[{'_index': 'mongo_index', '_id': 'Dv9p5IUB9nynXRNhQsyK', '_score': 3.0906205, '_ignored': ['content.keyword', 'log_entry.keyword'], '_source': {'log_entry': '{"_id"=>BSON::ObjectId(\'63cffa2a78b994746c729cef\'), "content"=>"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5-Level Paging and 5-Level EPT white paper\\n\\n\\nDocument Number: 335252-001\\n\\n5-Level Paging and 5-Level EPT\\nWhite Paper\\n\\nRevision 1.0\\n\\nDecember 2016\\n\\n\\n\\n2 Document Number: 335252-001, Revision: 1.0\\n\\nLegal Lines and DisclaimersIntel technologies’ features and benefits depend on system configuration and may require enabled hardware, software, or service \\nactivation. Learn more at intel.com, or from the OEM or retailer.\\nNo computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any \\ndamages resulting from such losses.\\nYou may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel \\nproducts described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted \\nwhich includes subject matter disclosed herein.\\nNo license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document.\\nThe products described may contain design defects or errors known as errata which may cause the product to deviate from \\npublished specifications. Current characterized errata are available on request.\\nThis document contains information on products, services and/or processes in development. All information provided here is \\nsubject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps.\\nIntel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for \\na particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or \\nusage in trade.\\nCopies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-\\n4725 or by visiting www.intel.com/design/literature.htm.\\nIntel, the Intel logo, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries.\\n*Other names and brands may be claimed as the property of others.\\nCopyright © 2016, Intel Corporation. All Rights Reserved.\\n\\nNotice: This document contains information on products in the design phase of development. The information here is subject to \\nchange without notice. Do not finalize a design with this information.\\n\\nhttp://www.intel.com/design/literature.htm\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 3\\n\\nContents\\n\\n1 Introduction ..............................................................................................................3\\n1.1 Existing Paging in IA-32e Mode .............................................................................3\\n1.2 Linear-Address Width and VMX Transitions .............................................................5\\n1.3 Existing Extended Page Tables (EPT)......................................................................6\\n\\n2 Expanding Linear Addresses: 5-Level Paging .............................................................7\\n2.1 5-Level Paging: Introduction.................................................................................7\\n2.2 Enumeration and Enabling ....................................................................................7\\n\\n2.2.1 Enumeration by CPUID..............................................................................7\\n2.2.2 Enabling by Software ................................................................................8\\n\\n2.3 Linear-Address Generation and Canonicality............................................................8\\n2.4 5-Level Paging: Linear-Address Translation.............................................................9\\n2.5 Linear-Address Registers and Canonicality ............................................................ 10\\n\\n2.5.1 Canonicality Checking on RIP Loads .......................................................... 11\\n2.5.2 Canonicality Checking on Other Loads ....................................................... 12\\n\\n2.6 Interactions with TLB-Invalidation Instructions ...................................................... 13\\n2.7 Interactions with Intel® MPX .............................................................................. 14\\n2.8 Interactions with Intel® SGX .............................................................................. 15\\n\\n3 Linear-Address Expansion and VMX Transitions....................................................... 17\\n3.1 Linear-Address Expansion and VM Entries............................................................. 17\\n3.2 Linear-Address Expansion and VM Exits................................................................ 17\\n\\n4 5-Level EPT ............................................................................................................. 19\\n4.1 4-Level EPT: Guest-Physical-Address Limit............................................................ 19\\n4.2 5-Level EPT: Enumeration and Enabling ............................................................... 19\\n\\n4.2.1 Enumeration.......................................................................................... 19\\n4.2.2 Enabling by Software .............................................................................. 20\\n\\n4.3 5-Level EPT: Guest-Physical-Address Translation ................................................... 20\\n4.4 5-Level EPT and EPTP Switching .......................................................................... 21\\n\\n5 Intel® Virtualization Technology for Directed I/O ................................................... 23\\n\\nFigures\\n1-1 Linear-Address Translation Using IA-32e Paging ......................................................4\\n2-1 Linear-Address Translation Using 5-Level Paging ................................................... 11\\n\\nTables\\n2-1 Format of a PML5 Entry (PML5E) that References a PML4 Table .................................9\\n4-1 Format of an EPT PML5 Entry (EPT PML5E) ........................................................... 20\\n\\n\\n\\n4 Document Number: 335252-001, Revision: 1.0\\n\\nRevision History\\n\\nDocument \\nNumber\\n\\nRevision \\nNumber Description Date\\n\\n335252-001 1.0 • Initial Release December 2016\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 3\\n\\n1 Introduction\\n\\nThis document describes planned extensions to the Intel 64 architecture to expand the \\nsize of addresses that can be translated through a processor’s memory-translation \\nhardware.\\n\\nModern operating systems use address-translation support called paging. Paging \\ntranslates linear addresses (also known as virtual addresses), which are used by \\nsoftware, to physical addresses, which are used to access memory (or memory-\\nmapped I/O). Section 1.1 describes the 64-bit paging hardware on Intel 64 processors. \\nExisting processors limit linear addresses to 48 bits. Chapter 2 describes paging \\nextensions that would relax that limit to 57 linear-address bits.\\n\\nVirtual-machine monitors (VMMs) use the virtual-machine extensions (VMX) to \\nsupport guest software operating in a virtual machine. VMX transitions are control-\\nflow transfers between the VMM and guest software. VMX transitions involve the \\nloading and storing of various processor registers. Some of these registers are defined \\nto contain linear addresses. Because of this, the operation of VMX transitions depends \\nin part on the linear-address width supported by the processor. Section 1.2 describes \\nthe existing treatment of linear-address registers by VMX transitions, while Chapter 3 \\ndescribes the changes required to support larger linear addresses.\\n\\nVMMs may also use additional address-translation support called extended page \\ntables (EPT). When EPT is used, paging produces guest-physical addresses, which \\nEPT translates to physical addresses. Section 1.3 describes the EPT hardware on \\nexisting Intel 64 processors, which limit guest-physical addresses to 48 bits. Chapter 4 \\ndescribes EPT extensions to support 57 guest-physical-address bits.\\n\\n1.1 Existing Paging in IA-32e Mode\\nOn processors supporting Intel 64 architecture, software typically references memory \\nusing linear addresses. Most modern operating systems configure processors to use \\npaging, which translates linear addresses to physical addresses. The processor uses \\nthe resulting physical addresses to access memory.\\n\\nIA-32e mode is a mode of processor execution that extends the older 32-bit \\noperation, known as legacy mode. Software can enter IA-32e mode with the following \\nalgorithm.\\n\\n1. Use the MOV CR instruction to set CR4.PAE[bit 5]. (Physical-address extension \\nmust be enabled to enter IA-32e mode.)\\n\\n2. Use the WRMSR instruction to set bit 8 (LME) of the IA32_EFER MSR (index \\nC0000080H).\\n\\n3. Use the MOV CR instruction to load CR3 with the address of a PML4 table (see \\nbelow).\\n\\n4. Use the MOV CR instruction to set CR0.PG[bit 31].\\n\\nA logical processor is in IA-32e mode whenever CR0.PG = 1 and IA32_EFER.LME = 1. \\nThis fact is reported in IA32_EFER.LMA[bit 10]. Software cannot set this bit directly; it \\nis always the logical-AND of CR0.PG and IA32_EFER.LME.\\n\\n\\n\\n4 Document Number: 335252-001, Revision: 1.0\\n\\nIn IA-32e mode, linear addresses are 64 bits in size.1 However, the corresponding \\npaging mode (currently called IA-32e paging) does not use all 64 linear-address bits.\\n\\nIA-32e paging does not use all 64 linear-address bits because processors limit the size \\nof linear addresses. This limit is enumerated by the CPUID instruction. Specifically, \\nCPUID.80000008H:EAX[bits 15:8] enumerates the number of linear-address bits (the \\nmaximum linear-address width) supported by the processor. Existing processors \\nenumerate this value as 48.\\n\\nNote: Processors also limit the size of physical addresses and enumerate the limit using \\nCPUID. CPUID.80000008H:EAX[bits 7:0] enumerates the number of physical-address \\nbits supported by the processor, the maximum physical-address width. Existing \\nprocessors have enumerated values up to 46. Software can use more than 32 physical-\\naddress bits only if physical-address extension has been enabled by setting \\nCR4.PAE, bit 5 of control register CR4.\\n\\nThe enumerated limitation on the linear-address width implies that paging translates \\nonly the low 48 bits of each 64-bit linear address. After a linear address is generated \\nbut before it is translated, the processor confirms that the address uses only the 48 bits \\nthat the processor supports.\\n\\nThe limitation to 48 linear-address bits results from the nature of IA-32e paging, which \\nis illustrated in Figure 1-1.\\n\\n1. IA-32e mode comprises two sub-modes: compatibility mode and 64-bit mode. In compatibility \\nmode, software uses 32-bit addresses, which the processor zero-extends to 64-bit linear \\naddresses. In 64-bit mode, software uses 64-bit addresses directly.\\n\\nFigure 1-1. Linear-Address Translation Using IA-32e Paging\\n\\nDirectory Ptr\\n\\nPTE\\n\\nLinear Address\\n\\nPage Table\\n\\nPDPTE\\n\\nCR3\\n\\n39 38\\n\\nPointer Table\\n\\n9\\n9\\n\\n40\\n\\n12\\n9\\n\\n40\\n\\n4-KByte Page\\n\\nOffset\\n\\nPhysical Addr\\n\\nPDE\\n\\nTable\\n011122021\\n\\nDirectory\\n30 29\\n\\nPage-Directory-\\n\\nPage-Directory\\n\\nPML4\\n47\\n\\n9\\n\\nPML4E\\n\\n40\\n\\n40\\n\\n40\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 5\\n\\nThe processor performs IA-32e paging by traversing a 4-level hierarchy of paging \\nstructures whose root structure resides at the physical address in control register \\nCR3. Each paging structure is 4-KBytes in size and comprises 512 8-byte entries. The \\nprocessor uses the upper 36 bits of a linear address (bits 47:12), 9 bits at a time, to \\nselect paging-structure entries from the hierarchy.\\n\\nNote: Figure 1-1 illustrates the translation of a linear address to a 4-KByte page. The paging \\nprocess can be configured so that the translation of some linear addresses stops one or \\ntwo levels earlier, translating instead to 2-MByte pages or 1-GByte pages.\\n\\nIn general, bits 51:12 of each paging-structure entry contain a 4-KByte aligned \\nphysical address. For each entry except the last, this address is that of the next paging \\nstructure; in the last entry, it is the physical address of a 4-KByte page frame. The \\nfinal physical address is obtained by combining this page-frame address with the page \\noffset, bits 11:0 of the original linear address.\\n\\nBecause only bits 47:0 of a linear address are used in address-translation, the \\nprocessor reserves bits 63:48 for future expansion using a concept known as \\ncanonicality. A linear address is canonical if bits 63:47 of the address are identical. \\n(Put differently, a linear address is canonical only if bits 63:48 are a sign-extension of \\nbit 47, which is the uppermost bit used in linear-address translation.)\\n\\nWhen a 64-bit linear address is generated to access memory, the processor first \\nconfirms that the address is canonical. If the address is not canonical, the memory \\naccess causes a fault, and the processor makes no attempt to translate the address.1\\n\\nIntel 64 architecture includes numerous registers that are defined to hold linear \\naddresses. These registers may be loaded using a variety of instructions. In most \\ncases, these instructions cause a general-protection exception (#GP) if an attempt is \\nmade to load one of these registers with a value that is not canonical.\\n\\nPhysical-address bits in a paging-structure entry beyond the enumerated physical-\\naddress width are reserved. A page-fault exception (#PF) results if an attempt is made \\nto access a linear address whose translation encounters a paging-structure entry that \\nsets any of those bits.\\n\\n1.2 Linear-Address Width and VMX Transitions\\nVM entries and VM exits manipulate numerous processor registers that contain linear \\naddresses. The transitions respect the processor’s linear-address width in a manner \\nbased on canonicality.\\n\\nCertain fields in the VMCS correspond to registers that contain linear addresses. \\nVM entries confirm that most of those fields contain values that are canonical. Some \\nregisters, such as RIP and the LDTR base address, receive special treatment.\\n\\nVM exits save into the VMCS the state of certain registers, some of which contain linear \\naddresses. Because the processor generally ensures that the values in these registers \\nare canonical (see Section 1.1), the values that VM exits save for these registers will \\ngenerally be canonical.\\n\\n1. In general, an attempt to access memory using a linear address that is not canonical causes a \\ngeneral-protection exception (#GP). A stack-fault exception — #SS — occurs instead if the \\nmemory access was made using the SS segment.\\n\\n\\n\\n6 Document Number: 335252-001, Revision: 1.0\\n\\nVM exits also load from the VMCS certain registers, some of which contain linear \\naddresses. Each VM exit ensures that the value of each of these registers is canonical. \\nSpecifically, bits 47:0 of the register are loaded from the field in the host-state area; \\nthe value of bit 47 is then sign-extended into bits 63:48 of the register.\\n\\n1.3 Existing Extended Page Tables (EPT)\\nMost Intel 64 processors supporting VMX also support an additional layer of address \\ntranslation called extended page tables (EPT).\\n\\nVM entry can be configured to activate EPT for guest software. When EPT is active, the \\naddresses used and produced by paging (Section 1.1) are not used as physical \\naddresses to reference in memory. Instead, the processor interprets them as guest-\\nphysical addresses, and translates them to physical addresses in a manner \\ndetermined by the VMM. (This translation from guest-physical to physical applies not \\nonly to the output of paging but also to the addresses that the processor uses to \\nreference the guest paging structures.)\\n\\nIf the EPT translation process cannot translate a guest-physical address, it causes an \\nEPT violation. (EPT violations may also occur when an access to a guest-physical \\naddress violates the permissions established by EPT for that guest-physical address.) \\nAn EPT violation is a VMX-specific exception, usually causing a VM exit.\\n\\nAs noted in Section 1.1, existing processors limit physical addresses to 46 bits. That \\nlimit applies also to guest-physical addresses. As a result, guest-physical addresses \\nthat set bits beyond this limit are not translated by EPT. (For example, a page fault \\nresults if linear-address translation encounters a paging-structure entry with such an \\naddress.) Because of this, existing EPT has been limited to translating only 48 guest-\\nphysical-address bits.\\n\\nThe existing EPT translation process is analogous to the paging process that was \\nillustrated earlier in Figure 1-1. Like 4-level paging, the processor implements EPT by \\ntraversing a 4-level hierarchy of 4-KByte EPT paging structures. The last EPT paging-\\nstructure entry contains the upper bits of the final physical address, while the lowest \\nbits come from the original guest-physical address.\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 7\\n\\n2 Expanding Linear Addresses: \\n5-Level Paging\\n\\n2.1 5-Level Paging: Introduction\\n5-level paging is a new paging mode that will be available in IA-32e mode. As its \\nname suggests, it will translate linear addresses by traversing a 5-level hierarchy of \\npaging structures. Because the process is otherwise unmodified, 5-level paging extends \\nthe processor’s linear-address width to 57 bits. (The additional 9 bits are used to select \\nan entry from the fifth level of the hierarchy.) For clarity, the paging mode formerly \\ncalled IA-32e paging will now be called 4-level paging.\\n\\nThe remainder of this chapter specifies the architectural changes that define and are \\nentailed by 5-level paging. Section 2.2 specifies how the CPU enumerates the new \\nfeature and how it is enabled by software. Section 2.3 describes changes to the process \\nof linear-address generation, as well as a revision to the concept of canonicality. \\nSection 2.4 details how 5-level paging translates linear addresses. Section 2.5 clarifies \\nhow the processor treats loads of registers containing linear addresses, while Section \\n2.6 to Section 2.8 consider interactions with various other features. (Interactions with \\nthe virtual-machine extensions are specified in Chapter 3.)\\n\\n2.2 Enumeration and Enabling\\nThis section describes how processors enumerate to software support for 5-level paging \\nand related features and also how software enables the processor to use that support.\\n\\n2.2.1 Enumeration by CPUID\\nProcessors supporting the Intel 64 architecture typically use the CPUID instruction to \\nenumerate to software specific processor functionality. Those processors that support \\n5-level paging enumerate that fact through a new feature flag as well as through \\nchanges in how related features are reported:\\n\\n• CPUID.(EAX=07H, ECX=0):ECX[bit 16] is a new feature flag that will enumerate \\nbasic support for 5-level paging. All older processors clear this bit. A processor will \\nset this bit if and only if it supports 5-level paging.\\n\\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 15:8] enumerates the \\nmaximum linear-address width supported by the processor. All older processors \\nthat support Intel 64 architecture enumerated this value as 48. Processors that \\nsupport 5-level paging will instead enumerate this value as 57.\\n\\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 7:0] enumerates the \\nmaximum physical-address width supported by the processor. Processors that \\nsupport Intel 64 architecture have enumerated at most 46 for this value. \\nProcessors that support 5-level paging are expected to enumerate higher values, \\nup to 52.\\n\\n• CPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17] is an existing field that \\nenumerates the user MPX address-width adjust (MAWAU). This value specifies the \\nnumber of linear-address bits above 48 on which the BNDLDX and BNDSTX \\ninstructions operate in 64-bit mode when CPL = 3.\\n\\n\\n\\n8 Document Number: 335252-001, Revision: 1.0\\n\\nOlder processors that support Intel® MPX enumerated 0 for this value. Processors \\nthat support 5-level paging may enumerate either 0 or 9, depending on \\nconfiguration by system software. See Section 2.7 for more details on how BNDLDX \\nand BNDSTX use MAWAU and how system software determines its value.\\n\\n• CPUID.(EAX=12H,ECX=0H):EDX[bits 15:8] is an existing field that enumerates \\ninformation that specifies the maximum supported size of a 64-bit enclave. If the \\nvalue enumerated is n, the maximum size is 2n. Older processors that support \\nIntel® SGX enumerated at most 47 for this value. Processors that support 5-level \\npaging are expected to enumerate this value as 56.\\n\\n2.2.2 Enabling by Software\\nSection 1.1 identified an algorithm by which software can enter IA-32e mode. On \\nprocessors that do not support 5-level paging, this algorithm enables 4-level paging. \\nOn processors that support 5-level paging, it can be adapted to enable 5-level paging \\ninstead.\\n\\nProcessors that support 5-level paging allow software to set a new enabling bit, \\nCR4.LA57[bit 12].1 A logical processor in IA-32e mode (IA32_EFER.LMA = 1) uses 5-\\nlevel paging if CR4.LA57 = 1. Outside of IA-32e mode (IA32_EFER.LMA = 0), the value \\nof CR4.LA57 does not affect paging operation.\\n\\nThe following items detail how a logical processor determines the current paging mode.\\n\\n• If CR0.PG = 0, paging is disabled.\\n\\n• If IA32_EFER.LMA = 0, one of the legacy 32-bit paging modes is used (depending \\non the value of legacy paging-mode bits in CR4).2\\n\\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 0, 4-level paging is used.\\n\\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 1, 5-level paging is used.\\n\\nSoftware can thus use the following algorithm to enter IA-32e mode with 5-level \\npaging.\\n\\n1. Use the MOV CR instruction to set CR4.PAE and CR4.LA57.\\n2. Use the WRMSR instruction to set IA32_EFER.LME.\\n\\n3. Use the MOV CR instruction to load CR3 with the address of a PML5 table (see \\nSection 2.4).\\n\\n4. Use the MOV CR instruction to set CR0.PG.\\n\\nThe processor allows software to modify CR4.LA57 only outside of IA-32e mode. In \\nIA-32e mode, an attempt to modify CR4.LA57 using the MOV CR instruction causes a \\ngeneral-protection exception (#GP).\\n\\n2.3 Linear-Address Generation and Canonicality\\nAs noted in Section 1.1, processors with a linear-address width of 48 bits reserve \\nlinear-address bits 63:48 for future expansion. Linear addresses that use only bits 47:0 \\n(because bits 63:48 are a sign-extension of bit 47) are called canonical.\\n\\n1. Software can set CR4.LA57 only if CPUID.(EAX=07H, ECX=0):ECX[bit 16] is enumerated as 1.\\n2. Recall that IA32_EFER.LMA is the logical-AND of CR0.PG and IA32_EFER.LME.\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 9\\n\\nWhen a 64-bit linear address is generated to access memory, the processor first \\nconfirms that the address is canonical. If the address is not canonical, the memory \\naccess causes a fault, and the address is not translated.\\n\\nProcessors that support 5-level paging can translate 57-bit linear addresses when 5-\\nlevel paging is enabled. But if software has enabled only 4-level paging, such a \\nprocessor can translate only 48-bit linear addresses. This fact motivates the definition \\nof two levels of canonicality.\\n\\nA linear address is 48-bit canonical if bits 63:47 of the address are identical. \\nSimilarly, an address is 57-bit canonical if bits 63:56 of the address are identical. Any \\nlinear address is that 48-bit canonical is also 57-bit canonical.\\n\\nWhen a 64-bit linear address is generated to access memory, a processor that supports \\n5-level paging checks for canonicality based on the current paging mode: if 4-level \\npaging is enabled, the address must be 48-bit canonical; if 5-level paging is enabled, \\nthe address need only be 57-bit canonical. If the appropriate canonicality is not \\nobserved, the memory access causes a fault.\\n\\n2.4 5-Level Paging: Linear-Address Translation\\nAs noted in Section 2.2.2, a logical processor uses 5-level paging if IA32_EFER.LMA = 1 \\nand CR4.LA57 = 1.\\n\\nLike 4-level paging, 5-level paging translates linear addresses using a hierarchy of in-\\nmemory paging structures. Because 5-level paging increases the linear-address width \\nto 57 bits (from the 48 bits supported by 4-level paging), 5-level paging allows up to \\n128 PBytes of linear-address space to be accessed at any given time.\\n\\nAlso like 4-level paging, 5-level paging uses CR3 to locate the first paging-structure in \\nthe hierarchy. (CR3 has the same mode-specific format with 5-level paging as it does \\nwith 4-level paging.) The following items describe in more detail the changes that 5-\\nlevel paging makes to the translation process.\\n\\n• Translation begins by identifying a 4-KByte naturally aligned PML5 table. It is \\nlocated at the physical address specified in bits 51:12 of CR3. A PML5 table \\ncomprises 512 64-bit entries (PML5Es). A PML5E is selected using the physical \\naddress defined as follows.\\n\\n— Bits 51:12 are from CR3.\\n— Bits 11:3 are bits 56:48 of the linear address.\\n— Bits 2:0 are all 0.\\n\\nBecause a PML5E is identified using bits 56:48 of the linear address, it controls \\naccess to a 256-TByte region of the linear-address space. The format of a PML5E is \\ngiven in Table 2-1.\\n\\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table\\n\\nBit Position(s) Contents\\n\\n0 (P) Present; must be 1 to reference a PML4 table.\\n\\n1 (R/W) Read/write; if 0, writes may not be allowed to the 256-TByte region controlled by this entry.\\n\\n2 (U/S) User/supervisor; if 0, user-mode accesses are not allowed to the 256-TByte region \\ncontrolled by this entry.\\n\\n3 (PWT) Page-level write-through; indirectly determines the memory type used to access the PML4 \\ntable referenced by this entry.\\n\\n\\n\\n10 Document Number: 335252-001, Revision: 1.0\\n\\n• The next step of the translation process identifies a 4-KByte naturally aligned PML4 \\ntable. It is located at the physical address specified in bits 51:12 of the PML5E (see \\nTable 2-1). A PML4 table comprises 512 64-bit entries (PML4Es). A PML4E is \\nselected using the physical address defined as follows.\\n\\n— Bits 51:12 are from the PML5E.\\n— Bits 11:3 are bits 47:39 of the linear address.\\n— Bits 2:0 are all 0.\\n\\nAs is normally the case when accessing a paging-structure entry, the memory type \\nused to access the PML4E is based in part on the PCD and PWT bits in the PML5E.\\n\\nBecause a PML4E is identified using bits 56:39 of the linear address, it controls \\naccess to a 512-GByte region of the linear-address space.\\n\\nOnce the PML4E is identified, bits 38:0 of the linear address determine the remainder \\nof the translation process exactly as is done for 4-level paging. As suggested in \\nTable 2-1, the values of bit 1, bit 2, and bit 63 of the PML5E are used normally (in \\ncombination with the corresponding bits in other paging-structure entries) to determine \\naccess rights. The accessed flag (bit 5) in the PML5E is updated as is done for other \\npaging-structure entries.\\n\\nThe operation of 5-level paging is illustrated in Figure 2-1.\\n\\n2.5 Linear-Address Registers and Canonicality\\nIntel 64 architecture includes numerous registers that are defined to hold linear \\naddresses. These registers may be loaded using a variety of instructions. As noted in \\nSection 1.1, each of these instructions typically causes a general-protection exception \\n(#GP) if an attempt is made to load a linear-address register with a value that is not \\ncanonical.\\n\\nAs noted in Section 2.3, processors that support 5-level paging use two definitions of \\ncanonicality: 48-bit canonicality and 57-bit canonicality. This section describes how \\nsuch a processor checks the canonicality of the values being loaded into the linear-\\naddress registers. One approach is used for operations that load RIP (the instruction \\npointer; see Section 2.5.1) and another is used for those that load other registers (see \\nSection 2.5.2).\\n\\n4 (PCD) Page-level cache disable; indirectly determines the memory type used to access the PML4 \\ntable referenced by this entry.\\n\\n5 (A) Accessed; indicates whether this entry has been used for linear-address translation.\\n\\n6 Ignored.\\n\\n7 (PS) Reserved (must be 0).\\n\\n11:8 Ignored.\\n\\nM–1:12 Physical address of 4-KByte aligned PML4 table referenced by this entry.\\n\\n51:M Reserved (must be 0).\\n\\n62:52 Ignored.\\n\\n63 If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not allowed from the \\n256-TByte region controlled by this entry); otherwise, reserved (must be 0).\\n\\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table (Continued)\\n\\nBit Position(s) Contents\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 11\\n\\n2.5.1 Canonicality Checking on RIP Loads\\nThe RIP register contains the offset of the current instruction pointer within the CS \\nsegment. Because the processor treats the CS base address as zero in 64-bit mode, the \\nvalue of the RIP register in that mode is the linear address of the instruction pointer.\\n\\nOperations that load RIP (including both instructions such as JMP as well as control \\ntransfers through the IDT) check first whether the value to be loaded is canonical \\nrelative to the current paging mode. If the processor determines that the address is not \\ncanonical, the RIP load is not performed and a general-protection exception (#GP) \\noccurs.\\n\\nNote: An instruction that would load RIP with a non-canonical address faults, meaning that \\nthe return instruction pointer of the fault handler is the address of the faulting \\ninstruction and not the non-canonical address whose load was attempted.\\n\\nThe canonicality checking performed by these operations uses 48-bit canonicality when \\n4-level paging is active. When 5-level paging is active, the checking is relaxed to \\nrequire only 57-bit canonicality.\\n\\nThe SYSCALL and SYSENTER instructions load RIP from the IA32_LSTAR and \\nIA32_SYSENTER_EIP MSRs, respectively. On processors that support only 4-level \\npaging, these instructions do not check that the values being loaded are canonical \\nbecause the WRMSR instruction ensures that each of these MSRs contains a value that \\nis 48-bit canonical. On processors that support 5-level paging, the checking by WRMSR \\nis relaxed to 57-bit canonicality (see Section 2.5.2). On such processors, an execution \\n\\nFigure 2-1. Linear-Address Translation Using 5-Level Paging\\n\\nPDE\\n\\nLinear Address\\n\\nPage Directory\\n\\nPML4E\\n\\nCR3\\n\\n39 38\\n\\n9 9\\n\\n40\\n\\n12\\n9\\n\\n40\\n\\n4-KByte Page\\n\\nOffset\\n\\nPhysical Addr\\n\\nPDPTE\\n\\n01112202130 29\\n\\nPage-Directory\\n\\n47\\n\\n9\\n\\nPML5E\\n\\n40\\n\\n40\\n\\n40\\n\\n56\\nTableDirectoryDirectory PtrPML4PML5\\n\\nPointer Table\\n\\nPage Table\\n\\nPTE\\n\\n9\\n\\n40\\n\\n\\n\\n12 Document Number: 335252-001, Revision: 1.0\\n\\nof SYSCALL or SYSENTER with 4-level paging checks that the value being loaded into \\nRIP is 48-bit canonical.1\\n\\nThe normal advancing of the instruction pointer to the next instruction boundary may \\nresult in the RIP register holding a non-canonical address. The fetch of the next \\ninstruction from that non-canonical address will result in a general-protection exception \\nas indicated in Section 2.3. In this case, the return instruction pointer of the fault \\nhandler will be that non-canonical address.\\n\\n2.5.2 Canonicality Checking on Other Loads\\nIn addition to RIP, the CPU maintains numerous other registers that hold linear \\naddresses:\\n\\n• GDTR and IDTR (in their base-address portions).\\n\\n• LDTR, TR, FS, and GS (in the base-address portions of their hidden descriptor \\ncaches).\\n\\n• The debug-address registers (DR0 through DR3), which hold the linear addresses \\nof breakpoints.\\n\\n• The following MSRs: IA32_BNDCFGS, IA32_DS_AREA, IA32_KERNEL_GS_BASE, \\nIA32_LSTAR, IA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \\nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \\nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, and \\nIA32_SYSENTER_ESP.\\n\\n• The x87 FPU instruction pointer (FIP).\\n\\n• The user-mode configuration register BNDCFGU, used by Intel® MPX.\\n\\nWith a few exceptions, the processor ensures that the addresses in these registers are \\nalways canonical in the following ways.\\n\\n• Some instructions fault on attempts to load a linear-address register with a non-\\ncanonical address:\\n\\n— An execution of the LGDT or LIDT instruction causes a general-protection \\nexception (#GP) if the base address specified in the instruction’s memory \\noperand is not canonical.\\n\\n— An execution of the LLDT or LTR instruction causes a #GP if the base address to \\nbe loaded from the GDT is not canonical.\\n\\n— An execution of WRMSR, WRFSBASE, or WRGSBASE causes a #GP if it would \\nload the base address of either FS or GS with a non-canonical address.\\n\\n— An execution of WRMSR causes a #GP if it would load any of the following MSRs \\nwith a non-canonical address: IA32_BNDCFGS, IA32_DS_AREA, \\nIA32_FS_BASE, IA32_GS_BASE, IA32_KERNEL_GS_BASE, IA32_LSTAR, \\nIA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \\nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \\nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, or \\nIA32_SYSENTER_ESP.2\\n\\n1. The SYSRET and SYSEXIT instructions, which complement SYSCALL and SYSENTER, load RIP \\nfrom RCX and RDX, respectively. Even before 5-level paging, these instructions checked the \\ncanonicality of the value to be loaded into RIP. As with other instructions that load RIP, this \\nchecking will be based on the current paging mode.\\n\\n2. Such canonicality checking may apply also when the WRMSR instruction is used to load some \\nnon-architectural MSRs (not listed here) that hold a linear address.\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 13\\n\\n— An execution of XRSTORS causes a #GP if it would load any of the following \\nMSRs with a non-canonical address: IA32_RTIT_ADDR0_A, \\nIA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, IA32_RTIT_ADDR1_B, \\nIA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, IA32_RTIT_ADDR3_A, and \\nIA32_RTIT_ADDR3_B.\\n\\nThis enforcement always uses the enumerated maximum linear-address width and \\nis independent of the current paging mode. Thus, a processor that supports 5-level \\npaging will allow the instructions mentioned above to load these registers with \\naddresses that are 57-bit canonical but not 48-bit canonical — even if 4-level \\npaging is active. (As a result, instructions that store these values — SGDT, SIDT, \\nSLDT, STR, RDFSBASE, RDGSBASE, RDMSR, XSAVE, XSAVEC, XSAVEOPT, and \\nXSAVES — may save addresses that are 57-bit canonical but not 48-bit canonical, \\neven if 4-level paging is active.)\\n\\n• The FXRSTOR, XRSTOR, and XRSTORS instructions ignore attempts to load some of \\nthese registers with non-canonical addresses:\\n\\n— Loads of FIP ignore any bits in the memory image beyond the enumerated \\nmaximum linear-address width. The processor sign-extends to most significant \\nbit (e.g., bit 56 on processors that support 5-level paging) to ensure that FIP is \\nalways canonical.\\n\\n— Loads of BNDCFGU (by XRSTOR or XRSTORS) ignore any bits in the memory \\nimage beyond the enumerated maximum linear-address width. The processor \\nsign-extends to most significant bit (e.g., bit 56 on processors that support 5-\\nlevel paging) to ensure that BNDCFGU is always canonical.\\n\\n• Every non-control x87 instruction loads FIP. The value loaded is always canonical \\nrelative to the current paging mode: 48-bit canonical if 4-level paging is active, and \\n57-bit canonical if 5-level paging is active.\\n\\nDR0 through DR3 can be loaded with the MOV to DR instruction. The instruction allows \\nthose registers to be loaded with non-canonical addresses. The MOV from DR \\ninstruction will return the value last loaded with the MOV to DR instruction, even if the \\naddress is not canonical. Breakpoint address matching is supported only for canonical \\nlinear addresses.\\n\\n2.6 Interactions with TLB-Invalidation Instructions\\nIntel 64 architecture includes three instructions that may invalidate TLB entries for the \\nlinear address of an instruction operand: INVLPG, INVPCID, and INVVPID. The following \\nitems describe how they are affected by linear-address width.\\n\\n• The INVLPG instruction takes a memory operand. It invalidates any TLB entries \\nthat the logical processor is caching for the linear address of that operand for the \\ncurrent linear address space. The instruction does not fault if that address is not \\ncanonical relative to the current paging mode (e.g., is not 48-bit canonical when 4-\\nlevel paging is active). However, no invalidation is performed because the processor \\ndoes not cache TLB entries for addresses that are not canonical relative to the \\ncurrent paging mode.\\n\\n• The INVPCID instruction takes a register operand (INVPCID type) and a memory \\noperand (INVPCID descriptor). If the INVPCID type is 0, the instruction invalidates \\nany TLB entries that the logical processor is caching for the linear address and PCID \\nspecified in the INVPCID descriptor. If the linear address is not canonical relative \\nthe linear-address width supported by the processor, the instruction causes a \\ngeneral-protection exception (#GP). If the processor supports 5-level paging, the \\ninstruction will not cause such a #GP for an address that is 57-bit canonical, \\nregardless of paging mode, even if 4-level paging is active and the address is not \\n48-bit canonical.\\n\\n\\n\\n14 Document Number: 335252-001, Revision: 1.0\\n\\n• The INVVPID instruction takes a register operand (INVVPID type) and a memory \\noperand (INVVPID descriptor). If the INVPCID type is 0, the instruction invalidates \\nany TLB entries that the logical processor is caching for the linear address and VPID \\nspecified in the INVVPID descriptor. If the linear address is not canonical relative \\nthe linear-address width supported by the processor, the instruction fails.1 If the \\nprocessor supports 5-level paging, the instruction will not fail for an address that is \\n57-bit canonical, regardless of paging mode, even if 4-level paging is active and the \\naddress is not 48-bit canonical.\\n\\n2.7 Interactions with Intel® MPX\\nThe Intel® Memory Protection Extensions (Intel® MPX) define a set of 4 bound \\nregisters, each of which software can associate with a specific pointer in memory. \\nIntel MPX includes two instructions — BNDLDX and BNDSTX — that allow software to \\nload from or store into memory the bounds associated with a particular pointer in \\nmemory.\\n\\nThe BNDLDX and BNDSTX instructions each take a bound register and a memory \\noperand (the associated pointer). Each of these parses the linear address of the \\nmemory operand to traverse a hierarchical data structure in memory. In 64-bit mode, \\nthese instructions do not necessarily use all the bits in the supplied 64-bit addresses. \\nThe number of bits used is 48 plus a value called the MPX address-width adjust \\n(MAWA).\\n\\nThe value of MAWA depends on CPL; the current paging mode (4-level paging or 5-level \\npaging); and, if 5-level paging is active, the value of a new MSR. Processors that \\nsupport both Intel MPX and 5-level paging support the IA32_MPX_LAX MSR (MSR index \\n1000H). Only bit 0 of the MSR is defined.\\n\\nIf CPL < 3, the supervisor MAWA (MAWAS) is used. The value of MAWAS is determined \\nby the setting of CR4.LA57. If CR4.LA57 = 0 (4-level paging is active; recall that MAWA \\nis relevant only in 64-bit mode), the value of MAWAS is 0. If CR4.LA57 = 1 (5-level \\npaging is active), the value of MAWAS is 9. The value of MAWAS is not enumerated by \\nthe CPUID instruction.\\n\\nIf CPL = 3, the user MAWA (MAWAU) is used. The value of MAWAU is determined as \\nfollows. If CR4.LA57 = 0 or IA32_MPX_LAX[bit 0] = 0, the value of MAWAU is 0. If \\nCR4.LA57 = 1 and IA32_MPX_LAX[bit 0] = 1, the value of MAWAU is 9. The current \\nvalue of MAWAU is enumerated in \\nCPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17].\\n\\nThe following items specify how an execution of the BNDLDX and BNDSTX instructions \\nin 64-bit mode parses a linear address to traverse a hierarchical data structure.\\n\\n• A bound directory is located at the 4-KByte aligned linear address specified in \\nbits 63:12 of BNDCFGx.2 A BDE is selected using the LAp (linear address of pointer \\nto a buffer) to construct a 64-bit offset as follows:\\n\\n— bits 63:31+MAWA are 0;\\n— bits 30+MAWA:3 are LAp[bits 47+MAWA:20]; and\\n— bits 2:0 are 0.\\n\\n1. INVVPID is a VMX instruction. In response to certain conditions, execution of a VMX may fail, \\nmeaning that it does not complete its normal operation. When a VMX instruction fails, control \\npasses to the next instruction (rather than to a fault handler) and a flag is set to report the \\nfailure.\\n\\n2. If CPL < 3, BNDCFGS is used; if CPL = 3, BNDCFGU is used.\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 15\\n\\nThe address of the BDE is the sum of the bound-directory base address (from \\nBNDCFGx) plus this 64-bit offset.\\n\\nIf either BNDLDX or BNDSTX is executed inside an enclave, the instruction operates \\nas if MAWAU = 0 (regardless of the values of CR4.LA57 and IA32_MPX_LAX[bit 0]).\\n\\n• The processor uses bits 63:3 of the BDE as the 8-byte aligned address of a bound \\ntable (BT). A BTE is selected using the LAp (linear address of pointer to a buffer) to \\nconstruct a 64-bit offset as follows:\\n\\n— bits 63:22 are 0;\\n— bits 21:5 are LAp[bits 19:3]; and\\n— bits 4:0 are 0.\\n\\nThe address of the BTE is the sum of the bound-table base address (from the BDE) \\nplus this 64-bit offset.\\n\\nA bound directory comprises 228+MAWA 64-bit entries (BDEs);1 thus, the size of a \\nbound directory in 64-bit mode is 21+MAWA GBytes. A bound table comprises 217 32-\\nbyte entries (BTEs); thus, the size of a bound table in 64-bit mode is 4 MBytes \\n(independent of MAWA).\\n\\n2.8 Interactions with Intel® SGX\\nIntel® Software Guard Extensions (Intel® SGX) define new processor functionality that \\nis implemented as SGX leaf functions within the ENCLS (supervisor) and ENCLU (user) \\ninstructions.\\n\\nThe SGX leaf functions include memory accesses using linear addresses normally. \\nWhen executed in 64-bit mode, the linear address are 64 bits in width and are subject \\nto the normal treatment of accesses to memory with 64-bit linear addresses (see \\nSection 2.3). In addition, some of the leaf functions apply specific architectural checks \\nrelated to linear-address width. The following items detail these checks and how they \\nare defined for processors that support 5-level paging.\\n\\n• The ECREATE leaf function of ENCLS creates a new enclave by creating a new SGX \\nenclave control structure (SECS). For a 64-bit enclave, the processor checks \\nwhether the enclave base linear address (specified in the SECS) is canonical, \\ngenerating a general-protection exception (#GP) if it is not. On processors that \\nsupport 5-level paging, this check is for 57-bit canonicality, regardless of the \\ncurrent paging mode.\\n\\nIn addition to checking the canonicality of the enclave base linear address, \\nECREATE confirms that the enclave size (specified in the SECS) is not greater than \\nthe maximum size supported by the processor (if the enclave size is too large, \\nECREATE generates a #GP). As noted in Section 2.2.1, older processors supported \\n64-bit enclaves with sizes up to 247 bytes; processors that support 5-level paging \\nare expected to support enclaves with sizes up to 256 bytes.\\n\\nIf bits 4:3 of the enclave’s XSAVE feature request mask (XFRM) are set (indicating \\nthat Intel MPX will be enabled during execution of the enclave), ECREATE generates \\na #GP if the enclave’s size is greater than 248 bytes, even if the processor \\nenumerates support for larger enclaves.\\n\\n• The EENTER and ERESUME leaf functions of ENCLU transfer control flow to an entry \\npoint within a specified enclave. For entry to a 64-bit enclave, the processor checks \\n\\n1. A bound directory used in a 64-bit enclave always comprises 228 64-bit BDEs and thus has a size \\nof 2 GBytes.\\n\\n\\n\\n16 Document Number: 335252-001, Revision: 1.0\\n\\nwhether certain linear addresses are canonical, generating a general-protection \\nexception (#GP) if any one is not. The following items detail these checks.\\n\\n— The linear address of the specified entry point must be canonical. If 4-level \\npaging is active, it must be 48-bit canonical; if 5-level paging is active, it must \\nbe 57-bit canonical.\\n\\n— The linear address of the asynchronous exit point (AEP — the address to which \\nthe processor transfers control on an asynchronous enclave exit) must be \\ncanonical. If 4-level paging is active, it must be 48-bit canonical; if 5-level \\npaging is active, it must be 57-bit canonical.\\n\\n— The enclave values for the base addresses of the FS and GS segments must be \\ncanonical. On processors that supports 5-level paging, these checks are for 57-\\nbit canonicality, regardless of the current paging mode.\\n\\n• The EEXIT leaf function exits the currently executing enclave and branches to a \\nspecified address. For an exit from a 64-bit enclave, the processor checks whether \\nthat target linear address is canonical, generating a general-protection exception \\n(#GP) if it is not. If 4-level paging is active, it must be 48-bit canonical; if 5-level \\npaging is active, it need only be 57-bit canonical.\\n\\nAs noted in Section 2.7, executions of BNDLDX and BNDSTX in a 64-bit enclave always \\noperate as if MAWAU = 0.\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 17\\n\\n3 Linear-Address Expansion and \\nVMX Transitions\\n\\nAs noted in Section 1.2, VM entries and VM exits manipulate numerous processor \\nregisters that contain linear addresses. The transitions respect the processor’s linear-\\naddress width in a manner based on canonicality.\\n\\nAs discussed in Chapter 2, processors that support 5-level paging expand the linear-\\naddress width from 48 bits to 57 bits. That expansion changes the operation of VMX \\ntransitions. Changes to VM entries are detailed in Section 3.1, while changes to \\nVM exits are given in Section 3.2.\\n\\n3.1 Linear-Address Expansion and VM Entries\\nCertain fields in the VMCS correspond to registers that contain linear addresses. \\nVM entries confirm those fields contain values that are canonical. This checking is \\nbased on the linear-address width supported by the processor (e.g., is based on 57-bit \\ncanonicality if the processor supports 5-level paging). The following are the fields to \\nwhich this applies.\\n\\n• In the host-state area:\\n\\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\\n\\n• In the guest-state area:\\n\\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\\n— The base-address field for LDTR (if LDTR will be usable).\\n— The field for the IA32_BNDCFGS MSR (if VM entry is loading that MSR).\\n\\nA VM entry to 64-bit mode also performs a check on the RIP field in the guest-state \\narea of the current VMCS. If the VM entry would result in 4-level paging, it checks that \\nbits 63:48 of the guest RIP field are identical; if it would result in 5-level paging, that \\ncheck is on bits 63:57.1\\n\\n3.2 Linear-Address Expansion and VM Exits\\nVM exits save the state of certain registers into the guest-state area of the VMCS. \\nSome of these registers contain linear addresses. As discussed in Section 1.1, the CPU \\ngenerally ensures that the values in these registers respect the CPU’s linear-address \\nwidth. As a result, the values the VM exits save for these registers will do the same.\\n\\n1. Note that these checks do not confirm that the guest RIP field is canonical relative to the paging \\nmode being entered. For example, bits 63:47 are identical in a 48-bit canonical address. However, \\nVM entry to 4-level paging may load RIP with a value in which bit 47 differs from that of \\nbits 63:48.\\n\\n\\n\\n18 Document Number: 335252-001, Revision: 1.0\\n\\nThere is a special case for LDTR base address. If LDTR was not usable at the time of a \\nVM exit, the value saved for the base address is undefined. However, this undefined \\nvalue is always 48-bit canonical on processors that do not support 5-level paging and is \\nalways 57-bit canonical on processors that do support 5-level paging.\\n\\nVM exits load the state of certain registers from the host-state area of the VMCS. Some \\nof these registers contain linear addresses. Each VM exit ensures that the value of each \\nof the following registers is canonical: the IA32_SYSENTER_EIP and \\nIA32_SYSENTER_ESP MSRs; and the base addresses for FS, GS, TR, GDTR, and IDTR. \\nHow this is done depends on whether the processor supports 5-level paging.\\n\\n• If the processor does not support 5-level paging, bits 47:0 of the register are \\nloaded from the field in the host-state area; the value of bit 47 is then sign-\\nextended into bits 63:48 of the register.\\n\\n• If the processor does support 5-level paging, bits 56:0 of the register are loaded \\nfrom the field in the host-state area; the value of bit 56 is then sign-extended into \\nbits 63:57 of the register.\\n\\nAgain, there is a special case for LDTR. LDTR is always unusable after a VM exit. Its \\nbase address may be loaded with an undefined value. This undefined value is always \\n48-bit canonical on processors that do not support 5-level paging and is always 57-bit \\ncanonical on processors that do support 5-level paging.\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 19\\n\\n4 5-Level EPT\\n\\n5-level EPT is a new mode for EPT. As its name suggests, it will translate guest-\\nphysical addresses by traversing a 5-level hierarchy of EPT paging structures. Because \\nthe process is otherwise unmodified, 5-level paging extends the processor’s guest-\\nphysical-address width to 57 bits. (The additional 9 bits are used to select an entry \\nfrom the fifth level of the hierarchy.) For clarity, the original EPT mode will now be \\ncalled 4-level EPT.\\n\\nThe remainder of this chapter specifies architectural changes to 4-level EPT as well as \\nthose that define and are entailed by 5-level EPT. Section 4.1 describes how the \\nexpansion of the guest-physical-address width affects 4-level EPT. Section 4.2 specifies \\nhow the CPU enumerates 5-level EPT and how the feature is enabled by software. \\nSection 4.3 details how 5-level EPT translates guest-physical addresses.\\n\\n4.1 4-Level EPT: Guest-Physical-Address Limit\\nAs explained in Section 1.3, 4-level EPT is limited to translating 48-bit guest-physical \\naddresses.\\n\\nThis is not a problem on existing processors, because they limit the physical-address \\nwidth to 46 bits (see Section 1.1). A processor’s physical-address width also limits \\nguest-physical addresses. That means that, on existing processors, any attempt to use \\na guest-physical address that sets a bit above the low 48 bits will cause a page-fault \\nexception (#PF).\\n\\nProcessors that support 5-level paging are expected to support 52 physical-address \\nbits. Such processors allow use of a guest-physical address that sets bits in the range \\n51:48; no #PF is generated.\\n\\nA guest-physical address that sets bits in the range 51:48 cannot be translated by 4-\\nlevel EPT. An attempt to access such an address when 4-level EPT is active causes an \\nEPT violation (see Section 1.3).\\n\\nEPT violations generate information about the exception in a value called the exit \\nqualification. In general, EPT violations caused by attempts to access a guest-physical \\naddress that is too wide establish the exit qualification as is currently done for other \\nEPT violations. Exceptions are made for bits 6:3 of the exit qualification, which report \\nthe access rights for the guest-physical address. The new EPT violations always clear \\nthese bits.\\n\\n4.2 5-Level EPT: Enumeration and Enabling\\nThis section describes how processors enumerate to software support for 5-level EPT \\nand how software enables the processor to use that support.\\n\\n4.2.1 Enumeration\\nProcessors supporting EPT enumerate details related to EPT in the \\nIA32_VMX_EPT_VPID_CAP MSR (index 48CH). Currently, \\nIA32_VMX_EPT_VPID_CAP[bit 6] enumerates support for 4-level EPT. Processors that \\nalso support 5-level EPT will enumerate that fact by also setting \\nIA32_VMX_EPT_VPID_CAP[bit 7].\\n\\n\\n\\n20 Document Number: 335252-001, Revision: 1.0\\n\\nThe guest-physical-address width supported by a processor is not enumerated using \\nthe IA32_VMX_EPT_VPID_CAP MSR. This is because that width is always the same as \\nthe processor’s maximum physical-address width as enumerated by \\nCPUID.80000008H:EAX[bits 7:0].\\n\\n4.2.2 Enabling by Software\\nA VMM enables EPT by setting the “enable EPT” VM-execution control in the current \\nVMCS before using the VMCS for VM entry.\\n\\nSpecific details of EPT operation are determined by the extended-page-table pointer \\nfield (EPTP) in the VMCS. In particular, EPTP[bits 5:3] contain a value that is 1 less than \\nthe number of levels used by the EPT. On existing processors, this value must be 3, \\nindicating 4-level EPT. (VM entry fails if a different value is used.) Processors that also \\nsupport 5-level EPT will also allow the value 4 (indicating 5-level EPT).\\n\\nIn summary, VM entry on a processor that supports 5-level check EPTP[bits 5:3]. If the \\nvalue is 3, the VM entry activates 4-level EPT. If the value is 4, the VM entry activates \\n5-level EPT. With any other value, VM entry fails.\\n\\n4.3 5-Level EPT: Guest-Physical-Address Translation\\nLike 4-level EPT, 5-level EPT translates guest-physical addresses using a hierarchy of \\nin-memory paging structures. Because 5-level EPT increases the guest-physical-\\naddress width to 57 bits (from the 48 bits supported by 4-level EPT), 5-level EPT allows \\nup to 128 PBytes of guest-physical-address space to be accessed at any given time.\\n\\nThe following items describe in more detail the changes that 5-level EPT makes to the \\ntranslation process.\\n\\n• Translation begins by identifying a 4-KByte naturally aligned EPT PML5 table. It is \\nlocated at the physical address specified in bits 51:12 of EPTP. An EPT PML5 table \\ncomprises 512 64-bit entries (EPT PML5Es). An EPT PML5E is selected using the \\nphysical address defined as follows.\\n\\n— Bits 63:52 are all 0.\\n— Bits 51:12 are from EPTP.\\n— Bits 11:3 are bits 56:48 of the guest-physical address.\\n— Bits 2:0 are all 0.\\n\\nBecause an EPT PML5E is identified using bits 56:48 of the guest-physical address, it \\ncontrols access to a 256-TByte region of the linear-address space. The format of an EPT \\nPML5E is given in Table 4-1.\\n\\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E)\\n\\nBit Position(s) Contents\\n\\n0 Read access; indicates whether reads are allowed from the 256-TByte region controlled by \\nthis entry.\\n\\n1 Write access; indicates whether writes are allowed from the 256-TByte region controlled by \\nthis entry.\\n\\n2 If the “mode-based execute control for EPT” VM-execution control is 0, execute access; \\nindicates whether instruction fetches are allowed from the 256-TByte region controlled by \\nthis entry.\\nIf that control is 1, execute access for supervisor-mode linear addresses; indicates whether \\ninstruction fetches are allowed from supervisor-mode linear addresses in the 256-TByte \\nregion controlled by this entry.\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 21\\n\\n• The next step of the translation process identifies a 4-KByte naturally aligned EPT \\nPML4 table. It is located at the physical address specified in bits 51:12 of the EPT \\nPML5E (see Table 4-1). An EPT PML4 table comprises 512 64-bit entries (EPT \\nPML4Es). An EPT PML4E is selected using the physical address defined as follows.\\n\\n— Bits 51:12 are from the EPT PML5E.\\n— Bits 11:3 are bits 47:39 of the guest-physical address.\\n— Bits 2:0 are all 0.\\n\\nBecause an EPT PML4E is identified using bits 56:39 of the guest-physical address, \\nit controls access to a 512-GByte region of the guest-physical-address space.\\n\\nOnce the EPT PML4E is identified, bits 38:0 of the guest-physical address determine the \\nremainder of the translation process exactly as is done for 4-level EPT. As suggested in \\nTable 4-1, the values of bits 2:0 and bit 10 of the EPT PML5E are used normally (in \\ncombination with the corresponding bits in other EPT paging-structure entries) to \\ndetermine whether EPT violations occur. The accessed flag (bit 8) in the EPT PML5E is \\nupdated as is done for other EPT paging-structure entries.\\n\\n4.4 5-Level EPT and EPTP Switching\\nThe value of EPTP may be modified in VMX non-root operation by invoking \\nVM function 0 (EPTP switching). This is done by executing the VMFUNC instruction with \\nvalue 0 in the EAX register. Invocation of VM function 0 loads EPTP with a value \\nselected from a data structure in memory.\\n\\nBefore loading EPTP in this way, the processor first confirms that the value to be loaded \\nis valid. The definition of a valid EPTP value depends on whether the processor supports \\n5-level EPT.\\n\\n• If the processor does not support 5-level EPT, an EPTP value in memory is \\nconsidered valid if it would not cause VM entry to fail (e.g., it does not set any \\nreserved bits).\\n\\n• If the processor does support 5-level EPT, an EPTP value in memory is considered \\nvalid only if it would not cause VM entry to fail (as above) and if its value in \\nbits 5:3 (which controls the number of EPT levels) is the same as that of the \\ncurrent value of EPTP.\\n\\nThe implication is that an invocation of VM function 0 cannot change the EPT mode \\nbetween 4-level EPT and 5-level EPT.\\n\\n7:3 Reserved (must be 0).\\n\\n8 If bit 6 of EPTP is 1, accessed flag for EPT; indicates whether software has accessed the \\n256-TByte region controlled by this entry. Ignored if bit 6 of EPTP is 0.\\n\\n9 Ignored.\\n\\n10 Execute access for user-mode linear addresses. If the “mode-based execute control for \\nEPT” VM-execution control is 1, indicates whether instruction fetches are allowed from user-\\nmode linear addresses in the 256-TByte region controlled by this entry. If that control is 0, \\nthis bit is ignored.\\n\\n11 Ignored.\\n\\nM–1:12 Physical address of 4-KByte aligned EPT PML4 table referenced by this entry.\\n\\n51:M Reserved (must be 0).\\n\\n63:52 Ignored.\\n\\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E) (Continued)\\n\\nBit Position(s) Contents\\n\\n\\n\\n22 Document Number: 335252-001, Revision: 1.0\\n\\n\\n\\nDocument Number: 335252-001, Revision: 1.0 23\\n\\n5 Intel® Virtualization \\nTechnology for Directed I/O\\n\\nIntel® Virtualization Technology for Directed I/O includes a feature called DMA \\nremapping.\\n\\nDMA remapping provides hardware support for isolation of device accesses to memory. \\nWhen a device attempts to access system memory, DMA-remapping hardware \\nintercepts the access and utilizes paging structures to determine whether the access \\ncan be permitted; it also determines the actual location to access.\\n\\nThe DMA-remapping hardware may support two levels of address translation. One level \\nmay translate a linear address to a guest-physical address, while a second level may \\nremap the guest-physical address to physical address.\\n\\nThe first-level translation uses paging structures with the same format as those used \\nfor ordinary paging. The second-level translation uses paging structures with the same \\nformat as those used for EPT.\\n\\nIt is expected that, on platforms that support wider linear and guest-physical addresses \\n(using 5-level paging and 5-level EPT, respectively), the DMA-remapping hardware will \\nbe similarly enhanced to support those wider addresses with 5-level translation \\nprocesses.\\n\\nThis enhanced support for DMA remapping will be detailed in a future revision of the \\nIntel® Virtualization Technology for Directed I/O Architecture Specification.\\n\\n\\n\\n24 Document Number: 335252-001, Revision: 1.0\\n\\n\\n\\t1 Introduction\\n\\t1.1 Existing Paging in IA-32e Mode\\n\\t1.2 Linear-Address Width and VMX Transitions\\n\\t1.3 Existing Extended Page Tables (EPT)\\n\\n\\t2 Expanding Linear Addresses: 5-Level Paging\\n\\t2.1 5-Level Paging: Introduction\\n\\t2.2 Enumeration and Enabling\\n\\t2.2.1 Enumeration by CPUID\\n\\t2.2.2 Enabling by Software\\n\\n\\t2.3 Linear-Address Generation and Canonicality\\n\\t2.4 5-Level Paging: Linear-Address Translation\\n\\t2.5 Linear-Address Registers and Canonicality\\n\\t2.5.1 Canonicality Checking on RIP Loads\\n\\t2.5.2 Canonicality Checking on Other Loads\\n\\n\\t2.6 Interactions with TLB-Invalidation Instructions\\n\\t2.7 Interactions with Intel® MPX\\n\\t2.8 Interactions with Intel® SGX\\n\\n\\t3 Linear-Address Expansion and VMX Transitions\\n\\t3.1 Linear-Address Expansion and VM Entries\\n\\t3.2 Linear-Address Expansion and VM Exits\\n\\n\\t4 5-Level EPT\\n\\t4.1 4-Level EPT: Guest-Physical-Address Limit\\n\\t4.2 5-Level EPT: Enumeration and Enabling\\n\\t4.2.1 Enumeration\\n\\t4.2.2 Enabling by Software\\n\\n\\t4.3 5-Level EPT: Guest-Physical-Address Translation\\n\\t4.4 5-Level EPT and EPTP Switching\\n\\n\\t5 Intel® Virtualization Technology for Directed I/O\\n\\n", "metadata"=>{"pdf:docinfo:title"=>"5-Level Paging and 5-Level EPT white paper", "pdf:docinfo:creator"=>"Intel Corporation", "resourceName"=>"b\'5-Level Paging and 5-Level EPT - Intel - Revision 1.0 (December, 2016).pdf\'", "pdf:docinfo:modified"=>"2016-12-02T22:28:56Z", "pdf:docinfo:created"=>"2016-12-02T08:52:25Z"}, "filename"=>"5-Level Paging and 5-Level EPT - Intel - Revision 1.0 (December, 2016).pdf"}', 'filename': '5-Level Paging and 5-Level EPT - Intel - Revision 1.0 (December, 2016).pdf', 'metadata_pdf:docinfo:created': '2016-12-02T08:52:25Z', 'mongo_id': '63cffa2a78b994746c729cef', '@version': '1', 'host': 'bdvm', 'logdate': '2023-01-24T15:32:58+00:00', '@timestamp': '2023-01-24T15:32:58.729640255Z', 'content': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5-Level Paging and 5-Level EPT white paper\n\n\nDocument Number: 335252-001\n\n5-Level Paging and 5-Level EPT\nWhite Paper\n\nRevision 1.0\n\nDecember 2016\n\n\n\n2 Document Number: 335252-001, Revision: 1.0\n\nLegal Lines and DisclaimersIntel technologies’ features and benefits depend on system configuration and may require enabled hardware, software, or service \nactivation. Learn more at intel.com, or from the OEM or retailer.\nNo computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any \ndamages resulting from such losses.\nYou may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel \nproducts described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted \nwhich includes subject matter disclosed herein.\nNo license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document.\nThe products described may contain design defects or errors known as errata which may cause the product to deviate from \npublished specifications. Current characterized errata are available on request.\nThis document contains information on products, services and/or processes in development. All information provided here is \nsubject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps.\nIntel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for \na particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or \nusage in trade.\nCopies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-\n4725 or by visiting www.intel.com/design/literature.htm.\nIntel, the Intel logo, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries.\n*Other names and brands may be claimed as the property of others.\nCopyright © 2016, Intel Corporation. All Rights Reserved.\n\nNotice: This document contains information on products in the design phase of development. The information here is subject to \nchange without notice. Do not finalize a design with this information.\n\nhttp://www.intel.com/design/literature.htm\n\n\nDocument Number: 335252-001, Revision: 1.0 3\n\nContents\n\n1 Introduction ..............................................................................................................3\n1.1 Existing Paging in IA-32e Mode .............................................................................3\n1.2 Linear-Address Width and VMX Transitions .............................................................5\n1.3 Existing Extended Page Tables (EPT)......................................................................6\n\n2 Expanding Linear Addresses: 5-Level Paging .............................................................7\n2.1 5-Level Paging: Introduction.................................................................................7\n2.2 Enumeration and Enabling ....................................................................................7\n\n2.2.1 Enumeration by CPUID..............................................................................7\n2.2.2 Enabling by Software ................................................................................8\n\n2.3 Linear-Address Generation and Canonicality............................................................8\n2.4 5-Level Paging: Linear-Address Translation.............................................................9\n2.5 Linear-Address Registers and Canonicality ............................................................ 10\n\n2.5.1 Canonicality Checking on RIP Loads .......................................................... 11\n2.5.2 Canonicality Checking on Other Loads ....................................................... 12\n\n2.6 Interactions with TLB-Invalidation Instructions ...................................................... 13\n2.7 Interactions with Intel® MPX .............................................................................. 14\n2.8 Interactions with Intel® SGX .............................................................................. 15\n\n3 Linear-Address Expansion and VMX Transitions....................................................... 17\n3.1 Linear-Address Expansion and VM Entries............................................................. 17\n3.2 Linear-Address Expansion and VM Exits................................................................ 17\n\n4 5-Level EPT ............................................................................................................. 19\n4.1 4-Level EPT: Guest-Physical-Address Limit............................................................ 19\n4.2 5-Level EPT: Enumeration and Enabling ............................................................... 19\n\n4.2.1 Enumeration.......................................................................................... 19\n4.2.2 Enabling by Software .............................................................................. 20\n\n4.3 5-Level EPT: Guest-Physical-Address Translation ................................................... 20\n4.4 5-Level EPT and EPTP Switching .......................................................................... 21\n\n5 Intel® Virtualization Technology for Directed I/O ................................................... 23\n\nFigures\n1-1 Linear-Address Translation Using IA-32e Paging ......................................................4\n2-1 Linear-Address Translation Using 5-Level Paging ................................................... 11\n\nTables\n2-1 Format of a PML5 Entry (PML5E) that References a PML4 Table .................................9\n4-1 Format of an EPT PML5 Entry (EPT PML5E) ........................................................... 20\n\n\n\n4 Document Number: 335252-001, Revision: 1.0\n\nRevision History\n\nDocument \nNumber\n\nRevision \nNumber Description Date\n\n335252-001 1.0 • Initial Release December 2016\n\n\n\nDocument Number: 335252-001, Revision: 1.0 3\n\n1 Introduction\n\nThis document describes planned extensions to the Intel 64 architecture to expand the \nsize of addresses that can be translated through a processor’s memory-translation \nhardware.\n\nModern operating systems use address-translation support called paging. Paging \ntranslates linear addresses (also known as virtual addresses), which are used by \nsoftware, to physical addresses, which are used to access memory (or memory-\nmapped I/O). Section 1.1 describes the 64-bit paging hardware on Intel 64 processors. \nExisting processors limit linear addresses to 48 bits. Chapter 2 describes paging \nextensions that would relax that limit to 57 linear-address bits.\n\nVirtual-machine monitors (VMMs) use the virtual-machine extensions (VMX) to \nsupport guest software operating in a virtual machine. VMX transitions are control-\nflow transfers between the VMM and guest software. VMX transitions involve the \nloading and storing of various processor registers. Some of these registers are defined \nto contain linear addresses. Because of this, the operation of VMX transitions depends \nin part on the linear-address width supported by the processor. Section 1.2 describes \nthe existing treatment of linear-address registers by VMX transitions, while Chapter 3 \ndescribes the changes required to support larger linear addresses.\n\nVMMs may also use additional address-translation support called extended page \ntables (EPT). When EPT is used, paging produces guest-physical addresses, which \nEPT translates to physical addresses. Section 1.3 describes the EPT hardware on \nexisting Intel 64 processors, which limit guest-physical addresses to 48 bits. Chapter 4 \ndescribes EPT extensions to support 57 guest-physical-address bits.\n\n1.1 Existing Paging in IA-32e Mode\nOn processors supporting Intel 64 architecture, software typically references memory \nusing linear addresses. Most modern operating systems configure processors to use \npaging, which translates linear addresses to physical addresses. The processor uses \nthe resulting physical addresses to access memory.\n\nIA-32e mode is a mode of processor execution that extends the older 32-bit \noperation, known as legacy mode. Software can enter IA-32e mode with the following \nalgorithm.\n\n1. Use the MOV CR instruction to set CR4.PAE[bit 5]. (Physical-address extension \nmust be enabled to enter IA-32e mode.)\n\n2. Use the WRMSR instruction to set bit 8 (LME) of the IA32_EFER MSR (index \nC0000080H).\n\n3. Use the MOV CR instruction to load CR3 with the address of a PML4 table (see \nbelow).\n\n4. Use the MOV CR instruction to set CR0.PG[bit 31].\n\nA logical processor is in IA-32e mode whenever CR0.PG = 1 and IA32_EFER.LME = 1. \nThis fact is reported in IA32_EFER.LMA[bit 10]. Software cannot set this bit directly; it \nis always the logical-AND of CR0.PG and IA32_EFER.LME.\n\n\n\n4 Document Number: 335252-001, Revision: 1.0\n\nIn IA-32e mode, linear addresses are 64 bits in size.1 However, the corresponding \npaging mode (currently called IA-32e paging) does not use all 64 linear-address bits.\n\nIA-32e paging does not use all 64 linear-address bits because processors limit the size \nof linear addresses. This limit is enumerated by the CPUID instruction. Specifically, \nCPUID.80000008H:EAX[bits 15:8] enumerates the number of linear-address bits (the \nmaximum linear-address width) supported by the processor. Existing processors \nenumerate this value as 48.\n\nNote: Processors also limit the size of physical addresses and enumerate the limit using \nCPUID. CPUID.80000008H:EAX[bits 7:0] enumerates the number of physical-address \nbits supported by the processor, the maximum physical-address width. Existing \nprocessors have enumerated values up to 46. Software can use more than 32 physical-\naddress bits only if physical-address extension has been enabled by setting \nCR4.PAE, bit 5 of control register CR4.\n\nThe enumerated limitation on the linear-address width implies that paging translates \nonly the low 48 bits of each 64-bit linear address. After a linear address is generated \nbut before it is translated, the processor confirms that the address uses only the 48 bits \nthat the processor supports.\n\nThe limitation to 48 linear-address bits results from the nature of IA-32e paging, which \nis illustrated in Figure 1-1.\n\n1. IA-32e mode comprises two sub-modes: compatibility mode and 64-bit mode. In compatibility \nmode, software uses 32-bit addresses, which the processor zero-extends to 64-bit linear \naddresses. In 64-bit mode, software uses 64-bit addresses directly.\n\nFigure 1-1. Linear-Address Translation Using IA-32e Paging\n\nDirectory Ptr\n\nPTE\n\nLinear Address\n\nPage Table\n\nPDPTE\n\nCR3\n\n39 38\n\nPointer Table\n\n9\n9\n\n40\n\n12\n9\n\n40\n\n4-KByte Page\n\nOffset\n\nPhysical Addr\n\nPDE\n\nTable\n011122021\n\nDirectory\n30 29\n\nPage-Directory-\n\nPage-Directory\n\nPML4\n47\n\n9\n\nPML4E\n\n40\n\n40\n\n40\n\n\n\nDocument Number: 335252-001, Revision: 1.0 5\n\nThe processor performs IA-32e paging by traversing a 4-level hierarchy of paging \nstructures whose root structure resides at the physical address in control register \nCR3. Each paging structure is 4-KBytes in size and comprises 512 8-byte entries. The \nprocessor uses the upper 36 bits of a linear address (bits 47:12), 9 bits at a time, to \nselect paging-structure entries from the hierarchy.\n\nNote: Figure 1-1 illustrates the translation of a linear address to a 4-KByte page. The paging \nprocess can be configured so that the translation of some linear addresses stops one or \ntwo levels earlier, translating instead to 2-MByte pages or 1-GByte pages.\n\nIn general, bits 51:12 of each paging-structure entry contain a 4-KByte aligned \nphysical address. For each entry except the last, this address is that of the next paging \nstructure; in the last entry, it is the physical address of a 4-KByte page frame. The \nfinal physical address is obtained by combining this page-frame address with the page \noffset, bits 11:0 of the original linear address.\n\nBecause only bits 47:0 of a linear address are used in address-translation, the \nprocessor reserves bits 63:48 for future expansion using a concept known as \ncanonicality. A linear address is canonical if bits 63:47 of the address are identical. \n(Put differently, a linear address is canonical only if bits 63:48 are a sign-extension of \nbit 47, which is the uppermost bit used in linear-address translation.)\n\nWhen a 64-bit linear address is generated to access memory, the processor first \nconfirms that the address is canonical. If the address is not canonical, the memory \naccess causes a fault, and the processor makes no attempt to translate the address.1\n\nIntel 64 architecture includes numerous registers that are defined to hold linear \naddresses. These registers may be loaded using a variety of instructions. In most \ncases, these instructions cause a general-protection exception (#GP) if an attempt is \nmade to load one of these registers with a value that is not canonical.\n\nPhysical-address bits in a paging-structure entry beyond the enumerated physical-\naddress width are reserved. A page-fault exception (#PF) results if an attempt is made \nto access a linear address whose translation encounters a paging-structure entry that \nsets any of those bits.\n\n1.2 Linear-Address Width and VMX Transitions\nVM entries and VM exits manipulate numerous processor registers that contain linear \naddresses. The transitions respect the processor’s linear-address width in a manner \nbased on canonicality.\n\nCertain fields in the VMCS correspond to registers that contain linear addresses. \nVM entries confirm that most of those fields contain values that are canonical. Some \nregisters, such as RIP and the LDTR base address, receive special treatment.\n\nVM exits save into the VMCS the state of certain registers, some of which contain linear \naddresses. Because the processor generally ensures that the values in these registers \nare canonical (see Section 1.1), the values that VM exits save for these registers will \ngenerally be canonical.\n\n1. In general, an attempt to access memory using a linear address that is not canonical causes a \ngeneral-protection exception (#GP). A stack-fault exception — #SS — occurs instead if the \nmemory access was made using the SS segment.\n\n\n\n6 Document Number: 335252-001, Revision: 1.0\n\nVM exits also load from the VMCS certain registers, some of which contain linear \naddresses. Each VM exit ensures that the value of each of these registers is canonical. \nSpecifically, bits 47:0 of the register are loaded from the field in the host-state area; \nthe value of bit 47 is then sign-extended into bits 63:48 of the register.\n\n1.3 Existing Extended Page Tables (EPT)\nMost Intel 64 processors supporting VMX also support an additional layer of address \ntranslation called extended page tables (EPT).\n\nVM entry can be configured to activate EPT for guest software. When EPT is active, the \naddresses used and produced by paging (Section 1.1) are not used as physical \naddresses to reference in memory. Instead, the processor interprets them as guest-\nphysical addresses, and translates them to physical addresses in a manner \ndetermined by the VMM. (This translation from guest-physical to physical applies not \nonly to the output of paging but also to the addresses that the processor uses to \nreference the guest paging structures.)\n\nIf the EPT translation process cannot translate a guest-physical address, it causes an \nEPT violation. (EPT violations may also occur when an access to a guest-physical \naddress violates the permissions established by EPT for that guest-physical address.) \nAn EPT violation is a VMX-specific exception, usually causing a VM exit.\n\nAs noted in Section 1.1, existing processors limit physical addresses to 46 bits. That \nlimit applies also to guest-physical addresses. As a result, guest-physical addresses \nthat set bits beyond this limit are not translated by EPT. (For example, a page fault \nresults if linear-address translation encounters a paging-structure entry with such an \naddress.) Because of this, existing EPT has been limited to translating only 48 guest-\nphysical-address bits.\n\nThe existing EPT translation process is analogous to the paging process that was \nillustrated earlier in Figure 1-1. Like 4-level paging, the processor implements EPT by \ntraversing a 4-level hierarchy of 4-KByte EPT paging structures. The last EPT paging-\nstructure entry contains the upper bits of the final physical address, while the lowest \nbits come from the original guest-physical address.\n\n\n\nDocument Number: 335252-001, Revision: 1.0 7\n\n2 Expanding Linear Addresses: \n5-Level Paging\n\n2.1 5-Level Paging: Introduction\n5-level paging is a new paging mode that will be available in IA-32e mode. As its \nname suggests, it will translate linear addresses by traversing a 5-level hierarchy of \npaging structures. Because the process is otherwise unmodified, 5-level paging extends \nthe processor’s linear-address width to 57 bits. (The additional 9 bits are used to select \nan entry from the fifth level of the hierarchy.) For clarity, the paging mode formerly \ncalled IA-32e paging will now be called 4-level paging.\n\nThe remainder of this chapter specifies the architectural changes that define and are \nentailed by 5-level paging. Section 2.2 specifies how the CPU enumerates the new \nfeature and how it is enabled by software. Section 2.3 describes changes to the process \nof linear-address generation, as well as a revision to the concept of canonicality. \nSection 2.4 details how 5-level paging translates linear addresses. Section 2.5 clarifies \nhow the processor treats loads of registers containing linear addresses, while Section \n2.6 to Section 2.8 consider interactions with various other features. (Interactions with \nthe virtual-machine extensions are specified in Chapter 3.)\n\n2.2 Enumeration and Enabling\nThis section describes how processors enumerate to software support for 5-level paging \nand related features and also how software enables the processor to use that support.\n\n2.2.1 Enumeration by CPUID\nProcessors supporting the Intel 64 architecture typically use the CPUID instruction to \nenumerate to software specific processor functionality. Those processors that support \n5-level paging enumerate that fact through a new feature flag as well as through \nchanges in how related features are reported:\n\n• CPUID.(EAX=07H, ECX=0):ECX[bit 16] is a new feature flag that will enumerate \nbasic support for 5-level paging. All older processors clear this bit. A processor will \nset this bit if and only if it supports 5-level paging.\n\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 15:8] enumerates the \nmaximum linear-address width supported by the processor. All older processors \nthat support Intel 64 architecture enumerated this value as 48. Processors that \nsupport 5-level paging will instead enumerate this value as 57.\n\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 7:0] enumerates the \nmaximum physical-address width supported by the processor. Processors that \nsupport Intel 64 architecture have enumerated at most 46 for this value. \nProcessors that support 5-level paging are expected to enumerate higher values, \nup to 52.\n\n• CPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17] is an existing field that \nenumerates the user MPX address-width adjust (MAWAU). This value specifies the \nnumber of linear-address bits above 48 on which the BNDLDX and BNDSTX \ninstructions operate in 64-bit mode when CPL = 3.\n\n\n\n8 Document Number: 335252-001, Revision: 1.0\n\nOlder processors that support Intel® MPX enumerated 0 for this value. Processors \nthat support 5-level paging may enumerate either 0 or 9, depending on \nconfiguration by system software. See Section 2.7 for more details on how BNDLDX \nand BNDSTX use MAWAU and how system software determines its value.\n\n• CPUID.(EAX=12H,ECX=0H):EDX[bits 15:8] is an existing field that enumerates \ninformation that specifies the maximum supported size of a 64-bit enclave. If the \nvalue enumerated is n, the maximum size is 2n. Older processors that support \nIntel® SGX enumerated at most 47 for this value. Processors that support 5-level \npaging are expected to enumerate this value as 56.\n\n2.2.2 Enabling by Software\nSection 1.1 identified an algorithm by which software can enter IA-32e mode. On \nprocessors that do not support 5-level paging, this algorithm enables 4-level paging. \nOn processors that support 5-level paging, it can be adapted to enable 5-level paging \ninstead.\n\nProcessors that support 5-level paging allow software to set a new enabling bit, \nCR4.LA57[bit 12].1 A logical processor in IA-32e mode (IA32_EFER.LMA = 1) uses 5-\nlevel paging if CR4.LA57 = 1. Outside of IA-32e mode (IA32_EFER.LMA = 0), the value \nof CR4.LA57 does not affect paging operation.\n\nThe following items detail how a logical processor determines the current paging mode.\n\n• If CR0.PG = 0, paging is disabled.\n\n• If IA32_EFER.LMA = 0, one of the legacy 32-bit paging modes is used (depending \non the value of legacy paging-mode bits in CR4).2\n\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 0, 4-level paging is used.\n\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 1, 5-level paging is used.\n\nSoftware can thus use the following algorithm to enter IA-32e mode with 5-level \npaging.\n\n1. Use the MOV CR instruction to set CR4.PAE and CR4.LA57.\n2. Use the WRMSR instruction to set IA32_EFER.LME.\n\n3. Use the MOV CR instruction to load CR3 with the address of a PML5 table (see \nSection 2.4).\n\n4. Use the MOV CR instruction to set CR0.PG.\n\nThe processor allows software to modify CR4.LA57 only outside of IA-32e mode. In \nIA-32e mode, an attempt to modify CR4.LA57 using the MOV CR instruction causes a \ngeneral-protection exception (#GP).\n\n2.3 Linear-Address Generation and Canonicality\nAs noted in Section 1.1, processors with a linear-address width of 48 bits reserve \nlinear-address bits 63:48 for future expansion. Linear addresses that use only bits 47:0 \n(because bits 63:48 are a sign-extension of bit 47) are called canonical.\n\n1. Software can set CR4.LA57 only if CPUID.(EAX=07H, ECX=0):ECX[bit 16] is enumerated as 1.\n2. Recall that IA32_EFER.LMA is the logical-AND of CR0.PG and IA32_EFER.LME.\n\n\n\nDocument Number: 335252-001, Revision: 1.0 9\n\nWhen a 64-bit linear address is generated to access memory, the processor first \nconfirms that the address is canonical. If the address is not canonical, the memory \naccess causes a fault, and the address is not translated.\n\nProcessors that support 5-level paging can translate 57-bit linear addresses when 5-\nlevel paging is enabled. But if software has enabled only 4-level paging, such a \nprocessor can translate only 48-bit linear addresses. This fact motivates the definition \nof two levels of canonicality.\n\nA linear address is 48-bit canonical if bits 63:47 of the address are identical. \nSimilarly, an address is 57-bit canonical if bits 63:56 of the address are identical. Any \nlinear address is that 48-bit canonical is also 57-bit canonical.\n\nWhen a 64-bit linear address is generated to access memory, a processor that supports \n5-level paging checks for canonicality based on the current paging mode: if 4-level \npaging is enabled, the address must be 48-bit canonical; if 5-level paging is enabled, \nthe address need only be 57-bit canonical. If the appropriate canonicality is not \nobserved, the memory access causes a fault.\n\n2.4 5-Level Paging: Linear-Address Translation\nAs noted in Section 2.2.2, a logical processor uses 5-level paging if IA32_EFER.LMA = 1 \nand CR4.LA57 = 1.\n\nLike 4-level paging, 5-level paging translates linear addresses using a hierarchy of in-\nmemory paging structures. Because 5-level paging increases the linear-address width \nto 57 bits (from the 48 bits supported by 4-level paging), 5-level paging allows up to \n128 PBytes of linear-address space to be accessed at any given time.\n\nAlso like 4-level paging, 5-level paging uses CR3 to locate the first paging-structure in \nthe hierarchy. (CR3 has the same mode-specific format with 5-level paging as it does \nwith 4-level paging.) The following items describe in more detail the changes that 5-\nlevel paging makes to the translation process.\n\n• Translation begins by identifying a 4-KByte naturally aligned PML5 table. It is \nlocated at the physical address specified in bits 51:12 of CR3. A PML5 table \ncomprises 512 64-bit entries (PML5Es). A PML5E is selected using the physical \naddress defined as follows.\n\n— Bits 51:12 are from CR3.\n— Bits 11:3 are bits 56:48 of the linear address.\n— Bits 2:0 are all 0.\n\nBecause a PML5E is identified using bits 56:48 of the linear address, it controls \naccess to a 256-TByte region of the linear-address space. The format of a PML5E is \ngiven in Table 2-1.\n\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table\n\nBit Position(s) Contents\n\n0 (P) Present; must be 1 to reference a PML4 table.\n\n1 (R/W) Read/write; if 0, writes may not be allowed to the 256-TByte region controlled by this entry.\n\n2 (U/S) User/supervisor; if 0, user-mode accesses are not allowed to the 256-TByte region \ncontrolled by this entry.\n\n3 (PWT) Page-level write-through; indirectly determines the memory type used to access the PML4 \ntable referenced by this entry.\n\n\n\n10 Document Number: 335252-001, Revision: 1.0\n\n• The next step of the translation process identifies a 4-KByte naturally aligned PML4 \ntable. It is located at the physical address specified in bits 51:12 of the PML5E (see \nTable 2-1). A PML4 table comprises 512 64-bit entries (PML4Es). A PML4E is \nselected using the physical address defined as follows.\n\n— Bits 51:12 are from the PML5E.\n— Bits 11:3 are bits 47:39 of the linear address.\n— Bits 2:0 are all 0.\n\nAs is normally the case when accessing a paging-structure entry, the memory type \nused to access the PML4E is based in part on the PCD and PWT bits in the PML5E.\n\nBecause a PML4E is identified using bits 56:39 of the linear address, it controls \naccess to a 512-GByte region of the linear-address space.\n\nOnce the PML4E is identified, bits 38:0 of the linear address determine the remainder \nof the translation process exactly as is done for 4-level paging. As suggested in \nTable 2-1, the values of bit 1, bit 2, and bit 63 of the PML5E are used normally (in \ncombination with the corresponding bits in other paging-structure entries) to determine \naccess rights. The accessed flag (bit 5) in the PML5E is updated as is done for other \npaging-structure entries.\n\nThe operation of 5-level paging is illustrated in Figure 2-1.\n\n2.5 Linear-Address Registers and Canonicality\nIntel 64 architecture includes numerous registers that are defined to hold linear \naddresses. These registers may be loaded using a variety of instructions. As noted in \nSection 1.1, each of these instructions typically causes a general-protection exception \n(#GP) if an attempt is made to load a linear-address register with a value that is not \ncanonical.\n\nAs noted in Section 2.3, processors that support 5-level paging use two definitions of \ncanonicality: 48-bit canonicality and 57-bit canonicality. This section describes how \nsuch a processor checks the canonicality of the values being loaded into the linear-\naddress registers. One approach is used for operations that load RIP (the instruction \npointer; see Section 2.5.1) and another is used for those that load other registers (see \nSection 2.5.2).\n\n4 (PCD) Page-level cache disable; indirectly determines the memory type used to access the PML4 \ntable referenced by this entry.\n\n5 (A) Accessed; indicates whether this entry has been used for linear-address translation.\n\n6 Ignored.\n\n7 (PS) Reserved (must be 0).\n\n11:8 Ignored.\n\nM–1:12 Physical address of 4-KByte aligned PML4 table referenced by this entry.\n\n51:M Reserved (must be 0).\n\n62:52 Ignored.\n\n63 If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not allowed from the \n256-TByte region controlled by this entry); otherwise, reserved (must be 0).\n\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table (Continued)\n\nBit Position(s) Contents\n\n\n\nDocument Number: 335252-001, Revision: 1.0 11\n\n2.5.1 Canonicality Checking on RIP Loads\nThe RIP register contains the offset of the current instruction pointer within the CS \nsegment. Because the processor treats the CS base address as zero in 64-bit mode, the \nvalue of the RIP register in that mode is the linear address of the instruction pointer.\n\nOperations that load RIP (including both instructions such as JMP as well as control \ntransfers through the IDT) check first whether the value to be loaded is canonical \nrelative to the current paging mode. If the processor determines that the address is not \ncanonical, the RIP load is not performed and a general-protection exception (#GP) \noccurs.\n\nNote: An instruction that would load RIP with a non-canonical address faults, meaning that \nthe return instruction pointer of the fault handler is the address of the faulting \ninstruction and not the non-canonical address whose load was attempted.\n\nThe canonicality checking performed by these operations uses 48-bit canonicality when \n4-level paging is active. When 5-level paging is active, the checking is relaxed to \nrequire only 57-bit canonicality.\n\nThe SYSCALL and SYSENTER instructions load RIP from the IA32_LSTAR and \nIA32_SYSENTER_EIP MSRs, respectively. On processors that support only 4-level \npaging, these instructions do not check that the values being loaded are canonical \nbecause the WRMSR instruction ensures that each of these MSRs contains a value that \nis 48-bit canonical. On processors that support 5-level paging, the checking by WRMSR \nis relaxed to 57-bit canonicality (see Section 2.5.2). On such processors, an execution \n\nFigure 2-1. Linear-Address Translation Using 5-Level Paging\n\nPDE\n\nLinear Address\n\nPage Directory\n\nPML4E\n\nCR3\n\n39 38\n\n9 9\n\n40\n\n12\n9\n\n40\n\n4-KByte Page\n\nOffset\n\nPhysical Addr\n\nPDPTE\n\n01112202130 29\n\nPage-Directory\n\n47\n\n9\n\nPML5E\n\n40\n\n40\n\n40\n\n56\nTableDirectoryDirectory PtrPML4PML5\n\nPointer Table\n\nPage Table\n\nPTE\n\n9\n\n40\n\n\n\n12 Document Number: 335252-001, Revision: 1.0\n\nof SYSCALL or SYSENTER with 4-level paging checks that the value being loaded into \nRIP is 48-bit canonical.1\n\nThe normal advancing of the instruction pointer to the next instruction boundary may \nresult in the RIP register holding a non-canonical address. The fetch of the next \ninstruction from that non-canonical address will result in a general-protection exception \nas indicated in Section 2.3. In this case, the return instruction pointer of the fault \nhandler will be that non-canonical address.\n\n2.5.2 Canonicality Checking on Other Loads\nIn addition to RIP, the CPU maintains numerous other registers that hold linear \naddresses:\n\n• GDTR and IDTR (in their base-address portions).\n\n• LDTR, TR, FS, and GS (in the base-address portions of their hidden descriptor \ncaches).\n\n• The debug-address registers (DR0 through DR3), which hold the linear addresses \nof breakpoints.\n\n• The following MSRs: IA32_BNDCFGS, IA32_DS_AREA, IA32_KERNEL_GS_BASE, \nIA32_LSTAR, IA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, and \nIA32_SYSENTER_ESP.\n\n• The x87 FPU instruction pointer (FIP).\n\n• The user-mode configuration register BNDCFGU, used by Intel® MPX.\n\nWith a few exceptions, the processor ensures that the addresses in these registers are \nalways canonical in the following ways.\n\n• Some instructions fault on attempts to load a linear-address register with a non-\ncanonical address:\n\n— An execution of the LGDT or LIDT instruction causes a general-protection \nexception (#GP) if the base address specified in the instruction’s memory \noperand is not canonical.\n\n— An execution of the LLDT or LTR instruction causes a #GP if the base address to \nbe loaded from the GDT is not canonical.\n\n— An execution of WRMSR, WRFSBASE, or WRGSBASE causes a #GP if it would \nload the base address of either FS or GS with a non-canonical address.\n\n— An execution of WRMSR causes a #GP if it would load any of the following MSRs \nwith a non-canonical address: IA32_BNDCFGS, IA32_DS_AREA, \nIA32_FS_BASE, IA32_GS_BASE, IA32_KERNEL_GS_BASE, IA32_LSTAR, \nIA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, or \nIA32_SYSENTER_ESP.2\n\n1. The SYSRET and SYSEXIT instructions, which complement SYSCALL and SYSENTER, load RIP \nfrom RCX and RDX, respectively. Even before 5-level paging, these instructions checked the \ncanonicality of the value to be loaded into RIP. As with other instructions that load RIP, this \nchecking will be based on the current paging mode.\n\n2. Such canonicality checking may apply also when the WRMSR instruction is used to load some \nnon-architectural MSRs (not listed here) that hold a linear address.\n\n\n\nDocument Number: 335252-001, Revision: 1.0 13\n\n— An execution of XRSTORS causes a #GP if it would load any of the following \nMSRs with a non-canonical address: IA32_RTIT_ADDR0_A, \nIA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, IA32_RTIT_ADDR1_B, \nIA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, IA32_RTIT_ADDR3_A, and \nIA32_RTIT_ADDR3_B.\n\nThis enforcement always uses the enumerated maximum linear-address width and \nis independent of the current paging mode. Thus, a processor that supports 5-level \npaging will allow the instructions mentioned above to load these registers with \naddresses that are 57-bit canonical but not 48-bit canonical — even if 4-level \npaging is active. (As a result, instructions that store these values — SGDT, SIDT, \nSLDT, STR, RDFSBASE, RDGSBASE, RDMSR, XSAVE, XSAVEC, XSAVEOPT, and \nXSAVES — may save addresses that are 57-bit canonical but not 48-bit canonical, \neven if 4-level paging is active.)\n\n• The FXRSTOR, XRSTOR, and XRSTORS instructions ignore attempts to load some of \nthese registers with non-canonical addresses:\n\n— Loads of FIP ignore any bits in the memory image beyond the enumerated \nmaximum linear-address width. The processor sign-extends to most significant \nbit (e.g., bit 56 on processors that support 5-level paging) to ensure that FIP is \nalways canonical.\n\n— Loads of BNDCFGU (by XRSTOR or XRSTORS) ignore any bits in the memory \nimage beyond the enumerated maximum linear-address width. The processor \nsign-extends to most significant bit (e.g., bit 56 on processors that support 5-\nlevel paging) to ensure that BNDCFGU is always canonical.\n\n• Every non-control x87 instruction loads FIP. The value loaded is always canonical \nrelative to the current paging mode: 48-bit canonical if 4-level paging is active, and \n57-bit canonical if 5-level paging is active.\n\nDR0 through DR3 can be loaded with the MOV to DR instruction. The instruction allows \nthose registers to be loaded with non-canonical addresses. The MOV from DR \ninstruction will return the value last loaded with the MOV to DR instruction, even if the \naddress is not canonical. Breakpoint address matching is supported only for canonical \nlinear addresses.\n\n2.6 Interactions with TLB-Invalidation Instructions\nIntel 64 architecture includes three instructions that may invalidate TLB entries for the \nlinear address of an instruction operand: INVLPG, INVPCID, and INVVPID. The following \nitems describe how they are affected by linear-address width.\n\n• The INVLPG instruction takes a memory operand. It invalidates any TLB entries \nthat the logical processor is caching for the linear address of that operand for the \ncurrent linear address space. The instruction does not fault if that address is not \ncanonical relative to the current paging mode (e.g., is not 48-bit canonical when 4-\nlevel paging is active). However, no invalidation is performed because the processor \ndoes not cache TLB entries for addresses that are not canonical relative to the \ncurrent paging mode.\n\n• The INVPCID instruction takes a register operand (INVPCID type) and a memory \noperand (INVPCID descriptor). If the INVPCID type is 0, the instruction invalidates \nany TLB entries that the logical processor is caching for the linear address and PCID \nspecified in the INVPCID descriptor. If the linear address is not canonical relative \nthe linear-address width supported by the processor, the instruction causes a \ngeneral-protection exception (#GP). If the processor supports 5-level paging, the \ninstruction will not cause such a #GP for an address that is 57-bit canonical, \nregardless of paging mode, even if 4-level paging is active and the address is not \n48-bit canonical.\n\n\n\n14 Document Number: 335252-001, Revision: 1.0\n\n• The INVVPID instruction takes a register operand (INVVPID type) and a memory \noperand (INVVPID descriptor). If the INVPCID type is 0, the instruction invalidates \nany TLB entries that the logical processor is caching for the linear address and VPID \nspecified in the INVVPID descriptor. If the linear address is not canonical relative \nthe linear-address width supported by the processor, the instruction fails.1 If the \nprocessor supports 5-level paging, the instruction will not fail for an address that is \n57-bit canonical, regardless of paging mode, even if 4-level paging is active and the \naddress is not 48-bit canonical.\n\n2.7 Interactions with Intel® MPX\nThe Intel® Memory Protection Extensions (Intel® MPX) define a set of 4 bound \nregisters, each of which software can associate with a specific pointer in memory. \nIntel MPX includes two instructions — BNDLDX and BNDSTX — that allow software to \nload from or store into memory the bounds associated with a particular pointer in \nmemory.\n\nThe BNDLDX and BNDSTX instructions each take a bound register and a memory \noperand (the associated pointer). Each of these parses the linear address of the \nmemory operand to traverse a hierarchical data structure in memory. In 64-bit mode, \nthese instructions do not necessarily use all the bits in the supplied 64-bit addresses. \nThe number of bits used is 48 plus a value called the MPX address-width adjust \n(MAWA).\n\nThe value of MAWA depends on CPL; the current paging mode (4-level paging or 5-level \npaging); and, if 5-level paging is active, the value of a new MSR. Processors that \nsupport both Intel MPX and 5-level paging support the IA32_MPX_LAX MSR (MSR index \n1000H). Only bit 0 of the MSR is defined.\n\nIf CPL < 3, the supervisor MAWA (MAWAS) is used. The value of MAWAS is determined \nby the setting of CR4.LA57. If CR4.LA57 = 0 (4-level paging is active; recall that MAWA \nis relevant only in 64-bit mode), the value of MAWAS is 0. If CR4.LA57 = 1 (5-level \npaging is active), the value of MAWAS is 9. The value of MAWAS is not enumerated by \nthe CPUID instruction.\n\nIf CPL = 3, the user MAWA (MAWAU) is used. The value of MAWAU is determined as \nfollows. If CR4.LA57 = 0 or IA32_MPX_LAX[bit 0] = 0, the value of MAWAU is 0. If \nCR4.LA57 = 1 and IA32_MPX_LAX[bit 0] = 1, the value of MAWAU is 9. The current \nvalue of MAWAU is enumerated in \nCPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17].\n\nThe following items specify how an execution of the BNDLDX and BNDSTX instructions \nin 64-bit mode parses a linear address to traverse a hierarchical data structure.\n\n• A bound directory is located at the 4-KByte aligned linear address specified in \nbits 63:12 of BNDCFGx.2 A BDE is selected using the LAp (linear address of pointer \nto a buffer) to construct a 64-bit offset as follows:\n\n— bits 63:31+MAWA are 0;\n— bits 30+MAWA:3 are LAp[bits 47+MAWA:20]; and\n— bits 2:0 are 0.\n\n1. INVVPID is a VMX instruction. In response to certain conditions, execution of a VMX may fail, \nmeaning that it does not complete its normal operation. When a VMX instruction fails, control \npasses to the next instruction (rather than to a fault handler) and a flag is set to report the \nfailure.\n\n2. If CPL < 3, BNDCFGS is used; if CPL = 3, BNDCFGU is used.\n\n\n\nDocument Number: 335252-001, Revision: 1.0 15\n\nThe address of the BDE is the sum of the bound-directory base address (from \nBNDCFGx) plus this 64-bit offset.\n\nIf either BNDLDX or BNDSTX is executed inside an enclave, the instruction operates \nas if MAWAU = 0 (regardless of the values of CR4.LA57 and IA32_MPX_LAX[bit 0]).\n\n• The processor uses bits 63:3 of the BDE as the 8-byte aligned address of a bound \ntable (BT). A BTE is selected using the LAp (linear address of pointer to a buffer) to \nconstruct a 64-bit offset as follows:\n\n— bits 63:22 are 0;\n— bits 21:5 are LAp[bits 19:3]; and\n— bits 4:0 are 0.\n\nThe address of the BTE is the sum of the bound-table base address (from the BDE) \nplus this 64-bit offset.\n\nA bound directory comprises 228+MAWA 64-bit entries (BDEs);1 thus, the size of a \nbound directory in 64-bit mode is 21+MAWA GBytes. A bound table comprises 217 32-\nbyte entries (BTEs); thus, the size of a bound table in 64-bit mode is 4 MBytes \n(independent of MAWA).\n\n2.8 Interactions with Intel® SGX\nIntel® Software Guard Extensions (Intel® SGX) define new processor functionality that \nis implemented as SGX leaf functions within the ENCLS (supervisor) and ENCLU (user) \ninstructions.\n\nThe SGX leaf functions include memory accesses using linear addresses normally. \nWhen executed in 64-bit mode, the linear address are 64 bits in width and are subject \nto the normal treatment of accesses to memory with 64-bit linear addresses (see \nSection 2.3). In addition, some of the leaf functions apply specific architectural checks \nrelated to linear-address width. The following items detail these checks and how they \nare defined for processors that support 5-level paging.\n\n• The ECREATE leaf function of ENCLS creates a new enclave by creating a new SGX \nenclave control structure (SECS). For a 64-bit enclave, the processor checks \nwhether the enclave base linear address (specified in the SECS) is canonical, \ngenerating a general-protection exception (#GP) if it is not. On processors that \nsupport 5-level paging, this check is for 57-bit canonicality, regardless of the \ncurrent paging mode.\n\nIn addition to checking the canonicality of the enclave base linear address, \nECREATE confirms that the enclave size (specified in the SECS) is not greater than \nthe maximum size supported by the processor (if the enclave size is too large, \nECREATE generates a #GP). As noted in Section 2.2.1, older processors supported \n64-bit enclaves with sizes up to 247 bytes; processors that support 5-level paging \nare expected to support enclaves with sizes up to 256 bytes.\n\nIf bits 4:3 of the enclave’s XSAVE feature request mask (XFRM) are set (indicating \nthat Intel MPX will be enabled during execution of the enclave), ECREATE generates \na #GP if the enclave’s size is greater than 248 bytes, even if the processor \nenumerates support for larger enclaves.\n\n• The EENTER and ERESUME leaf functions of ENCLU transfer control flow to an entry \npoint within a specified enclave. For entry to a 64-bit enclave, the processor checks \n\n1. A bound directory used in a 64-bit enclave always comprises 228 64-bit BDEs and thus has a size \nof 2 GBytes.\n\n\n\n16 Document Number: 335252-001, Revision: 1.0\n\nwhether certain linear addresses are canonical, generating a general-protection \nexception (#GP) if any one is not. The following items detail these checks.\n\n— The linear address of the specified entry point must be canonical. If 4-level \npaging is active, it must be 48-bit canonical; if 5-level paging is active, it must \nbe 57-bit canonical.\n\n— The linear address of the asynchronous exit point (AEP — the address to which \nthe processor transfers control on an asynchronous enclave exit) must be \ncanonical. If 4-level paging is active, it must be 48-bit canonical; if 5-level \npaging is active, it must be 57-bit canonical.\n\n— The enclave values for the base addresses of the FS and GS segments must be \ncanonical. On processors that supports 5-level paging, these checks are for 57-\nbit canonicality, regardless of the current paging mode.\n\n• The EEXIT leaf function exits the currently executing enclave and branches to a \nspecified address. For an exit from a 64-bit enclave, the processor checks whether \nthat target linear address is canonical, generating a general-protection exception \n(#GP) if it is not. If 4-level paging is active, it must be 48-bit canonical; if 5-level \npaging is active, it need only be 57-bit canonical.\n\nAs noted in Section 2.7, executions of BNDLDX and BNDSTX in a 64-bit enclave always \noperate as if MAWAU = 0.\n\n\n\nDocument Number: 335252-001, Revision: 1.0 17\n\n3 Linear-Address Expansion and \nVMX Transitions\n\nAs noted in Section 1.2, VM entries and VM exits manipulate numerous processor \nregisters that contain linear addresses. The transitions respect the processor’s linear-\naddress width in a manner based on canonicality.\n\nAs discussed in Chapter 2, processors that support 5-level paging expand the linear-\naddress width from 48 bits to 57 bits. That expansion changes the operation of VMX \ntransitions. Changes to VM entries are detailed in Section 3.1, while changes to \nVM exits are given in Section 3.2.\n\n3.1 Linear-Address Expansion and VM Entries\nCertain fields in the VMCS correspond to registers that contain linear addresses. \nVM entries confirm those fields contain values that are canonical. This checking is \nbased on the linear-address width supported by the processor (e.g., is based on 57-bit \ncanonicality if the processor supports 5-level paging). The following are the fields to \nwhich this applies.\n\n• In the host-state area:\n\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\n\n• In the guest-state area:\n\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\n— The base-address field for LDTR (if LDTR will be usable).\n— The field for the IA32_BNDCFGS MSR (if VM entry is loading that MSR).\n\nA VM entry to 64-bit mode also performs a check on the RIP field in the guest-state \narea of the current VMCS. If the VM entry would result in 4-level paging, it checks that \nbits 63:48 of the guest RIP field are identical; if it would result in 5-level paging, that \ncheck is on bits 63:57.1\n\n3.2 Linear-Address Expansion and VM Exits\nVM exits save the state of certain registers into the guest-state area of the VMCS. \nSome of these registers contain linear addresses. As discussed in Section 1.1, the CPU \ngenerally ensures that the values in these registers respect the CPU’s linear-address \nwidth. As a result, the values the VM exits save for these registers will do the same.\n\n1. Note that these checks do not confirm that the guest RIP field is canonical relative to the paging \nmode being entered. For example, bits 63:47 are identical in a 48-bit canonical address. However, \nVM entry to 4-level paging may load RIP with a value in which bit 47 differs from that of \nbits 63:48.\n\n\n\n18 Document Number: 335252-001, Revision: 1.0\n\nThere is a special case for LDTR base address. If LDTR was not usable at the time of a \nVM exit, the value saved for the base address is undefined. However, this undefined \nvalue is always 48-bit canonical on processors that do not support 5-level paging and is \nalways 57-bit canonical on processors that do support 5-level paging.\n\nVM exits load the state of certain registers from the host-state area of the VMCS. Some \nof these registers contain linear addresses. Each VM exit ensures that the value of each \nof the following registers is canonical: the IA32_SYSENTER_EIP and \nIA32_SYSENTER_ESP MSRs; and the base addresses for FS, GS, TR, GDTR, and IDTR. \nHow this is done depends on whether the processor supports 5-level paging.\n\n• If the processor does not support 5-level paging, bits 47:0 of the register are \nloaded from the field in the host-state area; the value of bit 47 is then sign-\nextended into bits 63:48 of the register.\n\n• If the processor does support 5-level paging, bits 56:0 of the register are loaded \nfrom the field in the host-state area; the value of bit 56 is then sign-extended into \nbits 63:57 of the register.\n\nAgain, there is a special case for LDTR. LDTR is always unusable after a VM exit. Its \nbase address may be loaded with an undefined value. This undefined value is always \n48-bit canonical on processors that do not support 5-level paging and is always 57-bit \ncanonical on processors that do support 5-level paging.\n\n\n\nDocument Number: 335252-001, Revision: 1.0 19\n\n4 5-Level EPT\n\n5-level EPT is a new mode for EPT. As its name suggests, it will translate guest-\nphysical addresses by traversing a 5-level hierarchy of EPT paging structures. Because \nthe process is otherwise unmodified, 5-level paging extends the processor’s guest-\nphysical-address width to 57 bits. (The additional 9 bits are used to select an entry \nfrom the fifth level of the hierarchy.) For clarity, the original EPT mode will now be \ncalled 4-level EPT.\n\nThe remainder of this chapter specifies architectural changes to 4-level EPT as well as \nthose that define and are entailed by 5-level EPT. Section 4.1 describes how the \nexpansion of the guest-physical-address width affects 4-level EPT. Section 4.2 specifies \nhow the CPU enumerates 5-level EPT and how the feature is enabled by software. \nSection 4.3 details how 5-level EPT translates guest-physical addresses.\n\n4.1 4-Level EPT: Guest-Physical-Address Limit\nAs explained in Section 1.3, 4-level EPT is limited to translating 48-bit guest-physical \naddresses.\n\nThis is not a problem on existing processors, because they limit the physical-address \nwidth to 46 bits (see Section 1.1). A processor’s physical-address width also limits \nguest-physical addresses. That means that, on existing processors, any attempt to use \na guest-physical address that sets a bit above the low 48 bits will cause a page-fault \nexception (#PF).\n\nProcessors that support 5-level paging are expected to support 52 physical-address \nbits. Such processors allow use of a guest-physical address that sets bits in the range \n51:48; no #PF is generated.\n\nA guest-physical address that sets bits in the range 51:48 cannot be translated by 4-\nlevel EPT. An attempt to access such an address when 4-level EPT is active causes an \nEPT violation (see Section 1.3).\n\nEPT violations generate information about the exception in a value called the exit \nqualification. In general, EPT violations caused by attempts to access a guest-physical \naddress that is too wide establish the exit qualification as is currently done for other \nEPT violations. Exceptions are made for bits 6:3 of the exit qualification, which report \nthe access rights for the guest-physical address. The new EPT violations always clear \nthese bits.\n\n4.2 5-Level EPT: Enumeration and Enabling\nThis section describes how processors enumerate to software support for 5-level EPT \nand how software enables the processor to use that support.\n\n4.2.1 Enumeration\nProcessors supporting EPT enumerate details related to EPT in the \nIA32_VMX_EPT_VPID_CAP MSR (index 48CH). Currently, \nIA32_VMX_EPT_VPID_CAP[bit 6] enumerates support for 4-level EPT. Processors that \nalso support 5-level EPT will enumerate that fact by also setting \nIA32_VMX_EPT_VPID_CAP[bit 7].\n\n\n\n20 Document Number: 335252-001, Revision: 1.0\n\nThe guest-physical-address width supported by a processor is not enumerated using \nthe IA32_VMX_EPT_VPID_CAP MSR. This is because that width is always the same as \nthe processor’s maximum physical-address width as enumerated by \nCPUID.80000008H:EAX[bits 7:0].\n\n4.2.2 Enabling by Software\nA VMM enables EPT by setting the “enable EPT” VM-execution control in the current \nVMCS before using the VMCS for VM entry.\n\nSpecific details of EPT operation are determined by the extended-page-table pointer \nfield (EPTP) in the VMCS. In particular, EPTP[bits 5:3] contain a value that is 1 less than \nthe number of levels used by the EPT. On existing processors, this value must be 3, \nindicating 4-level EPT. (VM entry fails if a different value is used.) Processors that also \nsupport 5-level EPT will also allow the value 4 (indicating 5-level EPT).\n\nIn summary, VM entry on a processor that supports 5-level check EPTP[bits 5:3]. If the \nvalue is 3, the VM entry activates 4-level EPT. If the value is 4, the VM entry activates \n5-level EPT. With any other value, VM entry fails.\n\n4.3 5-Level EPT: Guest-Physical-Address Translation\nLike 4-level EPT, 5-level EPT translates guest-physical addresses using a hierarchy of \nin-memory paging structures. Because 5-level EPT increases the guest-physical-\naddress width to 57 bits (from the 48 bits supported by 4-level EPT), 5-level EPT allows \nup to 128 PBytes of guest-physical-address space to be accessed at any given time.\n\nThe following items describe in more detail the changes that 5-level EPT makes to the \ntranslation process.\n\n• Translation begins by identifying a 4-KByte naturally aligned EPT PML5 table. It is \nlocated at the physical address specified in bits 51:12 of EPTP. An EPT PML5 table \ncomprises 512 64-bit entries (EPT PML5Es). An EPT PML5E is selected using the \nphysical address defined as follows.\n\n— Bits 63:52 are all 0.\n— Bits 51:12 are from EPTP.\n— Bits 11:3 are bits 56:48 of the guest-physical address.\n— Bits 2:0 are all 0.\n\nBecause an EPT PML5E is identified using bits 56:48 of the guest-physical address, it \ncontrols access to a 256-TByte region of the linear-address space. The format of an EPT \nPML5E is given in Table 4-1.\n\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E)\n\nBit Position(s) Contents\n\n0 Read access; indicates whether reads are allowed from the 256-TByte region controlled by \nthis entry.\n\n1 Write access; indicates whether writes are allowed from the 256-TByte region controlled by \nthis entry.\n\n2 If the “mode-based execute control for EPT” VM-execution control is 0, execute access; \nindicates whether instruction fetches are allowed from the 256-TByte region controlled by \nthis entry.\nIf that control is 1, execute access for supervisor-mode linear addresses; indicates whether \ninstruction fetches are allowed from supervisor-mode linear addresses in the 256-TByte \nregion controlled by this entry.\n\n\n\nDocument Number: 335252-001, Revision: 1.0 21\n\n• The next step of the translation process identifies a 4-KByte naturally aligned EPT \nPML4 table. It is located at the physical address specified in bits 51:12 of the EPT \nPML5E (see Table 4-1). An EPT PML4 table comprises 512 64-bit entries (EPT \nPML4Es). An EPT PML4E is selected using the physical address defined as follows.\n\n— Bits 51:12 are from the EPT PML5E.\n— Bits 11:3 are bits 47:39 of the guest-physical address.\n— Bits 2:0 are all 0.\n\nBecause an EPT PML4E is identified using bits 56:39 of the guest-physical address, \nit controls access to a 512-GByte region of the guest-physical-address space.\n\nOnce the EPT PML4E is identified, bits 38:0 of the guest-physical address determine the \nremainder of the translation process exactly as is done for 4-level EPT. As suggested in \nTable 4-1, the values of bits 2:0 and bit 10 of the EPT PML5E are used normally (in \ncombination with the corresponding bits in other EPT paging-structure entries) to \ndetermine whether EPT violations occur. The accessed flag (bit 8) in the EPT PML5E is \nupdated as is done for other EPT paging-structure entries.\n\n4.4 5-Level EPT and EPTP Switching\nThe value of EPTP may be modified in VMX non-root operation by invoking \nVM function 0 (EPTP switching). This is done by executing the VMFUNC instruction with \nvalue 0 in the EAX register. Invocation of VM function 0 loads EPTP with a value \nselected from a data structure in memory.\n\nBefore loading EPTP in this way, the processor first confirms that the value to be loaded \nis valid. The definition of a valid EPTP value depends on whether the processor supports \n5-level EPT.\n\n• If the processor does not support 5-level EPT, an EPTP value in memory is \nconsidered valid if it would not cause VM entry to fail (e.g., it does not set any \nreserved bits).\n\n• If the processor does support 5-level EPT, an EPTP value in memory is considered \nvalid only if it would not cause VM entry to fail (as above) and if its value in \nbits 5:3 (which controls the number of EPT levels) is the same as that of the \ncurrent value of EPTP.\n\nThe implication is that an invocation of VM function 0 cannot change the EPT mode \nbetween 4-level EPT and 5-level EPT.\n\n7:3 Reserved (must be 0).\n\n8 If bit 6 of EPTP is 1, accessed flag for EPT; indicates whether software has accessed the \n256-TByte region controlled by this entry. Ignored if bit 6 of EPTP is 0.\n\n9 Ignored.\n\n10 Execute access for user-mode linear addresses. If the “mode-based execute control for \nEPT” VM-execution control is 1, indicates whether instruction fetches are allowed from user-\nmode linear addresses in the 256-TByte region controlled by this entry. If that control is 0, \nthis bit is ignored.\n\n11 Ignored.\n\nM–1:12 Physical address of 4-KByte aligned EPT PML4 table referenced by this entry.\n\n51:M Reserved (must be 0).\n\n63:52 Ignored.\n\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E) (Continued)\n\nBit Position(s) Contents\n\n\n\n22 Document Number: 335252-001, Revision: 1.0\n\n\n\nDocument Number: 335252-001, Revision: 1.0 23\n\n5 Intel® Virtualization \nTechnology for Directed I/O\n\nIntel® Virtualization Technology for Directed I/O includes a feature called DMA \nremapping.\n\nDMA remapping provides hardware support for isolation of device accesses to memory. \nWhen a device attempts to access system memory, DMA-remapping hardware \nintercepts the access and utilizes paging structures to determine whether the access \ncan be permitted; it also determines the actual location to access.\n\nThe DMA-remapping hardware may support two levels of address translation. One level \nmay translate a linear address to a guest-physical address, while a second level may \nremap the guest-physical address to physical address.\n\nThe first-level translation uses paging structures with the same format as those used \nfor ordinary paging. The second-level translation uses paging structures with the same \nformat as those used for EPT.\n\nIt is expected that, on platforms that support wider linear and guest-physical addresses \n(using 5-level paging and 5-level EPT, respectively), the DMA-remapping hardware will \nbe similarly enhanced to support those wider addresses with 5-level translation \nprocesses.\n\nThis enhanced support for DMA remapping will be detailed in a future revision of the \nIntel® Virtualization Technology for Directed I/O Architecture Specification.\n\n\n\n24 Document Number: 335252-001, Revision: 1.0\n\n\n\t1 Introduction\n\t1.1 Existing Paging in IA-32e Mode\n\t1.2 Linear-Address Width and VMX Transitions\n\t1.3 Existing Extended Page Tables (EPT)\n\n\t2 Expanding Linear Addresses: 5-Level Paging\n\t2.1 5-Level Paging: Introduction\n\t2.2 Enumeration and Enabling\n\t2.2.1 Enumeration by CPUID\n\t2.2.2 Enabling by Software\n\n\t2.3 Linear-Address Generation and Canonicality\n\t2.4 5-Level Paging: Linear-Address Translation\n\t2.5 Linear-Address Registers and Canonicality\n\t2.5.1 Canonicality Checking on RIP Loads\n\t2.5.2 Canonicality Checking on Other Loads\n\n\t2.6 Interactions with TLB-Invalidation Instructions\n\t2.7 Interactions with Intel® MPX\n\t2.8 Interactions with Intel® SGX\n\n\t3 Linear-Address Expansion and VMX Transitions\n\t3.1 Linear-Address Expansion and VM Entries\n\t3.2 Linear-Address Expansion and VM Exits\n\n\t4 5-Level EPT\n\t4.1 4-Level EPT: Guest-Physical-Address Limit\n\t4.2 5-Level EPT: Enumeration and Enabling\n\t4.2.1 Enumeration\n\t4.2.2 Enabling by Software\n\n\t4.3 5-Level EPT: Guest-Physical-Address Translation\n\t4.4 5-Level EPT and EPTP Switching\n\n\t5 Intel® Virtualization Technology for Directed I/O\n\n', 'metadata_pdf:docinfo:creator': 'Intel Corporation', 'metadata_resourceName': "b'5-Level Paging and 5-Level EPT - Intel - Revision 1.0 (December, 2016).pdf'", 'metadata_pdf:docinfo:modified': '2016-12-02T22:28:56Z', 'metadata_pdf:docinfo:title': '5-Level Paging and 5-Level EPT white paper'}}, {'_index': 'mongo_index', '_id': 'Ff9p5IUB9nynXRNhVcwZ', '_score': 3.0906205, '_ignored': ['content.keyword', 'log_entry.keyword'], '_source': {'log_entry': '{"_id"=>BSON::ObjectId(\'63cffa2978b994746c729cec\'), "content"=>"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5-Level Paging and 5-Level EPT\\n\\n\\nDocument Number: 335252-002\\n\\n5-Level Paging and 5-Level EPT\\nWhite Paper\\n\\nRevision 1.1\\n\\nMay 2017\\n\\n\\n\\n2 Document Number: 335252-002, Revision: 1.1\\n\\nLegal Lines and DisclaimersIntel technologies’ features and benefits depend on system configuration and may require enabled hardware, software, or service \\nactivation. Learn more at intel.com, or from the OEM or retailer.\\nNo computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any \\ndamages resulting from such losses.\\nYou may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel \\nproducts described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted \\nwhich includes subject matter disclosed herein.\\nNo license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document.\\nThe products described may contain design defects or errors known as errata which may cause the product to deviate from \\npublished specifications. Current characterized errata are available on request.\\nThis document contains information on products, services and/or processes in development. All information provided here is \\nsubject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps.\\nIntel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for \\na particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or \\nusage in trade.\\nCopies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-\\n4725 or by visiting www.intel.com/design/literature.htm.\\nIntel, the Intel logo, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries.\\n*Other names and brands may be claimed as the property of others.\\nCopyright © 2016-2017, Intel Corporation. All Rights Reserved.\\n\\nNotice: This document contains information on products in the design phase of development. The information here is subject to \\nchange without notice. Do not finalize a design with this information.\\n\\nhttp://www.intel.com/design/literature.htm\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 3\\n\\nContents\\n\\n1 Introduction ..............................................................................................................8\\n1.1 Existing Paging in IA-32e Mode .............................................................................8\\n1.2 Linear-Address Width and VMX Transitions ........................................................... 10\\n1.3 Existing Extended Page Tables (EPT).................................................................... 11\\n\\n2 Expanding Linear Addresses: 5-Level Paging ........................................................... 12\\n2.1 5-Level Paging: Introduction............................................................................... 12\\n2.2 Enumeration and Enabling .................................................................................. 12\\n\\n2.2.1 Enumeration by CPUID............................................................................ 12\\n2.2.2 Enabling by Software .............................................................................. 13\\n\\n2.3 Linear-Address Generation and Canonicality.......................................................... 13\\n2.4 5-Level Paging: Linear-Address Translation........................................................... 14\\n2.5 Linear-Address Registers and Canonicality ............................................................ 15\\n\\n2.5.1 Canonicality Checking on RIP Loads .......................................................... 16\\n2.5.2 Canonicality Checking on Other Loads ....................................................... 17\\n\\n2.6 Interactions with TLB-Invalidation Instructions ...................................................... 18\\n2.7 Interactions with Intel® MPX .............................................................................. 19\\n2.8 Interactions with Intel® SGX .............................................................................. 20\\n\\n3 Linear-Address Expansion and VMX Transitions....................................................... 22\\n3.1 Linear-Address Expansion and VM Entries............................................................. 22\\n3.2 Linear-Address Expansion and VM Exits................................................................ 22\\n\\n4 5-Level EPT ............................................................................................................. 24\\n4.1 4-Level EPT: Guest-Physical-Address Limit............................................................ 24\\n4.2 5-Level EPT: Enumeration and Enabling ............................................................... 24\\n\\n4.2.1 Enumeration.......................................................................................... 24\\n4.2.2 Enabling by Software .............................................................................. 25\\n\\n4.3 5-Level EPT: Guest-Physical-Address Translation ................................................... 25\\n4.4 5-Level EPT and EPTP Switching .......................................................................... 26\\n\\n5 Intel® Virtualization Technology for Directed I/O ................................................... 27\\n\\nFigures\\n1-1 Linear-Address Translation Using IA-32e Paging ......................................................9\\n2-1 Linear-Address Translation Using 5-Level Paging ................................................... 16\\n\\nTables\\n2-1 Format of a PML5 Entry (PML5E) that References a PML4 Table ............................... 14\\n4-1 Format of an EPT PML5 Entry (EPT PML5E) ........................................................... 25\\n\\n\\n\\n4 Document Number: 335252-002, Revision: 1.1\\n\\nRevision History\\n\\nDocument \\nNumber\\n\\nRevision \\nNumber Description Date\\n\\n335252-001 1.0 • Initial Release November 2016\\n\\n335252-002 1.1 • Updates to chapter 2, section 2.5.2 “Canonicality Checking on Other \\nLoads”. May 2017\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 5\\n\\n2-1 Format of a PML5 Entry (PML5E) that References a PML4 Table ............................... 17\\n4-1 Format of an EPT PML5 Entry (EPT PML5E) ........................................................... 28\\n\\n\\n\\n6 Document Number: 335252-002, Revision: 1.1\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 7\\n\\n1-1 Linear-Address Translation Using IA-32e Paging .................................................... 12\\n2-1 Linear-Address Translation Using 5-Level Paging ................................................... 19\\n\\n\\n\\n8 Document Number: 335252-002, Revision: 1.1\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 9\\n\\n1 Introduction ............................................................................................................ 11\\n1.1 Existing Paging in IA-32e Mode ........................................................................... 11\\n1.2 Linear-Address Width and VMX Transitions ........................................................... 13\\n1.3 Existing Extended Page Tables (EPT).................................................................... 14\\n\\n2 Expanding Linear Addresses: 5-Level Paging ........................................................... 15\\n2.1 5-Level Paging: Introduction............................................................................... 15\\n2.2 Enumeration and Enabling .................................................................................. 15\\n\\n2.2.1 Enumeration by CPUID............................................................................ 15\\n2.2.2 Enabling by Software .............................................................................. 16\\n\\n2.3 Linear-Address Generation and Canonicality.......................................................... 16\\n2.4 5-Level Paging: Linear-Address Translation........................................................... 17\\n2.5 Linear-Address Registers and Canonicality ............................................................ 18\\n\\n2.5.1 Canonicality Checking on RIP Loads .......................................................... 19\\n2.5.2 Canonicality Checking on Other Loads ....................................................... 20\\n\\n2.6 Interactions with TLB-Invalidation Instructions ...................................................... 21\\n2.7 Interactions with Intel® MPX .............................................................................. 22\\n2.8 Interactions with Intel® SGX .............................................................................. 23\\n\\n3 Linear-Address Expansion and VMX Transitions....................................................... 25\\n3.1 Linear-Address Expansion and VM Entries............................................................. 25\\n3.2 Linear-Address Expansion and VM Exits................................................................ 25\\n\\n4 5-Level EPT ............................................................................................................. 27\\n4.1 4-Level EPT: Guest-Physical-Address Limit............................................................ 27\\n4.2 5-Level EPT: Enumeration and Enabling ............................................................... 27\\n\\n4.2.1 Enumeration.......................................................................................... 27\\n4.2.2 Enabling by Software .............................................................................. 28\\n\\n4.3 5-Level EPT: Guest-Physical-Address Translation ................................................... 28\\n4.4 5-Level EPT and EPTP Switching .......................................................................... 29\\n\\n5 Intel® Virtualization Technology for Directed I/O ................................................... 31\\n\\n\\n\\n10 Document Number: 335252-002, Revision: 1.1\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 11\\n\\n1 Introduction\\n\\nThis document describes planned extensions to the Intel 64 architecture to expand the \\nsize of addresses that can be translated through a processor’s memory-translation \\nhardware.\\n\\nModern operating systems use address-translation support called paging. Paging \\ntranslates linear addresses (also known as virtual addresses), which are used by \\nsoftware, to physical addresses, which are used to access memory (or memory-\\nmapped I/O). Section 1.1 describes the 64-bit paging hardware on Intel 64 processors. \\nExisting processors limit linear addresses to 48 bits. Chapter 2 describes paging \\nextensions that would relax that limit to 57 linear-address bits.\\n\\nVirtual-machine monitors (VMMs) use the virtual-machine extensions (VMX) to \\nsupport guest software operating in a virtual machine. VMX transitions are control-\\nflow transfers between the VMM and guest software. VMX transitions involve the \\nloading and storing of various processor registers. Some of these registers are defined \\nto contain linear addresses. Because of this, the operation of VMX transitions depends \\nin part on the linear-address width supported by the processor. Section 1.2 describes \\nthe existing treatment of linear-address registers by VMX transitions, while Chapter 3 \\ndescribes the changes required to support larger linear addresses.\\n\\nVMMs may also use additional address-translation support called extended page \\ntables (EPT). When EPT is used, paging produces guest-physical addresses, which \\nEPT translates to physical addresses. Section 1.3 describes the EPT hardware on \\nexisting Intel 64 processors, which limit guest-physical addresses to 48 bits. Chapter 4 \\ndescribes EPT extensions to support 57 guest-physical-address bits.\\n\\n1.1 Existing Paging in IA-32e Mode\\nOn processors supporting Intel 64 architecture, software typically references memory \\nusing linear addresses. Most modern operating systems configure processors to use \\npaging, which translates linear addresses to physical addresses. The processor uses \\nthe resulting physical addresses to access memory.\\n\\nIA-32e mode is a mode of processor execution that extends the older 32-bit \\noperation, known as legacy mode. Software can enter IA-32e mode with the following \\nalgorithm.\\n\\n1. Use the MOV CR instruction to set CR4.PAE[bit 5]. (Physical-address extension \\nmust be enabled to enter IA-32e mode.)\\n\\n2. Use the WRMSR instruction to set bit 8 (LME) of the IA32_EFER MSR (index \\nC0000080H).\\n\\n3. Use the MOV CR instruction to load CR3 with the address of a PML4 table (see \\nbelow).\\n\\n4. Use the MOV CR instruction to set CR0.PG[bit 31].\\n\\nA logical processor is in IA-32e mode whenever CR0.PG = 1 and IA32_EFER.LME = 1. \\nThis fact is reported in IA32_EFER.LMA[bit 10]. Software cannot set this bit directly; it \\nis always the logical-AND of CR0.PG and IA32_EFER.LME.\\n\\n\\n\\n12 Document Number: 335252-002, Revision: 1.1\\n\\nIn IA-32e mode, linear addresses are 64 bits in size.1 However, the corresponding \\npaging mode (currently called IA-32e paging) does not use all 64 linear-address bits.\\n\\nIA-32e paging does not use all 64 linear-address bits because processors limit the size \\nof linear addresses. This limit is enumerated by the CPUID instruction. Specifically, \\nCPUID.80000008H:EAX[bits 15:8] enumerates the number of linear-address bits (the \\nmaximum linear-address width) supported by the processor. Existing processors \\nenumerate this value as 48.\\n\\nNote: Processors also limit the size of physical addresses and enumerate the limit using \\nCPUID. CPUID.80000008H:EAX[bits 7:0] enumerates the number of physical-address \\nbits supported by the processor, the maximum physical-address width. Existing \\nprocessors have enumerated values up to 46. Software can use more than 32 physical-\\naddress bits only if physical-address extension has been enabled by setting \\nCR4.PAE, bit 5 of control register CR4.\\n\\nThe enumerated limitation on the linear-address width implies that paging translates \\nonly the low 48 bits of each 64-bit linear address. After a linear address is generated \\nbut before it is translated, the processor confirms that the address uses only the 48 bits \\nthat the processor supports.\\n\\nThe limitation to 48 linear-address bits results from the nature of IA-32e paging, which \\nis illustrated in Figure 1-1.\\n\\n1. IA-32e mode comprises two sub-modes: compatibility mode and 64-bit mode. In compatibility \\nmode, software uses 32-bit addresses, which the processor zero-extends to 64-bit linear \\naddresses. In 64-bit mode, software uses 64-bit addresses directly.\\n\\nFigure 1-1. Linear-Address Translation Using IA-32e Paging\\n\\nDirectory Ptr\\n\\nPTE\\n\\nLinear Address\\n\\nPage Table\\n\\nPDPTE\\n\\nCR3\\n\\n39 38\\n\\nPointer Table\\n\\n9\\n9\\n\\n40\\n\\n12\\n9\\n\\n40\\n\\n4-KByte Page\\n\\nOffset\\n\\nPhysical Addr\\n\\nPDE\\n\\nTable\\n011122021\\n\\nDirectory\\n30 29\\n\\nPage-Directory-\\n\\nPage-Directory\\n\\nPML4\\n47\\n\\n9\\n\\nPML4E\\n\\n40\\n\\n40\\n\\n40\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 13\\n\\nThe processor performs IA-32e paging by traversing a 4-level hierarchy of paging \\nstructures whose root structure resides at the physical address in control register \\nCR3. Each paging structure is 4-KBytes in size and comprises 512 8-byte entries. The \\nprocessor uses the upper 36 bits of a linear address (bits 47:12), 9 bits at a time, to \\nselect paging-structure entries from the hierarchy.\\n\\nNote: Figure 1-1 illustrates the translation of a linear address to a 4-KByte page. The paging \\nprocess can be configured so that the translation of some linear addresses stops one or \\ntwo levels earlier, translating instead to 2-MByte pages or 1-GByte pages.\\n\\nIn general, bits 51:12 of each paging-structure entry contain a 4-KByte aligned \\nphysical address. For each entry except the last, this address is that of the next paging \\nstructure; in the last entry, it is the physical address of a 4-KByte page frame. The \\nfinal physical address is obtained by combining this page-frame address with the page \\noffset, bits 11:0 of the original linear address.\\n\\nBecause only bits 47:0 of a linear address are used in address-translation, the \\nprocessor reserves bits 63:48 for future expansion using a concept known as \\ncanonicality. A linear address is canonical if bits 63:47 of the address are identical. \\n(Put differently, a linear address is canonical only if bits 63:48 are a sign-extension of \\nbit 47, which is the uppermost bit used in linear-address translation.)\\n\\nWhen a 64-bit linear address is generated to access memory, the processor first \\nconfirms that the address is canonical. If the address is not canonical, the memory \\naccess causes a fault, and the processor makes no attempt to translate the address.1\\n\\nIntel 64 architecture includes numerous registers that are defined to hold linear \\naddresses. These registers may be loaded using a variety of instructions. In most \\ncases, these instructions cause a general-protection exception (#GP) if an attempt is \\nmade to load one of these registers with a value that is not canonical.\\n\\nPhysical-address bits in a paging-structure entry beyond the enumerated physical-\\naddress width are reserved. A page-fault exception (#PF) results if an attempt is made \\nto access a linear address whose translation encounters a paging-structure entry that \\nsets any of those bits.\\n\\n1.2 Linear-Address Width and VMX Transitions\\nVM entries and VM exits manipulate numerous processor registers that contain linear \\naddresses. The transitions respect the processor’s linear-address width in a manner \\nbased on canonicality.\\n\\nCertain fields in the VMCS correspond to registers that contain linear addresses. \\nVM entries confirm that most of those fields contain values that are canonical. Some \\nregisters, such as RIP and the LDTR base address, receive special treatment.\\n\\nVM exits save into the VMCS the state of certain registers, some of which contain linear \\naddresses. Because the processor generally ensures that the values in these registers \\nare canonical (see Section 1.1), the values that VM exits save for these registers will \\ngenerally be canonical.\\n\\n1. In general, an attempt to access memory using a linear address that is not canonical causes a \\ngeneral-protection exception (#GP). A stack-fault exception — #SS — occurs instead if the \\nmemory access was made using the SS segment.\\n\\n\\n\\n14 Document Number: 335252-002, Revision: 1.1\\n\\nVM exits also load from the VMCS certain registers, some of which contain linear \\naddresses. Each VM exit ensures that the value of each of these registers is canonical. \\nSpecifically, bits 47:0 of the register are loaded from the field in the host-state area; \\nthe value of bit 47 is then sign-extended into bits 63:48 of the register.\\n\\n1.3 Existing Extended Page Tables (EPT)\\nMost Intel 64 processors supporting VMX also support an additional layer of address \\ntranslation called extended page tables (EPT).\\n\\nVM entry can be configured to activate EPT for guest software. When EPT is active, the \\naddresses used and produced by paging (Section 1.1) are not used as physical \\naddresses to reference in memory. Instead, the processor interprets them as guest-\\nphysical addresses, and translates them to physical addresses in a manner \\ndetermined by the VMM. (This translation from guest-physical to physical applies not \\nonly to the output of paging but also to the addresses that the processor uses to \\nreference the guest paging structures.)\\n\\nIf the EPT translation process cannot translate a guest-physical address, it causes an \\nEPT violation. (EPT violations may also occur when an access to a guest-physical \\naddress violates the permissions established by EPT for that guest-physical address.) \\nAn EPT violation is a VMX-specific exception, usually causing a VM exit.\\n\\nAs noted in Section 1.1, existing processors limit physical addresses to 46 bits. That \\nlimit applies also to guest-physical addresses. As a result, guest-physical addresses \\nthat set bits beyond this limit are not translated by EPT. (For example, a page fault \\nresults if linear-address translation encounters a paging-structure entry with such an \\naddress.) Because of this, existing EPT has been limited to translating only 48 guest-\\nphysical-address bits.\\n\\nThe existing EPT translation process is analogous to the paging process that was \\nillustrated earlier in Figure 1-1. Like 4-level paging, the processor implements EPT by \\ntraversing a 4-level hierarchy of 4-KByte EPT paging structures. The last EPT paging-\\nstructure entry contains the upper bits of the final physical address, while the lowest \\nbits come from the original guest-physical address.\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 15\\n\\n2 Expanding Linear Addresses: \\n5-Level Paging\\n\\n2.1 5-Level Paging: Introduction\\n5-level paging is a new paging mode that will be available in IA-32e mode. As its \\nname suggests, it will translate linear addresses by traversing a 5-level hierarchy of \\npaging structures. Because the process is otherwise unmodified, 5-level paging extends \\nthe processor’s linear-address width to 57 bits. (The additional 9 bits are used to select \\nan entry from the fifth level of the hierarchy.) For clarity, the paging mode formerly \\ncalled IA-32e paging will now be called 4-level paging.\\n\\nThe remainder of this chapter specifies the architectural changes that define and are \\nentailed by 5-level paging. Section 2.2 specifies how the CPU enumerates the new \\nfeature and how it is enabled by software. Section 2.3 describes changes to the process \\nof linear-address generation, as well as a revision to the concept of canonicality. \\nSection 2.4 details how 5-level paging translates linear addresses. Section 2.5 clarifies \\nhow the processor treats loads of registers containing linear addresses, while Section \\n2.6 to Section 2.8 consider interactions with various other features. (Interactions with \\nthe virtual-machine extensions are specified in Chapter 3.)\\n\\n2.2 Enumeration and Enabling\\nThis section describes how processors enumerate to software support for 5-level paging \\nand related features and also how software enables the processor to use that support.\\n\\n2.2.1 Enumeration by CPUID\\nProcessors supporting the Intel 64 architecture typically use the CPUID instruction to \\nenumerate to software specific processor functionality. Those processors that support \\n5-level paging enumerate that fact through a new feature flag as well as through \\nchanges in how related features are reported:\\n\\n• CPUID.(EAX=07H, ECX=0):ECX[bit 16] is a new feature flag that will enumerate \\nbasic support for 5-level paging. All older processors clear this bit. A processor will \\nset this bit if and only if it supports 5-level paging.\\n\\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 15:8] enumerates the \\nmaximum linear-address width supported by the processor. All older processors \\nthat support Intel 64 architecture enumerated this value as 48. Processors that \\nsupport 5-level paging will instead enumerate this value as 57.\\n\\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 7:0] enumerates the \\nmaximum physical-address width supported by the processor. Processors that \\nsupport Intel 64 architecture have enumerated at most 46 for this value. \\nProcessors that support 5-level paging are expected to enumerate higher values, \\nup to 52.\\n\\n• CPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17] is an existing field that \\nenumerates the user MPX address-width adjust (MAWAU). This value specifies the \\nnumber of linear-address bits above 48 on which the BNDLDX and BNDSTX \\ninstructions operate in 64-bit mode when CPL = 3.\\n\\n\\n\\n16 Document Number: 335252-002, Revision: 1.1\\n\\nOlder processors that support Intel® MPX enumerated 0 for this value. Processors \\nthat support 5-level paging may enumerate either 0 or 9, depending on \\nconfiguration by system software. See Section 2.7 for more details on how BNDLDX \\nand BNDSTX use MAWAU and how system software determines its value.\\n\\n• CPUID.(EAX=12H,ECX=0H):EDX[bits 15:8] is an existing field that enumerates \\ninformation that specifies the maximum supported size of a 64-bit enclave. If the \\nvalue enumerated is n, the maximum size is 2n. Older processors that support \\nIntel® SGX enumerated at most 47 for this value. Processors that support 5-level \\npaging are expected to enumerate this value as 56.\\n\\n2.2.2 Enabling by Software\\nSection 1.1 identified an algorithm by which software can enter IA-32e mode. On \\nprocessors that do not support 5-level paging, this algorithm enables 4-level paging. \\nOn processors that support 5-level paging, it can be adapted to enable 5-level paging \\ninstead.\\n\\nProcessors that support 5-level paging allow software to set a new enabling bit, \\nCR4.LA57[bit 12].1 A logical processor in IA-32e mode (IA32_EFER.LMA = 1) uses 5-\\nlevel paging if CR4.LA57 = 1. Outside of IA-32e mode (IA32_EFER.LMA = 0), the value \\nof CR4.LA57 does not affect paging operation.\\n\\nThe following items detail how a logical processor determines the current paging mode.\\n\\n• If CR0.PG = 0, paging is disabled.\\n\\n• If IA32_EFER.LMA = 0, one of the legacy 32-bit paging modes is used (depending \\non the value of legacy paging-mode bits in CR4).2\\n\\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 0, 4-level paging is used.\\n\\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 1, 5-level paging is used.\\n\\nSoftware can thus use the following algorithm to enter IA-32e mode with 5-level \\npaging.\\n\\n1. Use the MOV CR instruction to set CR4.PAE and CR4.LA57.\\n2. Use the WRMSR instruction to set IA32_EFER.LME.\\n\\n3. Use the MOV CR instruction to load CR3 with the address of a PML5 table (see \\nSection 2.4).\\n\\n4. Use the MOV CR instruction to set CR0.PG.\\n\\nThe processor allows software to modify CR4.LA57 only outside of IA-32e mode. In \\nIA-32e mode, an attempt to modify CR4.LA57 using the MOV CR instruction causes a \\ngeneral-protection exception (#GP).\\n\\n2.3 Linear-Address Generation and Canonicality\\nAs noted in Section 1.1, processors with a linear-address width of 48 bits reserve \\nlinear-address bits 63:48 for future expansion. Linear addresses that use only bits 47:0 \\n(because bits 63:48 are a sign-extension of bit 47) are called canonical.\\n\\n1. Software can set CR4.LA57 only if CPUID.(EAX=07H, ECX=0):ECX[bit 16] is enumerated as 1.\\n2. Recall that IA32_EFER.LMA is the logical-AND of CR0.PG and IA32_EFER.LME.\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 17\\n\\nWhen a 64-bit linear address is generated to access memory, the processor first \\nconfirms that the address is canonical. If the address is not canonical, the memory \\naccess causes a fault, and the address is not translated.\\n\\nProcessors that support 5-level paging can translate 57-bit linear addresses when 5-\\nlevel paging is enabled. But if software has enabled only 4-level paging, such a \\nprocessor can translate only 48-bit linear addresses. This fact motivates the definition \\nof two levels of canonicality.\\n\\nA linear address is 48-bit canonical if bits 63:47 of the address are identical. \\nSimilarly, an address is 57-bit canonical if bits 63:56 of the address are identical. Any \\nlinear address is that 48-bit canonical is also 57-bit canonical.\\n\\nWhen a 64-bit linear address is generated to access memory, a processor that supports \\n5-level paging checks for canonicality based on the current paging mode: if 4-level \\npaging is enabled, the address must be 48-bit canonical; if 5-level paging is enabled, \\nthe address need only be 57-bit canonical. If the appropriate canonicality is not \\nobserved, the memory access causes a fault.\\n\\n2.4 5-Level Paging: Linear-Address Translation\\nAs noted in Section 2.2.2, a logical processor uses 5-level paging if IA32_EFER.LMA = 1 \\nand CR4.LA57 = 1.\\n\\nLike 4-level paging, 5-level paging translates linear addresses using a hierarchy of in-\\nmemory paging structures. Because 5-level paging increases the linear-address width \\nto 57 bits (from the 48 bits supported by 4-level paging), 5-level paging allows up to \\n128 PBytes of linear-address space to be accessed at any given time.\\n\\nAlso like 4-level paging, 5-level paging uses CR3 to locate the first paging-structure in \\nthe hierarchy. (CR3 has the same mode-specific format with 5-level paging as it does \\nwith 4-level paging.) The following items describe in more detail the changes that 5-\\nlevel paging makes to the translation process.\\n\\n• Translation begins by identifying a 4-KByte naturally aligned PML5 table. It is \\nlocated at the physical address specified in bits 51:12 of CR3. A PML5 table \\ncomprises 512 64-bit entries (PML5Es). A PML5E is selected using the physical \\naddress defined as follows.\\n\\n— Bits 51:12 are from CR3.\\n— Bits 11:3 are bits 56:48 of the linear address.\\n— Bits 2:0 are all 0.\\n\\nBecause a PML5E is identified using bits 56:48 of the linear address, it controls \\naccess to a 256-TByte region of the linear-address space. The format of a PML5E is \\ngiven in Table 2-1.\\n\\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table\\n\\nBit Position(s) Contents\\n\\n0 (P) Present; must be 1 to reference a PML4 table.\\n\\n1 (R/W) Read/write; if 0, writes may not be allowed to the 256-TByte region controlled by this entry.\\n\\n2 (U/S) User/supervisor; if 0, user-mode accesses are not allowed to the 256-TByte region \\ncontrolled by this entry.\\n\\n3 (PWT) Page-level write-through; indirectly determines the memory type used to access the PML4 \\ntable referenced by this entry.\\n\\n\\n\\n18 Document Number: 335252-002, Revision: 1.1\\n\\n• The next step of the translation process identifies a 4-KByte naturally aligned PML4 \\ntable. It is located at the physical address specified in bits 51:12 of the PML5E (see \\nTable 2-1). A PML4 table comprises 512 64-bit entries (PML4Es). A PML4E is \\nselected using the physical address defined as follows.\\n\\n— Bits 51:12 are from the PML5E.\\n— Bits 11:3 are bits 47:39 of the linear address.\\n— Bits 2:0 are all 0.\\n\\nAs is normally the case when accessing a paging-structure entry, the memory type \\nused to access the PML4E is based in part on the PCD and PWT bits in the PML5E.\\n\\nBecause a PML4E is identified using bits 56:39 of the linear address, it controls \\naccess to a 512-GByte region of the linear-address space.\\n\\nOnce the PML4E is identified, bits 38:0 of the linear address determine the remainder \\nof the translation process exactly as is done for 4-level paging. As suggested in \\nTable 2-1, the values of bit 1, bit 2, and bit 63 of the PML5E are used normally (in \\ncombination with the corresponding bits in other paging-structure entries) to determine \\naccess rights. The accessed flag (bit 5) in the PML5E is updated as is done for other \\npaging-structure entries.\\n\\nThe operation of 5-level paging is illustrated in Figure 2-1.\\n\\n2.5 Linear-Address Registers and Canonicality\\nIntel 64 architecture includes numerous registers that are defined to hold linear \\naddresses. These registers may be loaded using a variety of instructions. As noted in \\nSection 1.1, each of these instructions typically causes a general-protection exception \\n(#GP) if an attempt is made to load a linear-address register with a value that is not \\ncanonical.\\n\\nAs noted in Section 2.3, processors that support 5-level paging use two definitions of \\ncanonicality: 48-bit canonicality and 57-bit canonicality. This section describes how \\nsuch a processor checks the canonicality of the values being loaded into the linear-\\naddress registers. One approach is used for operations that load RIP (the instruction \\npointer; see Section 2.5.1) and another is used for those that load other registers (see \\nSection 2.5.2).\\n\\n4 (PCD) Page-level cache disable; indirectly determines the memory type used to access the PML4 \\ntable referenced by this entry.\\n\\n5 (A) Accessed; indicates whether this entry has been used for linear-address translation.\\n\\n6 Ignored.\\n\\n7 (PS) Reserved (must be 0).\\n\\n11:8 Ignored.\\n\\nM–1:12 Physical address of 4-KByte aligned PML4 table referenced by this entry.\\n\\n51:M Reserved (must be 0).\\n\\n62:52 Ignored.\\n\\n63 If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not allowed from the \\n256-TByte region controlled by this entry); otherwise, reserved (must be 0).\\n\\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table (Continued)\\n\\nBit Position(s) Contents\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 19\\n\\n2.5.1 Canonicality Checking on RIP Loads\\nThe RIP register contains the offset of the current instruction pointer within the CS \\nsegment. Because the processor treats the CS base address as zero in 64-bit mode, the \\nvalue of the RIP register in that mode is the linear address of the instruction pointer.\\n\\nOperations that load RIP (including both instructions such as JMP as well as control \\ntransfers through the IDT) check first whether the value to be loaded is canonical \\nrelative to the current paging mode. If the processor determines that the address is not \\ncanonical, the RIP load is not performed and a general-protection exception (#GP) \\noccurs.\\n\\nNote: An instruction that would load RIP with a non-canonical address faults, meaning that \\nthe return instruction pointer of the fault handler is the address of the faulting \\ninstruction and not the non-canonical address whose load was attempted.\\n\\nThe canonicality checking performed by these operations uses 48-bit canonicality when \\n4-level paging is active. When 5-level paging is active, the checking is relaxed to \\nrequire only 57-bit canonicality.\\n\\nThe SYSCALL and SYSENTER instructions load RIP from the IA32_LSTAR and \\nIA32_SYSENTER_EIP MSRs, respectively. On processors that support only 4-level \\npaging, these instructions do not check that the values being loaded are canonical \\nbecause the WRMSR instruction ensures that each of these MSRs contains a value that \\nis 48-bit canonical. On processors that support 5-level paging, the checking by WRMSR \\nis relaxed to 57-bit canonicality (see Section 2.5.2). On such processors, an execution \\n\\nFigure 2-1. Linear-Address Translation Using 5-Level Paging\\n\\nPDE\\n\\nLinear Address\\n\\nPage Directory\\n\\nPML4E\\n\\nCR3\\n\\n39 38\\n\\n9 9\\n\\n40\\n\\n12\\n9\\n\\n40\\n\\n4-KByte Page\\n\\nOffset\\n\\nPhysical Addr\\n\\nPDPTE\\n\\n01112202130 29\\n\\nPage-Directory\\n\\n47\\n\\n9\\n\\nPML5E\\n\\n40\\n\\n40\\n\\n40\\n\\n56\\nTableDirectoryDirectory PtrPML4PML5\\n\\nPointer Table\\n\\nPage Table\\n\\nPTE\\n\\n9\\n\\n40\\n\\n\\n\\n20 Document Number: 335252-002, Revision: 1.1\\n\\nof SYSCALL or SYSENTER with 4-level paging checks that the value being loaded into \\nRIP is 48-bit canonical.1\\n\\nThe normal advancing of the instruction pointer to the next instruction boundary may \\nresult in the RIP register holding a non-canonical address. The fetch of the next \\ninstruction from that non-canonical address will result in a general-protection exception \\nas indicated in Section 2.3. In this case, the return instruction pointer of the fault \\nhandler will be that non-canonical address.\\n\\n2.5.2 Canonicality Checking on Other Loads\\nIn addition to RIP, the CPU maintains numerous other registers that hold linear \\naddresses:\\n\\n• GDTR and IDTR (in their base-address portions).\\n\\n• LDTR, TR, FS, and GS (in the base-address portions of their hidden descriptor \\ncaches).\\n\\n• The debug-address registers (DR0 through DR3), which hold the linear addresses \\nof breakpoints.\\n\\n• The following MSRs: IA32_BNDCFGS, IA32_DS_AREA, IA32_KERNEL_GS_BASE, \\nIA32_LSTAR, IA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \\nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \\nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, and \\nIA32_SYSENTER_ESP.\\n\\n• The x87 FPU instruction pointer (FIP).\\n\\n• The user-mode configuration register BNDCFGU, used by Intel® MPX.\\n\\nWith a few exceptions, the processor ensures that the addresses in these registers are \\nalways canonical in the following ways.\\n\\n• Some instructions fault on attempts to load a linear-address register with a non-\\ncanonical address:\\n\\n— An execution of the LGDT or LIDT instruction causes a general-protection \\nexception (#GP) if the base address specified in the instruction’s memory \\noperand is not canonical.\\n\\n— An execution of the LLDT or LTR instruction causes a #GP if the base address to \\nbe loaded from the GDT is not canonical.\\n\\n— An execution of WRMSR, WRFSBASE, or WRGSBASE causes a #GP if it would \\nload the base address of either FS or GS with a non-canonical address.\\n\\n— An execution of WRMSR causes a #GP if it would load any of the following MSRs \\nwith a non-canonical address: IA32_BNDCFGS, IA32_DS_AREA, \\nIA32_FS_BASE, IA32_GS_BASE, IA32_KERNEL_GS_BASE, IA32_LSTAR, \\nIA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \\nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \\nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, or \\nIA32_SYSENTER_ESP.2\\n\\n1. The SYSRET and SYSEXIT instructions, which complement SYSCALL and SYSENTER, load RIP \\nfrom RCX and RDX, respectively. Even before 5-level paging, these instructions checked the \\ncanonicality of the value to be loaded into RIP. As with other instructions that load RIP, this \\nchecking will be based on the current paging mode.\\n\\n2. Such canonicality checking may apply also when the WRMSR instruction is used to load some \\nnon-architectural MSRs (not listed here) that hold a linear address.\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 21\\n\\n— An execution of XRSTORS causes a #GP if it would load any of the following \\nMSRs with a non-canonical address: IA32_RTIT_ADDR0_A, \\nIA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, IA32_RTIT_ADDR1_B, \\nIA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, IA32_RTIT_ADDR3_A, and \\nIA32_RTIT_ADDR3_B.\\n\\nWith a small number of exceptions, this enforcement always uses the enumerated \\nmaximum linear-address width and is independent of the current paging mode. \\nThus, a processor that supports 5-level paging will allow the instructions mentioned \\nabove to load these registers with addresses that are 57-bit canonical but not 48-\\nbit canonical — even if 4-level paging is active. (As a result, instructions that \\nstore these values — SGDT, SIDT, SLDT, STR, RDFSBASE, RDGSBASE, RDMSR, \\nXSAVE, XSAVEC, XSAVEOPT, and XSAVES — may save addresses that are 57-bit \\ncanonical but not 48-bit canonical, even if 4-level paging is active.)\\n\\nThe WRFSBASE and WRGSBASE instructions, which load the base address of FS \\nand GS, respectively, operate differently. An execution of either of these \\ninstructions causes a #GP if it would load a base address with an address that is \\nnot canonical relative to the current paging mode. Thus, if 4-level paging is active, \\nthese instructions do not allow loading of addresses that are 57-bit canonical but \\nnot 48-bit canonical.\\n\\n• The FXRSTOR, XRSTOR, and XRSTORS instructions ignore attempts to load some of \\nthese registers with non-canonical addresses:\\n\\n— Loads of FIP ignore any bits in the memory image beyond the enumerated \\nmaximum linear-address width. The processor sign-extends to most significant \\nbit (e.g., bit 56 on processors that support 5-level paging) to ensure that FIP is \\nalways canonical.\\n\\n— Loads of BNDCFGU (by XRSTOR or XRSTORS) ignore any bits in the memory \\nimage beyond the enumerated maximum linear-address width. The processor \\nsign-extends to most significant bit (e.g., bit 56 on processors that support 5-\\nlevel paging) to ensure that BNDCFGU is always canonical.\\n\\n• Every non-control x87 instruction loads FIP. The value loaded is always canonical \\nrelative to the current paging mode: 48-bit canonical if 4-level paging is active, and \\n57-bit canonical if 5-level paging is active.\\n\\nDR0 through DR3 can be loaded with the MOV to DR instruction. The instruction allows \\nthose registers to be loaded with non-canonical addresses. The MOV from DR \\ninstruction will return the value last loaded with the MOV to DR instruction, even if the \\naddress is not canonical. Breakpoint address matching is supported only for canonical \\nlinear addresses.\\n\\n2.6 Interactions with TLB-Invalidation Instructions\\nIntel 64 architecture includes three instructions that may invalidate TLB entries for the \\nlinear address of an instruction operand: INVLPG, INVPCID, and INVVPID. The following \\nitems describe how they are affected by linear-address width.\\n\\n• The INVLPG instruction takes a memory operand. It invalidates any TLB entries \\nthat the logical processor is caching for the linear address of that operand for the \\ncurrent linear address space. The instruction does not fault if that address is not \\ncanonical relative to the current paging mode (e.g., is not 48-bit canonical when 4-\\nlevel paging is active). However, no invalidation is performed because the processor \\ndoes not cache TLB entries for addresses that are not canonical relative to the \\ncurrent paging mode.\\n\\n• The INVPCID instruction takes a register operand (INVPCID type) and a memory \\noperand (INVPCID descriptor). If the INVPCID type is 0, the instruction invalidates \\n\\n\\n\\n22 Document Number: 335252-002, Revision: 1.1\\n\\nany TLB entries that the logical processor is caching for the linear address and PCID \\nspecified in the INVPCID descriptor. If the linear address is not canonical relative \\nthe linear-address width supported by the processor, the instruction causes a \\ngeneral-protection exception (#GP). If the processor supports 5-level paging, the \\ninstruction will not cause such a #GP for an address that is 57-bit canonical, \\nregardless of paging mode, even if 4-level paging is active and the address is not \\n48-bit canonical.\\n\\n• The INVVPID instruction takes a register operand (INVVPID type) and a memory \\noperand (INVVPID descriptor). If the INVPCID type is 0, the instruction invalidates \\nany TLB entries that the logical processor is caching for the linear address and VPID \\nspecified in the INVVPID descriptor. If the linear address is not canonical relative \\nthe linear-address width supported by the processor, the instruction fails.1 If the \\nprocessor supports 5-level paging, the instruction will not fail for an address that is \\n57-bit canonical, regardless of paging mode, even if 4-level paging is active and the \\naddress is not 48-bit canonical.\\n\\n2.7 Interactions with Intel® MPX\\nThe Intel® Memory Protection Extensions (Intel® MPX) define a set of 4 bound \\nregisters, each of which software can associate with a specific pointer in memory. \\nIntel MPX includes two instructions — BNDLDX and BNDSTX — that allow software to \\nload from or store into memory the bounds associated with a particular pointer in \\nmemory.\\n\\nThe BNDLDX and BNDSTX instructions each take a bound register and a memory \\noperand (the associated pointer). Each of these parses the linear address of the \\nmemory operand to traverse a hierarchical data structure in memory. In 64-bit mode, \\nthese instructions do not necessarily use all the bits in the supplied 64-bit addresses. \\nThe number of bits used is 48 plus a value called the MPX address-width adjust \\n(MAWA).\\n\\nThe value of MAWA depends on CPL; the current paging mode (4-level paging or 5-level \\npaging); and, if 5-level paging is active, the value of a new MSR. Processors that \\nsupport both Intel MPX and 5-level paging support the IA32_MPX_LAX MSR (MSR index \\n1000H). Only bit 0 of the MSR is defined.\\n\\nIf CPL < 3, the supervisor MAWA (MAWAS) is used. The value of MAWAS is determined \\nby the setting of CR4.LA57. If CR4.LA57 = 0 (4-level paging is active; recall that MAWA \\nis relevant only in 64-bit mode), the value of MAWAS is 0. If CR4.LA57 = 1 (5-level \\npaging is active), the value of MAWAS is 9. The value of MAWAS is not enumerated by \\nthe CPUID instruction.\\n\\nIf CPL = 3, the user MAWA (MAWAU) is used. The value of MAWAU is determined as \\nfollows. If CR4.LA57 = 0 or IA32_MPX_LAX[bit 0] = 0, the value of MAWAU is 0. If \\nCR4.LA57 = 1 and IA32_MPX_LAX[bit 0] = 1, the value of MAWAU is 9. The current \\nvalue of MAWAU is enumerated in \\nCPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17].\\n\\nThe following items specify how an execution of the BNDLDX and BNDSTX instructions \\nin 64-bit mode parses a linear address to traverse a hierarchical data structure.\\n\\n1. INVVPID is a VMX instruction. In response to certain conditions, execution of a VMX may fail, \\nmeaning that it does not complete its normal operation. When a VMX instruction fails, control \\npasses to the next instruction (rather than to a fault handler) and a flag is set to report the \\nfailure.\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 23\\n\\n• A bound directory is located at the 4-KByte aligned linear address specified in \\nbits 63:12 of BNDCFGx.1 A BDE is selected using the LAp (linear address of pointer \\nto a buffer) to construct a 64-bit offset as follows:\\n\\n— bits 63:31+MAWA are 0;\\n— bits 30+MAWA:3 are LAp[bits 47+MAWA:20]; and\\n— bits 2:0 are 0.\\n\\nThe address of the BDE is the sum of the bound-directory base address (from \\nBNDCFGx) plus this 64-bit offset.\\n\\nIf either BNDLDX or BNDSTX is executed inside an enclave, the instruction operates \\nas if MAWAU = 0 (regardless of the values of CR4.LA57 and IA32_MPX_LAX[bit 0]).\\n\\n• The processor uses bits 63:3 of the BDE as the 8-byte aligned address of a bound \\ntable (BT). A BTE is selected using the LAp (linear address of pointer to a buffer) to \\nconstruct a 64-bit offset as follows:\\n\\n— bits 63:22 are 0;\\n— bits 21:5 are LAp[bits 19:3]; and\\n— bits 4:0 are 0.\\n\\nThe address of the BTE is the sum of the bound-table base address (from the BDE) \\nplus this 64-bit offset.\\n\\nA bound directory comprises 228+MAWA 64-bit entries (BDEs);2 thus, the size of a \\nbound directory in 64-bit mode is 21+MAWA GBytes. A bound table comprises 217 32-\\nbyte entries (BTEs); thus, the size of a bound table in 64-bit mode is 4 MBytes \\n(independent of MAWA).\\n\\n2.8 Interactions with Intel® SGX\\nIntel® Software Guard Extensions (Intel® SGX) define new processor functionality that \\nis implemented as SGX leaf functions within the ENCLS (supervisor) and ENCLU (user) \\ninstructions.\\n\\nThe SGX leaf functions include memory accesses using linear addresses normally. \\nWhen executed in 64-bit mode, the linear address are 64 bits in width and are subject \\nto the normal treatment of accesses to memory with 64-bit linear addresses (see \\nSection 2.3). In addition, some of the leaf functions apply specific architectural checks \\nrelated to linear-address width. The following items detail these checks and how they \\nare defined for processors that support 5-level paging.\\n\\n• The ECREATE leaf function of ENCLS creates a new enclave by creating a new SGX \\nenclave control structure (SECS). For a 64-bit enclave, the processor checks \\nwhether the enclave base linear address (specified in the SECS) is canonical, \\ngenerating a general-protection exception (#GP) if it is not. On processors that \\nsupport 5-level paging, this check is for 57-bit canonicality, regardless of the \\ncurrent paging mode.\\n\\nIn addition to checking the canonicality of the enclave base linear address, \\nECREATE confirms that the enclave size (specified in the SECS) is not greater than \\nthe maximum size supported by the processor (if the enclave size is too large, \\nECREATE generates a #GP). As noted in Section 2.2.1, older processors supported \\n\\n1. If CPL < 3, BNDCFGS is used; if CPL = 3, BNDCFGU is used.\\n2. A bound directory used in a 64-bit enclave always comprises 228 64-bit BDEs and thus has a size \\n\\nof 2 GBytes.\\n\\n\\n\\n24 Document Number: 335252-002, Revision: 1.1\\n\\n64-bit enclaves with sizes up to 247 bytes; processors that support 5-level paging \\nare expected to support enclaves with sizes up to 256 bytes.\\n\\nIf bits 4:3 of the enclave’s XSAVE feature request mask (XFRM) are set (indicating \\nthat Intel MPX will be enabled during execution of the enclave), ECREATE generates \\na #GP if the enclave’s size is greater than 248 bytes, even if the processor \\nenumerates support for larger enclaves.\\n\\n• The EENTER and ERESUME leaf functions of ENCLU transfer control flow to an entry \\npoint within a specified enclave. For entry to a 64-bit enclave, the processor checks \\nwhether certain linear addresses are canonical, generating a general-protection \\nexception (#GP) if any one is not. The following items detail these checks.\\n\\n— The linear address of the specified entry point must be canonical. If 4-level \\npaging is active, it must be 48-bit canonical; if 5-level paging is active, it must \\nbe 57-bit canonical.\\n\\n— The linear address of the asynchronous exit point (AEP — the address to which \\nthe processor transfers control on an asynchronous enclave exit) must be \\ncanonical. If 4-level paging is active, it must be 48-bit canonical; if 5-level \\npaging is active, it must be 57-bit canonical.\\n\\n— The enclave values for the base addresses of the FS and GS segments must be \\ncanonical. On processors that supports 5-level paging, these checks are for 57-\\nbit canonicality, regardless of the current paging mode.\\n\\n• The EEXIT leaf function exits the currently executing enclave and branches to a \\nspecified address. For an exit from a 64-bit enclave, the processor checks whether \\nthat target linear address is canonical, generating a general-protection exception \\n(#GP) if it is not. If 4-level paging is active, it must be 48-bit canonical; if 5-level \\npaging is active, it need only be 57-bit canonical.\\n\\nAs noted in Section 2.7, executions of BNDLDX and BNDSTX in a 64-bit enclave always \\noperate as if MAWAU = 0.\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 25\\n\\n3 Linear-Address Expansion and \\nVMX Transitions\\n\\nAs noted in Section 1.2, VM entries and VM exits manipulate numerous processor \\nregisters that contain linear addresses. The transitions respect the processor’s linear-\\naddress width in a manner based on canonicality.\\n\\nAs discussed in Chapter 2, processors that support 5-level paging expand the linear-\\naddress width from 48 bits to 57 bits. That expansion changes the operation of VMX \\ntransitions. Changes to VM entries are detailed in Section 3.1, while changes to \\nVM exits are given in Section 3.2.\\n\\n3.1 Linear-Address Expansion and VM Entries\\nCertain fields in the VMCS correspond to registers that contain linear addresses. \\nVM entries confirm those fields contain values that are canonical. This checking is \\nbased on the linear-address width supported by the processor (e.g., is based on 57-bit \\ncanonicality if the processor supports 5-level paging). The following are the fields to \\nwhich this applies.\\n\\n• In the host-state area:\\n\\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\\n\\n• In the guest-state area:\\n\\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\\n— The base-address field for LDTR (if LDTR will be usable).\\n— The field for the IA32_BNDCFGS MSR (if VM entry is loading that MSR).\\n\\nA VM entry to 64-bit mode also performs a check on the RIP field in the guest-state \\narea of the current VMCS. If the VM entry would result in 4-level paging, it checks that \\nbits 63:48 of the guest RIP field are identical; if it would result in 5-level paging, that \\ncheck is on bits 63:57.1\\n\\n3.2 Linear-Address Expansion and VM Exits\\nVM exits save the state of certain registers into the guest-state area of the VMCS. \\nSome of these registers contain linear addresses. As discussed in Section 1.1, the CPU \\ngenerally ensures that the values in these registers respect the CPU’s linear-address \\nwidth. As a result, the values the VM exits save for these registers will do the same.\\n\\n1. Note that these checks do not confirm that the guest RIP field is canonical relative to the paging \\nmode being entered. For example, bits 63:47 are identical in a 48-bit canonical address. However, \\nVM entry to 4-level paging may load RIP with a value in which bit 47 differs from that of \\nbits 63:48.\\n\\n\\n\\n26 Document Number: 335252-002, Revision: 1.1\\n\\nThere is a special case for LDTR base address. If LDTR was not usable at the time of a \\nVM exit, the value saved for the base address is undefined. However, this undefined \\nvalue is always 48-bit canonical on processors that do not support 5-level paging and is \\nalways 57-bit canonical on processors that do support 5-level paging.\\n\\nVM exits load the state of certain registers from the host-state area of the VMCS. Some \\nof these registers contain linear addresses. Each VM exit ensures that the value of each \\nof the following registers is canonical: the IA32_SYSENTER_EIP and \\nIA32_SYSENTER_ESP MSRs; and the base addresses for FS, GS, TR, GDTR, and IDTR. \\nHow this is done depends on whether the processor supports 5-level paging.\\n\\n• If the processor does not support 5-level paging, bits 47:0 of the register are \\nloaded from the field in the host-state area; the value of bit 47 is then sign-\\nextended into bits 63:48 of the register.\\n\\n• If the processor does support 5-level paging, bits 56:0 of the register are loaded \\nfrom the field in the host-state area; the value of bit 56 is then sign-extended into \\nbits 63:57 of the register.\\n\\nAgain, there is a special case for LDTR. LDTR is always unusable after a VM exit. Its \\nbase address may be loaded with an undefined value. This undefined value is always \\n48-bit canonical on processors that do not support 5-level paging and is always 57-bit \\ncanonical on processors that do support 5-level paging.\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 27\\n\\n4 5-Level EPT\\n\\n5-level EPT is a new mode for EPT. As its name suggests, it will translate guest-\\nphysical addresses by traversing a 5-level hierarchy of EPT paging structures. Because \\nthe process is otherwise unmodified, 5-level paging extends the processor’s guest-\\nphysical-address width to 57 bits. (The additional 9 bits are used to select an entry \\nfrom the fifth level of the hierarchy.) For clarity, the original EPT mode will now be \\ncalled 4-level EPT.\\n\\nThe remainder of this chapter specifies architectural changes to 4-level EPT as well as \\nthose that define and are entailed by 5-level EPT. Section 4.1 describes how the \\nexpansion of the guest-physical-address width affects 4-level EPT. Section 4.2 specifies \\nhow the CPU enumerates 5-level EPT and how the feature is enabled by software. \\nSection 4.3 details how 5-level EPT translates guest-physical addresses.\\n\\n4.1 4-Level EPT: Guest-Physical-Address Limit\\nAs explained in Section 1.3, 4-level EPT is limited to translating 48-bit guest-physical \\naddresses.\\n\\nThis is not a problem on existing processors, because they limit the physical-address \\nwidth to 46 bits (see Section 1.1). A processor’s physical-address width also limits \\nguest-physical addresses. That means that, on existing processors, any attempt to use \\na guest-physical address that sets a bit above the low 48 bits will cause a page-fault \\nexception (#PF).\\n\\nProcessors that support 5-level paging are expected to support 52 physical-address \\nbits. Such processors allow use of a guest-physical address that sets bits in the range \\n51:48; no #PF is generated.\\n\\nA guest-physical address that sets bits in the range 51:48 cannot be translated by 4-\\nlevel EPT. An attempt to access such an address when 4-level EPT is active causes an \\nEPT violation (see Section 1.3).\\n\\nEPT violations generate information about the exception in a value called the exit \\nqualification. In general, EPT violations caused by attempts to access a guest-physical \\naddress that is too wide establish the exit qualification as is currently done for other \\nEPT violations. Exceptions are made for bits 6:3 of the exit qualification, which report \\nthe access rights for the guest-physical address. The new EPT violations always clear \\nthese bits.\\n\\n4.2 5-Level EPT: Enumeration and Enabling\\nThis section describes how processors enumerate to software support for 5-level EPT \\nand how software enables the processor to use that support.\\n\\n4.2.1 Enumeration\\nProcessors supporting EPT enumerate details related to EPT in the \\nIA32_VMX_EPT_VPID_CAP MSR (index 48CH). Currently, \\nIA32_VMX_EPT_VPID_CAP[bit 6] enumerates support for 4-level EPT. Processors that \\nalso support 5-level EPT will enumerate that fact by also setting \\nIA32_VMX_EPT_VPID_CAP[bit 7].\\n\\n\\n\\n28 Document Number: 335252-002, Revision: 1.1\\n\\nThe guest-physical-address width supported by a processor is not enumerated using \\nthe IA32_VMX_EPT_VPID_CAP MSR. This is because that width is always the same as \\nthe processor’s maximum physical-address width as enumerated by \\nCPUID.80000008H:EAX[bits 7:0].\\n\\n4.2.2 Enabling by Software\\nA VMM enables EPT by setting the “enable EPT” VM-execution control in the current \\nVMCS before using the VMCS for VM entry.\\n\\nSpecific details of EPT operation are determined by the extended-page-table pointer \\nfield (EPTP) in the VMCS. In particular, EPTP[bits 5:3] contain a value that is 1 less than \\nthe number of levels used by the EPT. On existing processors, this value must be 3, \\nindicating 4-level EPT. (VM entry fails if a different value is used.) Processors that also \\nsupport 5-level EPT will also allow the value 4 (indicating 5-level EPT).\\n\\nIn summary, VM entry on a processor that supports 5-level check EPTP[bits 5:3]. If the \\nvalue is 3, the VM entry activates 4-level EPT. If the value is 4, the VM entry activates \\n5-level EPT. With any other value, VM entry fails.\\n\\n4.3 5-Level EPT: Guest-Physical-Address Translation\\nLike 4-level EPT, 5-level EPT translates guest-physical addresses using a hierarchy of \\nin-memory paging structures. Because 5-level EPT increases the guest-physical-\\naddress width to 57 bits (from the 48 bits supported by 4-level EPT), 5-level EPT allows \\nup to 128 PBytes of guest-physical-address space to be accessed at any given time.\\n\\nThe following items describe in more detail the changes that 5-level EPT makes to the \\ntranslation process.\\n\\n• Translation begins by identifying a 4-KByte naturally aligned EPT PML5 table. It is \\nlocated at the physical address specified in bits 51:12 of EPTP. An EPT PML5 table \\ncomprises 512 64-bit entries (EPT PML5Es). An EPT PML5E is selected using the \\nphysical address defined as follows.\\n\\n— Bits 63:52 are all 0.\\n— Bits 51:12 are from EPTP.\\n— Bits 11:3 are bits 56:48 of the guest-physical address.\\n— Bits 2:0 are all 0.\\n\\nBecause an EPT PML5E is identified using bits 56:48 of the guest-physical address, it \\ncontrols access to a 256-TByte region of the linear-address space. The format of an EPT \\nPML5E is given in Table 4-1.\\n\\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E)\\n\\nBit Position(s) Contents\\n\\n0 Read access; indicates whether reads are allowed from the 256-TByte region controlled by \\nthis entry.\\n\\n1 Write access; indicates whether writes are allowed from the 256-TByte region controlled by \\nthis entry.\\n\\n2 If the “mode-based execute control for EPT” VM-execution control is 0, execute access; \\nindicates whether instruction fetches are allowed from the 256-TByte region controlled by \\nthis entry.\\nIf that control is 1, execute access for supervisor-mode linear addresses; indicates whether \\ninstruction fetches are allowed from supervisor-mode linear addresses in the 256-TByte \\nregion controlled by this entry.\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 29\\n\\n• The next step of the translation process identifies a 4-KByte naturally aligned EPT \\nPML4 table. It is located at the physical address specified in bits 51:12 of the EPT \\nPML5E (see Table 4-1). An EPT PML4 table comprises 512 64-bit entries (EPT \\nPML4Es). An EPT PML4E is selected using the physical address defined as follows.\\n\\n— Bits 51:12 are from the EPT PML5E.\\n— Bits 11:3 are bits 47:39 of the guest-physical address.\\n— Bits 2:0 are all 0.\\n\\nBecause an EPT PML4E is identified using bits 56:39 of the guest-physical address, \\nit controls access to a 512-GByte region of the guest-physical-address space.\\n\\nOnce the EPT PML4E is identified, bits 38:0 of the guest-physical address determine the \\nremainder of the translation process exactly as is done for 4-level EPT. As suggested in \\nTable 4-1, the values of bits 2:0 and bit 10 of the EPT PML5E are used normally (in \\ncombination with the corresponding bits in other EPT paging-structure entries) to \\ndetermine whether EPT violations occur. The accessed flag (bit 8) in the EPT PML5E is \\nupdated as is done for other EPT paging-structure entries.\\n\\n4.4 5-Level EPT and EPTP Switching\\nThe value of EPTP may be modified in VMX non-root operation by invoking \\nVM function 0 (EPTP switching). This is done by executing the VMFUNC instruction with \\nvalue 0 in the EAX register. Invocation of VM function 0 loads EPTP with a value \\nselected from a data structure in memory.\\n\\nBefore loading EPTP in this way, the processor first confirms that the value to be loaded \\nis valid. The definition of a valid EPTP value depends on whether the processor supports \\n5-level EPT.\\n\\n• If the processor does not support 5-level EPT, an EPTP value in memory is \\nconsidered valid if it would not cause VM entry to fail (e.g., it does not set any \\nreserved bits).\\n\\n• If the processor does support 5-level EPT, an EPTP value in memory is considered \\nvalid only if it would not cause VM entry to fail (as above) and if its value in \\nbits 5:3 (which controls the number of EPT levels) is the same as that of the \\ncurrent value of EPTP.\\n\\nThe implication is that an invocation of VM function 0 cannot change the EPT mode \\nbetween 4-level EPT and 5-level EPT.\\n\\n7:3 Reserved (must be 0).\\n\\n8 If bit 6 of EPTP is 1, accessed flag for EPT; indicates whether software has accessed the \\n256-TByte region controlled by this entry. Ignored if bit 6 of EPTP is 0.\\n\\n9 Ignored.\\n\\n10 Execute access for user-mode linear addresses. If the “mode-based execute control for \\nEPT” VM-execution control is 1, indicates whether instruction fetches are allowed from user-\\nmode linear addresses in the 256-TByte region controlled by this entry. If that control is 0, \\nthis bit is ignored.\\n\\n11 Ignored.\\n\\nM–1:12 Physical address of 4-KByte aligned EPT PML4 table referenced by this entry.\\n\\n51:M Reserved (must be 0).\\n\\n63:52 Ignored.\\n\\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E) (Continued)\\n\\nBit Position(s) Contents\\n\\n\\n\\n30 Document Number: 335252-002, Revision: 1.1\\n\\n\\n\\nDocument Number: 335252-002, Revision: 1.1 31\\n\\n5 Intel® Virtualization \\nTechnology for Directed I/O\\n\\nIntel® Virtualization Technology for Directed I/O includes a feature called DMA \\nremapping.\\n\\nDMA remapping provides hardware support for isolation of device accesses to memory. \\nWhen a device attempts to access system memory, DMA-remapping hardware \\nintercepts the access and utilizes paging structures to determine whether the access \\ncan be permitted; it also determines the actual location to access.\\n\\nThe DMA-remapping hardware may support two levels of address translation. One level \\nmay translate a linear address to a guest-physical address, while a second level may \\nremap the guest-physical address to physical address.\\n\\nThe first-level translation uses paging structures with the same format as those used \\nfor ordinary paging. The second-level translation uses paging structures with the same \\nformat as those used for EPT.\\n\\nIt is expected that, on platforms that support wider linear and guest-physical addresses \\n(using 5-level paging and 5-level EPT, respectively), the DMA-remapping hardware will \\nbe similarly enhanced to support those wider addresses with 5-level translation \\nprocesses.\\n\\nThis enhanced support for DMA remapping will be detailed in a future revision of the \\nIntel® Virtualization Technology for Directed I/O Architecture Specification.\\n\\n\\n\\t1 Introduction\\n\\t1.1 Existing Paging in IA-32e Mode\\n\\t1.2 Linear-Address Width and VMX Transitions\\n\\t1.3 Existing Extended Page Tables (EPT)\\n\\n\\t2 Expanding Linear Addresses: 5-Level Paging\\n\\t2.1 5-Level Paging: Introduction\\n\\t2.2 Enumeration and Enabling\\n\\t2.2.1 Enumeration by CPUID\\n\\t2.2.2 Enabling by Software\\n\\n\\t2.3 Linear-Address Generation and Canonicality\\n\\t2.4 5-Level Paging: Linear-Address Translation\\n\\t2.5 Linear-Address Registers and Canonicality\\n\\t2.5.1 Canonicality Checking on RIP Loads\\n\\t2.5.2 Canonicality Checking on Other Loads\\n\\n\\t2.6 Interactions with TLB-Invalidation Instructions\\n\\t2.7 Interactions with Intel® MPX\\n\\t2.8 Interactions with Intel® SGX\\n\\n\\t3 Linear-Address Expansion and VMX Transitions\\n\\t3.1 Linear-Address Expansion and VM Entries\\n\\t3.2 Linear-Address Expansion and VM Exits\\n\\n\\t4 5-Level EPT\\n\\t4.1 4-Level EPT: Guest-Physical-Address Limit\\n\\t4.2 5-Level EPT: Enumeration and Enabling\\n\\t4.2.1 Enumeration\\n\\t4.2.2 Enabling by Software\\n\\n\\t4.3 5-Level EPT: Guest-Physical-Address Translation\\n\\t4.4 5-Level EPT and EPTP Switching\\n\\n\\t5 Intel® Virtualization Technology for Directed I/O\\n\\n", "metadata"=>{"pdf:docinfo:title"=>"5-Level Paging and 5-Level EPT", "pdf:docinfo:creator"=>"Intel Corporation", "resourceName"=>"b\'5-Level Paging and 5-Level EPT - Intel - Revision 1.1 (May, 2017).pdf\'", "pdf:docinfo:modified"=>"2018-04-02T21:22:31Z", "pdf:docinfo:created"=>"2017-05-25T08:58:09Z"}, "filename"=>"5-Level Paging and 5-Level EPT - Intel - Revision 1.1 (May, 2017).pdf"}', 'filename': '5-Level Paging and 5-Level EPT - Intel - Revision 1.1 (May, 2017).pdf', 'metadata_pdf:docinfo:created': '2017-05-25T08:58:09Z', 'mongo_id': '63cffa2978b994746c729cec', '@version': '1', 'host': 'bdvm', 'logdate': '2023-01-24T15:32:57+00:00', '@timestamp': '2023-01-24T15:32:58.708024966Z', 'content': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5-Level Paging and 5-Level EPT\n\n\nDocument Number: 335252-002\n\n5-Level Paging and 5-Level EPT\nWhite Paper\n\nRevision 1.1\n\nMay 2017\n\n\n\n2 Document Number: 335252-002, Revision: 1.1\n\nLegal Lines and DisclaimersIntel technologies’ features and benefits depend on system configuration and may require enabled hardware, software, or service \nactivation. Learn more at intel.com, or from the OEM or retailer.\nNo computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any \ndamages resulting from such losses.\nYou may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel \nproducts described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted \nwhich includes subject matter disclosed herein.\nNo license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document.\nThe products described may contain design defects or errors known as errata which may cause the product to deviate from \npublished specifications. Current characterized errata are available on request.\nThis document contains information on products, services and/or processes in development. All information provided here is \nsubject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps.\nIntel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for \na particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or \nusage in trade.\nCopies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-\n4725 or by visiting www.intel.com/design/literature.htm.\nIntel, the Intel logo, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries.\n*Other names and brands may be claimed as the property of others.\nCopyright © 2016-2017, Intel Corporation. All Rights Reserved.\n\nNotice: This document contains information on products in the design phase of development. The information here is subject to \nchange without notice. Do not finalize a design with this information.\n\nhttp://www.intel.com/design/literature.htm\n\n\nDocument Number: 335252-002, Revision: 1.1 3\n\nContents\n\n1 Introduction ..............................................................................................................8\n1.1 Existing Paging in IA-32e Mode .............................................................................8\n1.2 Linear-Address Width and VMX Transitions ........................................................... 10\n1.3 Existing Extended Page Tables (EPT).................................................................... 11\n\n2 Expanding Linear Addresses: 5-Level Paging ........................................................... 12\n2.1 5-Level Paging: Introduction............................................................................... 12\n2.2 Enumeration and Enabling .................................................................................. 12\n\n2.2.1 Enumeration by CPUID............................................................................ 12\n2.2.2 Enabling by Software .............................................................................. 13\n\n2.3 Linear-Address Generation and Canonicality.......................................................... 13\n2.4 5-Level Paging: Linear-Address Translation........................................................... 14\n2.5 Linear-Address Registers and Canonicality ............................................................ 15\n\n2.5.1 Canonicality Checking on RIP Loads .......................................................... 16\n2.5.2 Canonicality Checking on Other Loads ....................................................... 17\n\n2.6 Interactions with TLB-Invalidation Instructions ...................................................... 18\n2.7 Interactions with Intel® MPX .............................................................................. 19\n2.8 Interactions with Intel® SGX .............................................................................. 20\n\n3 Linear-Address Expansion and VMX Transitions....................................................... 22\n3.1 Linear-Address Expansion and VM Entries............................................................. 22\n3.2 Linear-Address Expansion and VM Exits................................................................ 22\n\n4 5-Level EPT ............................................................................................................. 24\n4.1 4-Level EPT: Guest-Physical-Address Limit............................................................ 24\n4.2 5-Level EPT: Enumeration and Enabling ............................................................... 24\n\n4.2.1 Enumeration.......................................................................................... 24\n4.2.2 Enabling by Software .............................................................................. 25\n\n4.3 5-Level EPT: Guest-Physical-Address Translation ................................................... 25\n4.4 5-Level EPT and EPTP Switching .......................................................................... 26\n\n5 Intel® Virtualization Technology for Directed I/O ................................................... 27\n\nFigures\n1-1 Linear-Address Translation Using IA-32e Paging ......................................................9\n2-1 Linear-Address Translation Using 5-Level Paging ................................................... 16\n\nTables\n2-1 Format of a PML5 Entry (PML5E) that References a PML4 Table ............................... 14\n4-1 Format of an EPT PML5 Entry (EPT PML5E) ........................................................... 25\n\n\n\n4 Document Number: 335252-002, Revision: 1.1\n\nRevision History\n\nDocument \nNumber\n\nRevision \nNumber Description Date\n\n335252-001 1.0 • Initial Release November 2016\n\n335252-002 1.1 • Updates to chapter 2, section 2.5.2 “Canonicality Checking on Other \nLoads”. May 2017\n\n\n\nDocument Number: 335252-002, Revision: 1.1 5\n\n2-1 Format of a PML5 Entry (PML5E) that References a PML4 Table ............................... 17\n4-1 Format of an EPT PML5 Entry (EPT PML5E) ........................................................... 28\n\n\n\n6 Document Number: 335252-002, Revision: 1.1\n\n\n\nDocument Number: 335252-002, Revision: 1.1 7\n\n1-1 Linear-Address Translation Using IA-32e Paging .................................................... 12\n2-1 Linear-Address Translation Using 5-Level Paging ................................................... 19\n\n\n\n8 Document Number: 335252-002, Revision: 1.1\n\n\n\nDocument Number: 335252-002, Revision: 1.1 9\n\n1 Introduction ............................................................................................................ 11\n1.1 Existing Paging in IA-32e Mode ........................................................................... 11\n1.2 Linear-Address Width and VMX Transitions ........................................................... 13\n1.3 Existing Extended Page Tables (EPT).................................................................... 14\n\n2 Expanding Linear Addresses: 5-Level Paging ........................................................... 15\n2.1 5-Level Paging: Introduction............................................................................... 15\n2.2 Enumeration and Enabling .................................................................................. 15\n\n2.2.1 Enumeration by CPUID............................................................................ 15\n2.2.2 Enabling by Software .............................................................................. 16\n\n2.3 Linear-Address Generation and Canonicality.......................................................... 16\n2.4 5-Level Paging: Linear-Address Translation........................................................... 17\n2.5 Linear-Address Registers and Canonicality ............................................................ 18\n\n2.5.1 Canonicality Checking on RIP Loads .......................................................... 19\n2.5.2 Canonicality Checking on Other Loads ....................................................... 20\n\n2.6 Interactions with TLB-Invalidation Instructions ...................................................... 21\n2.7 Interactions with Intel® MPX .............................................................................. 22\n2.8 Interactions with Intel® SGX .............................................................................. 23\n\n3 Linear-Address Expansion and VMX Transitions....................................................... 25\n3.1 Linear-Address Expansion and VM Entries............................................................. 25\n3.2 Linear-Address Expansion and VM Exits................................................................ 25\n\n4 5-Level EPT ............................................................................................................. 27\n4.1 4-Level EPT: Guest-Physical-Address Limit............................................................ 27\n4.2 5-Level EPT: Enumeration and Enabling ............................................................... 27\n\n4.2.1 Enumeration.......................................................................................... 27\n4.2.2 Enabling by Software .............................................................................. 28\n\n4.3 5-Level EPT: Guest-Physical-Address Translation ................................................... 28\n4.4 5-Level EPT and EPTP Switching .......................................................................... 29\n\n5 Intel® Virtualization Technology for Directed I/O ................................................... 31\n\n\n\n10 Document Number: 335252-002, Revision: 1.1\n\n\n\nDocument Number: 335252-002, Revision: 1.1 11\n\n1 Introduction\n\nThis document describes planned extensions to the Intel 64 architecture to expand the \nsize of addresses that can be translated through a processor’s memory-translation \nhardware.\n\nModern operating systems use address-translation support called paging. Paging \ntranslates linear addresses (also known as virtual addresses), which are used by \nsoftware, to physical addresses, which are used to access memory (or memory-\nmapped I/O). Section 1.1 describes the 64-bit paging hardware on Intel 64 processors. \nExisting processors limit linear addresses to 48 bits. Chapter 2 describes paging \nextensions that would relax that limit to 57 linear-address bits.\n\nVirtual-machine monitors (VMMs) use the virtual-machine extensions (VMX) to \nsupport guest software operating in a virtual machine. VMX transitions are control-\nflow transfers between the VMM and guest software. VMX transitions involve the \nloading and storing of various processor registers. Some of these registers are defined \nto contain linear addresses. Because of this, the operation of VMX transitions depends \nin part on the linear-address width supported by the processor. Section 1.2 describes \nthe existing treatment of linear-address registers by VMX transitions, while Chapter 3 \ndescribes the changes required to support larger linear addresses.\n\nVMMs may also use additional address-translation support called extended page \ntables (EPT). When EPT is used, paging produces guest-physical addresses, which \nEPT translates to physical addresses. Section 1.3 describes the EPT hardware on \nexisting Intel 64 processors, which limit guest-physical addresses to 48 bits. Chapter 4 \ndescribes EPT extensions to support 57 guest-physical-address bits.\n\n1.1 Existing Paging in IA-32e Mode\nOn processors supporting Intel 64 architecture, software typically references memory \nusing linear addresses. Most modern operating systems configure processors to use \npaging, which translates linear addresses to physical addresses. The processor uses \nthe resulting physical addresses to access memory.\n\nIA-32e mode is a mode of processor execution that extends the older 32-bit \noperation, known as legacy mode. Software can enter IA-32e mode with the following \nalgorithm.\n\n1. Use the MOV CR instruction to set CR4.PAE[bit 5]. (Physical-address extension \nmust be enabled to enter IA-32e mode.)\n\n2. Use the WRMSR instruction to set bit 8 (LME) of the IA32_EFER MSR (index \nC0000080H).\n\n3. Use the MOV CR instruction to load CR3 with the address of a PML4 table (see \nbelow).\n\n4. Use the MOV CR instruction to set CR0.PG[bit 31].\n\nA logical processor is in IA-32e mode whenever CR0.PG = 1 and IA32_EFER.LME = 1. \nThis fact is reported in IA32_EFER.LMA[bit 10]. Software cannot set this bit directly; it \nis always the logical-AND of CR0.PG and IA32_EFER.LME.\n\n\n\n12 Document Number: 335252-002, Revision: 1.1\n\nIn IA-32e mode, linear addresses are 64 bits in size.1 However, the corresponding \npaging mode (currently called IA-32e paging) does not use all 64 linear-address bits.\n\nIA-32e paging does not use all 64 linear-address bits because processors limit the size \nof linear addresses. This limit is enumerated by the CPUID instruction. Specifically, \nCPUID.80000008H:EAX[bits 15:8] enumerates the number of linear-address bits (the \nmaximum linear-address width) supported by the processor. Existing processors \nenumerate this value as 48.\n\nNote: Processors also limit the size of physical addresses and enumerate the limit using \nCPUID. CPUID.80000008H:EAX[bits 7:0] enumerates the number of physical-address \nbits supported by the processor, the maximum physical-address width. Existing \nprocessors have enumerated values up to 46. Software can use more than 32 physical-\naddress bits only if physical-address extension has been enabled by setting \nCR4.PAE, bit 5 of control register CR4.\n\nThe enumerated limitation on the linear-address width implies that paging translates \nonly the low 48 bits of each 64-bit linear address. After a linear address is generated \nbut before it is translated, the processor confirms that the address uses only the 48 bits \nthat the processor supports.\n\nThe limitation to 48 linear-address bits results from the nature of IA-32e paging, which \nis illustrated in Figure 1-1.\n\n1. IA-32e mode comprises two sub-modes: compatibility mode and 64-bit mode. In compatibility \nmode, software uses 32-bit addresses, which the processor zero-extends to 64-bit linear \naddresses. In 64-bit mode, software uses 64-bit addresses directly.\n\nFigure 1-1. Linear-Address Translation Using IA-32e Paging\n\nDirectory Ptr\n\nPTE\n\nLinear Address\n\nPage Table\n\nPDPTE\n\nCR3\n\n39 38\n\nPointer Table\n\n9\n9\n\n40\n\n12\n9\n\n40\n\n4-KByte Page\n\nOffset\n\nPhysical Addr\n\nPDE\n\nTable\n011122021\n\nDirectory\n30 29\n\nPage-Directory-\n\nPage-Directory\n\nPML4\n47\n\n9\n\nPML4E\n\n40\n\n40\n\n40\n\n\n\nDocument Number: 335252-002, Revision: 1.1 13\n\nThe processor performs IA-32e paging by traversing a 4-level hierarchy of paging \nstructures whose root structure resides at the physical address in control register \nCR3. Each paging structure is 4-KBytes in size and comprises 512 8-byte entries. The \nprocessor uses the upper 36 bits of a linear address (bits 47:12), 9 bits at a time, to \nselect paging-structure entries from the hierarchy.\n\nNote: Figure 1-1 illustrates the translation of a linear address to a 4-KByte page. The paging \nprocess can be configured so that the translation of some linear addresses stops one or \ntwo levels earlier, translating instead to 2-MByte pages or 1-GByte pages.\n\nIn general, bits 51:12 of each paging-structure entry contain a 4-KByte aligned \nphysical address. For each entry except the last, this address is that of the next paging \nstructure; in the last entry, it is the physical address of a 4-KByte page frame. The \nfinal physical address is obtained by combining this page-frame address with the page \noffset, bits 11:0 of the original linear address.\n\nBecause only bits 47:0 of a linear address are used in address-translation, the \nprocessor reserves bits 63:48 for future expansion using a concept known as \ncanonicality. A linear address is canonical if bits 63:47 of the address are identical. \n(Put differently, a linear address is canonical only if bits 63:48 are a sign-extension of \nbit 47, which is the uppermost bit used in linear-address translation.)\n\nWhen a 64-bit linear address is generated to access memory, the processor first \nconfirms that the address is canonical. If the address is not canonical, the memory \naccess causes a fault, and the processor makes no attempt to translate the address.1\n\nIntel 64 architecture includes numerous registers that are defined to hold linear \naddresses. These registers may be loaded using a variety of instructions. In most \ncases, these instructions cause a general-protection exception (#GP) if an attempt is \nmade to load one of these registers with a value that is not canonical.\n\nPhysical-address bits in a paging-structure entry beyond the enumerated physical-\naddress width are reserved. A page-fault exception (#PF) results if an attempt is made \nto access a linear address whose translation encounters a paging-structure entry that \nsets any of those bits.\n\n1.2 Linear-Address Width and VMX Transitions\nVM entries and VM exits manipulate numerous processor registers that contain linear \naddresses. The transitions respect the processor’s linear-address width in a manner \nbased on canonicality.\n\nCertain fields in the VMCS correspond to registers that contain linear addresses. \nVM entries confirm that most of those fields contain values that are canonical. Some \nregisters, such as RIP and the LDTR base address, receive special treatment.\n\nVM exits save into the VMCS the state of certain registers, some of which contain linear \naddresses. Because the processor generally ensures that the values in these registers \nare canonical (see Section 1.1), the values that VM exits save for these registers will \ngenerally be canonical.\n\n1. In general, an attempt to access memory using a linear address that is not canonical causes a \ngeneral-protection exception (#GP). A stack-fault exception — #SS — occurs instead if the \nmemory access was made using the SS segment.\n\n\n\n14 Document Number: 335252-002, Revision: 1.1\n\nVM exits also load from the VMCS certain registers, some of which contain linear \naddresses. Each VM exit ensures that the value of each of these registers is canonical. \nSpecifically, bits 47:0 of the register are loaded from the field in the host-state area; \nthe value of bit 47 is then sign-extended into bits 63:48 of the register.\n\n1.3 Existing Extended Page Tables (EPT)\nMost Intel 64 processors supporting VMX also support an additional layer of address \ntranslation called extended page tables (EPT).\n\nVM entry can be configured to activate EPT for guest software. When EPT is active, the \naddresses used and produced by paging (Section 1.1) are not used as physical \naddresses to reference in memory. Instead, the processor interprets them as guest-\nphysical addresses, and translates them to physical addresses in a manner \ndetermined by the VMM. (This translation from guest-physical to physical applies not \nonly to the output of paging but also to the addresses that the processor uses to \nreference the guest paging structures.)\n\nIf the EPT translation process cannot translate a guest-physical address, it causes an \nEPT violation. (EPT violations may also occur when an access to a guest-physical \naddress violates the permissions established by EPT for that guest-physical address.) \nAn EPT violation is a VMX-specific exception, usually causing a VM exit.\n\nAs noted in Section 1.1, existing processors limit physical addresses to 46 bits. That \nlimit applies also to guest-physical addresses. As a result, guest-physical addresses \nthat set bits beyond this limit are not translated by EPT. (For example, a page fault \nresults if linear-address translation encounters a paging-structure entry with such an \naddress.) Because of this, existing EPT has been limited to translating only 48 guest-\nphysical-address bits.\n\nThe existing EPT translation process is analogous to the paging process that was \nillustrated earlier in Figure 1-1. Like 4-level paging, the processor implements EPT by \ntraversing a 4-level hierarchy of 4-KByte EPT paging structures. The last EPT paging-\nstructure entry contains the upper bits of the final physical address, while the lowest \nbits come from the original guest-physical address.\n\n\n\nDocument Number: 335252-002, Revision: 1.1 15\n\n2 Expanding Linear Addresses: \n5-Level Paging\n\n2.1 5-Level Paging: Introduction\n5-level paging is a new paging mode that will be available in IA-32e mode. As its \nname suggests, it will translate linear addresses by traversing a 5-level hierarchy of \npaging structures. Because the process is otherwise unmodified, 5-level paging extends \nthe processor’s linear-address width to 57 bits. (The additional 9 bits are used to select \nan entry from the fifth level of the hierarchy.) For clarity, the paging mode formerly \ncalled IA-32e paging will now be called 4-level paging.\n\nThe remainder of this chapter specifies the architectural changes that define and are \nentailed by 5-level paging. Section 2.2 specifies how the CPU enumerates the new \nfeature and how it is enabled by software. Section 2.3 describes changes to the process \nof linear-address generation, as well as a revision to the concept of canonicality. \nSection 2.4 details how 5-level paging translates linear addresses. Section 2.5 clarifies \nhow the processor treats loads of registers containing linear addresses, while Section \n2.6 to Section 2.8 consider interactions with various other features. (Interactions with \nthe virtual-machine extensions are specified in Chapter 3.)\n\n2.2 Enumeration and Enabling\nThis section describes how processors enumerate to software support for 5-level paging \nand related features and also how software enables the processor to use that support.\n\n2.2.1 Enumeration by CPUID\nProcessors supporting the Intel 64 architecture typically use the CPUID instruction to \nenumerate to software specific processor functionality. Those processors that support \n5-level paging enumerate that fact through a new feature flag as well as through \nchanges in how related features are reported:\n\n• CPUID.(EAX=07H, ECX=0):ECX[bit 16] is a new feature flag that will enumerate \nbasic support for 5-level paging. All older processors clear this bit. A processor will \nset this bit if and only if it supports 5-level paging.\n\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 15:8] enumerates the \nmaximum linear-address width supported by the processor. All older processors \nthat support Intel 64 architecture enumerated this value as 48. Processors that \nsupport 5-level paging will instead enumerate this value as 57.\n\n• As noted in Section 1.1, CPUID.80000008H:EAX[bits 7:0] enumerates the \nmaximum physical-address width supported by the processor. Processors that \nsupport Intel 64 architecture have enumerated at most 46 for this value. \nProcessors that support 5-level paging are expected to enumerate higher values, \nup to 52.\n\n• CPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17] is an existing field that \nenumerates the user MPX address-width adjust (MAWAU). This value specifies the \nnumber of linear-address bits above 48 on which the BNDLDX and BNDSTX \ninstructions operate in 64-bit mode when CPL = 3.\n\n\n\n16 Document Number: 335252-002, Revision: 1.1\n\nOlder processors that support Intel® MPX enumerated 0 for this value. Processors \nthat support 5-level paging may enumerate either 0 or 9, depending on \nconfiguration by system software. See Section 2.7 for more details on how BNDLDX \nand BNDSTX use MAWAU and how system software determines its value.\n\n• CPUID.(EAX=12H,ECX=0H):EDX[bits 15:8] is an existing field that enumerates \ninformation that specifies the maximum supported size of a 64-bit enclave. If the \nvalue enumerated is n, the maximum size is 2n. Older processors that support \nIntel® SGX enumerated at most 47 for this value. Processors that support 5-level \npaging are expected to enumerate this value as 56.\n\n2.2.2 Enabling by Software\nSection 1.1 identified an algorithm by which software can enter IA-32e mode. On \nprocessors that do not support 5-level paging, this algorithm enables 4-level paging. \nOn processors that support 5-level paging, it can be adapted to enable 5-level paging \ninstead.\n\nProcessors that support 5-level paging allow software to set a new enabling bit, \nCR4.LA57[bit 12].1 A logical processor in IA-32e mode (IA32_EFER.LMA = 1) uses 5-\nlevel paging if CR4.LA57 = 1. Outside of IA-32e mode (IA32_EFER.LMA = 0), the value \nof CR4.LA57 does not affect paging operation.\n\nThe following items detail how a logical processor determines the current paging mode.\n\n• If CR0.PG = 0, paging is disabled.\n\n• If IA32_EFER.LMA = 0, one of the legacy 32-bit paging modes is used (depending \non the value of legacy paging-mode bits in CR4).2\n\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 0, 4-level paging is used.\n\n• If IA32_EFER.LMA = 1 and CR4.LA57 = 1, 5-level paging is used.\n\nSoftware can thus use the following algorithm to enter IA-32e mode with 5-level \npaging.\n\n1. Use the MOV CR instruction to set CR4.PAE and CR4.LA57.\n2. Use the WRMSR instruction to set IA32_EFER.LME.\n\n3. Use the MOV CR instruction to load CR3 with the address of a PML5 table (see \nSection 2.4).\n\n4. Use the MOV CR instruction to set CR0.PG.\n\nThe processor allows software to modify CR4.LA57 only outside of IA-32e mode. In \nIA-32e mode, an attempt to modify CR4.LA57 using the MOV CR instruction causes a \ngeneral-protection exception (#GP).\n\n2.3 Linear-Address Generation and Canonicality\nAs noted in Section 1.1, processors with a linear-address width of 48 bits reserve \nlinear-address bits 63:48 for future expansion. Linear addresses that use only bits 47:0 \n(because bits 63:48 are a sign-extension of bit 47) are called canonical.\n\n1. Software can set CR4.LA57 only if CPUID.(EAX=07H, ECX=0):ECX[bit 16] is enumerated as 1.\n2. Recall that IA32_EFER.LMA is the logical-AND of CR0.PG and IA32_EFER.LME.\n\n\n\nDocument Number: 335252-002, Revision: 1.1 17\n\nWhen a 64-bit linear address is generated to access memory, the processor first \nconfirms that the address is canonical. If the address is not canonical, the memory \naccess causes a fault, and the address is not translated.\n\nProcessors that support 5-level paging can translate 57-bit linear addresses when 5-\nlevel paging is enabled. But if software has enabled only 4-level paging, such a \nprocessor can translate only 48-bit linear addresses. This fact motivates the definition \nof two levels of canonicality.\n\nA linear address is 48-bit canonical if bits 63:47 of the address are identical. \nSimilarly, an address is 57-bit canonical if bits 63:56 of the address are identical. Any \nlinear address is that 48-bit canonical is also 57-bit canonical.\n\nWhen a 64-bit linear address is generated to access memory, a processor that supports \n5-level paging checks for canonicality based on the current paging mode: if 4-level \npaging is enabled, the address must be 48-bit canonical; if 5-level paging is enabled, \nthe address need only be 57-bit canonical. If the appropriate canonicality is not \nobserved, the memory access causes a fault.\n\n2.4 5-Level Paging: Linear-Address Translation\nAs noted in Section 2.2.2, a logical processor uses 5-level paging if IA32_EFER.LMA = 1 \nand CR4.LA57 = 1.\n\nLike 4-level paging, 5-level paging translates linear addresses using a hierarchy of in-\nmemory paging structures. Because 5-level paging increases the linear-address width \nto 57 bits (from the 48 bits supported by 4-level paging), 5-level paging allows up to \n128 PBytes of linear-address space to be accessed at any given time.\n\nAlso like 4-level paging, 5-level paging uses CR3 to locate the first paging-structure in \nthe hierarchy. (CR3 has the same mode-specific format with 5-level paging as it does \nwith 4-level paging.) The following items describe in more detail the changes that 5-\nlevel paging makes to the translation process.\n\n• Translation begins by identifying a 4-KByte naturally aligned PML5 table. It is \nlocated at the physical address specified in bits 51:12 of CR3. A PML5 table \ncomprises 512 64-bit entries (PML5Es). A PML5E is selected using the physical \naddress defined as follows.\n\n— Bits 51:12 are from CR3.\n— Bits 11:3 are bits 56:48 of the linear address.\n— Bits 2:0 are all 0.\n\nBecause a PML5E is identified using bits 56:48 of the linear address, it controls \naccess to a 256-TByte region of the linear-address space. The format of a PML5E is \ngiven in Table 2-1.\n\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table\n\nBit Position(s) Contents\n\n0 (P) Present; must be 1 to reference a PML4 table.\n\n1 (R/W) Read/write; if 0, writes may not be allowed to the 256-TByte region controlled by this entry.\n\n2 (U/S) User/supervisor; if 0, user-mode accesses are not allowed to the 256-TByte region \ncontrolled by this entry.\n\n3 (PWT) Page-level write-through; indirectly determines the memory type used to access the PML4 \ntable referenced by this entry.\n\n\n\n18 Document Number: 335252-002, Revision: 1.1\n\n• The next step of the translation process identifies a 4-KByte naturally aligned PML4 \ntable. It is located at the physical address specified in bits 51:12 of the PML5E (see \nTable 2-1). A PML4 table comprises 512 64-bit entries (PML4Es). A PML4E is \nselected using the physical address defined as follows.\n\n— Bits 51:12 are from the PML5E.\n— Bits 11:3 are bits 47:39 of the linear address.\n— Bits 2:0 are all 0.\n\nAs is normally the case when accessing a paging-structure entry, the memory type \nused to access the PML4E is based in part on the PCD and PWT bits in the PML5E.\n\nBecause a PML4E is identified using bits 56:39 of the linear address, it controls \naccess to a 512-GByte region of the linear-address space.\n\nOnce the PML4E is identified, bits 38:0 of the linear address determine the remainder \nof the translation process exactly as is done for 4-level paging. As suggested in \nTable 2-1, the values of bit 1, bit 2, and bit 63 of the PML5E are used normally (in \ncombination with the corresponding bits in other paging-structure entries) to determine \naccess rights. The accessed flag (bit 5) in the PML5E is updated as is done for other \npaging-structure entries.\n\nThe operation of 5-level paging is illustrated in Figure 2-1.\n\n2.5 Linear-Address Registers and Canonicality\nIntel 64 architecture includes numerous registers that are defined to hold linear \naddresses. These registers may be loaded using a variety of instructions. As noted in \nSection 1.1, each of these instructions typically causes a general-protection exception \n(#GP) if an attempt is made to load a linear-address register with a value that is not \ncanonical.\n\nAs noted in Section 2.3, processors that support 5-level paging use two definitions of \ncanonicality: 48-bit canonicality and 57-bit canonicality. This section describes how \nsuch a processor checks the canonicality of the values being loaded into the linear-\naddress registers. One approach is used for operations that load RIP (the instruction \npointer; see Section 2.5.1) and another is used for those that load other registers (see \nSection 2.5.2).\n\n4 (PCD) Page-level cache disable; indirectly determines the memory type used to access the PML4 \ntable referenced by this entry.\n\n5 (A) Accessed; indicates whether this entry has been used for linear-address translation.\n\n6 Ignored.\n\n7 (PS) Reserved (must be 0).\n\n11:8 Ignored.\n\nM–1:12 Physical address of 4-KByte aligned PML4 table referenced by this entry.\n\n51:M Reserved (must be 0).\n\n62:52 Ignored.\n\n63 If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not allowed from the \n256-TByte region controlled by this entry); otherwise, reserved (must be 0).\n\nTable 2-1. Format of a PML5 Entry (PML5E) that References a PML4 Table (Continued)\n\nBit Position(s) Contents\n\n\n\nDocument Number: 335252-002, Revision: 1.1 19\n\n2.5.1 Canonicality Checking on RIP Loads\nThe RIP register contains the offset of the current instruction pointer within the CS \nsegment. Because the processor treats the CS base address as zero in 64-bit mode, the \nvalue of the RIP register in that mode is the linear address of the instruction pointer.\n\nOperations that load RIP (including both instructions such as JMP as well as control \ntransfers through the IDT) check first whether the value to be loaded is canonical \nrelative to the current paging mode. If the processor determines that the address is not \ncanonical, the RIP load is not performed and a general-protection exception (#GP) \noccurs.\n\nNote: An instruction that would load RIP with a non-canonical address faults, meaning that \nthe return instruction pointer of the fault handler is the address of the faulting \ninstruction and not the non-canonical address whose load was attempted.\n\nThe canonicality checking performed by these operations uses 48-bit canonicality when \n4-level paging is active. When 5-level paging is active, the checking is relaxed to \nrequire only 57-bit canonicality.\n\nThe SYSCALL and SYSENTER instructions load RIP from the IA32_LSTAR and \nIA32_SYSENTER_EIP MSRs, respectively. On processors that support only 4-level \npaging, these instructions do not check that the values being loaded are canonical \nbecause the WRMSR instruction ensures that each of these MSRs contains a value that \nis 48-bit canonical. On processors that support 5-level paging, the checking by WRMSR \nis relaxed to 57-bit canonicality (see Section 2.5.2). On such processors, an execution \n\nFigure 2-1. Linear-Address Translation Using 5-Level Paging\n\nPDE\n\nLinear Address\n\nPage Directory\n\nPML4E\n\nCR3\n\n39 38\n\n9 9\n\n40\n\n12\n9\n\n40\n\n4-KByte Page\n\nOffset\n\nPhysical Addr\n\nPDPTE\n\n01112202130 29\n\nPage-Directory\n\n47\n\n9\n\nPML5E\n\n40\n\n40\n\n40\n\n56\nTableDirectoryDirectory PtrPML4PML5\n\nPointer Table\n\nPage Table\n\nPTE\n\n9\n\n40\n\n\n\n20 Document Number: 335252-002, Revision: 1.1\n\nof SYSCALL or SYSENTER with 4-level paging checks that the value being loaded into \nRIP is 48-bit canonical.1\n\nThe normal advancing of the instruction pointer to the next instruction boundary may \nresult in the RIP register holding a non-canonical address. The fetch of the next \ninstruction from that non-canonical address will result in a general-protection exception \nas indicated in Section 2.3. In this case, the return instruction pointer of the fault \nhandler will be that non-canonical address.\n\n2.5.2 Canonicality Checking on Other Loads\nIn addition to RIP, the CPU maintains numerous other registers that hold linear \naddresses:\n\n• GDTR and IDTR (in their base-address portions).\n\n• LDTR, TR, FS, and GS (in the base-address portions of their hidden descriptor \ncaches).\n\n• The debug-address registers (DR0 through DR3), which hold the linear addresses \nof breakpoints.\n\n• The following MSRs: IA32_BNDCFGS, IA32_DS_AREA, IA32_KERNEL_GS_BASE, \nIA32_LSTAR, IA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, and \nIA32_SYSENTER_ESP.\n\n• The x87 FPU instruction pointer (FIP).\n\n• The user-mode configuration register BNDCFGU, used by Intel® MPX.\n\nWith a few exceptions, the processor ensures that the addresses in these registers are \nalways canonical in the following ways.\n\n• Some instructions fault on attempts to load a linear-address register with a non-\ncanonical address:\n\n— An execution of the LGDT or LIDT instruction causes a general-protection \nexception (#GP) if the base address specified in the instruction’s memory \noperand is not canonical.\n\n— An execution of the LLDT or LTR instruction causes a #GP if the base address to \nbe loaded from the GDT is not canonical.\n\n— An execution of WRMSR, WRFSBASE, or WRGSBASE causes a #GP if it would \nload the base address of either FS or GS with a non-canonical address.\n\n— An execution of WRMSR causes a #GP if it would load any of the following MSRs \nwith a non-canonical address: IA32_BNDCFGS, IA32_DS_AREA, \nIA32_FS_BASE, IA32_GS_BASE, IA32_KERNEL_GS_BASE, IA32_LSTAR, \nIA32_RTIT_ADDR0_A, IA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, \nIA32_RTIT_ADDR1_B, IA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, \nIA32_RTIT_ADDR3_A, IA32_RTIT_ADDR3_B, IA32_SYSENTER_EIP, or \nIA32_SYSENTER_ESP.2\n\n1. The SYSRET and SYSEXIT instructions, which complement SYSCALL and SYSENTER, load RIP \nfrom RCX and RDX, respectively. Even before 5-level paging, these instructions checked the \ncanonicality of the value to be loaded into RIP. As with other instructions that load RIP, this \nchecking will be based on the current paging mode.\n\n2. Such canonicality checking may apply also when the WRMSR instruction is used to load some \nnon-architectural MSRs (not listed here) that hold a linear address.\n\n\n\nDocument Number: 335252-002, Revision: 1.1 21\n\n— An execution of XRSTORS causes a #GP if it would load any of the following \nMSRs with a non-canonical address: IA32_RTIT_ADDR0_A, \nIA32_RTIT_ADDR0_B, IA32_RTIT_ADDR1_A, IA32_RTIT_ADDR1_B, \nIA32_RTIT_ADDR2_A, IA32_RTIT_ADDR2_B, IA32_RTIT_ADDR3_A, and \nIA32_RTIT_ADDR3_B.\n\nWith a small number of exceptions, this enforcement always uses the enumerated \nmaximum linear-address width and is independent of the current paging mode. \nThus, a processor that supports 5-level paging will allow the instructions mentioned \nabove to load these registers with addresses that are 57-bit canonical but not 48-\nbit canonical — even if 4-level paging is active. (As a result, instructions that \nstore these values — SGDT, SIDT, SLDT, STR, RDFSBASE, RDGSBASE, RDMSR, \nXSAVE, XSAVEC, XSAVEOPT, and XSAVES — may save addresses that are 57-bit \ncanonical but not 48-bit canonical, even if 4-level paging is active.)\n\nThe WRFSBASE and WRGSBASE instructions, which load the base address of FS \nand GS, respectively, operate differently. An execution of either of these \ninstructions causes a #GP if it would load a base address with an address that is \nnot canonical relative to the current paging mode. Thus, if 4-level paging is active, \nthese instructions do not allow loading of addresses that are 57-bit canonical but \nnot 48-bit canonical.\n\n• The FXRSTOR, XRSTOR, and XRSTORS instructions ignore attempts to load some of \nthese registers with non-canonical addresses:\n\n— Loads of FIP ignore any bits in the memory image beyond the enumerated \nmaximum linear-address width. The processor sign-extends to most significant \nbit (e.g., bit 56 on processors that support 5-level paging) to ensure that FIP is \nalways canonical.\n\n— Loads of BNDCFGU (by XRSTOR or XRSTORS) ignore any bits in the memory \nimage beyond the enumerated maximum linear-address width. The processor \nsign-extends to most significant bit (e.g., bit 56 on processors that support 5-\nlevel paging) to ensure that BNDCFGU is always canonical.\n\n• Every non-control x87 instruction loads FIP. The value loaded is always canonical \nrelative to the current paging mode: 48-bit canonical if 4-level paging is active, and \n57-bit canonical if 5-level paging is active.\n\nDR0 through DR3 can be loaded with the MOV to DR instruction. The instruction allows \nthose registers to be loaded with non-canonical addresses. The MOV from DR \ninstruction will return the value last loaded with the MOV to DR instruction, even if the \naddress is not canonical. Breakpoint address matching is supported only for canonical \nlinear addresses.\n\n2.6 Interactions with TLB-Invalidation Instructions\nIntel 64 architecture includes three instructions that may invalidate TLB entries for the \nlinear address of an instruction operand: INVLPG, INVPCID, and INVVPID. The following \nitems describe how they are affected by linear-address width.\n\n• The INVLPG instruction takes a memory operand. It invalidates any TLB entries \nthat the logical processor is caching for the linear address of that operand for the \ncurrent linear address space. The instruction does not fault if that address is not \ncanonical relative to the current paging mode (e.g., is not 48-bit canonical when 4-\nlevel paging is active). However, no invalidation is performed because the processor \ndoes not cache TLB entries for addresses that are not canonical relative to the \ncurrent paging mode.\n\n• The INVPCID instruction takes a register operand (INVPCID type) and a memory \noperand (INVPCID descriptor). If the INVPCID type is 0, the instruction invalidates \n\n\n\n22 Document Number: 335252-002, Revision: 1.1\n\nany TLB entries that the logical processor is caching for the linear address and PCID \nspecified in the INVPCID descriptor. If the linear address is not canonical relative \nthe linear-address width supported by the processor, the instruction causes a \ngeneral-protection exception (#GP). If the processor supports 5-level paging, the \ninstruction will not cause such a #GP for an address that is 57-bit canonical, \nregardless of paging mode, even if 4-level paging is active and the address is not \n48-bit canonical.\n\n• The INVVPID instruction takes a register operand (INVVPID type) and a memory \noperand (INVVPID descriptor). If the INVPCID type is 0, the instruction invalidates \nany TLB entries that the logical processor is caching for the linear address and VPID \nspecified in the INVVPID descriptor. If the linear address is not canonical relative \nthe linear-address width supported by the processor, the instruction fails.1 If the \nprocessor supports 5-level paging, the instruction will not fail for an address that is \n57-bit canonical, regardless of paging mode, even if 4-level paging is active and the \naddress is not 48-bit canonical.\n\n2.7 Interactions with Intel® MPX\nThe Intel® Memory Protection Extensions (Intel® MPX) define a set of 4 bound \nregisters, each of which software can associate with a specific pointer in memory. \nIntel MPX includes two instructions — BNDLDX and BNDSTX — that allow software to \nload from or store into memory the bounds associated with a particular pointer in \nmemory.\n\nThe BNDLDX and BNDSTX instructions each take a bound register and a memory \noperand (the associated pointer). Each of these parses the linear address of the \nmemory operand to traverse a hierarchical data structure in memory. In 64-bit mode, \nthese instructions do not necessarily use all the bits in the supplied 64-bit addresses. \nThe number of bits used is 48 plus a value called the MPX address-width adjust \n(MAWA).\n\nThe value of MAWA depends on CPL; the current paging mode (4-level paging or 5-level \npaging); and, if 5-level paging is active, the value of a new MSR. Processors that \nsupport both Intel MPX and 5-level paging support the IA32_MPX_LAX MSR (MSR index \n1000H). Only bit 0 of the MSR is defined.\n\nIf CPL < 3, the supervisor MAWA (MAWAS) is used. The value of MAWAS is determined \nby the setting of CR4.LA57. If CR4.LA57 = 0 (4-level paging is active; recall that MAWA \nis relevant only in 64-bit mode), the value of MAWAS is 0. If CR4.LA57 = 1 (5-level \npaging is active), the value of MAWAS is 9. The value of MAWAS is not enumerated by \nthe CPUID instruction.\n\nIf CPL = 3, the user MAWA (MAWAU) is used. The value of MAWAU is determined as \nfollows. If CR4.LA57 = 0 or IA32_MPX_LAX[bit 0] = 0, the value of MAWAU is 0. If \nCR4.LA57 = 1 and IA32_MPX_LAX[bit 0] = 1, the value of MAWAU is 9. The current \nvalue of MAWAU is enumerated in \nCPUID.(EAX=07H,ECX=0H):ECX.MAWAU[bits 21:17].\n\nThe following items specify how an execution of the BNDLDX and BNDSTX instructions \nin 64-bit mode parses a linear address to traverse a hierarchical data structure.\n\n1. INVVPID is a VMX instruction. In response to certain conditions, execution of a VMX may fail, \nmeaning that it does not complete its normal operation. When a VMX instruction fails, control \npasses to the next instruction (rather than to a fault handler) and a flag is set to report the \nfailure.\n\n\n\nDocument Number: 335252-002, Revision: 1.1 23\n\n• A bound directory is located at the 4-KByte aligned linear address specified in \nbits 63:12 of BNDCFGx.1 A BDE is selected using the LAp (linear address of pointer \nto a buffer) to construct a 64-bit offset as follows:\n\n— bits 63:31+MAWA are 0;\n— bits 30+MAWA:3 are LAp[bits 47+MAWA:20]; and\n— bits 2:0 are 0.\n\nThe address of the BDE is the sum of the bound-directory base address (from \nBNDCFGx) plus this 64-bit offset.\n\nIf either BNDLDX or BNDSTX is executed inside an enclave, the instruction operates \nas if MAWAU = 0 (regardless of the values of CR4.LA57 and IA32_MPX_LAX[bit 0]).\n\n• The processor uses bits 63:3 of the BDE as the 8-byte aligned address of a bound \ntable (BT). A BTE is selected using the LAp (linear address of pointer to a buffer) to \nconstruct a 64-bit offset as follows:\n\n— bits 63:22 are 0;\n— bits 21:5 are LAp[bits 19:3]; and\n— bits 4:0 are 0.\n\nThe address of the BTE is the sum of the bound-table base address (from the BDE) \nplus this 64-bit offset.\n\nA bound directory comprises 228+MAWA 64-bit entries (BDEs);2 thus, the size of a \nbound directory in 64-bit mode is 21+MAWA GBytes. A bound table comprises 217 32-\nbyte entries (BTEs); thus, the size of a bound table in 64-bit mode is 4 MBytes \n(independent of MAWA).\n\n2.8 Interactions with Intel® SGX\nIntel® Software Guard Extensions (Intel® SGX) define new processor functionality that \nis implemented as SGX leaf functions within the ENCLS (supervisor) and ENCLU (user) \ninstructions.\n\nThe SGX leaf functions include memory accesses using linear addresses normally. \nWhen executed in 64-bit mode, the linear address are 64 bits in width and are subject \nto the normal treatment of accesses to memory with 64-bit linear addresses (see \nSection 2.3). In addition, some of the leaf functions apply specific architectural checks \nrelated to linear-address width. The following items detail these checks and how they \nare defined for processors that support 5-level paging.\n\n• The ECREATE leaf function of ENCLS creates a new enclave by creating a new SGX \nenclave control structure (SECS). For a 64-bit enclave, the processor checks \nwhether the enclave base linear address (specified in the SECS) is canonical, \ngenerating a general-protection exception (#GP) if it is not. On processors that \nsupport 5-level paging, this check is for 57-bit canonicality, regardless of the \ncurrent paging mode.\n\nIn addition to checking the canonicality of the enclave base linear address, \nECREATE confirms that the enclave size (specified in the SECS) is not greater than \nthe maximum size supported by the processor (if the enclave size is too large, \nECREATE generates a #GP). As noted in Section 2.2.1, older processors supported \n\n1. If CPL < 3, BNDCFGS is used; if CPL = 3, BNDCFGU is used.\n2. A bound directory used in a 64-bit enclave always comprises 228 64-bit BDEs and thus has a size \n\nof 2 GBytes.\n\n\n\n24 Document Number: 335252-002, Revision: 1.1\n\n64-bit enclaves with sizes up to 247 bytes; processors that support 5-level paging \nare expected to support enclaves with sizes up to 256 bytes.\n\nIf bits 4:3 of the enclave’s XSAVE feature request mask (XFRM) are set (indicating \nthat Intel MPX will be enabled during execution of the enclave), ECREATE generates \na #GP if the enclave’s size is greater than 248 bytes, even if the processor \nenumerates support for larger enclaves.\n\n• The EENTER and ERESUME leaf functions of ENCLU transfer control flow to an entry \npoint within a specified enclave. For entry to a 64-bit enclave, the processor checks \nwhether certain linear addresses are canonical, generating a general-protection \nexception (#GP) if any one is not. The following items detail these checks.\n\n— The linear address of the specified entry point must be canonical. If 4-level \npaging is active, it must be 48-bit canonical; if 5-level paging is active, it must \nbe 57-bit canonical.\n\n— The linear address of the asynchronous exit point (AEP — the address to which \nthe processor transfers control on an asynchronous enclave exit) must be \ncanonical. If 4-level paging is active, it must be 48-bit canonical; if 5-level \npaging is active, it must be 57-bit canonical.\n\n— The enclave values for the base addresses of the FS and GS segments must be \ncanonical. On processors that supports 5-level paging, these checks are for 57-\nbit canonicality, regardless of the current paging mode.\n\n• The EEXIT leaf function exits the currently executing enclave and branches to a \nspecified address. For an exit from a 64-bit enclave, the processor checks whether \nthat target linear address is canonical, generating a general-protection exception \n(#GP) if it is not. If 4-level paging is active, it must be 48-bit canonical; if 5-level \npaging is active, it need only be 57-bit canonical.\n\nAs noted in Section 2.7, executions of BNDLDX and BNDSTX in a 64-bit enclave always \noperate as if MAWAU = 0.\n\n\n\nDocument Number: 335252-002, Revision: 1.1 25\n\n3 Linear-Address Expansion and \nVMX Transitions\n\nAs noted in Section 1.2, VM entries and VM exits manipulate numerous processor \nregisters that contain linear addresses. The transitions respect the processor’s linear-\naddress width in a manner based on canonicality.\n\nAs discussed in Chapter 2, processors that support 5-level paging expand the linear-\naddress width from 48 bits to 57 bits. That expansion changes the operation of VMX \ntransitions. Changes to VM entries are detailed in Section 3.1, while changes to \nVM exits are given in Section 3.2.\n\n3.1 Linear-Address Expansion and VM Entries\nCertain fields in the VMCS correspond to registers that contain linear addresses. \nVM entries confirm those fields contain values that are canonical. This checking is \nbased on the linear-address width supported by the processor (e.g., is based on 57-bit \ncanonicality if the processor supports 5-level paging). The following are the fields to \nwhich this applies.\n\n• In the host-state area:\n\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\n\n• In the guest-state area:\n\n— The fields for the IA32_SYSENTER_EIP and IA32_SYSENTER_ESP MSRs.\n— The base-address fields for FS, GS, TR, GDTR, and IDTR.\n— The base-address field for LDTR (if LDTR will be usable).\n— The field for the IA32_BNDCFGS MSR (if VM entry is loading that MSR).\n\nA VM entry to 64-bit mode also performs a check on the RIP field in the guest-state \narea of the current VMCS. If the VM entry would result in 4-level paging, it checks that \nbits 63:48 of the guest RIP field are identical; if it would result in 5-level paging, that \ncheck is on bits 63:57.1\n\n3.2 Linear-Address Expansion and VM Exits\nVM exits save the state of certain registers into the guest-state area of the VMCS. \nSome of these registers contain linear addresses. As discussed in Section 1.1, the CPU \ngenerally ensures that the values in these registers respect the CPU’s linear-address \nwidth. As a result, the values the VM exits save for these registers will do the same.\n\n1. Note that these checks do not confirm that the guest RIP field is canonical relative to the paging \nmode being entered. For example, bits 63:47 are identical in a 48-bit canonical address. However, \nVM entry to 4-level paging may load RIP with a value in which bit 47 differs from that of \nbits 63:48.\n\n\n\n26 Document Number: 335252-002, Revision: 1.1\n\nThere is a special case for LDTR base address. If LDTR was not usable at the time of a \nVM exit, the value saved for the base address is undefined. However, this undefined \nvalue is always 48-bit canonical on processors that do not support 5-level paging and is \nalways 57-bit canonical on processors that do support 5-level paging.\n\nVM exits load the state of certain registers from the host-state area of the VMCS. Some \nof these registers contain linear addresses. Each VM exit ensures that the value of each \nof the following registers is canonical: the IA32_SYSENTER_EIP and \nIA32_SYSENTER_ESP MSRs; and the base addresses for FS, GS, TR, GDTR, and IDTR. \nHow this is done depends on whether the processor supports 5-level paging.\n\n• If the processor does not support 5-level paging, bits 47:0 of the register are \nloaded from the field in the host-state area; the value of bit 47 is then sign-\nextended into bits 63:48 of the register.\n\n• If the processor does support 5-level paging, bits 56:0 of the register are loaded \nfrom the field in the host-state area; the value of bit 56 is then sign-extended into \nbits 63:57 of the register.\n\nAgain, there is a special case for LDTR. LDTR is always unusable after a VM exit. Its \nbase address may be loaded with an undefined value. This undefined value is always \n48-bit canonical on processors that do not support 5-level paging and is always 57-bit \ncanonical on processors that do support 5-level paging.\n\n\n\nDocument Number: 335252-002, Revision: 1.1 27\n\n4 5-Level EPT\n\n5-level EPT is a new mode for EPT. As its name suggests, it will translate guest-\nphysical addresses by traversing a 5-level hierarchy of EPT paging structures. Because \nthe process is otherwise unmodified, 5-level paging extends the processor’s guest-\nphysical-address width to 57 bits. (The additional 9 bits are used to select an entry \nfrom the fifth level of the hierarchy.) For clarity, the original EPT mode will now be \ncalled 4-level EPT.\n\nThe remainder of this chapter specifies architectural changes to 4-level EPT as well as \nthose that define and are entailed by 5-level EPT. Section 4.1 describes how the \nexpansion of the guest-physical-address width affects 4-level EPT. Section 4.2 specifies \nhow the CPU enumerates 5-level EPT and how the feature is enabled by software. \nSection 4.3 details how 5-level EPT translates guest-physical addresses.\n\n4.1 4-Level EPT: Guest-Physical-Address Limit\nAs explained in Section 1.3, 4-level EPT is limited to translating 48-bit guest-physical \naddresses.\n\nThis is not a problem on existing processors, because they limit the physical-address \nwidth to 46 bits (see Section 1.1). A processor’s physical-address width also limits \nguest-physical addresses. That means that, on existing processors, any attempt to use \na guest-physical address that sets a bit above the low 48 bits will cause a page-fault \nexception (#PF).\n\nProcessors that support 5-level paging are expected to support 52 physical-address \nbits. Such processors allow use of a guest-physical address that sets bits in the range \n51:48; no #PF is generated.\n\nA guest-physical address that sets bits in the range 51:48 cannot be translated by 4-\nlevel EPT. An attempt to access such an address when 4-level EPT is active causes an \nEPT violation (see Section 1.3).\n\nEPT violations generate information about the exception in a value called the exit \nqualification. In general, EPT violations caused by attempts to access a guest-physical \naddress that is too wide establish the exit qualification as is currently done for other \nEPT violations. Exceptions are made for bits 6:3 of the exit qualification, which report \nthe access rights for the guest-physical address. The new EPT violations always clear \nthese bits.\n\n4.2 5-Level EPT: Enumeration and Enabling\nThis section describes how processors enumerate to software support for 5-level EPT \nand how software enables the processor to use that support.\n\n4.2.1 Enumeration\nProcessors supporting EPT enumerate details related to EPT in the \nIA32_VMX_EPT_VPID_CAP MSR (index 48CH). Currently, \nIA32_VMX_EPT_VPID_CAP[bit 6] enumerates support for 4-level EPT. Processors that \nalso support 5-level EPT will enumerate that fact by also setting \nIA32_VMX_EPT_VPID_CAP[bit 7].\n\n\n\n28 Document Number: 335252-002, Revision: 1.1\n\nThe guest-physical-address width supported by a processor is not enumerated using \nthe IA32_VMX_EPT_VPID_CAP MSR. This is because that width is always the same as \nthe processor’s maximum physical-address width as enumerated by \nCPUID.80000008H:EAX[bits 7:0].\n\n4.2.2 Enabling by Software\nA VMM enables EPT by setting the “enable EPT” VM-execution control in the current \nVMCS before using the VMCS for VM entry.\n\nSpecific details of EPT operation are determined by the extended-page-table pointer \nfield (EPTP) in the VMCS. In particular, EPTP[bits 5:3] contain a value that is 1 less than \nthe number of levels used by the EPT. On existing processors, this value must be 3, \nindicating 4-level EPT. (VM entry fails if a different value is used.) Processors that also \nsupport 5-level EPT will also allow the value 4 (indicating 5-level EPT).\n\nIn summary, VM entry on a processor that supports 5-level check EPTP[bits 5:3]. If the \nvalue is 3, the VM entry activates 4-level EPT. If the value is 4, the VM entry activates \n5-level EPT. With any other value, VM entry fails.\n\n4.3 5-Level EPT: Guest-Physical-Address Translation\nLike 4-level EPT, 5-level EPT translates guest-physical addresses using a hierarchy of \nin-memory paging structures. Because 5-level EPT increases the guest-physical-\naddress width to 57 bits (from the 48 bits supported by 4-level EPT), 5-level EPT allows \nup to 128 PBytes of guest-physical-address space to be accessed at any given time.\n\nThe following items describe in more detail the changes that 5-level EPT makes to the \ntranslation process.\n\n• Translation begins by identifying a 4-KByte naturally aligned EPT PML5 table. It is \nlocated at the physical address specified in bits 51:12 of EPTP. An EPT PML5 table \ncomprises 512 64-bit entries (EPT PML5Es). An EPT PML5E is selected using the \nphysical address defined as follows.\n\n— Bits 63:52 are all 0.\n— Bits 51:12 are from EPTP.\n— Bits 11:3 are bits 56:48 of the guest-physical address.\n— Bits 2:0 are all 0.\n\nBecause an EPT PML5E is identified using bits 56:48 of the guest-physical address, it \ncontrols access to a 256-TByte region of the linear-address space. The format of an EPT \nPML5E is given in Table 4-1.\n\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E)\n\nBit Position(s) Contents\n\n0 Read access; indicates whether reads are allowed from the 256-TByte region controlled by \nthis entry.\n\n1 Write access; indicates whether writes are allowed from the 256-TByte region controlled by \nthis entry.\n\n2 If the “mode-based execute control for EPT” VM-execution control is 0, execute access; \nindicates whether instruction fetches are allowed from the 256-TByte region controlled by \nthis entry.\nIf that control is 1, execute access for supervisor-mode linear addresses; indicates whether \ninstruction fetches are allowed from supervisor-mode linear addresses in the 256-TByte \nregion controlled by this entry.\n\n\n\nDocument Number: 335252-002, Revision: 1.1 29\n\n• The next step of the translation process identifies a 4-KByte naturally aligned EPT \nPML4 table. It is located at the physical address specified in bits 51:12 of the EPT \nPML5E (see Table 4-1). An EPT PML4 table comprises 512 64-bit entries (EPT \nPML4Es). An EPT PML4E is selected using the physical address defined as follows.\n\n— Bits 51:12 are from the EPT PML5E.\n— Bits 11:3 are bits 47:39 of the guest-physical address.\n— Bits 2:0 are all 0.\n\nBecause an EPT PML4E is identified using bits 56:39 of the guest-physical address, \nit controls access to a 512-GByte region of the guest-physical-address space.\n\nOnce the EPT PML4E is identified, bits 38:0 of the guest-physical address determine the \nremainder of the translation process exactly as is done for 4-level EPT. As suggested in \nTable 4-1, the values of bits 2:0 and bit 10 of the EPT PML5E are used normally (in \ncombination with the corresponding bits in other EPT paging-structure entries) to \ndetermine whether EPT violations occur. The accessed flag (bit 8) in the EPT PML5E is \nupdated as is done for other EPT paging-structure entries.\n\n4.4 5-Level EPT and EPTP Switching\nThe value of EPTP may be modified in VMX non-root operation by invoking \nVM function 0 (EPTP switching). This is done by executing the VMFUNC instruction with \nvalue 0 in the EAX register. Invocation of VM function 0 loads EPTP with a value \nselected from a data structure in memory.\n\nBefore loading EPTP in this way, the processor first confirms that the value to be loaded \nis valid. The definition of a valid EPTP value depends on whether the processor supports \n5-level EPT.\n\n• If the processor does not support 5-level EPT, an EPTP value in memory is \nconsidered valid if it would not cause VM entry to fail (e.g., it does not set any \nreserved bits).\n\n• If the processor does support 5-level EPT, an EPTP value in memory is considered \nvalid only if it would not cause VM entry to fail (as above) and if its value in \nbits 5:3 (which controls the number of EPT levels) is the same as that of the \ncurrent value of EPTP.\n\nThe implication is that an invocation of VM function 0 cannot change the EPT mode \nbetween 4-level EPT and 5-level EPT.\n\n7:3 Reserved (must be 0).\n\n8 If bit 6 of EPTP is 1, accessed flag for EPT; indicates whether software has accessed the \n256-TByte region controlled by this entry. Ignored if bit 6 of EPTP is 0.\n\n9 Ignored.\n\n10 Execute access for user-mode linear addresses. If the “mode-based execute control for \nEPT” VM-execution control is 1, indicates whether instruction fetches are allowed from user-\nmode linear addresses in the 256-TByte region controlled by this entry. If that control is 0, \nthis bit is ignored.\n\n11 Ignored.\n\nM–1:12 Physical address of 4-KByte aligned EPT PML4 table referenced by this entry.\n\n51:M Reserved (must be 0).\n\n63:52 Ignored.\n\nTable 4-1. Format of an EPT PML5 Entry (EPT PML5E) (Continued)\n\nBit Position(s) Contents\n\n\n\n30 Document Number: 335252-002, Revision: 1.1\n\n\n\nDocument Number: 335252-002, Revision: 1.1 31\n\n5 Intel® Virtualization \nTechnology for Directed I/O\n\nIntel® Virtualization Technology for Directed I/O includes a feature called DMA \nremapping.\n\nDMA remapping provides hardware support for isolation of device accesses to memory. \nWhen a device attempts to access system memory, DMA-remapping hardware \nintercepts the access and utilizes paging structures to determine whether the access \ncan be permitted; it also determines the actual location to access.\n\nThe DMA-remapping hardware may support two levels of address translation. One level \nmay translate a linear address to a guest-physical address, while a second level may \nremap the guest-physical address to physical address.\n\nThe first-level translation uses paging structures with the same format as those used \nfor ordinary paging. The second-level translation uses paging structures with the same \nformat as those used for EPT.\n\nIt is expected that, on platforms that support wider linear and guest-physical addresses \n(using 5-level paging and 5-level EPT, respectively), the DMA-remapping hardware will \nbe similarly enhanced to support those wider addresses with 5-level translation \nprocesses.\n\nThis enhanced support for DMA remapping will be detailed in a future revision of the \nIntel® Virtualization Technology for Directed I/O Architecture Specification.\n\n\n\t1 Introduction\n\t1.1 Existing Paging in IA-32e Mode\n\t1.2 Linear-Address Width and VMX Transitions\n\t1.3 Existing Extended Page Tables (EPT)\n\n\t2 Expanding Linear Addresses: 5-Level Paging\n\t2.1 5-Level Paging: Introduction\n\t2.2 Enumeration and Enabling\n\t2.2.1 Enumeration by CPUID\n\t2.2.2 Enabling by Software\n\n\t2.3 Linear-Address Generation and Canonicality\n\t2.4 5-Level Paging: Linear-Address Translation\n\t2.5 Linear-Address Registers and Canonicality\n\t2.5.1 Canonicality Checking on RIP Loads\n\t2.5.2 Canonicality Checking on Other Loads\n\n\t2.6 Interactions with TLB-Invalidation Instructions\n\t2.7 Interactions with Intel® MPX\n\t2.8 Interactions with Intel® SGX\n\n\t3 Linear-Address Expansion and VMX Transitions\n\t3.1 Linear-Address Expansion and VM Entries\n\t3.2 Linear-Address Expansion and VM Exits\n\n\t4 5-Level EPT\n\t4.1 4-Level EPT: Guest-Physical-Address Limit\n\t4.2 5-Level EPT: Enumeration and Enabling\n\t4.2.1 Enumeration\n\t4.2.2 Enabling by Software\n\n\t4.3 5-Level EPT: Guest-Physical-Address Translation\n\t4.4 5-Level EPT and EPTP Switching\n\n\t5 Intel® Virtualization Technology for Directed I/O\n\n', 'metadata_pdf:docinfo:creator': 'Intel Corporation', 'metadata_resourceName': "b'5-Level Paging and 5-Level EPT - Intel - Revision 1.1 (May, 2017).pdf'", 'metadata_pdf:docinfo:modified': '2018-04-02T21:22:31Z', 'metadata_pdf:docinfo:title': '5-Level Paging and 5-Level EPT'}}, {'_index': 'mongo_index', '_id': 'F_9p5IUB9nynXRNhVczX', '_score': 2.7891173, '_ignored': ['content.keyword', 'log_entry.keyword'], '_source': {'log_entry': '{"_id"=>BSON::ObjectId(\'63cffa2978b994746c729ce8\'), "content"=>"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n100G Networking Toronto.key\\n\\n\\n100G Networking Technology Overview\\nChristopher Lameter <cl@linux.com> \\nFernando Garcia <fgarcia@dasgunt.com> \\n\\nToronto, August 23, 2016\\n\\n1\\n\\n\\n\\nWhy 100G now?\\n\\n• Capacity and speed requirements on data links keep increasing. \\n\\n• Fiber link reuse in the Connectivity providers (Allows Telcos to \\nmake better use of WAN links) \\n\\n• Servers have begun to be capable of sustaining 100G to memory \\n(Intel Skylake, IBM Power8+) \\n\\n• Machine Learning Algorithms require more bandwidth \\n\\n• Exascale Vision  for 2020 of the US DoE to move the industry \\nahead.\\n\\n2\\n\\n\\n\\n100G Networking Technologies\\n• 10 x 10G Link old standard CFP C??. Expensive. Lots of cabling. Has been in use for awhile for \\n\\nspecialized uses. \\n\\n• New 4 x 28G link standards \\"QSFP28\\". Brings down price to ranges of SFP and QSFP.\\u2028\\nCompact and designed to replace 10G and 40G networking. \\n\\n• Infiniband (EDR) \\no Standard pushed by Mellanox. \\no Transitioning to lower Infiniband speeds through switches. \\no Most mature technology to date. Switches and NICs are available. \\n\\n• Ethernet \\no Early deployment in 2015. \\no  But most widely used chipset for switches recalled to be respun.  \\no NICs are under development. Mature one is the Mellanox EDR adapter that can run in 100G \\n\\nEthernet mode. \\no Maybe ready mid 2016. \\n\\n• Omnipath (Intel) \\n\\n• Redesigned serialization. No legacy issues with Infiniband. More nodes. Designed for Exascale \\nvision. Immature. Vendor claims production readiness but what is available has the character of \\nan alpha release with limited functionality. Estimate that this is going to be more mature at the \\nend of 2016.\\n\\n3\\n\\n\\n\\nCFP vs QSFP28: 100G Connectors \\n\\n4\\n\\n\\n\\nSplitting 100G Ethernet to 25G and 50G\\n\\n• 100G is actually 4x25g (QSFP28), so 100G Ports can be split with \\n“octopus cables” to lower speed. \\n\\n• 50G (2x25) and 25G (1x25G) speeds are available which doubles or \\nquadruples the port density of switches. \\n\\n• Some switches can handle 32 links of 100G, 64 of 50G and 128 of 25G. \\n\\n• It was a late idea. So 25G Ethernet standards are scheduled to be \\ncompleted in 2016 only. Vendors are designing to a proposed standard. \\n\\n• 50G Ethernet standard is in the works (2018-2020). May be the default \\nin the future since storage speeds and memory speeds increase. \\n\\n• 100G Ethernet done \\n\\n• 25G Ethernet has a new connector standard called SFP28 5\\n\\n\\n\\n100G Cabling and Connectors\\n\\n6\\n\\n\\n\\n100G Switches\\n\\n7\\n\\nPorts Status Name\\n\\nMellanox \\nInfiniband\\n\\nEDR x 36 Released. Stable. 7700 Series\\n\\nBroadcom 100G x 32 \\n50G x 64 \\n25G x 128\\n\\nRereleased after \\nsilicon problem.\\n\\nTomahawk Chip\\n\\nMellanox \\nEthernet\\n\\n100G x 32 \\n50G x 64\\n\\n2Q ? Spectrum\\n\\nIntel Omnipath x 48 Available 100 Series\\n\\n\\n\\nOverwhelmed by data\\n\\n8\\n\\nEthernet 10M 100M (Fast) 1G (Gigabit) 10G 100G\\n\\nTime per bit 100 ns 10 ns 1 ns 0.1 ns 0.01 ns\\n\\nTime for a MTU size \\nframe 1500 bytes\\n\\n1500 us 150 us 15 us 1.5 us 150 ns\\n\\nTime for a 64 byte \\npacket\\n\\n64 us 6.4 us 640 ns 64 ns 6.4 ns\\n\\nPackets per second ~10 K ~100 K ~1 M ~10 M ~100 M\\n\\nPackets per 10 us 2 (small) 20 \\n(small)\\n\\n6 (MTU) 60 (MTU)\\n\\n\\n\\nNo time to process what you get?\\n\\n9\\n\\n• NICs have the problem of how to \\nget  the data to the application \\n\\n• Flow Steering in the kernel allows \\nthe distribution of packets to \\nmultiple processors so that the \\nprocessing scales. But there are not \\nenough processing cores for 100G. \\n\\n• NICs have extensive logic to offload \\noperations and distribute the load. \\n\\n• One NIC supports multiple servers \\nof diverse architectures \\nsimultaneously. \\n\\n• Support for virtualization. SR-IOV \\netc. \\n\\n• Switch like logic on the chip.\\n\\n1 us = 1 microsecond \\n= 1/1000000 seconds \\n\\n1 ns = 1 nanosecond \\n= 1/1000 us \\n\\nNetwork send or receive syscall: \\n10-20 us \\n\\nMain memory access: \\n~100 ns\\n\\n\\n\\nAvailable 100G NICs\\n• Mellanox ConnectX4 Adapter \\n• 100G Ethernet \\n• EDR Infiniband \\n\\n• Sophisticated offloads. \\n• Multi-Host \\n\\n• Evolution of ConnectX3 \\n\\n• Intel Omnipath Adapter \\n• Focus on MPI \\n• Omnipath only \\n\\n• Redesign of IB protocol to be a “Fabric” \\n• “Fabric Adapter”. New Fabric APIs. \\n\\n• More nodes larger transfer sizes\\n\\n10\\n\\n\\n\\nApplication Interfaces and 100G\\n1.Socket API (Posix)\\u2028\\n\\nRun existing apps. Large code base. Large set of developers that know how to use the programming interface \\n2.Block level File I/O \\u2028\\n\\nAnother POSIX API. Remote filesystems like NFS may use NFSoRDMA etc \\n3.RDMA API \\n\\n1.One sided transfers \\n2.Receive/SendQ in user space \\n3.Talk directly to the hardware. \\n\\n4.OFI\\u2028\\nFabric API designed for application interaction not with the network but the “Fabric” \\n\\n5.DPDK \\u2028\\nLow level access to NIC from user space.\\n\\n11\\n\\n\\n\\nUsing the Socket APIs with 100G\\n\\n• Problem of queuing if you have a fast talker. \\n\\n• Flow steering to scale to multipe processors \\n\\n• Per processor queues to scale sending. \\n\\n• Exploit offloads to send / receive large amounts \\nof data \\n\\n• May use protocol with congestion control (TCP) \\nbut then not able to use full bandwidth. \\nCongestion control not tested with 100G. \\n\\n• Restricted to Ethernet 100G. Use on non \\nEthernet Fabrics (IPoIB, IPoFabric) has various \\nnon Ethernet semantics. F.e. Layer 2 behaves \\ndifferently and may offer up surprises. 12\\n\\n\\n\\nRDMA / Infiniband API\\n\\n• Allow use of native Infiniband functionality designed for higher speed. \\n\\n• Supports both Infiniband and Onmipath. \\n\\n• Single sided transfers via memory registration or classic messaging. \\n\\n• Offload behavior by having RX and TX rings in user space. \\n\\n• Group send / receive possible. \\n\\n• Control of RDMA/Infiniband from user space with API that is process \\nsafe but allows direct interaction with an instance of the NIC. \\n\\n• Can be used on Ethernet via ROCE and/or ROCEv2 \\n\\n• Generally traffic is not routable (ROCE V2 and Ethernet messaging of \\ncourse is). Problem of getting into and out of fabric. Requires \\nspecialized gateways.\\n\\n13\\n\\n\\n\\nOFI (aka libfabric)\\n\\n• Recent project by OFA to design a new API. \\n\\n• Based on RDMA concepts. \\n\\n• Driven by Intel to have an easier API than the ugly RDMA \\nAPIs. OFI is focusing on the application needs from a Fabric. \\n\\n• Tested and developed for the needs of MPI at this point. \\n\\n• Ability to avoid the RDMA kernel API via “native” drivers. \\nDrivers can define API to their own user space libraries. \\n\\n• Lack of some general functionality like Multicast. \\n\\n• Immature at this point. Promise for the future to solve some \\nof the issue coming with 100G networking.\\n\\n14\\n\\n\\n\\nSoftware Support for 100G technology\\n\\nEDR via Mellanox ConnectX4 Adapter \\n - Linux 4.3. Redhat 7.2 \\n\\nEthernet via Mellanox ConnectX4 Adapter \\n - Linux 4.5. Redhat 7.3.\\u2028\\n (7.2 has only socket layer support). \\n\\nOmnipath via Intel OPA adapter \\n - Out of tree drivers, in Linux 4.4 staging. \\n\\nCurrently supported via Intel OFED \\ndistribution \\n\\n15\\n\\n\\n\\nTest Hardware\\no Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz \\n\\n• Adapters \\no Intel Omni-Path Host Fabric Interface Adapter \\n\\n• Driver Version: 0.11-162 \\n• Opa Version: 10.1.1.0.9 \\n\\no Mellanox ConnectX-4 VPI\xa0Adapter \\n• Driver Version: Stock RHEL 7.2 \\n• Firmware Version: 12.16.1020 \\n\\n• Switches \\no Intel 100 OPA Edge 48p \\n\\n• Firmware Version: 10.1.0.0.133 \\no Mellanox SB7700 \\n\\n• Firmware Version: 3.4.3050\\n16\\n\\n\\n\\nLatency Tests via RDMA APIs(ib_send_lat)\\n\\n17\\n\\nTy\\npi\\nca\\nl\\tL\\nat\\nen\\n\\ncy\\n\\t(u\\n\\nse\\nc)\\n\\n0.00\\n\\n2.75\\n\\n5.50\\n\\n8.25\\n\\n11.00\\n\\nMsg\\tSize\\t(bytes)\\n\\n2 4 8 16 32 64 128 256 512 1024 2048 4096\\n\\nEDR\\nOmnipath\\n100GbE\\n10GbE\\n1GbE\\n\\n- Consistently low latency below 50% of 1G Ethernet. \\n- Higher packet sizes benefit significantly. \\n- Omnipath has higher latency due to software processing of send/receive \\n\\nrequests.\\u2028\\n\\n\\n\\nBandwidth Tests using RDMA APIs (ib_send_bw)\\n\\n18\\n\\nBW\\n\\ta\\nve\\nra\\nge\\n\\t(M\\n\\nB/\\nse\\nc)\\n\\n0.00\\n\\n3000.00\\n\\n6000.00\\n\\n9000.00\\n\\n12000.00\\n\\nMsg\\tSize\\t(bytes)\\n\\n2 4 8 16 32 64 128 256 512 1024 2048 4096\\n\\nEDR\\nOmnipath\\n100GbE\\n10GbE\\n1GbE\\n\\n- EDR can reach line saturation (~12GB/sec) at ~ 2k packet size \\n- Small packet processing is superior on EDR. \\n- 10GE (1GB/sec) and 1G (120GB/sec) saturate the line with small packets \\n\\nearly\\n\\n\\n\\nMulticast latency tests\\n\\n19\\n\\nLa\\nte\\nnc\\ny\\t\\n(u\\ns)\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\nEDR Omnipath 100GbE 10GbE 1GbE\\n\\n- Lowest latency with 100G and 10G Ethernet \\n- Slightly higher latency of EDR due to Forward Error Correction \\n- Software processing increases packet latency on Omnipath\\n\\n\\n\\nRDMA vs. Posix Sockets (30 byte payload)\\n\\n20\\n\\nLa\\nte\\nnc\\ny\\t\\n(u\\ns)\\n\\n0\\n\\n4.5\\n\\n9\\n\\n13.5\\n\\n18\\n\\nEDR Omnipath 100GbE 10GbE 1GbE\\n\\nSocket\\nRDMA\\n\\n\\n\\nRDMA vs. Posix Sockets (1000 byte Payload)\\n\\n21\\n\\nLa\\nte\\nnc\\ny\\t\\n(u\\ns)\\n\\n0\\n\\n7.5\\n\\n15\\n\\n22.5\\n\\n30\\n\\nEDR Omnipath 100GbE 10GbE 1GbE\\n\\nSockets\\nRDMA\\n\\n\\n\\nFurther Reading Material\\n\\nhttp://presentations.interop.com/events/las-vegas/2015/open-to-all---keynote-\\npresentations/download/2709 \\n\\nhttps://en.wikipedia.org/wiki/100_Gigabit_Ethernet \\n\\nhttp://www.ieee802.org/3/index.html \\n\\n22\\n\\nhttp://presentations.interop.com/events/las-vegas/2015/open-to-all---keynote-presentations/download/2709\\nhttps://en.wikipedia.org/wiki/100_Gigabit_Ethernet\\nhttp://www.ieee802.org/3/index.html\\n\\n\\nMemory Performance issues with 100G\\n\\n• 100G NIC can give you 12.5G Byte per second of \\nthroughput \\n\\n• DDR3 memory in basic configuration at 6.5 Gbyte \\nper sec. High end at 17G byte per second. \\n\\n• DDR4 12.8G - 19.2G byte per sec. \\n\\n• Some adapter have dual 100G connectors.  \\n\\n• Memory via the NIC traffic may be able to saturate \\nthe system.\\u2028\\n\\n23\\n\\n\\n\\nLooking Ahead\\n• 100G is maturing. \\n\\n• 200G available in 2017/2018. \\n\\n• Terabit links by 2022. \\n\\n• Software needs to mature. Especially the \\nOS network stack to handle these speeds. \\n\\n• Issues \\no Memory throughput \\no Proper APIs \\no Deeper integration of cpu/memory/io\\n\\n24\\n\\n\\n\\nQ&A\\n\\n• Issues \\n\\n• Getting involved \\n\\n• How to scale the OS and software \\n\\n• What impact will this speed have on software \\n\\n• Contact information\\u2028\\n  cl@linux.com\\n\\n25\\n\\nmailto:cl@linux.com\\n\\n", "metadata"=>{"pdf:docinfo:title"=>"100G Networking Toronto.key", "pdf:docinfo:modified"=>"2016-08-21T02:40:43Z", "resourceName"=>"b\'100G Networking Technology Overview - Slides - Toronto (August 2016).pdf\'", "pdf:docinfo:created"=>"2016-08-21T02:40:43Z"}, "filename"=>"100G Networking Technology Overview - Slides - Toronto (August 2016).pdf"}', 'filename': '100G Networking Technology Overview - Slides - Toronto (August 2016).pdf', 'metadata_pdf:docinfo:created': '2016-08-21T02:40:43Z', 'mongo_id': '63cffa2978b994746c729ce8', '@version': '1', 'host': 'bdvm', 'logdate': '2023-01-24T15:32:57+00:00', '@timestamp': '2023-01-24T15:32:58.659339470Z', 'content': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100G Networking Toronto.key\n\n\n100G Networking Technology Overview\nChristopher Lameter <cl@linux.com> \nFernando Garcia <fgarcia@dasgunt.com> \n\nToronto, August 23, 2016\n\n1\n\n\n\nWhy 100G now?\n\n• Capacity and speed requirements on data links keep increasing. \n\n• Fiber link reuse in the Connectivity providers (Allows Telcos to \nmake better use of WAN links) \n\n• Servers have begun to be capable of sustaining 100G to memory \n(Intel Skylake, IBM Power8+) \n\n• Machine Learning Algorithms require more bandwidth \n\n• Exascale Vision  for 2020 of the US DoE to move the industry \nahead.\n\n2\n\n\n\n100G Networking Technologies\n• 10 x 10G Link old standard CFP C??. Expensive. Lots of cabling. Has been in use for awhile for \n\nspecialized uses. \n\n• New 4 x 28G link standards "QSFP28". Brings down price to ranges of SFP and QSFP.\u2028\nCompact and designed to replace 10G and 40G networking. \n\n• Infiniband (EDR) \no Standard pushed by Mellanox. \no Transitioning to lower Infiniband speeds through switches. \no Most mature technology to date. Switches and NICs are available. \n\n• Ethernet \no Early deployment in 2015. \no  But most widely used chipset for switches recalled to be respun.  \no NICs are under development. Mature one is the Mellanox EDR adapter that can run in 100G \n\nEthernet mode. \no Maybe ready mid 2016. \n\n• Omnipath (Intel) \n\n• Redesigned serialization. No legacy issues with Infiniband. More nodes. Designed for Exascale \nvision. Immature. Vendor claims production readiness but what is available has the character of \nan alpha release with limited functionality. Estimate that this is going to be more mature at the \nend of 2016.\n\n3\n\n\n\nCFP vs QSFP28: 100G Connectors \n\n4\n\n\n\nSplitting 100G Ethernet to 25G and 50G\n\n• 100G is actually 4x25g (QSFP28), so 100G Ports can be split with \n“octopus cables” to lower speed. \n\n• 50G (2x25) and 25G (1x25G) speeds are available which doubles or \nquadruples the port density of switches. \n\n• Some switches can handle 32 links of 100G, 64 of 50G and 128 of 25G. \n\n• It was a late idea. So 25G Ethernet standards are scheduled to be \ncompleted in 2016 only. Vendors are designing to a proposed standard. \n\n• 50G Ethernet standard is in the works (2018-2020). May be the default \nin the future since storage speeds and memory speeds increase. \n\n• 100G Ethernet done \n\n• 25G Ethernet has a new connector standard called SFP28 5\n\n\n\n100G Cabling and Connectors\n\n6\n\n\n\n100G Switches\n\n7\n\nPorts Status Name\n\nMellanox \nInfiniband\n\nEDR x 36 Released. Stable. 7700 Series\n\nBroadcom 100G x 32 \n50G x 64 \n25G x 128\n\nRereleased after \nsilicon problem.\n\nTomahawk Chip\n\nMellanox \nEthernet\n\n100G x 32 \n50G x 64\n\n2Q ? Spectrum\n\nIntel Omnipath x 48 Available 100 Series\n\n\n\nOverwhelmed by data\n\n8\n\nEthernet 10M 100M (Fast) 1G (Gigabit) 10G 100G\n\nTime per bit 100 ns 10 ns 1 ns 0.1 ns 0.01 ns\n\nTime for a MTU size \nframe 1500 bytes\n\n1500 us 150 us 15 us 1.5 us 150 ns\n\nTime for a 64 byte \npacket\n\n64 us 6.4 us 640 ns 64 ns 6.4 ns\n\nPackets per second ~10 K ~100 K ~1 M ~10 M ~100 M\n\nPackets per 10 us 2 (small) 20 \n(small)\n\n6 (MTU) 60 (MTU)\n\n\n\nNo time to process what you get?\n\n9\n\n• NICs have the problem of how to \nget  the data to the application \n\n• Flow Steering in the kernel allows \nthe distribution of packets to \nmultiple processors so that the \nprocessing scales. But there are not \nenough processing cores for 100G. \n\n• NICs have extensive logic to offload \noperations and distribute the load. \n\n• One NIC supports multiple servers \nof diverse architectures \nsimultaneously. \n\n• Support for virtualization. SR-IOV \netc. \n\n• Switch like logic on the chip.\n\n1 us = 1 microsecond \n= 1/1000000 seconds \n\n1 ns = 1 nanosecond \n= 1/1000 us \n\nNetwork send or receive syscall: \n10-20 us \n\nMain memory access: \n~100 ns\n\n\n\nAvailable 100G NICs\n• Mellanox ConnectX4 Adapter \n• 100G Ethernet \n• EDR Infiniband \n\n• Sophisticated offloads. \n• Multi-Host \n\n• Evolution of ConnectX3 \n\n• Intel Omnipath Adapter \n• Focus on MPI \n• Omnipath only \n\n• Redesign of IB protocol to be a “Fabric” \n• “Fabric Adapter”. New Fabric APIs. \n\n• More nodes larger transfer sizes\n\n10\n\n\n\nApplication Interfaces and 100G\n1.Socket API (Posix)\u2028\n\nRun existing apps. Large code base. Large set of developers that know how to use the programming interface \n2.Block level File I/O \u2028\n\nAnother POSIX API. Remote filesystems like NFS may use NFSoRDMA etc \n3.RDMA API \n\n1.One sided transfers \n2.Receive/SendQ in user space \n3.Talk directly to the hardware. \n\n4.OFI\u2028\nFabric API designed for application interaction not with the network but the “Fabric” \n\n5.DPDK \u2028\nLow level access to NIC from user space.\n\n11\n\n\n\nUsing the Socket APIs with 100G\n\n• Problem of queuing if you have a fast talker. \n\n• Flow steering to scale to multipe processors \n\n• Per processor queues to scale sending. \n\n• Exploit offloads to send / receive large amounts \nof data \n\n• May use protocol with congestion control (TCP) \nbut then not able to use full bandwidth. \nCongestion control not tested with 100G. \n\n• Restricted to Ethernet 100G. Use on non \nEthernet Fabrics (IPoIB, IPoFabric) has various \nnon Ethernet semantics. F.e. Layer 2 behaves \ndifferently and may offer up surprises. 12\n\n\n\nRDMA / Infiniband API\n\n• Allow use of native Infiniband functionality designed for higher speed. \n\n• Supports both Infiniband and Onmipath. \n\n• Single sided transfers via memory registration or classic messaging. \n\n• Offload behavior by having RX and TX rings in user space. \n\n• Group send / receive possible. \n\n• Control of RDMA/Infiniband from user space with API that is process \nsafe but allows direct interaction with an instance of the NIC. \n\n• Can be used on Ethernet via ROCE and/or ROCEv2 \n\n• Generally traffic is not routable (ROCE V2 and Ethernet messaging of \ncourse is). Problem of getting into and out of fabric. Requires \nspecialized gateways.\n\n13\n\n\n\nOFI (aka libfabric)\n\n• Recent project by OFA to design a new API. \n\n• Based on RDMA concepts. \n\n• Driven by Intel to have an easier API than the ugly RDMA \nAPIs. OFI is focusing on the application needs from a Fabric. \n\n• Tested and developed for the needs of MPI at this point. \n\n• Ability to avoid the RDMA kernel API via “native” drivers. \nDrivers can define API to their own user space libraries. \n\n• Lack of some general functionality like Multicast. \n\n• Immature at this point. Promise for the future to solve some \nof the issue coming with 100G networking.\n\n14\n\n\n\nSoftware Support for 100G technology\n\nEDR via Mellanox ConnectX4 Adapter \n - Linux 4.3. Redhat 7.2 \n\nEthernet via Mellanox ConnectX4 Adapter \n - Linux 4.5. Redhat 7.3.\u2028\n (7.2 has only socket layer support). \n\nOmnipath via Intel OPA adapter \n - Out of tree drivers, in Linux 4.4 staging. \n\nCurrently supported via Intel OFED \ndistribution \n\n15\n\n\n\nTest Hardware\no Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz \n\n• Adapters \no Intel Omni-Path Host Fabric Interface Adapter \n\n• Driver Version: 0.11-162 \n• Opa Version: 10.1.1.0.9 \n\no Mellanox ConnectX-4 VPI\xa0Adapter \n• Driver Version: Stock RHEL 7.2 \n• Firmware Version: 12.16.1020 \n\n• Switches \no Intel 100 OPA Edge 48p \n\n• Firmware Version: 10.1.0.0.133 \no Mellanox SB7700 \n\n• Firmware Version: 3.4.3050\n16\n\n\n\nLatency Tests via RDMA APIs(ib_send_lat)\n\n17\n\nTy\npi\nca\nl\tL\nat\nen\n\ncy\n\t(u\n\nse\nc)\n\n0.00\n\n2.75\n\n5.50\n\n8.25\n\n11.00\n\nMsg\tSize\t(bytes)\n\n2 4 8 16 32 64 128 256 512 1024 2048 4096\n\nEDR\nOmnipath\n100GbE\n10GbE\n1GbE\n\n- Consistently low latency below 50% of 1G Ethernet. \n- Higher packet sizes benefit significantly. \n- Omnipath has higher latency due to software processing of send/receive \n\nrequests.\u2028\n\n\n\nBandwidth Tests using RDMA APIs (ib_send_bw)\n\n18\n\nBW\n\ta\nve\nra\nge\n\t(M\n\nB/\nse\nc)\n\n0.00\n\n3000.00\n\n6000.00\n\n9000.00\n\n12000.00\n\nMsg\tSize\t(bytes)\n\n2 4 8 16 32 64 128 256 512 1024 2048 4096\n\nEDR\nOmnipath\n100GbE\n10GbE\n1GbE\n\n- EDR can reach line saturation (~12GB/sec) at ~ 2k packet size \n- Small packet processing is superior on EDR. \n- 10GE (1GB/sec) and 1G (120GB/sec) saturate the line with small packets \n\nearly\n\n\n\nMulticast latency tests\n\n19\n\nLa\nte\nnc\ny\t\n(u\ns)\n\n0\n\n1\n\n2\n\n3\n\n4\n\nEDR Omnipath 100GbE 10GbE 1GbE\n\n- Lowest latency with 100G and 10G Ethernet \n- Slightly higher latency of EDR due to Forward Error Correction \n- Software processing increases packet latency on Omnipath\n\n\n\nRDMA vs. Posix Sockets (30 byte payload)\n\n20\n\nLa\nte\nnc\ny\t\n(u\ns)\n\n0\n\n4.5\n\n9\n\n13.5\n\n18\n\nEDR Omnipath 100GbE 10GbE 1GbE\n\nSocket\nRDMA\n\n\n\nRDMA vs. Posix Sockets (1000 byte Payload)\n\n21\n\nLa\nte\nnc\ny\t\n(u\ns)\n\n0\n\n7.5\n\n15\n\n22.5\n\n30\n\nEDR Omnipath 100GbE 10GbE 1GbE\n\nSockets\nRDMA\n\n\n\nFurther Reading Material\n\nhttp://presentations.interop.com/events/las-vegas/2015/open-to-all---keynote-\npresentations/download/2709 \n\nhttps://en.wikipedia.org/wiki/100_Gigabit_Ethernet \n\nhttp://www.ieee802.org/3/index.html \n\n22\n\nhttp://presentations.interop.com/events/las-vegas/2015/open-to-all---keynote-presentations/download/2709\nhttps://en.wikipedia.org/wiki/100_Gigabit_Ethernet\nhttp://www.ieee802.org/3/index.html\n\n\nMemory Performance issues with 100G\n\n• 100G NIC can give you 12.5G Byte per second of \nthroughput \n\n• DDR3 memory in basic configuration at 6.5 Gbyte \nper sec. High end at 17G byte per second. \n\n• DDR4 12.8G - 19.2G byte per sec. \n\n• Some adapter have dual 100G connectors.  \n\n• Memory via the NIC traffic may be able to saturate \nthe system.\u2028\n\n23\n\n\n\nLooking Ahead\n• 100G is maturing. \n\n• 200G available in 2017/2018. \n\n• Terabit links by 2022. \n\n• Software needs to mature. Especially the \nOS network stack to handle these speeds. \n\n• Issues \no Memory throughput \no Proper APIs \no Deeper integration of cpu/memory/io\n\n24\n\n\n\nQ&A\n\n• Issues \n\n• Getting involved \n\n• How to scale the OS and software \n\n• What impact will this speed have on software \n\n• Contact information\u2028\n  cl@linux.com\n\n25\n\nmailto:cl@linux.com\n\n', 'metadata_resourceName': "b'100G Networking Technology Overview - Slides - Toronto (August 2016).pdf'", 'metadata_pdf:docinfo:modified': '2016-08-21T02:40:43Z', 'metadata_pdf:docinfo:title': '100G Networking Toronto.key'}}, {'_index': 'mongo_index', '_id': 'EP9p5IUB9nynXRNhU8ye', '_score': 1.5127712, '_ignored': ['content.keyword', 'log_entry.keyword'], '_source': {'log_entry': '{"_id"=>BSON::ObjectId(\'63cffa2978b994746c729ce7\'), "content"=>"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbstract Algebra\\n\\n\\nAbstract Algebra\\n\\nTheory and Applications\\n\\n\\n\\n\\n\\nAbstract Algebra\\nTheory and Applications\\n\\nThomas W. Judson\\nStephen F. Austin State University\\n\\nSage Exercises for Abstract Algebra\\nRobert A. Beezer\\n\\nUniversity of Puget Sound\\n\\nAugust 9, 2016\\n\\n\\n\\nWebsite: abstract.pugetsound.edu\\n\\n© 1997–2016 Thomas W. Judson, Robert A. Beezer\\n\\nPermission is granted to copy, distribute and/or modify this document under the terms\\nof the GNU Free Documentation License, Version 1.2 or any later version published by\\nthe Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no\\nBack-Cover Texts. A copy of the license is included in the appendix entitled “GNU Free\\nDocumentation License.”\\n\\nhttp://abstract.pugetsound.edu\\n\\n\\nAcknowledgements\\n\\nI would like to acknowledge the following reviewers for their helpful comments and sugges-\\ntions.\\n\\n• David Anderson, University of Tennessee, Knoxville\\n\\n• Robert Beezer, University of Puget Sound\\n\\n• Myron Hood, California Polytechnic State University\\n\\n• Herbert Kasube, Bradley University\\n\\n• John Kurtzke, University of Portland\\n\\n• Inessa Levi, University of Louisville\\n\\n• Geoffrey Mason, University of California, Santa Cruz\\n\\n• Bruce Mericle, Mankato State University\\n\\n• Kimmo Rosenthal, Union College\\n\\n• Mark Teply, University of Wisconsin\\n\\nI would also like to thank Steve Quigley, Marnie Pommett, Cathie Griffin, Kelle Karshick,\\nand the rest of the staff at PWS Publishing for their guidance throughout this project. It\\nhas been a pleasure to work with them.\\n\\nRobert Beezer encouraged me to make Abstract Algebra: Theory and Applications avail-\\nable as an open source textbook, a decision that I have never regretted. With his assistance,\\nthe book has been rewritten in MathBook XML (http://mathbook.pugetsound.edu), mak-\\ning it possible to quickly output print, web, pdf versions and more from the same source.\\nThe open source version of this book has received support from the National Science Foun-\\ndation (Award #DUE-1020957).\\n\\nv\\n\\nhttp://mathbook.pugetsound.edu\\n\\n\\nPreface\\n\\nThis text is intended for a one or two-semester undergraduate course in abstract algebra.\\nTraditionally, these courses have covered the theoretical aspects of groups, rings, and fields.\\nHowever, with the development of computing in the last several decades, applications that\\ninvolve abstract algebra and discrete mathematics have become increasingly important,\\nand many science, engineering, and computer science students are now electing to minor in\\nmathematics. Though theory still occupies a central role in the subject of abstract algebra\\nand no student should go through such a course without a good notion of what a proof is, the\\nimportance of applications such as coding theory and cryptography has grown significantly.\\n\\nUntil recently most abstract algebra texts included few if any applications. However,\\none of the major problems in teaching an abstract algebra course is that for many students it\\nis their first encounter with an environment that requires them to do rigorous proofs. Such\\nstudents often find it hard to see the use of learning to prove theorems and propositions;\\napplied examples help the instructor provide motivation.\\n\\nThis text contains more material than can possibly be covered in a single semester.\\nCertainly there is adequate material for a two-semester course, and perhaps more; however,\\nfor a one-semester course it would be quite easy to omit selected chapters and still have a\\nuseful text. The order of presentation of topics is standard: groups, then rings, and finally\\nfields. Emphasis can be placed either on theory or on applications. A typical one-semester\\ncourse might cover groups and rings while briefly touching on field theory, using Chapters 1\\nthrough 6, 9, 10, 11, 13 (the first part), 16, 17, 18 (the first part), 20, and 21. Parts of\\nthese chapters could be deleted and applications substituted according to the interests of\\nthe students and the instructor. A two-semester course emphasizing theory might cover\\nChapters 1 through 6, 9, 10, 11, 13 through 18, 20, 21, 22 (the first part), and 23. On\\nthe other hand, if applications are to be emphasized, the course might cover Chapters 1\\nthrough 14, and 16 through 22. In an applied course, some of the more theoretical results\\ncould be assumed or omitted. A chapter dependency chart appears below. (A broken line\\nindicates a partial dependency.)\\n\\nvi\\n\\n\\n\\nvii\\n\\nChapter 23\\n\\nChapter 22\\n\\nChapter 21\\n\\nChapter 18 Chapter 20 Chapter 19\\n\\nChapter 17 Chapter 15\\n\\nChapter 13 Chapter 16 Chapter 12 Chapter 14\\n\\nChapter 11\\n\\nChapter 10\\n\\nChapter 8 Chapter 9 Chapter 7\\n\\nChapters 1–6\\n\\nThough there are no specific prerequisites for a course in abstract algebra, students\\nwho have had other higher-level courses in mathematics will generally be more prepared\\nthan those who have not, because they will possess a bit more mathematical sophistication.\\nOccasionally, we shall assume some basic linear algebra; that is, we shall take for granted an\\nelementary knowledge of matrices and determinants. This should present no great problem,\\nsince most students taking a course in abstract algebra have been introduced to matrices\\nand determinants elsewhere in their career, if they have not already taken a sophomore or\\njunior-level course in linear algebra.\\n\\nExercise sections are the heart of any mathematics text. An exercise set appears at the\\nend of each chapter. The nature of the exercises ranges over several categories; computa-\\ntional, conceptual, and theoretical problems are included. A section presenting hints and\\nsolutions to many of the exercises appears at the end of the text. Often in the solutions\\na proof is only sketched, and it is up to the student to provide the details. The exercises\\nrange in difficulty from very easy to very challenging. Many of the more substantial prob-\\nlems require careful thought, so the student should not be discouraged if the solution is not\\nforthcoming after a few minutes of work.\\n\\nThere are additional exercises or computer projects at the ends of many of the chapters.\\nThe computer projects usually require a knowledge of programming. All of these exercises\\nand projects are more substantial in nature and allow the exploration of new results and\\ntheory.\\n\\nSage (sagemath.org) is a free, open source, software system for advanced mathematics,\\nwhich is ideal for assisting with a study of abstract algebra. Sage can be used either on\\nyour own computer, a local server, or on SageMathCloud (https://cloud.sagemath.com).\\nRobert Beezer has written a comprehensive introduction to Sage and a selection of relevant\\nexercises that appear at the end of each chapter, including live Sage cells in the web version\\n\\nhttp://sagemath.org\\nhttps://cloud.sagemath.com\\n\\n\\nviii\\n\\nof the book. The Sage code has been tested for accuracy with the most recent version\\navailable at this time: Sage Version 7.3 (released 2016-08-04).\\n\\nThomas W. Judson\\nNacogdoches, Texas 2015\\n\\n\\n\\nContents\\n\\nAcknowledgements v\\n\\nPreface vi\\n\\n1 Preliminaries 1\\n1.1 A Short Note on Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Sets and Equivalence Relations . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n1.4 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 15\\n1.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n1.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\n2 The Integers 22\\n2.1 Mathematical Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2.2 The Division Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n2.4 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n\\n3 Groups 36\\n3.1 Integer Equivalence Classes and Symmetries . . . . . . . . . . . . . . . . . . 36\\n3.2 Definitions and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n3.3 Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n3.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n3.5 Additional Exercises: Detecting Errors . . . . . . . . . . . . . . . . . . . . . 50\\n3.6 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 52\\n3.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\n3.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n\\n4 Cyclic Groups 59\\n4.1 Cyclic Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n4.2 Multiplicative Group of Complex Numbers . . . . . . . . . . . . . . . . . . 62\\n4.3 The Method of Repeated Squares . . . . . . . . . . . . . . . . . . . . . . . . 65\\n4.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n4.5 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n4.6 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 70\\n4.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n\\nix\\n\\n\\n\\nx CONTENTS\\n\\n4.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n\\n5 Permutation Groups 80\\n5.1 Definitions and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n5.2 Dihedral Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n5.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\\n5.4 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\\n5.5 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n\\n6 Cosets and Lagrange’s Theorem 101\\n6.1 Cosets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n6.2 Lagrange’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n6.3 Fermat’s and Euler’s Theorems . . . . . . . . . . . . . . . . . . . . . . . . . 104\\n6.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n6.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n6.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\\n\\n7 Introduction to Cryptography 113\\n7.1 Private Key Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\\n7.2 Public Key Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n7.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n7.4 Additional Exercises: Primality and Factoring . . . . . . . . . . . . . . . . . 119\\n7.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 121\\n7.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n7.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n\\n8 Algebraic Coding Theory 126\\n8.1 Error-Detecting and Correcting Codes . . . . . . . . . . . . . . . . . . . . . 126\\n8.2 Linear Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n8.3 Parity-Check and Generator Matrices . . . . . . . . . . . . . . . . . . . . . 135\\n8.4 Efficient Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\\n8.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\\n8.6 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\n8.7 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 147\\n8.8 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\n8.9 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n\\n9 Isomorphisms 152\\n9.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n9.2 Direct Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n9.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n9.4 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\\n9.5 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n\\n10 Normal Subgroups and Factor Groups 168\\n10.1 Factor Groups and Normal Subgroups . . . . . . . . . . . . . . . . . . . . . 168\\n10.2 The Simplicity of the Alternating Group . . . . . . . . . . . . . . . . . . . . 170\\n10.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n10.4 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n10.5 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\\n\\n\\n\\nCONTENTS xi\\n\\n11 Homomorphisms 179\\n11.1 Group Homomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n11.2 The Isomorphism Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\n11.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\\n11.4 Additional Exercises: Automorphisms . . . . . . . . . . . . . . . . . . . . . 185\\n11.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n11.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\\n\\n12 Matrix Groups and Symmetry 192\\n12.1 Matrix Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n12.2 Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\n12.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\n12.4 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 207\\n12.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\n12.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\n\\n13 The Structure of Groups 208\\n13.1 Finite Abelian Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\\n13.2 Solvable Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\\n13.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\\n13.4 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\\n13.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 216\\n13.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\\n13.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\\n\\n14 Group Actions 220\\n14.1 Groups Acting on Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\\n14.2 The Class Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\\n14.3 Burnside’s Counting Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 224\\n14.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\\n14.5 Programming Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\\n14.6 References and Suggested Reading . . . . . . . . . . . . . . . . . . . . . . . 232\\n14.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\\n14.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\\n\\n15 The Sylow Theorems 239\\n15.1 The Sylow Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\\n15.2 Examples and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\\n15.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\\n15.4 A Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\\n15.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 247\\n15.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\n15.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\\n\\n16 Rings 255\\n16.1 Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\\n16.2 Integral Domains and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\\n16.3 Ring Homomorphisms and Ideals . . . . . . . . . . . . . . . . . . . . . . . . 260\\n16.4 Maximal and Prime Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n16.5 An Application to Software Design . . . . . . . . . . . . . . . . . . . . . . . 265\\n16.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\\n\\n\\n\\nxii CONTENTS\\n\\n16.7 Programming Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\\n16.8 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 273\\n16.9 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\\n16.10Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n\\n17 Polynomials 283\\n17.1 Polynomial Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\\n17.2 The Division Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\\n17.3 Irreducible Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\\n17.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\\n17.5 Additional Exercises: Solving the Cubic and Quartic Equations . . . . . . . 296\\n17.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\\n17.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\\n\\n18 Integral Domains 304\\n18.1 Fields of Fractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\\n18.2 Factorization in Integral Domains . . . . . . . . . . . . . . . . . . . . . . . . 307\\n18.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\\n18.4 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 316\\n18.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\\n18.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\\n\\n19 Lattices and Boolean Algebras 320\\n19.1 Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320\\n19.2 Boolean Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\\n19.3 The Algebra of Electrical Circuits . . . . . . . . . . . . . . . . . . . . . . . . 328\\n19.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\\n19.5 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\\n19.6 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 333\\n19.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\\n19.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\\n\\n20 Vector Spaces 340\\n20.1 Definitions and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\\n20.2 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\\n20.3 Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\\n20.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\\n20.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 346\\n20.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\\n20.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\\n\\n21 Fields 354\\n21.1 Extension Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\n21.2 Splitting Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\\n21.3 Geometric Constructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\\n21.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\\n21.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 371\\n21.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n21.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\\n\\n\\n\\nCONTENTS xiii\\n\\n22 Finite Fields 380\\n22.1 Structure of a Finite Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\\n22.2 Polynomial Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\\n22.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390\\n22.4 Additional Exercises: Error Correction for BCH Codes . . . . . . . . . . . . 392\\n22.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 393\\n22.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\\n22.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\\n\\n23 Galois Theory 397\\n23.1 Field Automorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397\\n23.2 The Fundamental Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . 401\\n23.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407\\n23.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\\n23.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 413\\n23.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414\\n23.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\\n\\nA GNU Free Documentation License 428\\n\\nB Hints and Solutions to Selected Exercises 435\\n\\nC Notation 447\\n\\nIndex 451\\n\\n\\n\\nxiv CONTENTS\\n\\n\\n\\n1\\n\\nPreliminaries\\n\\nA certain amount of mathematical maturity is necessary to find and study applications\\nof abstract algebra. A basic knowledge of set theory, mathematical induction, equivalence\\nrelations, and matrices is a must. Even more important is the ability to read and understand\\nmathematical proofs. In this chapter we will outline the background needed for a course in\\nabstract algebra.\\n\\n1.1 A Short Note on Proofs\\nAbstract mathematics is different from other sciences. In laboratory sciences such as chem-\\nistry and physics, scientists perform experiments to discover new principles and verify theo-\\nries. Although mathematics is often motivated by physical experimentation or by computer\\nsimulations, it is made rigorous through the use of logical arguments. In studying abstract\\nmathematics, we take what is called an axiomatic approach; that is, we take a collection\\nof objects S and assume some rules about their structure. These rules are called axioms.\\nUsing the axioms for S, we wish to derive other information about S by using logical argu-\\nments. We require that our axioms be consistent; that is, they should not contradict one\\nanother. We also demand that there not be too many axioms. If a system of axioms is too\\nrestrictive, there will be few examples of the mathematical structure.\\n\\nA statement in logic or mathematics is an assertion that is either true or false. Consider\\nthe following examples:\\n\\n• 3 + 56− 13 + 8/2.\\n\\n• All cats are black.\\n\\n• 2 + 3 = 5.\\n\\n• 2x = 6 exactly when x = 4.\\n\\n• If ax2 + bx+ c = 0 and a ̸= 0, then\\n\\nx =\\n−b±\\n\\n√\\nb2 − 4ac\\n\\n2a\\n.\\n\\n• x3 − 4x2 + 5x− 6.\\n\\nAll but the first and last examples are statements, and must be either true or false.\\nA mathematical proof is nothing more than a convincing argument about the accuracy\\n\\nof a statement. Such an argument should contain enough detail to convince the audience; for\\ninstance, we can see that the statement “2x = 6 exactly when x = 4” is false by evaluating\\n2 · 4 and noting that 6 ̸= 8, an argument that would satisfy anyone. Of course, audiences\\n\\n1\\n\\n\\n\\n2 CHAPTER 1. PRELIMINARIES\\n\\nmay vary widely: proofs can be addressed to another student, to a professor, or to the\\nreader of a text. If more detail than needed is presented in the proof, then the explanation\\nwill be either long-winded or poorly written. If too much detail is omitted, then the proof\\nmay not be convincing. Again it is important to keep the audience in mind. High school\\nstudents require much more detail than do graduate students. A good rule of thumb for an\\nargument in an introductory abstract algebra course is that it should be written to convince\\none’s peers, whether those peers be other students or other readers of the text.\\n\\nLet us examine different types of statements. A statement could be as simple as “10/5 =\\n2;” however, mathematicians are usually interested in more complex statements such as “If\\np, then q,” where p and q are both statements. If certain statements are known or assumed\\nto be true, we wish to know what we can say about other statements. Here p is called\\nthe hypothesis and q is known as the conclusion. Consider the following statement: If\\nax2 + bx+ c = 0 and a ̸= 0, then\\n\\nx =\\n−b±\\n\\n√\\nb2 − 4ac\\n\\n2a\\n.\\n\\nThe hypothesis is ax2 + bx+ c = 0 and a ̸= 0; the conclusion is\\n\\nx =\\n−b±\\n\\n√\\nb2 − 4ac\\n\\n2a\\n.\\n\\nNotice that the statement says nothing about whether or not the hypothesis is true. How-\\never, if this entire statement is true and we can show that ax2 + bx + c = 0 with a ̸= 0 is\\ntrue, then the conclusion must be true. A proof of this statement might simply be a series\\nof equations:\\n\\nax2 + bx+ c = 0\\n\\nx2 +\\nb\\n\\na\\nx = − c\\n\\na\\n\\nx2 +\\nb\\n\\na\\nx+\\n\\n(\\nb\\n\\n2a\\n\\n)2\\n\\n=\\n\\n(\\nb\\n\\n2a\\n\\n)2\\n\\n− c\\n\\na(\\nx+\\n\\nb\\n\\n2a\\n\\n)2\\n\\n=\\nb2 − 4ac\\n\\n4a2\\n\\nx+\\nb\\n\\n2a\\n=\\n\\n±\\n√\\nb2 − 4ac\\n\\n2a\\n\\nx =\\n−b±\\n\\n√\\nb2 − 4ac\\n\\n2a\\n.\\n\\nIf we can prove a statement true, then that statement is called a proposition. A\\nproposition of major importance is called a theorem. Sometimes instead of proving a\\ntheorem or proposition all at once, we break the proof down into modules; that is, we\\nprove several supporting propositions, which are called lemmas, and use the results of\\nthese propositions to prove the main result. If we can prove a proposition or a theorem,\\nwe will often, with very little effort, be able to derive other related propositions called\\ncorollaries.\\n\\nSome Cautions and Suggestions\\nThere are several different strategies for proving propositions. In addition to using different\\nmethods of proof, students often make some common mistakes when they are first learning\\nhow to prove theorems. To aid students who are studying abstract mathematics for the\\n\\n\\n\\n1.2. SETS AND EQUIVALENCE RELATIONS 3\\n\\nfirst time, we list here some of the difficulties that they may encounter and some of the\\nstrategies of proof available to them. It is a good idea to keep referring back to this list as\\na reminder. (Other techniques of proof will become apparent throughout this chapter and\\nthe remainder of the text.)\\n\\n• A theorem cannot be proved by example; however, the standard way to show that a\\nstatement is not a theorem is to provide a counterexample.\\n\\n• Quantifiers are important. Words and phrases such as only, for all, for every, and for\\nsome possess different meanings.\\n\\n• Never assume any hypothesis that is not explicitly stated in the theorem. You cannot\\ntake things for granted.\\n\\n• Suppose you wish to show that an object exists and is unique. First show that there\\nactually is such an object. To show that it is unique, assume that there are two such\\nobjects, say r and s, and then show that r = s.\\n\\n• Sometimes it is easier to prove the contrapositive of a statement. Proving the state-\\nment “If p, then q” is exactly the same as proving the statement “If not q, then not\\np.”\\n\\n• Although it is usually better to find a direct proof of a theorem, this task can some-\\ntimes be difficult. It may be easier to assume that the theorem that you are trying\\nto prove is false, and to hope that in the course of your argument you are forced to\\nmake some statement that cannot possibly be true.\\n\\nRemember that one of the main objectives of higher mathematics is proving theorems.\\nTheorems are tools that make new and productive applications of mathematics possible. We\\nuse examples to give insight into existing theorems and to foster intuitions as to what new\\ntheorems might be true. Applications, examples, and proofs are tightly interconnected—\\nmuch more so than they may seem at first appearance.\\n\\n1.2 Sets and Equivalence Relations\\nSet Theory\\nA set is a well-defined collection of objects; that is, it is defined in such a manner that we\\ncan determine for any given object x whether or not x belongs to the set. The objects that\\nbelong to a set are called its elements or members. We will denote sets by capital letters,\\nsuch as A or X; if a is an element of the set A, we write a ∈ A.\\n\\nA set is usually specified either by listing all of its elements inside a pair of braces or\\nby stating the property that determines whether or not an object x belongs to the set. We\\nmight write\\n\\nX = {x1, x2, . . . , xn}\\n\\nfor a set containing elements x1, x2, . . . , xn or\\n\\nX = {x : x satisfies P}\\n\\nif each x in X satisfies a certain property P. For example, if E is the set of even positive\\nintegers, we can describe E by writing either\\n\\nE = {2, 4, 6, . . .} or E = {x : x is an even integer and x > 0}.\\n\\n\\n\\n4 CHAPTER 1. PRELIMINARIES\\n\\nWe write 2 ∈ E when we want to say that 2 is in the set E, and −3 /∈ E to say that −3 is\\nnot in the set E.\\n\\nSome of the more important sets that we will consider are the following:\\n\\nN = {n : n is a natural number} = {1, 2, 3, . . .};\\nZ = {n : n is an integer} = {. . . ,−1, 0, 1, 2, . . .};\\n\\nQ = {r : r is a rational number} = {p/q : p, q ∈ Z where q ̸= 0};\\nR = {x : x is a real number};\\n\\nC = {z : z is a complex number}.\\n\\nWe can find various relations between sets as well as perform operations on sets. A set\\nA is a subset of B, written A ⊂ B or B ⊃ A, if every element of A is also an element of B.\\nFor example,\\n\\n{4, 5, 8} ⊂ {2, 3, 4, 5, 6, 7, 8, 9}\\n\\nand\\nN ⊂ Z ⊂ Q ⊂ R ⊂ C.\\n\\nTrivially, every set is a subset of itself. A set B is a proper subset of a set A if B ⊂ A but\\nB ̸= A. If A is not a subset of B, we write A ̸⊂ B; for example, {4, 7, 9} ̸⊂ {2, 4, 5, 8, 9}.\\nTwo sets are equal, written A = B, if we can show that A ⊂ B and B ⊂ A.\\n\\nIt is convenient to have a set with no elements in it. This set is called the empty set\\nand is denoted by ∅. Note that the empty set is a subset of every set.\\n\\nTo construct new sets out of old sets, we can perform certain operations: the union\\nA ∪B of two sets A and B is defined as\\n\\nA ∪B = {x : x ∈ A or x ∈ B};\\n\\nthe intersection of A and B is defined by\\n\\nA ∩B = {x : x ∈ A and x ∈ B}.\\n\\nIf A = {1, 3, 5} and B = {1, 2, 3, 9}, then\\n\\nA ∪B = {1, 2, 3, 5, 9} and A ∩B = {1, 3}.\\n\\nWe can consider the union and the intersection of more than two sets. In this case we write\\nn∪\\n\\ni=1\\n\\nAi = A1 ∪ . . . ∪An\\n\\nand\\nn∩\\n\\ni=1\\n\\nAi = A1 ∩ . . . ∩An\\n\\nfor the union and intersection, respectively, of the sets A1, . . . , An.\\nWhen two sets have no elements in common, they are said to be disjoint; for example,\\n\\nif E is the set of even integers and O is the set of odd integers, then E and O are disjoint.\\nTwo sets A and B are disjoint exactly when A ∩B = ∅.\\n\\nSometimes we will work within one fixed set U , called the universal set. For any set\\nA ⊂ U , we define the complement of A, denoted by A′, to be the set\\n\\nA′ = {x : x ∈ U and x /∈ A}.\\n\\n\\n\\n1.2. SETS AND EQUIVALENCE RELATIONS 5\\n\\nWe define the difference of two sets A and B to be\\n\\nA \\\\B = A ∩B′ = {x : x ∈ A and x /∈ B}.\\n\\nExample 1.1. Let R be the universal set and suppose that\\n\\nA = {x ∈ R : 0 < x ≤ 3} and B = {x ∈ R : 2 ≤ x < 4}.\\n\\nThen\\n\\nA ∩B = {x ∈ R : 2 ≤ x ≤ 3}\\nA ∪B = {x ∈ R : 0 < x < 4}\\nA \\\\B = {x ∈ R : 0 < x < 2}\\n\\nA′ = {x ∈ R : x ≤ 0 or x > 3}.\\n\\nProposition 1.2. Let A, B, and C be sets. Then\\n\\n1. A ∪A = A, A ∩A = A, and A \\\\A = ∅;\\n\\n2. A ∪ ∅ = A and A ∩ ∅ = ∅;\\n\\n3. A ∪ (B ∪ C) = (A ∪B) ∪ C and A ∩ (B ∩ C) = (A ∩B) ∩ C;\\n\\n4. A ∪B = B ∪A and A ∩B = B ∩A;\\n\\n5. A ∪ (B ∩ C) = (A ∪B) ∩ (A ∪ C);\\n\\n6. A ∩ (B ∪ C) = (A ∩B) ∪ (A ∩ C).\\n\\nProof. We will prove (1) and (3) and leave the remaining results to be proven in the\\nexercises.\\n\\n(1) Observe that\\n\\nA ∪A = {x : x ∈ A or x ∈ A}\\n= {x : x ∈ A}\\n= A\\n\\nand\\n\\nA ∩A = {x : x ∈ A and x ∈ A}\\n= {x : x ∈ A}\\n= A.\\n\\nAlso, A \\\\A = A ∩A′ = ∅.\\n(3) For sets A, B, and C,\\n\\nA ∪ (B ∪ C) = A ∪ {x : x ∈ B or x ∈ C}\\n= {x : x ∈ A or x ∈ B, or x ∈ C}\\n= {x : x ∈ A or x ∈ B} ∪ C\\n= (A ∪B) ∪ C.\\n\\nA similar argument proves that A ∩ (B ∩ C) = (A ∩B) ∩ C.\\n\\nTheorem 1.3 (De Morgan’s Laws). Let A and B be sets. Then\\n\\n\\n\\n6 CHAPTER 1. PRELIMINARIES\\n\\n1. (A ∪B)′ = A′ ∩B′;\\n\\n2. (A ∩B)′ = A′ ∪B′.\\n\\nProof. (1) If A∪B = ∅, then the theorem follows immediately since both A and B are the\\nempty set. Otherwise, we must show that (A ∪B)′ ⊂ A′ ∩B′ and (A ∪B)′ ⊃ A′ ∩B′. Let\\nx ∈ (A∪B)′. Then x /∈ A∪B. So x is neither in A nor in B, by the definition of the union\\nof sets. By the definition of the complement, x ∈ A′ and x ∈ B′. Therefore, x ∈ A′ ∩ B′\\n\\nand we have (A ∪B)′ ⊂ A′ ∩B′.\\nTo show the reverse inclusion, suppose that x ∈ A′ ∩B′. Then x ∈ A′ and x ∈ B′, and\\n\\nso x /∈ A and x /∈ B. Thus x /∈ A ∪B and so x ∈ (A ∪B)′. Hence, (A ∪B)′ ⊃ A′ ∩B′ and\\nso (A ∪B)′ = A′ ∩B′.\\n\\nThe proof of (2) is left as an exercise.\\n\\nExample 1.4. Other relations between sets often hold true. For example,\\n\\n(A \\\\B) ∩ (B \\\\A) = ∅.\\n\\nTo see that this is true, observe that\\n\\n(A \\\\B) ∩ (B \\\\A) = (A ∩B′) ∩ (B ∩A′)\\n\\n= A ∩A′ ∩B ∩B′\\n\\n= ∅.\\n\\nCartesian Products and Mappings\\nGiven sets A and B, we can define a new set A × B, called the Cartesian product of A\\nand B, as a set of ordered pairs. That is,\\n\\nA×B = {(a, b) : a ∈ A and b ∈ B}.\\n\\nExample 1.5. If A = {x, y}, B = {1, 2, 3}, and C = ∅, then A×B is the set\\n\\n{(x, 1), (x, 2), (x, 3), (y, 1), (y, 2), (y, 3)}\\n\\nand\\nA× C = ∅.\\n\\nWe define the Cartesian product of n sets to be\\n\\nA1 × · · · ×An = {(a1, . . . , an) : ai ∈ Ai for i = 1, . . . , n}.\\n\\nIf A = A1 = A2 = · · · = An, we often write An for A× · · · × A (where A would be written\\nn times). For example, the set R3 consists of all of 3-tuples of real numbers.\\n\\nSubsets of A×B are called relations. We will define a mapping or function f ⊂ A×B\\nfrom a set A to a set B to be the special type of relation where (a, b) ∈ f if for every element\\na ∈ A there exists a unique element b ∈ B. Another way of saying this is that for every\\nelement in A, f assigns a unique element in B. We usually write f : A → B or A f→ B.\\nInstead of writing down ordered pairs (a, b) ∈ A× B, we write f(a) = b or f : a 7→ b. The\\nset A is called the domain of f and\\n\\nf(A) = {f(a) : a ∈ A} ⊂ B\\n\\nis called the range or image of f . We can think of the elements in the function’s domain\\nas input values and the elements in the function’s range as output values.\\n\\n\\n\\n1.2. SETS AND EQUIVALENCE RELATIONS 7\\n\\n1\\n\\n2\\n\\n3\\n\\na\\n\\nb\\n\\nc\\n\\n1\\n\\n2\\n\\n3\\n\\na\\n\\nb\\n\\nc\\n\\nA B\\n\\nA Bg\\n\\nf\\n\\nFigure 1.6: Mappings and relations\\n\\nExample 1.7. Suppose A = {1, 2, 3} and B = {a, b, c}. In Figure 1.6 we define relations f\\nand g from A to B. The relation f is a mapping, but g is not because 1 ∈ A is not assigned\\nto a unique element in B; that is, g(1) = a and g(1) = b.\\n\\nGiven a function f : A → B, it is often possible to write a list describing what the\\nfunction does to each specific element in the domain. However, not all functions can be\\ndescribed in this manner. For example, the function f : R → R that sends each real number\\nto its cube is a mapping that must be described by writing f(x) = x3 or f : x 7→ x3.\\n\\nConsider the relation f : Q → Z given by f(p/q) = p. We know that 1/2 = 2/4, but\\nis f(1/2) = 1 or 2? This relation cannot be a mapping because it is not well-defined. A\\nrelation is well-defined if each element in the domain is assigned to a unique element in\\nthe range.\\n\\nIf f : A→ B is a map and the image of f is B, i.e., f(A) = B, then f is said to be onto\\nor surjective. In other words, if there exists an a ∈ A for each b ∈ B such that f(a) = b,\\nthen f is onto. A map is one-to-one or injective if a1 ̸= a2 implies f(a1) ̸= f(a2).\\nEquivalently, a function is one-to-one if f(a1) = f(a2) implies a1 = a2. A map that is both\\none-to-one and onto is called bijective.\\n\\nExample 1.8. Let f : Z → Q be defined by f(n) = n/1. Then f is one-to-one but not\\nonto. Define g : Q → Z by g(p/q) = p where p/q is a rational number expressed in its lowest\\nterms with a positive denominator. The function g is onto but not one-to-one.\\n\\nGiven two functions, we can construct a new function by using the range of the first\\nfunction as the domain of the second function. Let f : A→ B and g : B → C be mappings.\\nDefine a new map, the composition of f and g from A to C, by (g ◦ f)(x) = g(f(x)).\\n\\n\\n\\n8 CHAPTER 1. PRELIMINARIES\\n\\nA B C\\n\\n1\\n\\n2\\n\\n3\\n\\na\\n\\nb\\n\\nc\\n\\nX\\n\\nY\\n\\nZ\\n\\nf g\\n\\nA C\\n\\n1\\n\\n2\\n\\n3\\n\\nX\\n\\nY\\n\\nZ\\n\\ng ◦ f\\n\\nFigure 1.9: Composition of maps\\n\\nExample 1.10. Consider the functions f : A → B and g : B → C that are defined in\\nFigure 1.9 (top). The composition of these functions, g ◦f : A→ C, is defined in Figure 1.9\\n(bottom).\\n\\nExample 1.11. Let f(x) = x2 and g(x) = 2x+ 5. Then\\n\\n(f ◦ g)(x) = f(g(x)) = (2x+ 5)2 = 4x2 + 20x+ 25\\n\\nand\\n(g ◦ f)(x) = g(f(x)) = 2x2 + 5.\\n\\nIn general, order makes a difference; that is, in most cases f ◦ g ̸= g ◦ f .\\n\\nExample 1.12. Sometimes it is the case that f ◦ g = g ◦ f . Let f(x) = x3 and g(x) = 3\\n√\\nx.\\n\\nThen\\n(f ◦ g)(x) = f(g(x)) = f( 3\\n\\n√\\nx ) = ( 3\\n\\n√\\nx )3 = x\\n\\nand\\n(g ◦ f)(x) = g(f(x)) = g(x3) =\\n\\n3\\n√\\nx3 = x.\\n\\nExample 1.13. Given a 2× 2 matrix\\n\\nA =\\n\\n(\\na b\\n\\nc d\\n\\n)\\n,\\n\\nwe can define a map TA : R2 → R2 by\\n\\nTA(x, y) = (ax+ by, cx+ dy)\\n\\nfor (x, y) in R2. This is actually matrix multiplication; that is,(\\na b\\n\\nc d\\n\\n)(\\nx\\n\\ny\\n\\n)\\n=\\n\\n(\\nax+ by\\n\\ncx+ dy\\n\\n)\\n.\\n\\nMaps from Rn to Rm given by matrices are called linear maps or linear transforma-\\ntions.\\n\\n\\n\\n1.2. SETS AND EQUIVALENCE RELATIONS 9\\n\\nExample 1.14. Suppose that S = {1, 2, 3}. Define a map π : S → S by\\n\\nπ(1) = 2, π(2) = 1, π(3) = 3.\\n\\nThis is a bijective map. An alternative way to write π is(\\n1 2 3\\n\\nπ(1) π(2) π(3)\\n\\n)\\n=\\n\\n(\\n1 2 3\\n\\n2 1 3\\n\\n)\\n.\\n\\nFor any set S, a one-to-one and onto mapping π : S → S is called a permutation of S.\\n\\nTheorem 1.15. Let f : A→ B, g : B → C, and h : C → D. Then\\n\\n1. The composition of mappings is associative; that is, (h ◦ g) ◦ f = h ◦ (g ◦ f);\\n\\n2. If f and g are both one-to-one, then the mapping g ◦ f is one-to-one;\\n\\n3. If f and g are both onto, then the mapping g ◦ f is onto;\\n\\n4. If f and g are bijective, then so is g ◦ f .\\n\\nProof. We will prove (1) and (3). Part (2) is left as an exercise. Part (4) follows directly\\nfrom (2) and (3).\\n\\n(1) We must show that\\nh ◦ (g ◦ f) = (h ◦ g) ◦ f.\\n\\nFor a ∈ A we have\\n\\n(h ◦ (g ◦ f))(a) = h((g ◦ f)(a))\\n= h(g(f(a)))\\n\\n= (h ◦ g)(f(a))\\n= ((h ◦ g) ◦ f)(a).\\n\\n(3) Assume that f and g are both onto functions. Given c ∈ C, we must show that\\nthere exists an a ∈ A such that (g ◦ f)(a) = g(f(a)) = c. However, since g is onto, there\\nis an element b ∈ B such that g(b) = c. Similarly, there is an a ∈ A such that f(a) = b.\\nAccordingly,\\n\\n(g ◦ f)(a) = g(f(a)) = g(b) = c.\\n\\nIf S is any set, we will use idS or id to denote the identity mapping from S to itself.\\nDefine this map by id(s) = s for all s ∈ S. A map g : B → A is an inverse mapping\\nof f : A → B if g ◦ f = idA and f ◦ g = idB; in other words, the inverse function of a\\nfunction simply “undoes” the function. A map is said to be invertible if it has an inverse.\\nWe usually write f−1 for the inverse of f .\\n\\nExample 1.16. The function f(x) = x3 has inverse f−1(x) = 3\\n√\\nx by Example 1.12.\\n\\nExample 1.17. The natural logarithm and the exponential functions, f(x) = lnx and\\nf−1(x) = ex, are inverses of each other provided that we are careful about choosing domains.\\nObserve that\\n\\nf(f−1(x)) = f(ex) = ln ex = x\\n\\nand\\nf−1(f(x)) = f−1(lnx) = elnx = x\\n\\nwhenever composition makes sense.\\n\\n\\n\\n10 CHAPTER 1. PRELIMINARIES\\n\\nExample 1.18. Suppose that\\n\\nA =\\n\\n(\\n3 1\\n\\n5 2\\n\\n)\\n.\\n\\nThen A defines a map from R2 to R2 by\\n\\nTA(x, y) = (3x+ y, 5x+ 2y).\\n\\nWe can find an inverse map of TA by simply inverting the matrix A; that is, T−1\\nA = TA−1 .\\n\\nIn this example,\\n\\nA−1 =\\n\\n(\\n2 −1\\n\\n−5 3\\n\\n)\\n;\\n\\nhence, the inverse map is given by\\n\\nT−1\\nA (x, y) = (2x− y,−5x+ 3y).\\n\\nIt is easy to check that\\n\\nT−1\\nA ◦ TA(x, y) = TA ◦ T−1\\n\\nA (x, y) = (x, y).\\n\\nNot every map has an inverse. If we consider the map\\n\\nTB(x, y) = (3x, 0)\\n\\ngiven by the matrix\\n\\nB =\\n\\n(\\n3 0\\n\\n0 0\\n\\n)\\n,\\n\\nthen an inverse map would have to be of the form\\n\\nT−1\\nB (x, y) = (ax+ by, cx+ dy)\\n\\nand\\n(x, y) = T ◦ T−1\\n\\nB (x, y) = (3ax+ 3by, 0)\\n\\nfor all x and y. Clearly this is impossible because y might not be 0.\\n\\nExample 1.19. Given the permutation\\n\\nπ =\\n\\n(\\n1 2 3\\n\\n2 3 1\\n\\n)\\non S = {1, 2, 3}, it is easy to see that the permutation defined by\\n\\nπ−1 =\\n\\n(\\n1 2 3\\n\\n3 1 2\\n\\n)\\nis the inverse of π. In fact, any bijective mapping possesses an inverse, as we will see in the\\nnext theorem.\\n\\nTheorem 1.20. A mapping is invertible if and only if it is both one-to-one and onto.\\n\\nProof. Suppose first that f : A → B is invertible with inverse g : B → A. Then g ◦ f =\\nidA is the identity map; that is, g(f(a)) = a. If a1, a2 ∈ A with f(a1) = f(a2), then\\na1 = g(f(a1)) = g(f(a2)) = a2. Consequently, f is one-to-one. Now suppose that b ∈ B.\\nTo show that f is onto, it is necessary to find an a ∈ A such that f(a) = b, but f(g(b)) = b\\nwith g(b) ∈ A. Let a = g(b).\\n\\nConversely, let f be bijective and let b ∈ B. Since f is onto, there exists an a ∈ A such\\nthat f(a) = b. Because f is one-to-one, a must be unique. Define g by letting g(b) = a. We\\nhave now constructed the inverse of f .\\n\\n\\n\\n1.2. SETS AND EQUIVALENCE RELATIONS 11\\n\\nEquivalence Relations and Partitions\\nA fundamental notion in mathematics is that of equality. We can generalize equality with\\nequivalence relations and equivalence classes. An equivalence relation on a set X is a\\nrelation R ⊂ X ×X such that\\n\\n• (x, x) ∈ R for all x ∈ X (reflexive property);\\n\\n• (x, y) ∈ R implies (y, x) ∈ R (symmetric property);\\n\\n• (x, y) and (y, z) ∈ R imply (x, z) ∈ R (transitive property).\\n\\nGiven an equivalence relation R on a set X, we usually write x ∼ y instead of (x, y) ∈ R.\\nIf the equivalence relation already has an associated notation such as =, ≡, or ∼=, we will\\nuse that notation.\\n\\nExample 1.21. Let p, q, r, and s be integers, where q and s are nonzero. Define p/q ∼ r/s\\nif ps = qr. Clearly ∼ is reflexive and symmetric. To show that it is also transitive, suppose\\nthat p/q ∼ r/s and r/s ∼ t/u, with q, s, and u all nonzero. Then ps = qr and ru = st.\\nTherefore,\\n\\npsu = qru = qst.\\n\\nSince s ̸= 0, pu = qt. Consequently, p/q ∼ t/u.\\n\\nExample 1.22. Suppose that f and g are differentiable functions on R. We can define an\\nequivalence relation on such functions by letting f(x) ∼ g(x) if f ′(x) = g′(x). It is clear that\\n∼ is both reflexive and symmetric. To demonstrate transitivity, suppose that f(x) ∼ g(x)\\nand g(x) ∼ h(x). From calculus we know that f(x)−g(x) = c1 and g(x)−h(x) = c2, where\\nc1 and c2 are both constants. Hence,\\n\\nf(x)− h(x) = (f(x)− g(x)) + (g(x)− h(x)) = c1 − c2\\n\\nand f ′(x)− h′(x) = 0. Therefore, f(x) ∼ h(x).\\n\\nExample 1.23. For (x1, y1) and (x2, y2) in R2, define (x1, y1) ∼ (x2, y2) if x21+y21 = x22+y\\n2\\n2.\\n\\nThen ∼ is an equivalence relation on R2.\\n\\nExample 1.24. Let A and B be 2× 2 matrices with entries in the real numbers. We can\\ndefine an equivalence relation on the set of 2× 2 matrices, by saying A ∼ B if there exists\\nan invertible matrix P such that PAP−1 = B. For example, if\\n\\nA =\\n\\n(\\n1 2\\n\\n−1 1\\n\\n)\\nand B =\\n\\n(\\n−18 33\\n\\n−11 20\\n\\n)\\n,\\n\\nthen A ∼ B since PAP−1 = B for\\n\\nP =\\n\\n(\\n2 5\\n\\n1 3\\n\\n)\\n.\\n\\nLet I be the 2× 2 identity matrix; that is,\\n\\nI =\\n\\n(\\n1 0\\n\\n0 1\\n\\n)\\n.\\n\\nThen IAI−1 = IAI = A; therefore, the relation is reflexive. To show symmetry, suppose\\nthat A ∼ B. Then there exists an invertible matrix P such that PAP−1 = B. So\\n\\nA = P−1BP = P−1B(P−1)−1.\\n\\n\\n\\n12 CHAPTER 1. PRELIMINARIES\\n\\nFinally, suppose that A ∼ B and B ∼ C. Then there exist invertible matrices P and Q\\nsuch that PAP−1 = B and QBQ−1 = C. Since\\n\\nC = QBQ−1 = QPAP−1Q−1 = (QP )A(QP )−1,\\n\\nthe relation is transitive. Two matrices that are equivalent in this manner are said to be\\nsimilar.\\n\\nA partition P of a set X is a collection of nonempty sets X1, X2, . . . such that Xi∩Xj =\\n∅ for i ̸= j and\\n\\n∪\\nkXk = X. Let ∼ be an equivalence relation on a set X and let x ∈ X.\\n\\nThen [x] = {y ∈ X : y ∼ x} is called the equivalence class of x. We will see that\\nan equivalence relation gives rise to a partition via equivalence classes. Also, whenever\\na partition of a set exists, there is some natural underlying equivalence relation, as the\\nfollowing theorem demonstrates.\\n\\nTheorem 1.25. Given an equivalence relation ∼ on a set X, the equivalence classes of X\\nform a partition of X. Conversely, if P = {Xi} is a partition of a set X, then there is an\\nequivalence relation on X with equivalence classes Xi.\\n\\nProof. Suppose there exists an equivalence relation ∼ on the set X. For any x ∈ X, the\\nreflexive property shows that x ∈ [x] and so [x] is nonempty. Clearly X =\\n\\n∪\\nx∈X [x]. Now\\n\\nlet x, y ∈ X. We need to show that either [x] = [y] or [x] ∩ [y] = ∅. Suppose that the\\nintersection of [x] and [y] is not empty and that z ∈ [x] ∩ [y]. Then z ∼ x and z ∼ y. By\\nsymmetry and transitivity x ∼ y; hence, [x] ⊂ [y]. Similarly, [y] ⊂ [x] and so [x] = [y].\\nTherefore, any two equivalence classes are either disjoint or exactly the same.\\n\\nConversely, suppose that P = {Xi} is a partition of a set X. Let two elements be\\nequivalent if they are in the same partition. Clearly, the relation is reflexive. If x is in the\\nsame partition as y, then y is in the same partition as x, so x ∼ y implies y ∼ x. Finally,\\nif x is in the same partition as y and y is in the same partition as z, then x must be in the\\nsame partition as z, and transitivity holds.\\n\\nCorollary 1.26. Two equivalence classes of an equivalence relation are either disjoint or\\nequal.\\n\\nLet us examine some of the partitions given by the equivalence classes in the last set of\\nexamples.\\n\\nExample 1.27. In the equivalence relation in Example 1.21, two pairs of integers, (p, q)\\nand (r, s), are in the same equivalence class when they reduce to the same fraction in its\\nlowest terms.\\n\\nExample 1.28. In the equivalence relation in Example 1.22, two functions f(x) and g(x)\\nare in the same partition when they differ by a constant.\\n\\nExample 1.29. We defined an equivalence class on R2 by (x1, y1) ∼ (x2, y2) if x21 + y21 =\\nx22 + y22. Two pairs of real numbers are in the same partition when they lie on the same\\ncircle about the origin.\\n\\nExample 1.30. Let r and s be two integers and suppose that n ∈ N. We say that r is\\ncongruent to s modulo n, or r is congruent to s mod n, if r − s is evenly divisible by n;\\nthat is, r − s = nk for some k ∈ Z. In this case we write r ≡ s (mod n). For example,\\n41 ≡ 17 (mod 8) since 41 − 17 = 24 is divisible by 8. We claim that congruence modulo\\nn forms an equivalence relation of Z. Certainly any integer r is equivalent to itself since\\nr − r = 0 is divisible by n. We will now show that the relation is symmetric. If r ≡ s\\n\\n\\n\\n1.3. EXERCISES 13\\n\\n(mod n), then r − s = −(s − r) is divisible by n. So s − r is divisible by n and s ≡ r\\n(mod n). Now suppose that r ≡ s (mod n) and s ≡ t (mod n). Then there exist integers\\nk and l such that r − s = kn and s− t = ln. To show transitivity, it is necessary to prove\\nthat r − t is divisible by n. However,\\n\\nr − t = r − s+ s− t = kn+ ln = (k + l)n,\\n\\nand so r − t is divisible by n.\\nIf we consider the equivalence relation established by the integers modulo 3, then\\n\\n[0] = {. . . ,−3, 0, 3, 6, . . .},\\n[1] = {. . . ,−2, 1, 4, 7, . . .},\\n[2] = {. . . ,−1, 2, 5, 8, . . .}.\\n\\nNotice that [0] ∪ [1] ∪ [2] = Z and also that the sets are disjoint. The sets [0], [1], and [2]\\nform a partition of the integers.\\n\\nThe integers modulo n are a very important example in the study of abstract algebra\\nand will become quite useful in our investigation of various algebraic structures such as\\ngroups and rings. In our discussion of the integers modulo n we have actually assumed a\\nresult known as the division algorithm, which will be stated and proved in Chapter 2.\\n\\n1.3 Exercises\\n1. Suppose that\\n\\nA = {x : x ∈ N and x is even},\\nB = {x : x ∈ N and x is prime},\\nC = {x : x ∈ N and x is a multiple of 5}.\\n\\nDescribe each of the following sets.\\n\\n(a) A ∩B\\n(b) B ∩ C\\n\\n(c) A ∪B\\n(d) A ∩ (B ∪ C)\\n\\n2. If A = {a, b, c}, B = {1, 2, 3}, C = {x}, and D = ∅, list all of the elements in each of the\\nfollowing sets.\\n\\n(a) A×B\\n\\n(b) B ×A\\n\\n(c) A×B × C\\n\\n(d) A×D\\n\\n3. Find an example of two nonempty sets A and B for which A×B = B ×A is true.\\n\\n4. Prove A ∪ ∅ = A and A ∩ ∅ = ∅.\\n\\n5. Prove A ∪B = B ∪A and A ∩B = B ∩A.\\n\\n6. Prove A ∪ (B ∩ C) = (A ∪B) ∩ (A ∪ C).\\n\\n7. Prove A ∩ (B ∪ C) = (A ∩B) ∪ (A ∩ C).\\n\\n8. Prove A ⊂ B if and only if A ∩B = A.\\n\\n\\n\\n14 CHAPTER 1. PRELIMINARIES\\n\\n9. Prove (A ∩B)′ = A′ ∪B′.\\n\\n10. Prove A ∪B = (A ∩B) ∪ (A \\\\B) ∪ (B \\\\A).\\n\\n11. Prove (A ∪B)× C = (A× C) ∪ (B × C).\\n\\n12. Prove (A ∩B) \\\\B = ∅.\\n\\n13. Prove (A ∪B) \\\\B = A \\\\B.\\n\\n14. Prove A \\\\ (B ∪ C) = (A \\\\B) ∩ (A \\\\ C).\\n\\n15. Prove A ∩ (B \\\\ C) = (A ∩B) \\\\ (A ∩ C).\\n\\n16. Prove (A \\\\B) ∪ (B \\\\A) = (A ∪B) \\\\ (A ∩B).\\n\\n17. Which of the following relations f : Q → Q define a mapping? In each case, supply a\\nreason why f is or is not a mapping.\\n\\n(a) f(p/q) =\\np+ 1\\n\\np− 2\\n\\n(b) f(p/q) =\\n3p\\n\\n3q\\n\\n(c) f(p/q) =\\np+ q\\n\\nq2\\n\\n(d) f(p/q) =\\n3p2\\n\\n7q2\\n− p\\n\\nq\\n\\n18. Determine which of the following functions are one-to-one and which are onto. If the\\nfunction is not onto, determine its range.\\n(a) f : R → R defined by f(x) = ex\\n\\n(b) f : Z → Z defined by f(n) = n2 + 3\\n\\n(c) f : R → R defined by f(x) = sinx\\n(d) f : Z → Z defined by f(x) = x2\\n\\n19. Let f : A→ B and g : B → C be invertible mappings; that is, mappings such that f−1\\n\\nand g−1 exist. Show that (g ◦ f)−1 = f−1 ◦ g−1.\\n\\n20.\\n(a) Define a function f : N → N that is one-to-one but not onto.\\n(b) Define a function f : N → N that is onto but not one-to-one.\\n\\n21. Prove the relation defined on R2 by (x1, y1) ∼ (x2, y2) if x21 + y21 = x22 + y22 is an\\nequivalence relation.\\n\\n22. Let f : A→ B and g : B → C be maps.\\n(a) If f and g are both one-to-one functions, show that g ◦ f is one-to-one.\\n(b) If g ◦ f is onto, show that g is onto.\\n(c) If g ◦ f is one-to-one, show that f is one-to-one.\\n(d) If g ◦ f is one-to-one and f is onto, show that g is one-to-one.\\n(e) If g ◦ f is onto and g is one-to-one, show that f is onto.\\n\\n23. Define a function on the real numbers by\\n\\nf(x) =\\nx+ 1\\n\\nx− 1\\n.\\n\\nWhat are the domain and range of f? What is the inverse of f? Compute f ◦ f−1 and\\nf−1 ◦ f .\\n\\n\\n\\n1.4. REFERENCES AND SUGGESTED READINGS 15\\n\\n24. Let f : X → Y be a map with A1, A2 ⊂ X and B1, B2 ⊂ Y .\\n(a) Prove f(A1 ∪A2) = f(A1) ∪ f(A2).\\n(b) Prove f(A1 ∩A2) ⊂ f(A1) ∩ f(A2). Give an example in which equality fails.\\n(c) Prove f−1(B1 ∪B2) = f−1(B1) ∪ f−1(B2), where\\n\\nf−1(B) = {x ∈ X : f(x) ∈ B}.\\n\\n(d) Prove f−1(B1 ∩B2) = f−1(B1) ∩ f−1(B2).\\n(e) Prove f−1(Y \\\\B1) = X \\\\ f−1(B1).\\n\\n25. Determine whether or not the following relations are equivalence relations on the given\\nset. If the relation is an equivalence relation, describe the partition given by it. If the\\nrelation is not an equivalence relation, state why it fails to be one.\\n\\n(a) x ∼ y in R if x ≥ y\\n\\n(b) m ∼ n in Z if mn > 0\\n\\n(c) x ∼ y in R if |x− y| ≤ 4\\n\\n(d) m ∼ n in Z if m ≡ n (mod 6)\\n\\n26. Define a relation ∼ on R2 by stating that (a, b) ∼ (c, d) if and only if a2 + b2 ≤ c2 + d2.\\nShow that ∼ is reflexive and transitive but not symmetric.\\n\\n27. Show that an m× n matrix gives rise to a well-defined map from Rn to Rm.\\n\\n28. Find the error in the following argument by providing a counterexample. “The reflexive\\nproperty is redundant in the axioms for an equivalence relation. If x ∼ y, then y ∼ x by\\nthe symmetric property. Using the transitive property, we can deduce that x ∼ x.”\\n\\n29. (Projective Real Line) Define a relation on R2 \\\\ {(0, 0)} by letting (x1, y1) ∼ (x2, y2) if\\nthere exists a nonzero real number λ such that (x1, y1) = (λx2, λy2). Prove that ∼ defines\\nan equivalence relation on R2 \\\\(0, 0). What are the corresponding equivalence classes? This\\nequivalence relation defines the projective line, denoted by P(R), which is very important\\nin geometry.\\n\\n1.4 References and Suggested Readings\\n[1] Artin, M. Abstract Algebra. 2nd ed. Pearson, Upper Saddle River, NJ, 2011.\\n[2] Childs, L. A Concrete Introduction to Higher Algebra. 2nd ed. Springer-Verlag, New\\n\\nYork, 1995.\\n[3] Dummit, D. and Foote, R. Abstract Algebra. 3rd ed. Wiley, New York, 2003.\\n[4] Ehrlich, G. Fundamental Concepts of Algebra. PWS-KENT, Boston, 1991.\\n[5] Fraleigh, J. B. A First Course in Abstract Algebra. 7th ed. Pearson, Upper Saddle\\n\\nRiver, NJ, 2003.\\n[6] Gallian, J. A. Contemporary Abstract Algebra. 7th ed. Brooks/Cole, Belmont, CA,\\n\\n2009.\\n[7] Halmos, P. Naive Set Theory. Springer, New York, 1991. One of the best references\\n\\nfor set theory.\\n[8] Herstein, I. N. Abstract Algebra. 3rd ed. Wiley, New York, 1996.\\n[9] Hungerford, T. W. Algebra. Springer, New York, 1974. One of the standard graduate\\n\\nalgebra texts.\\n\\n\\n\\n16 CHAPTER 1. PRELIMINARIES\\n\\n[10] Lang, S. Algebra. 3rd ed. Springer, New York, 2002. Another standard graduate text.\\n[11] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\\n[12] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\\n[13] Nickelson, W. K. Introduction to Abstract Algebra. 3rd ed. Wiley, New York, 2006.\\n[14] Solow, D. How to Read and Do Proofs. 5th ed. Wiley, New York, 2009.\\n[15] van der Waerden, B. L. A History of Algebra. Springer-Verlag, New York, 1985. An\\n\\naccount of the historical development of algebra.\\n\\n1.5 Sage\\nSage is a powerful system for studying and exploring many different areas of mathematics.\\nIn this textbook, you will study a variety of algebraic structures, such as groups, rings and\\nfields. Sage does an excellent job of implementing many features of these objects as we will\\nsee in the chapters ahead. But here and now, in this initial chapter, we will concentrate on\\na few general ways of getting the most out of working with Sage.\\n\\nYou may use Sage several different ways. It may be used as a command-line program\\nwhen installed on your own computer. Or it might be a web application such as the\\nSageMathCloud. Our writing will assume that you are reading this as a worksheet within\\nthe Sage Notebook (a web browser interface), or this is a section of the entire book presented\\nas web pages, and you are employing the Sage Cell Server via those pages. After the first\\nfew chapters the explanations should work equally well for whatever vehicle you use to\\nexecute Sage commands.\\n\\nExecuting Sage Commands\\n\\nMost of your interaction will be by typing commands into a compute cell. If you are reading\\nthis in the Sage Notebook or as a webpage version of the book, then you will see a compute\\ncell just below this paragraph. Click once inside the compute cell and if you are in the Sage\\nNotebook, you will get a more distinctive border around it, a blinking cursor inside, plus a\\ncute little “evaluate” link below.\\n\\nAt the cursor, type 2+2 and then click on the evaluate link. Did a 4 appear below the\\ncell? If so, you have successfully sent a command off for Sage to evaluate and you have\\nreceived back the (correct) answer.\\n\\nHere is another compute cell. Try evaluating the command factorial(300) here.\\nHmmmmm. That is quite a big integer! If you see slashes at the end of each line, this\\n\\nmeans the result is continued onto the next line, since there are 615 total digits in the result.\\nTo make new compute cells in the Sage Notebook (only), hover your mouse just above\\n\\nanother compute cell, or just below some output from a compute cell. When you see a\\nskinny blue bar across the width of your worksheet, click and you will open up a new\\ncompute cell, ready for input. Note that your worksheet will remember any calculations\\nyou make, in the order you make them, no matter where you put the cells, so it is best to\\nstay organized and add new cells at the bottom.\\n\\nTry placing your cursor just below the monstrous value of 300! that you have. Click on\\nthe blue bar and try another factorial computation in the new compute cell.\\n\\nEach compute cell will show output due to only the very last command in the cell. Try\\nto predict the following output before evaluating the cell.\\n\\na = 10\\nb = 6\\n\\n\\n\\n1.5. SAGE 17\\n\\nb = b - 10\\na = a + 20\\na\\n\\n30\\n\\nThe following compute cell will not print anything since the one command does not\\ncreate output. But it will have an effect, as you can see when you execute the subsequent\\ncell. Notice how this uses the value of b from above. Execute this compute cell once.\\nExactly once. Even if it appears to do nothing. If you execute the cell twice, your credit\\ncard may be charged twice.\\n\\nb = b + 50\\n\\nNow execute this cell, which will produce some output.\\nb + 20\\n\\n66\\n\\nSo b came into existence as 6. We subtracted 10 immediately afterward. Then a subse-\\nquent cell added 50. This assumes you executed this cell exactly once! In the last cell we\\ncreate b+20 (but do not save it) and it is this value (66) that is output, while b is still 46.\\n\\nYou can combine several commands on one line with a semi-colon. This is a great way\\nto get multiple outputs from a compute cell. The syntax for building a matrix should be\\nsomewhat obvious when you see the output, but if not, it is not particularly important to\\nunderstand now.\\n\\nA = matrix ([[3, 1], [5 ,2]]); A\\n\\n[3 1]\\n[5 2]\\n\\nprint A; print ; A.inverse ()\\n\\n[3 1]\\n[5 2]\\n<BLANKLINE >\\n[ 2 -1]\\n[-5 3]\\n\\nImmediate Help\\nSome commands in Sage are “functions,” an example is factorial() above. Other commands\\nare “methods” of an object and are like characteristics of objects, an example is .inverse()\\n\\nas a method of a matrix. Once you know how to create an object (such as a matrix), then\\nit is easy to see all the available methods. Write the name of the object, place a period\\n(“dot”) and hit the TAB key. If you have A defined from above, then the compute cell below\\nis ready to go, click into it and then hit TAB (not “evaluate”!). You should get a long list\\nof possible methods.\\n\\nA.\\n\\nTo get some help on how to use a method with an object, write its name after a dot (with\\nno parentheses) and then use a question-mark and hit TAB. (Hit the escape key “ESC” to\\nremove the list, or click on the text for a method.)\\n\\n\\n\\n18 CHAPTER 1. PRELIMINARIES\\n\\nA.inverse?\\n\\nWith one more question-mark and a TAB you can see the actual computer instructions\\nthat were programmed into Sage to make the method work, once you scoll down past the\\ndocumentation delimited by the triple quotes (\\"\\"\\"):\\n\\nA.inverse ??\\n\\nIt is worthwhile to see what Sage does when there is an error. You will probably see a\\nlot of these at first, and initially they will be a bit intimidating. But with time, you will\\nlearn how to use them effectively and you will also become more proficient with Sage and\\nsee them less often. Execute the compute cell below, it asks for the inverse of a matrix that\\nhas no inverse. Then reread the commentary.\\n\\nB = matrix ([[2, 20], [5, 50]])\\nB.inverse ()\\n\\nTraceback (most recent call last):\\n...\\nZeroDivisionError: Matrix is singular\\n\\nClick just to the left of the error message to expand it fully (another click hides it totally,\\nand a third click brings back the abbreviated form). Read the bottom of an error message\\nfirst, it is your best explanation. Here a ZeroDivisionError is not 100% accurate, but is\\nclose. The matrix is not invertible, not dissimilar to how we cannot divide scalars by zero.\\nThe remainder of the message begins at the top showing were the error first happened in\\nyour code and then the various places where intermediate functions were called, until the\\nactual piece of Sage where the problem occurred. Sometimes this information will give you\\nsome clues, sometimes it is totally undecipherable. So do not let it scare you if it seems\\nmysterious, but do remember to always read the last line first, then go back and read the\\nfirst few lines for something that looks like your code.\\n\\nAnnotating Your Work\\nIt is easy to comment on your work when you use the Sage Notebook. (The following only\\napplies if you are reading this within a Sage Notebook. If you are not, then perhaps you\\ncan go open up a worksheet in the Sage Notebook and experiment there.) You can open up\\na small word-processor by hovering your mouse until you get a skinny blue bar again, but\\nnow when you click, also hold the SHIFT key at the same time. Experiment with fonts,\\ncolors, bullet lists, etc and then click the “Save changes” button to exit. Double-click on\\nyour text if you need to go back and edit it later.\\n\\nOpen the word-processor again to create a new bit of text (maybe next to the empty\\ncompute cell just below). Type all of the following exactly:\\n\\nPythagorean Theorem: $c^2=a^2+b^2$\\n\\nand save your changes. The symbols between the dollar signs are written according to\\nthe mathematical typesetting language known as TEX — cruise the internet to learn more\\nabout this very popular tool. (Well, it is extremely popular among mathematicians and\\nphysical scientists.)\\n\\nLists\\nMuch of our interaction with sets will be through Sage lists. These are not really sets — they\\nallow duplicates, and order matters. But they are so close to sets, and so easy and powerful\\n\\n\\n\\n1.5. SAGE 19\\n\\nto use that we will use them regularly. We will use a fun made-up list for practice, the\\nquote marks mean the items are just text, with no special mathematical meaning. Execute\\nthese compute cells as we work through them.\\n\\nzoo = [ \' snake \' , \' parrot \' , \' elephant \' , \' baboon \' , \' beetle \' ]\\nzoo\\n\\n[ \' snake \' , \' parrot \' , \' elephant \' , \' baboon \' , \' beetle \' ]\\n\\nSo the square brackets define the boundaries of our list, commas separate items, and we\\ncan give the list a name. To work with just one element of the list, we use the name and\\na pair of brackets with an index. Notice that lists have indices that begin counting at zero.\\nThis will seem odd at first and will seem very natural later.\\n\\nzoo [2]\\n\\n\' elephant \'\\n\\nWe can add a new creature to the zoo, it is joined up at the far right end.\\nzoo.append( \' ostrich \' ); zoo\\n\\n[ \' snake \' , \' parrot \' , \' elephant \' , \' baboon \' , \' beetle \' , \' ostrich \' ]\\n\\nWe can remove a creature.\\nzoo.remove( \' parrot \' )\\nzoo\\n\\n[ \' snake \' , \' elephant \' , \' baboon \' , \' beetle \' , \' ostrich \' ]\\n\\nWe can extract a sublist. Here we start with element 1 (the elephant) and go all the\\nway up to, but not including, element 3 (the beetle). Again a bit odd, but it will feel natural\\nlater. For now, notice that we are extracting two elements of the lists, exactly 3 − 1 = 2\\nelements.\\n\\nmammals = zoo [1:3]\\nmammals\\n\\n[ \' elephant \' , \' baboon \' ]\\n\\nOften we will want to see if two lists are equal. To do that we will need to sort a list\\nfirst. A function creates a new, sorted list, leaving the original alone. So we need to save\\nthe new one with a new name.\\n\\nnewzoo = sorted(zoo)\\nnewzoo\\n\\n[ \' baboon \' , \' beetle \' , \' elephant \' , \' ostrich \' , \' snake \' ]\\n\\nzoo.sort()\\nzoo\\n\\n[ \' baboon \' , \' beetle \' , \' elephant \' , \' ostrich \' , \' snake \' ]\\n\\nNotice that if you run this last compute cell your zoo has changed and some commands\\nabove will not necessarily execute the same way. If you want to experiment, go all the way\\nback to the first creation of the zoo and start executing cells again from there with a fresh\\nzoo.\\n\\n\\n\\n20 CHAPTER 1. PRELIMINARIES\\n\\nA construction called a list comprehension is especially powerful, especially since it\\nalmost exactly mirrors notation we use to describe sets. Suppose we want to form the plural\\nof the names of the creatures in our zoo. We build a new list, based on all of the elements\\nof our old list.\\n\\nplurality_zoo = [animal+ \' s \' for animal in zoo]\\nplurality_zoo\\n\\n[ \' baboons \' , \' beetles \' , \' elephants \' , \' ostrichs \' , \' snakes \' ]\\n\\nAlmost like it says: we add an “s” to each animal name, for each animal in the zoo, and\\nplace them in a new list. Perfect. (Except for getting the plural of “ostrich” wrong.)\\n\\nLists of Integers\\nOne final type of list, with numbers this time. The srange() function will create lists of\\nintegers. (The “s” in the name stands for “Sage” and so will produce integers that Sage\\nunderstands best. Many early difficulties with Sage and group theory can be alleviated by\\nusing only this command to create lists of integers.) In its simplest form an invocation\\nlike srange(12) will create a list of 12 integers, starting at zero and working up to, but not\\nincluding, 12. Does this sound familiar?\\n\\ndozen = srange (12); dozen\\n\\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\\n\\nHere are two other forms, that you should be able to understand by studying the exam-\\nples.\\n\\nteens = srange (13, 20); teens\\n\\n[13, 14, 15, 16, 17, 18, 19]\\n\\ndecades = srange (1900, 2000, 10); decades\\n\\n[1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990]\\n\\nSaving and Sharing Your Work\\nThere is a “Save” button in the upper-right corner of the Sage Notebook. This will save a\\ncurrent copy of your worksheet that you can retrieve your work from within your notebook\\nagain later, though you have to re-execute all the cells when you re-open the worksheet.\\n\\nThere is also a “File” drop-down list, on the left, just above your very top compute cell\\n(not be confused with your browser’s File menu item!). You will see a choice here labeled\\n“Save worksheet to a file...” When you do this, you are creating a copy of your worksheet\\nin the sws format (short for “Sage WorkSheet”). You can email this file, or post it on a\\nwebsite, for other Sage users and they can use the “Upload” link on the homepage of their\\nnotebook to incorporate a copy of your worksheet into their notebook.\\n\\nThere are other ways to share worksheets that you can experiment with, but this gives\\nyou one way to share any worksheet with anybody almost anywhere.\\n\\nWe have covered a lot here in this section, so come back later to pick up tidbits you\\nmight have missed. There are also many more features in the Sage Notebook that we have\\nnot covered.\\n\\n\\n\\n1.6. SAGE EXERCISES 21\\n\\n1.6 Sage Exercises\\n1. This exercise is just about making sure you know how to use Sage. Login to a Sage\\nNotebook server and create a new worksheet. Do some non-trivial computation, maybe\\na pretty plot or some gruesome numerical computation to an insane precision. Create an\\ninteresting list and experiment with it some. Maybe include some nicely formatted text or\\nTEX using the included mini-word-processor of the Sage Notebook (hover until a blue bar\\nappears between cells and then shift-click).\\nUse whatever mechanism your instructor has in place for submitting your work. Or save\\nyour worksheet and then trade worksheets via email (or another electronic method) with a\\nclassmate.\\n\\n\\n\\n2\\n\\nThe Integers\\n\\nThe integers are the building blocks of mathematics. In this chapter we will investigate\\nthe fundamental properties of the integers, including mathematical induction, the division\\nalgorithm, and the Fundamental Theorem of Arithmetic.\\n\\n2.1 Mathematical Induction\\nSuppose we wish to show that\\n\\n1 + 2 + · · ·+ n =\\nn(n+ 1)\\n\\n2\\n\\nfor any natural number n. This formula is easily verified for small numbers such as n = 1,\\n2, 3, or 4, but it is impossible to verify for all natural numbers on a case-by-case basis. To\\nprove the formula true in general, a more generic method is required.\\n\\nSuppose we have verified the equation for the first n cases. We will attempt to show\\nthat we can generate the formula for the (n+ 1)th case from this knowledge. The formula\\nis true for n = 1 since\\n\\n1 =\\n1(1 + 1)\\n\\n2\\n.\\n\\nIf we have verified the first n cases, then\\n\\n1 + 2 + · · ·+ n+ (n+ 1) =\\nn(n+ 1)\\n\\n2\\n+ n+ 1\\n\\n=\\nn2 + 3n+ 2\\n\\n2\\n\\n=\\n(n+ 1)[(n+ 1) + 1]\\n\\n2\\n.\\n\\nThis is exactly the formula for the (n+ 1)th case.\\nThis method of proof is known as mathematical induction. Instead of attempting to\\n\\nverify a statement about some subset S of the positive integers N on a case-by-case basis, an\\nimpossible task if S is an infinite set, we give a specific proof for the smallest integer being\\nconsidered, followed by a generic argument showing that if the statement holds for a given\\ncase, then it must also hold for the next case in the sequence. We summarize mathematical\\ninduction in the following axiom.\\n\\nPrinciple 2.1 (First Principle of Mathematical Induction). Let S(n) be a statement about\\nintegers for n ∈ N and suppose S(n0) is true for some integer n0. If for all integers k with\\nk ≥ n0, S(k) implies that S(k + 1) is true, then S(n) is true for all integers n greater than\\nor equal to n0.\\n\\n22\\n\\n\\n\\n2.1. MATHEMATICAL INDUCTION 23\\n\\nExample 2.2. For all integers n ≥ 3, 2n > n+ 4. Since\\n\\n8 = 23 > 3 + 4 = 7,\\n\\nthe statement is true for n0 = 3. Assume that 2k > k + 4 for k ≥ 3. Then 2k+1 = 2 · 2k >\\n2(k + 4). But\\n\\n2(k + 4) = 2k + 8 > k + 5 = (k + 1) + 4\\n\\nsince k is positive. Hence, by induction, the statement holds for all integers n ≥ 3.\\n\\nExample 2.3. Every integer 10n+1 + 3 · 10n + 5 is divisible by 9 for n ∈ N. For n = 1,\\n\\n101+1 + 3 · 10 + 5 = 135 = 9 · 15\\n\\nis divisible by 9. Suppose that 10k+1 + 3 · 10k + 5 is divisible by 9 for k ≥ 1. Then\\n\\n10(k+1)+1 + 3 · 10k+1 + 5 = 10k+2 + 3 · 10k+1 + 50− 45\\n\\n= 10(10k+1 + 3 · 10k + 5)− 45\\n\\nis divisible by 9.\\n\\nExample 2.4. We will prove the binomial theorem using mathematical induction; that is,\\n\\n(a+ b)n =\\n\\nn∑\\nk=0\\n\\n(\\nn\\n\\nk\\n\\n)\\nakbn−k,\\n\\nwhere a and b are real numbers, n ∈ N, and\\n\\n(\\nn\\n\\nk\\n\\n)\\n=\\n\\nn!\\n\\nk!(n− k)!\\n\\nis the binomial coefficient. We first show that(\\nn+ 1\\n\\nk\\n\\n)\\n=\\n\\n(\\nn\\n\\nk\\n\\n)\\n+\\n\\n(\\nn\\n\\nk − 1\\n\\n)\\n.\\n\\nThis result follows from(\\nn\\n\\nk\\n\\n)\\n+\\n\\n(\\nn\\n\\nk − 1\\n\\n)\\n=\\n\\nn!\\n\\nk!(n− k)!\\n+\\n\\nn!\\n\\n(k − 1)!(n− k + 1)!\\n\\n=\\n(n+ 1)!\\n\\nk!(n+ 1− k)!\\n\\n=\\n\\n(\\nn+ 1\\n\\nk\\n\\n)\\n.\\n\\nIf n = 1, the binomial theorem is easy to verify. Now assume that the result is true for n\\n\\n\\n\\n24 CHAPTER 2. THE INTEGERS\\n\\ngreater than or equal to 1. Then\\n\\n(a+ b)n+1 = (a+ b)(a+ b)n\\n\\n= (a+ b)\\n\\n(\\nn∑\\n\\nk=0\\n\\n(\\nn\\n\\nk\\n\\n)\\nakbn−k\\n\\n)\\n\\n=\\nn∑\\n\\nk=0\\n\\n(\\nn\\n\\nk\\n\\n)\\nak+1bn−k +\\n\\nn∑\\nk=0\\n\\n(\\nn\\n\\nk\\n\\n)\\nakbn+1−k\\n\\n= an+1 +\\nn∑\\n\\nk=1\\n\\n(\\nn\\n\\nk − 1\\n\\n)\\nakbn+1−k +\\n\\nn∑\\nk=1\\n\\n(\\nn\\n\\nk\\n\\n)\\nakbn+1−k + bn+1\\n\\n= an+1 +\\n\\nn∑\\nk=1\\n\\n[(\\nn\\n\\nk − 1\\n\\n)\\n+\\n\\n(\\nn\\n\\nk\\n\\n)]\\nakbn+1−k + bn+1\\n\\n=\\nn+1∑\\nk=0\\n\\n(\\nn+ 1\\n\\nk\\n\\n)\\nakbn+1−k.\\n\\nWe have an equivalent statement of the Principle of Mathematical Induction that is\\noften very useful.\\n\\nPrinciple 2.5 (Second Principle of Mathematical Induction). Let S(n) be a statement\\nabout integers for n ∈ N and suppose S(n0) is true for some integer n0. If S(n0), S(n0 +\\n1), . . . , S(k) imply that S(k+1) for k ≥ n0, then the statement S(n) is true for all integers\\nn ≥ n0.\\n\\nA nonempty subset S of Z is well-ordered if S contains a least element. Notice that\\nthe set Z is not well-ordered since it does not contain a smallest element. However, the\\nnatural numbers are well-ordered.\\n\\nPrinciple 2.6 (Principle of Well-Ordering). Every nonempty subset of the natural numbers\\nis well-ordered.\\n\\nThe Principle of Well-Ordering is equivalent to the Principle of Mathematical Induction.\\n\\nLemma 2.7. The Principle of Mathematical Induction implies that 1 is the least positive\\nnatural number.\\n\\nProof. Let S = {n ∈ N : n ≥ 1}. Then 1 ∈ S. Now assume that n ∈ S; that is, n ≥ 1.\\nSince n + 1 ≥ 1, n + 1 ∈ S; hence, by induction, every natural number is greater than or\\nequal to 1.\\n\\nTheorem 2.8. The Principle of Mathematical Induction implies the Principle of Well-\\nOrdering. That is, every nonempty subset of N contains a least element.\\n\\nProof. We must show that if S is a nonempty subset of the natural numbers, then S\\ncontains a least element. If S contains 1, then the theorem is true by Lemma 2.7. Assume\\nthat if S contains an integer k such that 1 ≤ k ≤ n, then S contains a least element. We\\nwill show that if a set S contains an integer less than or equal to n+ 1, then S has a least\\nelement. If S does not contain an integer less than n+ 1, then n+ 1 is the smallest integer\\nin S. Otherwise, since S is nonempty, S must contain an integer less than or equal to n. In\\nthis case, by induction, S contains a least element.\\n\\nInduction can also be very useful in formulating definitions. For instance, there are two\\nways to define n!, the factorial of a positive integer n.\\n\\n\\n\\n2.2. THE DIVISION ALGORITHM 25\\n\\n• The explicit definition: n! = 1 · 2 · 3 · · · (n− 1) · n.\\n\\n• The inductive or recursive definition: 1! = 1 and n! = n(n− 1)! for n > 1.\\n\\nEvery good mathematician or computer scientist knows that looking at problems recursively,\\nas opposed to explicitly, often results in better understanding of complex issues.\\n\\n2.2 The Division Algorithm\\nAn application of the Principle of Well-Ordering that we will use often is the division\\nalgorithm.\\n\\nTheorem 2.9 (Division Algorithm). Let a and b be integers, with b > 0. Then there exist\\nunique integers q and r such that\\n\\na = bq + r\\n\\nwhere 0 ≤ r < b.\\n\\nProof. This is a perfect example of the existence-and-uniqueness type of proof. We must\\nfirst prove that the numbers q and r actually exist. Then we must show that if q′ and r′\\n\\nare two other such numbers, then q = q′ and r = r′.\\nExistence of q and r. Let\\n\\nS = {a− bk : k ∈ Z and a− bk ≥ 0}.\\n\\nIf 0 ∈ S, then b divides a, and we can let q = a/b and r = 0. If 0 /∈ S, we can use the Well-\\nOrdering Principle. We must first show that S is nonempty. If a > 0, then a − b · 0 ∈ S.\\nIf a < 0, then a − b(2a) = a(1 − 2b) ∈ S. In either case S ̸= ∅. By the Well-Ordering\\nPrinciple, S must have a smallest member, say r = a − bq. Therefore, a = bq + r, r ≥ 0.\\nWe now show that r < b. Suppose that r > b. Then\\n\\na− b(q + 1) = a− bq − b = r − b > 0.\\n\\nIn this case we would have a− b(q + 1) in the set S. But then a− b(q + 1) < a− bq, which\\nwould contradict the fact that r = a − bq is the smallest member of S. So r ≤ b. Since\\n0 /∈ S, r ̸= b and so r < b.\\n\\nUniqueness of q and r. Suppose there exist integers r, r′, q, and q′ such that\\n\\na = bq + r, 0 ≤ r < b and a = bq′ + r′, 0 ≤ r′ < b.\\n\\nThen bq+r = bq′+r′. Assume that r′ ≥ r. From the last equation we have b(q−q′) = r′−r;\\ntherefore, b must divide r′ − r and 0 ≤ r′ − r ≤ r′ < b. This is possible only if r′ − r = 0.\\nHence, r = r′ and q = q′.\\n\\nLet a and b be integers. If b = ak for some integer k, we write a | b. An integer d is\\ncalled a common divisor of a and b if d | a and d | b. The greatest common divisor of\\nintegers a and b is a positive integer d such that d is a common divisor of a and b and if d′\\nis any other common divisor of a and b, then d′ | d. We write d = gcd(a, b); for example,\\ngcd(24, 36) = 12 and gcd(120, 102) = 6. We say that two integers a and b are relatively\\nprime if gcd(a, b) = 1.\\n\\nTheorem 2.10. Let a and b be nonzero integers. Then there exist integers r and s such\\nthat\\n\\ngcd(a, b) = ar + bs.\\n\\nFurthermore, the greatest common divisor of a and b is unique.\\n\\n\\n\\n26 CHAPTER 2. THE INTEGERS\\n\\nProof. Let\\nS = {am+ bn : m,n ∈ Z and am+ bn > 0}.\\n\\nClearly, the set S is nonempty; hence, by the Well-Ordering Principle S must have a smallest\\nmember, say d = ar+ bs. We claim that d = gcd(a, b). Write a = dq+ r′ where 0 ≤ r′ < d.\\nIf r′ > 0, then\\n\\nr′ = a− dq\\n\\n= a− (ar + bs)q\\n\\n= a− arq − bsq\\n\\n= a(1− rq) + b(−sq),\\n\\nwhich is in S. But this would contradict the fact that d is the smallest member of S. Hence,\\nr′ = 0 and d divides a. A similar argument shows that d divides b. Therefore, d is a common\\ndivisor of a and b.\\n\\nSuppose that d′ is another common divisor of a and b, and we want to show that d′ | d.\\nIf we let a = d′h and b = d′k, then\\n\\nd = ar + bs = d′hr + d′ks = d′(hr + ks).\\n\\nSo d′ must divide d. Hence, d must be the unique greatest common divisor of a and b.\\n\\nCorollary 2.11. Let a and b be two integers that are relatively prime. Then there exist\\nintegers r and s such that ar + bs = 1.\\n\\nThe Euclidean Algorithm\\nAmong other things, Theorem 2.10 allows us to compute the greatest common divisor of\\ntwo integers.\\n\\nExample 2.12. Let us compute the greatest common divisor of 945 and 2415. First observe\\nthat\\n\\n2415 = 945 · 2 + 525\\n\\n945 = 525 · 1 + 420\\n\\n525 = 420 · 1 + 105\\n\\n420 = 105 · 4 + 0.\\n\\nReversing our steps, 105 divides 420, 105 divides 525, 105 divides 945, and 105 divides 2415.\\nHence, 105 divides both 945 and 2415. If d were another common divisor of 945 and 2415,\\nthen d would also have to divide 105. Therefore, gcd(945, 2415) = 105.\\n\\nIf we work backward through the above sequence of equations, we can also obtain\\nnumbers r and s such that 945r + 2415s = 105. Observe that\\n\\n105 = 525 + (−1) · 420\\n= 525 + (−1) · [945 + (−1) · 525]\\n= 2 · 525 + (−1) · 945\\n= 2 · [2415 + (−2) · 945] + (−1) · 945\\n= 2 · 2415 + (−5) · 945.\\n\\nSo r = −5 and s = 2. Notice that r and s are not unique, since r = 41 and s = −16 would\\nalso work.\\n\\n\\n\\n2.2. THE DIVISION ALGORITHM 27\\n\\nTo compute gcd(a, b) = d, we are using repeated divisions to obtain a decreasing se-\\nquence of positive integers r1 > r2 > · · · > rn = d; that is,\\n\\nb = aq1 + r1\\n\\na = r1q2 + r2\\n\\nr1 = r2q3 + r3\\n...\\n\\nrn−2 = rn−1qn + rn\\n\\nrn−1 = rnqn+1.\\n\\nTo find r and s such that ar + bs = d, we begin with this last equation and substitute\\nresults obtained from the previous equations:\\n\\nd = rn\\n\\n= rn−2 − rn−1qn\\n\\n= rn−2 − qn(rn−3 − qn−1rn−2)\\n\\n= −qnrn−3 + (1 + qnqn−1)rn−2\\n\\n...\\n= ra+ sb.\\n\\nThe algorithm that we have just used to find the greatest common divisor d of two integers\\na and b and to write d as the linear combination of a and b is known as the Euclidean\\nalgorithm.\\n\\nPrime Numbers\\nLet p be an integer such that p > 1. We say that p is a prime number, or simply p is\\nprime, if the only positive numbers that divide p are 1 and p itself. An integer n > 1 that\\nis not prime is said to be composite.\\n\\nLemma 2.13 (Euclid). Let a and b be integers and p be a prime number. If p | ab, then\\neither p | a or p | b.\\n\\nProof. Suppose that p does not divide a. We must show that p | b. Since gcd(a, p) = 1,\\nthere exist integers r and s such that ar + ps = 1. So\\n\\nb = b(ar + ps) = (ab)r + p(bs).\\n\\nSince p divides both ab and itself, p must divide b = (ab)r + p(bs).\\n\\nTheorem 2.14 (Euclid). There exist an infinite number of primes.\\n\\nProof. We will prove this theorem by contradiction. Suppose that there are only a finite\\nnumber of primes, say p1, p2, . . . , pn. Let P = p1p2 · · · pn + 1. Then P must be divisible\\nby some pi for 1 ≤ i ≤ n. In this case, pi must divide P − p1p2 · · · pn = 1, which is a\\ncontradiction. Hence, either P is prime or there exists an additional prime number p ̸= pi\\nthat divides P .\\n\\nTheorem 2.15 (Fundamental Theorem of Arithmetic). Let n be an integer such that n > 1.\\nThen\\n\\nn = p1p2 · · · pk,\\n\\n\\n\\n28 CHAPTER 2. THE INTEGERS\\n\\nwhere p1, . . . , pk are primes (not necessarily distinct). Furthermore, this factorization is\\nunique; that is, if\\n\\nn = q1q2 · · · ql,\\n\\nthen k = l and the qi’s are just the pi’s rearranged.\\n\\nProof. Uniqueness. To show uniqueness we will use induction on n. The theorem is\\ncertainly true for n = 2 since in this case n is prime. Now assume that the result holds for\\nall integers m such that 1 ≤ m < n, and\\n\\nn = p1p2 · · · pk = q1q2 · · · ql,\\n\\nwhere p1 ≤ p2 ≤ · · · ≤ pk and q1 ≤ q2 ≤ · · · ≤ ql. By Lemma 2.13, p1 | qi for some\\ni = 1, . . . , l and q1 | pj for some j = 1, . . . , k. Since all of the pi’s and qi’s are prime, p1 = qi\\nand q1 = pj . Hence, p1 = q1 since p1 ≤ pj = q1 ≤ qi = p1. By the induction hypothesis,\\n\\nn′ = p2 · · · pk = q2 · · · ql\\n\\nhas a unique factorization. Hence, k = l and qi = pi for i = 1, . . . , k.\\nExistence. To show existence, suppose that there is some integer that cannot be written\\n\\nas the product of primes. Let S be the set of all such numbers. By the Principle of Well-\\nOrdering, S has a smallest number, say a. If the only positive factors of a are a and 1, then\\na is prime, which is a contradiction. Hence, a = a1a2 where 1 < a1 < a and 1 < a2 < a.\\nNeither a1 ∈ S nor a2 ∈ S, since a is the smallest element in S. So\\n\\na1 = p1 · · · pr\\na2 = q1 · · · qs.\\n\\nTherefore,\\na = a1a2 = p1 · · · prq1 · · · qs.\\n\\nSo a /∈ S, which is a contradiction.\\n\\nHistorical Note\\n\\nPrime numbers were first studied by the ancient Greeks. Two important results from\\nantiquity are Euclid’s proof that an infinite number of primes exist and the Sieve of Eratos-\\nthenes, a method of computing all of the prime numbers less than a fixed positive integer\\nn. One problem in number theory is to find a function f such that f(n) is prime for each\\ninteger n. Pierre Fermat (1601?–1665) conjectured that 22\\n\\nn\\n+ 1 was prime for all n, but\\n\\nlater it was shown by Leonhard Euler (1707–1783) that\\n\\n22\\n5\\n+ 1 = 4,294,967,297\\n\\nis a composite number. One of the many unproven conjectures about prime numbers is\\nGoldbach’s Conjecture. In a letter to Euler in 1742, Christian Goldbach stated the conjec-\\nture that every even integer with the exception of 2 seemed to be the sum of two primes:\\n4 = 2 + 2, 6 = 3 + 3, 8 = 3 + 5, . . .. Although the conjecture has been verified for the\\nnumbers up through 4× 1018, it has yet to be proven in general. Since prime numbers play\\nan important role in public key cryptography, there is currently a great deal of interest in\\ndetermining whether or not a large number is prime.\\n\\n\\n\\n2.3. EXERCISES 29\\n\\n2.3 Exercises\\n1. Prove that\\n\\n12 + 22 + · · ·+ n2 =\\nn(n+ 1)(2n+ 1)\\n\\n6\\nfor n ∈ N.\\n\\n2. Prove that\\n13 + 23 + · · ·+ n3 =\\n\\nn2(n+ 1)2\\n\\n4\\nfor n ∈ N.\\n\\n3. Prove that n! > 2n for n ≥ 4.\\n\\n4. Prove that\\nx+ 4x+ 7x+ · · ·+ (3n− 2)x =\\n\\nn(3n− 1)x\\n\\n2\\nfor n ∈ N.\\n\\n5. Prove that 10n+1 + 10n + 1 is divisible by 3 for n ∈ N.\\n\\n6. Prove that 4 · 102n + 9 · 102n−1 + 5 is divisible by 99 for n ∈ N.\\n\\n7. Show that\\nn\\n√\\na1a2 · · · an ≤ 1\\n\\nn\\n\\nn∑\\nk=1\\n\\nak.\\n\\n8. Prove the Leibniz rule for f (n)(x), where f (n) is the nth derivative of f ; that is, show\\nthat\\n\\n(fg)(n)(x) =\\nn∑\\n\\nk=0\\n\\n(\\nn\\n\\nk\\n\\n)\\nf (k)(x)g(n−k)(x).\\n\\n9. Use induction to prove that 1 + 2 + 22 + · · ·+ 2n = 2n+1 − 1 for n ∈ N.\\n\\n10. Prove that\\n1\\n\\n2\\n+\\n\\n1\\n\\n6\\n+ · · ·+ 1\\n\\nn(n+ 1)\\n=\\n\\nn\\n\\nn+ 1\\n\\nfor n ∈ N.\\n\\n11. If x is a nonnegative real number, then show that (1 + x)n − 1 ≥ nx for n = 0, 1, 2, . . ..\\n\\n12. (Power Sets) Let X be a set. Define the power set of X, denoted P(X), to be the\\nset of all subsets of X. For example,\\n\\nP({a, b}) = {∅, {a}, {b}, {a, b}}.\\n\\nFor every positive integer n, show that a set with exactly n elements has a power set with\\nexactly 2n elements.\\n\\n13. Prove that the two principles of mathematical induction stated in Section 2.1 are equiv-\\nalent.\\n\\n14. Show that the Principle of Well-Ordering for the natural numbers implies that 1 is the\\nsmallest natural number. Use this result to show that the Principle of Well-Ordering implies\\nthe Principle of Mathematical Induction; that is, show that if S ⊂ N such that 1 ∈ S and\\nn+ 1 ∈ S whenever n ∈ S, then S = N.\\n\\n15. For each of the following pairs of numbers a and b, calculate gcd(a, b) and find integers\\nr and s such that gcd(a, b) = ra+ sb.\\n\\n\\n\\n30 CHAPTER 2. THE INTEGERS\\n\\n(a) 14 and 39\\n(b) 234 and 165\\n(c) 1739 and 9923\\n\\n(d) 471 and 562\\n(e) 23,771 and 19,945\\n(f) −4357 and 3754\\n\\n16. Let a and b be nonzero integers. If there exist integers r and s such that ar + bs = 1,\\nshow that a and b are relatively prime.\\n\\n17. (Fibonacci Numbers) The Fibonacci numbers are\\n\\n1, 1, 2, 3, 5, 8, 13, 21, . . . .\\n\\nWe can define them inductively by f1 = 1, f2 = 1, and fn+2 = fn+1 + fn for n ∈ N.\\n(a) Prove that fn < 2n.\\n(b) Prove that fn+1fn−1 = f2n + (−1)n, n ≥ 2.\\n(c) Prove that fn = [(1 +\\n\\n√\\n5 )n − (1−\\n\\n√\\n5 )n]/2n\\n\\n√\\n5.\\n\\n(d) Show that limn→∞ fn/fn+1 = (\\n√\\n5− 1)/2.\\n\\n(e) Prove that fn and fn+1 are relatively prime.\\n\\n18. Let a and b be integers such that gcd(a, b) = 1. Let r and s be integers such that\\nar + bs = 1. Prove that\\n\\ngcd(a, s) = gcd(r, b) = gcd(r, s) = 1.\\n\\n19. Let x, y ∈ N be relatively prime. If xy is a perfect square, prove that x and y must\\nboth be perfect squares.\\n\\n20. Using the division algorithm, show that every perfect square is of the form 4k or 4k+1\\nfor some nonnegative integer k.\\n\\n21. Suppose that a, b, r, s are pairwise relatively prime and that\\n\\na2 + b2 = r2\\n\\na2 − b2 = s2.\\n\\nProve that a, r, and s are odd and b is even.\\n\\n22. Let n ∈ N. Use the division algorithm to prove that every integer is congruent mod n\\nto precisely one of the integers 0, 1, . . . , n − 1. Conclude that if r is an integer, then there\\nis exactly one s in Z such that 0 ≤ s < n and [r] = [s]. Hence, the integers are indeed\\npartitioned by congruence mod n.\\n\\n23. Define the least common multiple of two nonzero integers a and b, denoted by\\nlcm(a, b), to be the nonnegative integer m such that both a and b divide m, and if a and b\\ndivide any other integer n, then m also divides n. Prove there exists a unique least common\\nmultiple for any two integers a and b.\\n\\n24. If d = gcd(a, b) and m = lcm(a, b), prove that dm = |ab|.\\n\\n25. Show that lcm(a, b) = ab if and only if gcd(a, b) = 1.\\n\\n26. Prove that gcd(a, c) = gcd(b, c) = 1 if and only if gcd(ab, c) = 1 for integers a, b, and c.\\n\\n\\n\\n2.4. PROGRAMMING EXERCISES 31\\n\\n27. Let a, b, c ∈ Z. Prove that if gcd(a, b) = 1 and a | bc, then a | c.\\n\\n28. Let p ≥ 2. Prove that if 2p − 1 is prime, then p must also be prime.\\n\\n29. Prove that there are an infinite number of primes of the form 6n+ 5.\\n\\n30. Prove that there are an infinite number of primes of the form 4n− 1.\\n\\n31. Using the fact that 2 is prime, show that there do not exist integers p and q such that\\np2 = 2q2. Demonstrate that therefore\\n\\n√\\n2 cannot be a rational number.\\n\\n2.4 Programming Exercises\\n1. (The Sieve of Eratosthenes) One method of computing all of the prime numbers less\\nthan a certain fixed positive integer N is to list all of the numbers n such that 1 < n < N .\\nBegin by eliminating all of the multiples of 2. Next eliminate all of the multiples of 3. Now\\neliminate all of the multiples of 5. Notice that 4 has already been crossed out. Continue in\\nthis manner, noticing that we do not have to go all the way to N ; it suffices to stop at\\n\\n√\\nN .\\n\\nUsing this method, compute all of the prime numbers less than N = 250. We can also use\\nthis method to find all of the integers that are relatively prime to an integer N . Simply\\neliminate the prime factors of N and all of their multiples. Using this method, find all of\\nthe numbers that are relatively prime to N = 120. Using the Sieve of Eratosthenes, write\\na program that will compute all of the primes less than an integer N .\\n\\n2. Let N0 = N ∪ {0}. Ackermann’s function is the function A : N0 × N0 → N0 defined by\\nthe equations\\n\\nA(0, y) = y + 1,\\n\\nA(x+ 1, 0) = A(x, 1),\\n\\nA(x+ 1, y + 1) = A(x,A(x+ 1, y)).\\n\\nUse this definition to compute A(3, 1). Write a program to evaluate Ackermann’s function.\\nModify the program to count the number of statements executed in the program when\\nAckermann’s function is evaluated. How many statements are executed in the evaluation\\nof A(4, 1)? What about A(5, 1)?\\n\\n3. Write a computer program that will implement the Euclidean algorithm. The program\\nshould accept two positive integers a and b as input and should output gcd(a, b) as well as\\nintegers r and s such that\\n\\ngcd(a, b) = ra+ sb.\\n\\n2.5 References and Suggested Readings\\n[1] Brookshear, J. G. Theory of Computation: Formal Languages, Automata, and Com-\\n\\nplexity. Benjamin/Cummings, Redwood City, CA, 1989. Shows the relationships of\\nthe theoretical aspects of computer science to set theory and the integers.\\n\\n[2] Hardy, G. H. and Wright, E. M. An Introduction to the Theory of Numbers. 6th ed.\\nOxford University Press, New York, 2008.\\n\\n[3] Niven, I. and Zuckerman, H. S. An Introduction to the Theory of Numbers. 5th ed.\\nWiley, New York, 1991.\\n\\n[4] Vanden Eynden, C. Elementary Number Theory. 2nd ed. Waveland Press, Long\\nGrove IL, 2001.\\n\\n\\n\\n32 CHAPTER 2. THE INTEGERS\\n\\n2.6 Sage\\nMany properties of the algebraic objects we will study can be determined from properties\\nof associated integers. And Sage has many powerful functions for analyzing integers.\\n\\nDivision Algorithm\\n\\nThe code a % b will return the remainder upon division of a by b. In other words, the result\\nis the unique integer r such that (1) 0 ≤ r < b, and (2) a = bq + r for some integer q (the\\nquotient), as guaranteed by the Division Algorithm (Theorem 2.9). Then (a − r)/b will\\nequal q. For example,\\n\\nr = 14 % 3\\nr\\n\\n2\\n\\nq = (14 - r)/3\\nq\\n\\n4\\n\\nIt is also possible to get both the quotient and remainder at the same time with the\\n.quo_rem() method (quotient and remainder).\\n\\na = 14\\nb = 3\\na.quo_rem(b)\\n\\n(4, 2)\\n\\nA remainder of zero indicates divisibility. So (a % b)== 0 will return True if b divides a,\\nand will otherwise return False.\\n\\n(20 % 5) == 0\\n\\nTrue\\n\\n(17 % 4) == 0\\n\\nFalse\\n\\nThe .divides() method is another option.\\n\\nc = 5\\nc.divides (20)\\n\\nTrue\\n\\nd = 4\\nd.divides (17)\\n\\nFalse\\n\\n\\n\\n2.6. SAGE 33\\n\\nGreatest Common Divisor\\nThe greatest common divisor of a and b is obtained with the command gcd(a, b), where\\nin our first uses, a and b are integers. Later, a and b can be other objects with a notion of\\ndivisibility and “greatness,” such as polynomials. For example,\\n\\ngcd (2776 , 2452)\\n\\n4\\n\\nWe can use the gcd command to determine if a pair of integers are relatively prime.\\na = 31049\\nb = 2105\\ngcd(a, b) == 1\\n\\nTrue\\n\\na = 3563\\nb = 2947\\ngcd(a, b) == 1\\n\\nFalse\\n\\nThe command xgcd(a,b) (“eXtended GCD”) returns a triple where the first element is\\nthe greatest common divisor of a and b (as with the gcd(a,b) command above), but the\\nnext two elements are values of r and s such that ra+ sb = gcd(a, b).\\n\\nxgcd (633 ,331)\\n\\n(1, -137, 262)\\n\\nPortions of the triple can be extracted using [ ] (“indexing”) to access the entries of the\\ntriple, starting with the first as number 0. For example, the following should always return\\nthe result True, even if you change the values of a and b. Try changing the values of a and\\nb below, to see that the result is always True.\\n\\na = 633\\nb = 331\\nextended = xgcd(a, b)\\ng = extended [0]\\nr = extended [1]\\ns = extended [2]\\ng == r*a + s*b\\n\\nTrue\\n\\nStudying this block of code will go a long way towards helping you get the most out of\\nSage’s output. Note that = is how a value is assigned to a variable, while as in the last line,\\n== is how we compare two items for equality.\\n\\nPrimes and Factoring\\nThe method .is_prime() will determine if an integer is prime or not.\\n\\na = 117371\\na.is_prime ()\\n\\n\\n\\n34 CHAPTER 2. THE INTEGERS\\n\\nTrue\\n\\nb = 14547073\\nb.is_prime ()\\n\\nFalse\\n\\nb == 1597 * 9109\\n\\nTrue\\n\\nThe command random_prime(a, proof=True) will generate a random prime number be-\\ntween 2 and a. Experiment by executing the following two compute cells several times.\\n(Replacing proof=True by proof=False will speed up the search, but there will be a very,\\nvery, very small probability the result will not be prime.)\\n\\na = random_prime (10^21 , proof=True)\\na\\n\\n424729101793542195193\\n\\na.is_prime ()\\n\\nTrue\\n\\nThe command prime_range(a, b) returns an ordered list of all the primes from a to b−1,\\ninclusive. For example,\\n\\nprime_range (500, 550)\\n\\n[503, 509, 521, 523, 541, 547]\\n\\nThe commands next_prime(a) and previous_prime(a) are other ways to get a single prime\\nnumber of a desired size. Give them a try below if you have an empty compute cell there\\n(as you will if you are reading in the Sage Notebook, or are reading the online version).\\n(The hash symbol, #, is used to indicate a “comment” line, which will not be evaluated by\\nSage. So erase this line, or start on the one below it.)\\n\\nIn addition to checking if integers are prime or not, or generating prime numbers, Sage\\ncan also decompose any integer into its prime factors, as described by the Fundamental\\nTheorem of Arithmetic (Theorem 2.15).\\n\\na = 2600\\na.factor ()\\n\\n2^3 * 5^2 * 13\\n\\nSo 2600 = 23 × 52 × 13 and this is the unique way to write 2600 as a product of prime\\nnumbers (other than rearranging the order of the primes themselves in the product).\\n\\nWhile Sage will print a factorization nicely, it is carried internally as a list of pairs of\\nintegers, with each pair being a base (a prime number) and an exponent (a positive integer).\\nStudy the following carefully, as it is another good exercise in working with Sage output in\\nthe form of lists.\\n\\na = 2600\\nfactored = a.factor ()\\nfirst_term = factored [0]\\nfirst_term\\n\\n\\n\\n2.7. SAGE EXERCISES 35\\n\\n(2, 3)\\n\\nsecond_term = factored [1]\\nsecond_term\\n\\n(5, 2)\\n\\nthird_term = factored [2]\\nthird_term\\n\\n(13, 1)\\n\\nfirst_prime = first_term [0]\\nfirst_prime\\n\\n2\\n\\nfirst_exponent = first_term [1]\\nfirst_exponent\\n\\n3\\n\\nThe next compute cell reveals the internal version of the factorization by asking for the\\nactual list. And we show how you could determine exactly how many terms the factorization\\nhas by using the length command, len().\\n\\nlist(factored)\\n\\n[(2, 3), (5, 2), (13, 1)]\\n\\nlen(factored)\\n\\n3\\n\\nCan you extract the next two primes, and their exponents, from a?\\n\\n2.7 Sage Exercises\\nThese exercises are about investigating basic properties of the integers, something we will\\nfrequently do when investigating groups. Use the editing capabilities of a Sage worksheet\\nto annotate and explain your work.\\n1. Use the next_prime() command to construct two different 8-digit prime numbers and\\nsave them in variables named a and b.\\n\\n2. Use the .is_prime() method to verify that your primes a and b are really prime.\\n\\n3. Verify that 1 is the greatest common divisor of your two primes from the previous\\nexercises.\\n\\n4. Find two integers that make a “linear combination” of your two primes equal to 1.\\nInclude a verification of your result.\\n\\n5. Determine a factorization into powers of primes for c = 4598 037 234.\\n\\n6. Write a compute cell that defines the same value of c again, and then defines a candidate\\ndivisor of c named d. The third line of the cell should return True if and only if d is a divisor\\nof c. Illustrate the use of your cell by testing your code with d = 7 and in a new copy of\\nthe cell, testing your code with d = 11.\\n\\n\\n\\n3\\n\\nGroups\\n\\nWe begin our study of algebraic structures by investigating sets associated with single\\noperations that satisfy certain reasonable axioms; that is, we want to define an operation\\non a set in a way that will generalize such familiar structures as the integers Z together\\nwith the single operation of addition, or invertible 2 × 2 matrices together with the single\\noperation of matrix multiplication. The integers and the 2×2 matrices, together with their\\nrespective single operations, are examples of algebraic structures known as groups.\\n\\nThe theory of groups occupies a central position in mathematics. Modern group theory\\narose from an attempt to find the roots of a polynomial in terms of its coefficients. Groups\\nnow play a central role in such areas as coding theory, counting, and the study of symmetries;\\nmany areas of biology, chemistry, and physics have benefited from group theory.\\n\\n3.1 Integer Equivalence Classes and Symmetries\\n\\nLet us now investigate some mathematical structures that can be viewed as sets with single\\noperations.\\n\\nThe Integers mod n\\n\\nThe integers mod n have become indispensable in the theory and applications of algebra.\\nIn mathematics they are used in cryptography, coding theory, and the detection of errors\\nin identification codes.\\n\\nWe have already seen that two integers a and b are equivalent mod n if n divides\\na − b. The integers mod n also partition Z into n different equivalence classes; we will\\ndenote the set of these equivalence classes by Zn. Consider the integers modulo 12 and the\\ncorresponding partition of the integers:\\n\\n[0] = {. . . ,−12, 0, 12, 24, . . .},\\n[1] = {. . . ,−11, 1, 13, 25, . . .},\\n\\n...\\n[11] = {. . . ,−1, 11, 23, 35, . . .}.\\n\\nWhen no confusion can arise, we will use 0, 1, . . . , 11 to indicate the equivalence classes\\n[0], [1], . . . , [11] respectively. We can do arithmetic on Zn. For two integers a and b, define\\naddition modulo n to be (a + b) (mod n); that is, the remainder when a + b is divided by\\nn. Similarly, multiplication modulo n is defined as (ab) (mod n), the remainder when ab is\\ndivided by n.\\n\\n36\\n\\n\\n\\n3.1. INTEGER EQUIVALENCE CLASSES AND SYMMETRIES 37\\n\\nExample 3.1. The following examples illustrate integer arithmetic modulo n:\\n\\n7 + 4 ≡ 1 (mod 5) 7 · 3 ≡ 1 (mod 5)\\n\\n3 + 5 ≡ 0 (mod 8) 3 · 5 ≡ 7 (mod 8)\\n\\n3 + 4 ≡ 7 (mod 12) 3 · 4 ≡ 0 (mod 12)\\n\\nIn particular, notice that it is possible that the product of two nonzero numbers modulo n\\ncan be equivalent to 0 modulo n.\\n\\nExample 3.2. Most, but not all, of the usual laws of arithmetic hold for addition and\\nmultiplication in Zn. For instance, it is not necessarily true that there is a multiplicative\\ninverse. Consider the multiplication table for Z8 in Table 3.3. Notice that 2, 4, and 6 do\\nnot have multiplicative inverses; that is, for n = 2, 4, or 6, there is no integer k such that\\nkn ≡ 1 (mod 8).\\n\\n· 0 1 2 3 4 5 6 7\\n\\n0 0 0 0 0 0 0 0 0\\n\\n1 0 1 2 3 4 5 6 7\\n\\n2 0 2 4 6 0 2 4 6\\n\\n3 0 3 6 1 4 7 2 5\\n\\n4 0 4 0 4 0 4 0 4\\n\\n5 0 5 2 7 4 1 6 3\\n\\n6 0 6 4 2 0 6 4 2\\n\\n7 0 7 6 5 4 3 2 1\\n\\nTable 3.3: Multiplication table for Z8\\n\\nProposition 3.4. Let Zn be the set of equivalence classes of the integers mod n and\\na, b, c ∈ Zn.\\n\\n1. Addition and multiplication are commutative:\\n\\na+ b ≡ b+ a (mod n)\\n\\nab ≡ ba (mod n).\\n\\n2. Addition and multiplication are associative:\\n\\n(a+ b) + c ≡ a+ (b+ c) (mod n)\\n\\n(ab)c ≡ a(bc) (mod n).\\n\\n3. There are both additive and multiplicative identities:\\n\\na+ 0 ≡ a (mod n)\\n\\na · 1 ≡ a (mod n).\\n\\n4. Multiplication distributes over addition:\\n\\na(b+ c) ≡ ab+ ac (mod n).\\n\\n\\n\\n38 CHAPTER 3. GROUPS\\n\\n5. For every integer a there is an additive inverse −a:\\n\\na+ (−a) ≡ 0 (mod n).\\n\\n6. Let a be a nonzero integer. Then gcd(a, n) = 1 if and only if there exists a multiplicative\\ninverse b for a (mod n); that is, a nonzero integer b such that\\n\\nab ≡ 1 (mod n).\\n\\nProof. We will prove (1) and (6) and leave the remaining properties to be proven in the\\nexercises.\\n\\n(1) Addition and multiplication are commutative modulo n since the remainder of a+ b\\ndivided by n is the same as the remainder of b+ a divided by n.\\n\\n(6) Suppose that gcd(a, n) = 1. Then there exist integers r and s such that ar+ns = 1.\\nSince ns = 1 − ar, it must be the case that ar ≡ 1 (mod n). Letting b be the equivalence\\nclass of r, ab ≡ 1 (mod n).\\n\\nConversely, suppose that there exists an integer b such that ab ≡ 1 (mod n). Then n\\ndivides ab − 1, so there is an integer k such that ab − nk = 1. Let d = gcd(a, n). Since d\\ndivides ab− nk, d must also divide 1; hence, d = 1.\\n\\nSymmetries\\n\\nreflection\\nhorizontal axis\\n\\nA\\n\\nD\\n\\nB\\n\\nC\\n\\nC\\n\\nB\\n\\nD\\n\\nA\\n\\nreflection\\nvertical axis\\n\\nA\\n\\nD\\n\\nB\\n\\nC\\n\\nA\\n\\nD\\n\\nB\\n\\nC\\n\\n180◦\\n\\nrotation\\n\\nA\\n\\nD\\n\\nB\\n\\nC\\n\\nD\\n\\nA\\n\\nC\\n\\nB\\n\\nidentityA\\n\\nD\\n\\nB\\n\\nC\\n\\nB\\n\\nC\\n\\nA\\n\\nD\\n\\nFigure 3.5: Rigid motions of a rectangle\\n\\nA symmetry of a geometric figure is a rearrangement of the figure preserving the\\narrangement of its sides and vertices as well as its distances and angles. A map from the\\nplane to itself preserving the symmetry of an object is called a rigid motion. For example,\\nif we look at the rectangle in Figure 3.5, it is easy to see that a rotation of 180◦ or 360◦\\n\\nreturns a rectangle in the plane with the same orientation as the original rectangle and the\\nsame relationship among the vertices. A reflection of the rectangle across either the vertical\\n\\n\\n\\n3.1. INTEGER EQUIVALENCE CLASSES AND SYMMETRIES 39\\n\\naxis or the horizontal axis can also be seen to be a symmetry. However, a 90◦ rotation in\\neither direction cannot be a symmetry unless the rectangle is a square.\\n\\nA\\n\\nB\\n\\nC\\n\\nreflection\\n\\nB C\\n\\nA\\n\\nµ3 =\\n\\n(\\nA B C\\n\\nB A C\\n\\n)\\nA\\n\\nB\\n\\nC\\n\\nreflection\\n\\nC A\\n\\nB\\n\\nµ2 =\\n\\n(\\nA B C\\n\\nC B A\\n\\n)\\nA\\n\\nB\\n\\nC\\n\\nreflection\\n\\nA B\\n\\nC\\n\\nµ1 =\\n\\n(\\nA B C\\n\\nA C B\\n\\n)\\nA\\n\\nB\\n\\nC\\n\\nrotation\\n\\nB A\\n\\nC\\n\\nρ2 =\\n\\n(\\nA B C\\n\\nC A B\\n\\n)\\nA\\n\\nB\\n\\nC\\n\\nrotation\\n\\nC B\\n\\nA\\n\\nρ1 =\\n\\n(\\nA B C\\n\\nB C A\\n\\n)\\nA\\n\\nB\\n\\nC\\n\\nidentity\\n\\nA C\\n\\nB\\n\\nid =\\n\\n(\\nA B C\\n\\nA B C\\n\\n)\\n\\nFigure 3.6: Symmetries of a triangle\\n\\nLet us find the symmetries of the equilateral triangle △ABC. To find a symmetry of\\n△ABC, we must first examine the permutations of the vertices A, B, and C and then ask\\nif a permutation extends to a symmetry of the triangle. Recall that a permutation of a\\nset S is a one-to-one and onto map π : S → S. The three vertices have 3! = 6 permutations,\\nso the triangle has at most six symmetries. To see that there are six permutations, observe\\nthere are three different possibilities for the first vertex, and two for the second, and the\\nremaining vertex is determined by the placement of the first two. So we have 3 ·2 ·1 = 3! = 6\\ndifferent arrangements. To denote the permutation of the vertices of an equilateral triangle\\nthat sends A to B, B to C, and C to A, we write the array(\\n\\nA B C\\n\\nB C A\\n\\n)\\n.\\n\\nNotice that this particular permutation corresponds to the rigid motion of rotating the\\ntriangle by 120◦ in a clockwise direction. In fact, every permutation gives rise to a symmetry\\nof the triangle. All of these symmetries are shown in Figure 3.6.\\n\\nA natural question to ask is what happens if one motion of the triangle △ABC is\\nfollowed by another. Which symmetry is µ1ρ1; that is, what happens when we do the\\npermutation ρ1 and then the permutation µ1? Remember that we are composing functions\\n\\n\\n\\n40 CHAPTER 3. GROUPS\\n\\nhere. Although we usually multiply left to right, we compose functions right to left. We have\\n\\n(µ1ρ1)(A) = µ1(ρ1(A)) = µ1(B) = C\\n\\n(µ1ρ1)(B) = µ1(ρ1(B)) = µ1(C) = B\\n\\n(µ1ρ1)(C) = µ1(ρ1(C)) = µ1(A) = A.\\n\\nThis is the same symmetry as µ2. Suppose we do these motions in the opposite order,\\nρ1 then µ1. It is easy to determine that this is the same as the symmetry µ3; hence,\\nρ1µ1 ̸= µ1ρ1. A multiplication table for the symmetries of an equilateral triangle △ABC is\\ngiven in Table 3.7.\\n\\nNotice that in the multiplication table for the symmetries of an equilateral triangle, for\\nevery motion of the triangle α there is another motion β such that αβ = id; that is, for\\nevery motion there is another motion that takes the triangle back to its original orientation.\\n\\n◦ id ρ1 ρ2 µ1 µ2 µ3\\nid id ρ1 ρ2 µ1 µ2 µ3\\nρ1 ρ1 ρ2 id µ3 µ1 µ2\\nρ2 ρ2 id ρ1 µ2 µ3 µ1\\nµ1 µ1 µ2 µ3 id ρ1 ρ2\\nµ2 µ2 µ3 µ1 ρ2 id ρ1\\nµ3 µ3 µ1 µ2 ρ1 ρ2 id\\n\\nTable 3.7: Symmetries of an equilateral triangle\\n\\n3.2 Definitions and Examples\\nThe integers mod n and the symmetries of a triangle or a rectangle are examples of groups.\\nA binary operation or law of composition on a set G is a function G × G → G that\\nassigns to each pair (a, b) ∈ G×G a unique element a◦ b, or ab in G, called the composition\\nof a and b. A group (G, ◦) is a set G together with a law of composition (a, b) 7→ a ◦ b that\\nsatisfies the following axioms.\\n\\n• The law of composition is associative. That is,\\n\\n(a ◦ b) ◦ c = a ◦ (b ◦ c)\\n\\nfor a, b, c ∈ G.\\n\\n• There exists an element e ∈ G, called the identity element, such that for any element\\na ∈ G\\n\\ne ◦ a = a ◦ e = a.\\n\\n• For each element a ∈ G, there exists an inverse element in G, denoted by a−1, such\\nthat\\n\\na ◦ a−1 = a−1 ◦ a = e.\\n\\nA group G with the property that a ◦ b = b ◦ a for all a, b ∈ G is called abelian or\\ncommutative. Groups not satisfying this property are said to be nonabelian or non-\\ncommutative.\\n\\n\\n\\n3.2. DEFINITIONS AND EXAMPLES 41\\n\\nExample 3.8. The integers Z = {. . . ,−1, 0, 1, 2, . . .} form a group under the operation\\nof addition. The binary operation on two integers m,n ∈ Z is just their sum. Since the\\nintegers under addition already have a well-established notation, we will use the operator +\\ninstead of ◦; that is, we shall write m+n instead of m◦n. The identity is 0, and the inverse\\nof n ∈ Z is written as −n instead of n−1. Notice that the set of integers under addition\\nhave the additional property that m+ n = n+m and therefore form an abelian group.\\n\\nMost of the time we will write ab instead of a ◦ b; however, if the group already has a\\nnatural operation such as addition in the integers, we will use that operation. That is, if\\nwe are adding two integers, we still write m+ n, −n for the inverse, and 0 for the identity\\nas usual. We also write m− n instead of m+ (−n).\\n\\nIt is often convenient to describe a group in terms of an addition or multiplication table.\\nSuch a table is called a Cayley table.\\n\\nExample 3.9. The integers mod n form a group under addition modulo n. Consider\\nZ5, consisting of the equivalence classes of the integers 0, 1, 2, 3, and 4. We define the\\ngroup operation on Z5 by modular addition. We write the binary operation on the group\\nadditively; that is, we write m + n. The element 0 is the identity of the group and each\\nelement in Z5 has an inverse. For instance, 2 + 3 = 3 + 2 = 0. Table 3.10 is a Cayley table\\nfor Z5. By Proposition 3.4, Zn = {0, 1, . . . , n− 1} is a group under the binary operation of\\naddition mod n.\\n\\n+ 0 1 2 3 4\\n\\n0 0 1 2 3 4\\n\\n1 1 2 3 4 0\\n\\n2 2 3 4 0 1\\n\\n3 3 4 0 1 2\\n\\n4 4 0 1 2 3\\n\\nTable 3.10: Cayley table for (Z5,+)\\n\\nExample 3.11. Not every set with a binary operation is a group. For example, if we let\\nmodular multiplication be the binary operation on Zn, then Zn fails to be a group. The\\nelement 1 acts as a group identity since 1 · k = k · 1 = k for any k ∈ Zn; however, a\\nmultiplicative inverse for 0 does not exist since 0 · k = k · 0 = 0 for every k in Zn. Even if\\nwe consider the set Zn \\\\ {0}, we still may not have a group. For instance, let 2 ∈ Z6. Then\\n2 has no multiplicative inverse since\\n\\n0 · 2 = 0 1 · 2 = 2\\n\\n2 · 2 = 4 3 · 2 = 0\\n\\n4 · 2 = 2 5 · 2 = 4.\\n\\nBy Proposition 3.4, every nonzero k does have an inverse in Zn if k is relatively prime to\\nn. Denote the set of all such nonzero elements in Zn by U(n). Then U(n) is a group called\\nthe group of units of Zn. Table 3.12 is a Cayley table for the group U(8).\\n\\n\\n\\n42 CHAPTER 3. GROUPS\\n\\n· 1 3 5 7\\n\\n1 1 3 5 7\\n\\n3 3 1 7 5\\n\\n5 5 7 1 3\\n\\n7 7 5 3 1\\n\\nTable 3.12: Multiplication table for U(8)\\n\\nExample 3.13. The symmetries of an equilateral triangle described in Section 3.1 form\\na nonabelian group. As we observed, it is not necessarily true that αβ = βα for two\\nsymmetries α and β. Using Table 3.7, which is a Cayley table for this group, we can easily\\ncheck that the symmetries of an equilateral triangle are indeed a group. We will denote this\\ngroup by either S3 or D3, for reasons that will be explained later.\\nExample 3.14. We use M2(R) to denote the set of all 2× 2 matrices. Let GL2(R) be the\\nsubset of M2(R) consisting of invertible matrices; that is, a matrix\\n\\nA =\\n\\n(\\na b\\n\\nc d\\n\\n)\\nis in GL2(R) if there exists a matrix A−1 such that AA−1 = A−1A = I, where I is the 2×2\\nidentity matrix. For A to have an inverse is equivalent to requiring that the determinant\\nof A be nonzero; that is, detA = ad− bc ̸= 0. The set of invertible matrices forms a group\\ncalled the general linear group. The identity of the group is the identity matrix\\n\\nI =\\n\\n(\\n1 0\\n\\n0 1\\n\\n)\\n.\\n\\nThe inverse of A ∈ GL2(R) is\\n\\nA−1 =\\n1\\n\\nad− bc\\n\\n(\\nd −b\\n−c a\\n\\n)\\n.\\n\\nThe product of two invertible matrices is again invertible. Matrix multiplication is associa-\\ntive, satisfying the other group axiom. For matrices it is not true in general that AB = BA;\\nhence, GL2(R) is another example of a nonabelian group.\\nExample 3.15. Let\\n\\n1 =\\n\\n(\\n1 0\\n\\n0 1\\n\\n)\\nI =\\n\\n(\\n0 1\\n\\n−1 0\\n\\n)\\nJ =\\n\\n(\\n0 i\\n\\ni 0\\n\\n)\\nK =\\n\\n(\\ni 0\\n\\n0 −i\\n\\n)\\n,\\n\\nwhere i2 = −1. Then the relations I2 = J2 = K2 = −1, IJ = K, JK = I, KI = J ,\\nJI = −K, KJ = −I, and IK = −J hold. The set Q8 = {±1,±I,±J,±K} is a group\\ncalled the quaternion group. Notice that Q8 is noncommutative.\\nExample 3.16. Let C∗be the set of nonzero complex numbers. Under the operation of\\nmultiplication C∗ forms a group. The identity is 1. If z = a + bi is a nonzero complex\\nnumber, then\\n\\nz−1 =\\na− bi\\n\\na2 + b2\\n\\nis the inverse of z. It is easy to see that the remaining group axioms hold.\\n\\n\\n\\n3.2. DEFINITIONS AND EXAMPLES 43\\n\\nA group is finite, or has finite order, if it contains a finite number of elements;\\notherwise, the group is said to be infinite or to have infinite order. The order of a finite\\ngroup is the number of elements that it contains. If G is a group containing n elements,\\nwe write |G| = n. The group Z5 is a finite group of order 5; the integers Z form an infinite\\ngroup under addition, and we sometimes write |Z| = ∞.\\n\\nBasic Properties of Groups\\n\\nProposition 3.17. The identity element in a group G is unique; that is, there exists only\\none element e ∈ G such that eg = ge = g for all g ∈ G.\\n\\nProof. Suppose that e and e′ are both identities in G. Then eg = ge = g and e′g = ge′ = g\\nfor all g ∈ G. We need to show that e = e′. If we think of e as the identity, then ee′ = e′; but\\nif e′ is the identity, then ee′ = e. Combining these two equations, we have e = ee′ = e′.\\n\\nInverses in a group are also unique. If g′ and g′′ are both inverses of an element g\\nin a group G, then gg′ = g′g = e and gg′′ = g′′g = e. We want to show that g′ = g′′,\\nbut g′ = g′e = g′(gg′′) = (g′g)g′′ = eg′′ = g′′. We summarize this fact in the following\\nproposition.\\n\\nProposition 3.18. If g is any element in a group G, then the inverse of g, denoted by g−1,\\nis unique.\\n\\nProposition 3.19. Let G be a group. If a, b ∈ G, then (ab)−1 = b−1a−1.\\n\\nProof. Let a, b ∈ G. Then abb−1a−1 = aea−1 = aa−1 = e. Similarly, b−1a−1ab = e. But\\nby the previous proposition, inverses are unique; hence, (ab)−1 = b−1a−1.\\n\\nProposition 3.20. Let G be a group. For any a ∈ G, (a−1)−1 = a.\\n\\nProof. Observe that a−1(a−1)−1 = e. Consequently, multiplying both sides of this equa-\\ntion by a, we have\\n\\n(a−1)−1 = e(a−1)−1 = aa−1(a−1)−1 = ae = a.\\n\\nIt makes sense to write equations with group elements and group operations. If a and b\\nare two elements in a group G, does there exist an element x ∈ G such that ax = b? If such\\nan x does exist, is it unique? The following proposition answers both of these questions\\npositively.\\n\\nProposition 3.21. Let G be a group and a and b be any two elements in G. Then the\\nequations ax = b and xa = b have unique solutions in G.\\n\\nProof. Suppose that ax = b. We must show that such an x exists. We can multiply both\\nsides of ax = b by a−1 to find x = ex = a−1ax = a−1b.\\n\\nTo show uniqueness, suppose that x1 and x2 are both solutions of ax = b; then ax1 =\\nb = ax2. So x1 = a−1ax1 = a−1ax2 = x2. The proof for the existence and uniqueness of\\nthe solution of xa = b is similar.\\n\\nProposition 3.22. If G is a group and a, b, c ∈ G, then ba = ca implies b = c and ab = ac\\nimplies b = c.\\n\\n\\n\\n44 CHAPTER 3. GROUPS\\n\\nThis proposition tells us that the right and left cancellation laws are true in groups.\\nWe leave the proof as an exercise.\\n\\nWe can use exponential notation for groups just as we do in ordinary algebra. If G is a\\ngroup and g ∈ G, then we define g0 = e. For n ∈ N, we define\\n\\ngn = g · g · · · g︸ ︷︷ ︸\\nn times\\n\\nand\\ng−n = g−1 · g−1 · · · g−1︸ ︷︷ ︸\\n\\nn times\\n\\n.\\n\\nTheorem 3.23. In a group, the usual laws of exponents hold; that is, for all g, h ∈ G,\\n\\n1. gmgn = gm+n for all m,n ∈ Z;\\n\\n2. (gm)n = gmn for all m,n ∈ Z;\\n\\n3. (gh)n = (h−1g−1)−n for all n ∈ Z. Furthermore, if G is abelian, then (gh)n = gnhn.\\n\\nWe will leave the proof of this theorem as an exercise. Notice that (gh)n ̸= gnhn in\\ngeneral, since the group may not be abelian. If the group is Z or Zn, we write the group\\noperation additively and the exponential operation multiplicatively; that is, we write ng\\ninstead of gn. The laws of exponents now become\\n\\n1. mg + ng = (m+ n)g for all m,n ∈ Z;\\n\\n2. m(ng) = (mn)g for all m,n ∈ Z;\\n\\n3. m(g + h) = mg +mh for all n ∈ Z.\\n\\nIt is important to realize that the last statement can be made only because Z and Zn\\n\\nare commutative groups.\\n\\nHistorical Note\\n\\nAlthough the first clear axiomatic definition of a group was not given until the late\\n1800s, group-theoretic methods had been employed before this time in the development of\\nmany areas of mathematics, including geometry and the theory of algebraic equations.\\n\\nJoseph-Louis Lagrange used group-theoretic methods in a 1770–1771 memoir to study\\nmethods of solving polynomial equations. Later, Évariste Galois (1811–1832) succeeded\\nin developing the mathematics necessary to determine exactly which polynomial equations\\ncould be solved in terms of the polynomials’coefficients. Galois’ primary tool was group\\ntheory.\\n\\nThe study of geometry was revolutionized in 1872 when Felix Klein proposed that ge-\\nometric spaces should be studied by examining those properties that are invariant under\\na transformation of the space. Sophus Lie, a contemporary of Klein, used group theory\\nto study solutions of partial differential equations. One of the first modern treatments of\\ngroup theory appeared in William Burnside’s The Theory of Groups of Finite Order [1],\\nfirst published in 1897.\\n\\n\\n\\n3.3. SUBGROUPS 45\\n\\n3.3 Subgroups\\nDefinitions and Examples\\nSometimes we wish to investigate smaller groups sitting inside a larger group. The set of\\neven integers 2Z = {. . . ,−2, 0, 2, 4, . . .} is a group under the operation of addition. This\\nsmaller group sits naturally inside of the group of integers under addition. We define a\\nsubgroup H of a group G to be a subset H of G such that when the group operation of\\nG is restricted to H, H is a group in its own right. Observe that every group G with at\\nleast two elements will always have at least two subgroups, the subgroup consisting of the\\nidentity element alone and the entire group itself. The subgroup H = {e} of a group G is\\ncalled the trivial subgroup. A subgroup that is a proper subset of G is called a proper\\nsubgroup. In many of the examples that we have investigated up to this point, there exist\\nother subgroups besides the trivial and improper subgroups.\\nExample 3.24. Consider the set of nonzero real numbers, R∗, with the group operation of\\nmultiplication. The identity of this group is 1 and the inverse of any element a ∈ R∗ is just\\n1/a. We will show that\\n\\nQ∗ = {p/q : p and q are nonzero integers}\\n\\nis a subgroup of R∗. The identity of R∗ is 1; however, 1 = 1/1 is the quotient of two nonzero\\nintegers. Hence, the identity of R∗ is in Q∗. Given two elements in Q∗, say p/q and r/s,\\ntheir product pr/qs is also in Q∗. The inverse of any element p/q ∈ Q∗ is again in Q∗ since\\n(p/q)−1 = q/p. Since multiplication in R∗ is associative, multiplication in Q∗ is associative.\\nExample 3.25. Recall that C∗ is the multiplicative group of nonzero complex numbers.\\nLet H = {1,−1, i,−i}. Then H is a subgroup of C∗. It is quite easy to verify that H is a\\ngroup under multiplication and that H ⊂ C∗.\\nExample 3.26. Let SL2(R) be the subset of GL2(R)consisting of matrices of determinant\\none; that is, a matrix\\n\\nA =\\n\\n(\\na b\\n\\nc d\\n\\n)\\nis in SL2(R) exactly when ad− bc = 1. To show that SL2(R) is a subgroup of the general\\nlinear group, we must show that it is a group under matrix multiplication. The 2×2 identity\\nmatrix is in SL2(R), as is the inverse of the matrix A:\\n\\nA−1 =\\n\\n(\\nd −b\\n−c a\\n\\n)\\n.\\n\\nIt remains to show that multiplication is closed; that is, that the product of two matrices\\nof determinant one also has determinant one. We will leave this task as an exercise. The\\ngroup SL2(R) is called the special linear group.\\nExample 3.27. It is important to realize that a subset H of a group G can be a group\\nwithout being a subgroup of G. For H to be a subgroup of G it must inherit G’s binary\\noperation. The set of all 2 × 2 matrices, M2(R), forms a group under the operation of\\naddition. The 2× 2 general linear group is a subset of M2(R) and is a group under matrix\\nmultiplication, but it is not a subgroup of M2(R). If we add two invertible matrices, we do\\nnot necessarily obtain another invertible matrix. Observe that(\\n\\n1 0\\n\\n0 1\\n\\n)\\n+\\n\\n(\\n−1 0\\n\\n0 −1\\n\\n)\\n=\\n\\n(\\n0 0\\n\\n0 0\\n\\n)\\n,\\n\\nbut the zero matrix is not in GL2(R).\\n\\n\\n\\n46 CHAPTER 3. GROUPS\\n\\nExample 3.28. One way of telling whether or not two groups are the same is by examining\\ntheir subgroups. Other than the trivial subgroup and the group itself, the group Z4 has\\na single subgroup consisting of the elements 0 and 2. From the group Z2, we can form\\nanother group of four elements as follows. As a set this group is Z2 × Z2. We perform the\\ngroup operation coordinatewise; that is, (a, b) + (c, d) = (a + c, b + d). Table 3.29 is an\\naddition table for Z2 × Z2. Since there are three nontrivial proper subgroups of Z2 × Z2,\\nH1 = {(0, 0), (0, 1)}, H2 = {(0, 0), (1, 0)}, and H3 = {(0, 0), (1, 1)}, Z4 and Z2 ×Z2 must be\\ndifferent groups.\\n\\n+ (0, 0) (0, 1) (1, 0) (1, 1)\\n\\n(0, 0) (0, 0) (0, 1) (1, 0) (1, 1)\\n\\n(0, 1) (0, 1) (0, 0) (1, 1) (1, 0)\\n\\n(1, 0) (1, 0) (1, 1) (0, 0) (0, 1)\\n\\n(1, 1) (1, 1) (1, 0) (0, 1) (0, 0)\\n\\nTable 3.29: Addition table for Z2 × Z2\\n\\nSome Subgroup Theorems\\nLet us examine some criteria for determining exactly when a subset of a group is a subgroup.\\n\\nProposition 3.30. A subset H of G is a subgroup if and only if it satisfies the following\\nconditions.\\n\\n1. The identity e of G is in H.\\n\\n2. If h1, h2 ∈ H, then h1h2 ∈ H.\\n\\n3. If h ∈ H, then h−1 ∈ H.\\n\\nProof. First suppose that H is a subgroup of G. We must show that the three conditions\\nhold. Since H is a group, it must have an identity eH . We must show that eH = e, where\\ne is the identity of G. We know that eHeH = eH and that eeH = eHe = eH ; hence,\\neeH = eHeH . By right-hand cancellation, e = eH . The second condition holds since a\\nsubgroup H is a group. To prove the third condition, let h ∈ H. Since H is a group, there\\nis an element h′ ∈ H such that hh′ = h′h = e. By the uniqueness of the inverse in G,\\nh′ = h−1.\\n\\nConversely, if the three conditions hold, we must show that H is a group under the same\\noperation as G; however, these conditions plus the associativity of the binary operation are\\nexactly the axioms stated in the definition of a group.\\n\\nProposition 3.31. Let H be a subset of a group G. Then H is a subgroup of G if and only\\nif H ̸= ∅, and whenever g, h ∈ H then gh−1 is in H.\\n\\nProof. First assume that H is a subgroup of G. We wish to show that gh−1 ∈ H whenever\\ng and h are in H. Since h is in H, its inverse h−1 must also be in H. Because of the closure\\nof the group operation, gh−1 ∈ H.\\n\\nConversely, suppose that H ⊂ G such that H ̸= ∅ and gh−1 ∈ H whenever g, h ∈ H. If\\ng ∈ H, then gg−1 = e is in H. If g ∈ H, then eg−1 = g−1 is also in H. Now let h1, h2 ∈ H.\\nWe must show that their product is also in H. However, h1(h−1\\n\\n2 )−1 = h1h2 ∈ H. Hence,\\nH is a subgroup of G.\\n\\n\\n\\n3.4. EXERCISES 47\\n\\n3.4 Exercises\\n1. Find all x ∈ Z satisfying each of the following equations.\\n\\n(a) 3x ≡ 2 (mod 7)\\n\\n(b) 5x+ 1 ≡ 13 (mod 23)\\n\\n(c) 5x+ 1 ≡ 13 (mod 26)\\n\\n(d) 9x ≡ 3 (mod 5)\\n\\n(e) 5x ≡ 1 (mod 6)\\n\\n(f) 3x ≡ 1 (mod 6)\\n\\n2. Which of the following multiplication tables defined on the set G = {a, b, c, d} form a\\ngroup? Support your answer in each case.\\n\\n(a)\\n◦ a b c d\\n\\na a c d a\\n\\nb b b c d\\n\\nc c d a b\\n\\nd d a b c\\n\\n(b)\\n◦ a b c d\\n\\na a b c d\\n\\nb b a d c\\n\\nc c d a b\\n\\nd d c b a\\n\\n(c)\\n◦ a b c d\\n\\na a b c d\\n\\nb b c d a\\n\\nc c d a b\\n\\nd d a b c\\n\\n(d)\\n◦ a b c d\\n\\na a b c d\\n\\nb b a c d\\n\\nc c b a d\\n\\nd d d b c\\n\\n3. Write out Cayley tables for groups formed by the symmetries of a rectangle and for\\n(Z4,+). How many elements are in each group? Are the groups the same? Why or why\\nnot?\\n\\n4. Describe the symmetries of a rhombus and prove that the set of symmetries forms a\\ngroup. Give Cayley tables for both the symmetries of a rectangle and the symmetries of a\\nrhombus. Are the symmetries of a rectangle and those of a rhombus the same?\\n\\n5. Describe the symmetries of a square and prove that the set of symmetries is a group.\\nGive a Cayley table for the symmetries. How many ways can the vertices of a square be\\npermuted? Is each permutation necessarily a symmetry of the square? The symmetry group\\nof the square is denoted by D4.\\n\\n6. Give a multiplication table for the group U(12).\\n\\n7. Let S = R \\\\ {−1} and define a binary operation on S by a ∗ b = a+ b+ ab. Prove that\\n(S, ∗) is an abelian group.\\n\\n8. Give an example of two elements A and B in GL2(R) with AB ̸= BA.\\n\\n9. Prove that the product of two matrices in SL2(R) has determinant one.\\n\\n10. Prove that the set of matrices of the form\uf8eb\uf8ed1 x y\\n\\n0 1 z\\n\\n0 0 1\\n\\n\uf8f6\uf8f8\\n\\n\\n\\n48 CHAPTER 3. GROUPS\\n\\nis a group under matrix multiplication. This group, known as the Heisenberg group, is\\nimportant in quantum physics. Matrix multiplication in the Heisenberg group is defined by\uf8eb\uf8ed1 x y\\n\\n0 1 z\\n\\n0 0 1\\n\\n\uf8f6\uf8f8\uf8eb\uf8ed1 x′ y′\\n\\n0 1 z′\\n\\n0 0 1\\n\\n\uf8f6\uf8f8 =\\n\\n\uf8eb\uf8ed1 x+ x′ y + y′ + xz′\\n\\n0 1 z + z′\\n\\n0 0 1\\n\\n\uf8f6\uf8f8 .\\n\\n11. Prove that det(AB) = det(A)det(B) in GL2(R). Use this result to show that the\\nbinary operation in the group GL2(R) is closed; that is, if A and B are in GL2(R), then\\nAB ∈ GL2(R).\\n\\n12. Let Zn\\n2 = {(a1, a2, . . . , an) : ai ∈ Z2}. Define a binary operation on Zn\\n\\n2 by\\n\\n(a1, a2, . . . , an) + (b1, b2, . . . , bn) = (a1 + b1, a2 + b2, . . . , an + bn).\\n\\nProve that Zn\\n2 is a group under this operation. This group is important in algebraic coding\\n\\ntheory.\\n\\n13. Show that R∗ = R \\\\ {0} is a group under the operation of multiplication.\\n\\n14. Given the groups R∗ and Z, let G = R∗ × Z. Define a binary operation ◦ on G by\\n(a,m) ◦ (b, n) = (ab,m+ n). Show that G is a group under this operation.\\n\\n15. Prove or disprove that every group containing six elements is abelian.\\n\\n16. Give a specific example of some group G and elements g, h ∈ G where (gh)n ̸= gnhn.\\n\\n17. Give an example of three different groups with eight elements. Why are the groups\\ndifferent?\\n\\n18. Show that there are n! permutations of a set containing n items.\\n\\n19. Show that\\n0 + a ≡ a+ 0 ≡ a (mod n)\\n\\nfor all a ∈ Zn.\\n\\n20. Prove that there is a multiplicative identity for the integers modulo n:\\n\\na · 1 ≡ a (mod n).\\n\\n21. For each a ∈ Zn find an element b ∈ Zn such that\\n\\na+ b ≡ b+ a ≡ 0 (mod n).\\n\\n22. Show that addition and multiplication mod n are well defined operations. That is, show\\nthat the operations do not depend on the choice of the representative from the equivalence\\nclasses mod n.\\n\\n23. Show that addition and multiplication mod n are associative operations.\\n\\n24. Show that multiplication distributes over addition modulo n:\\n\\na(b+ c) ≡ ab+ ac (mod n).\\n\\n25. Let a and b be elements in a group G. Prove that abna−1 = (aba−1)n for n ∈ Z.\\n\\n\\n\\n3.4. EXERCISES 49\\n\\n26. Let U(n) be the group of units in Zn. If n > 2, prove that there is an element k ∈ U(n)\\nsuch that k2 = 1 and k ̸= 1.\\n\\n27. Prove that the inverse of g1g2 · · · gn is g−1\\nn g−1\\n\\nn−1 · · · g\\n−1\\n1 .\\n\\n28. Prove the remainder of Proposition 3.21: if G is a group and a, b ∈ G, then the equation\\nxa = b has a unique solution in G.\\n\\n29. Prove Theorem 3.23.\\n\\n30. Prove the right and left cancellation laws for a group G; that is, show that in the group\\nG, ba = ca implies b = c and ab = ac implies b = c for elements a, b, c ∈ G.\\n\\n31. Show that if a2 = e for all elements a in a group G, then G must be abelian.\\n\\n32. Show that if G is a finite group of even order, then there is an a ∈ G such that a is not\\nthe identity and a2 = e.\\n\\n33. Let G be a group and suppose that (ab)2 = a2b2 for all a and b in G. Prove that G is\\nan abelian group.\\n\\n34. Find all the subgroups of Z3 × Z3. Use this information to show that Z3 × Z3 is not\\nthe same group as Z9. (See Example 3.28 for a short description of the product of groups.)\\n\\n35. Find all the subgroups of the symmetry group of an equilateral triangle.\\n\\n36. Compute the subgroups of the symmetry group of a square.\\n\\n37. Let H = {2k : k ∈ Z}. Show that H is a subgroup of Q∗.\\n\\n38. Let n = 0, 1, 2, . . . and nZ = {nk : k ∈ Z}. Prove that nZ is a subgroup of Z. Show\\nthat these subgroups are the only subgroups of Z.\\n\\n39. Let T = {z ∈ C∗ : |z| = 1}. Prove that T is a subgroup of C∗.\\n\\n40. (\\ncos θ − sin θ\\nsin θ cos θ\\n\\n)\\nwhere θ ∈ R. Prove that G is a subgroup of SL2(R).\\n\\n41. Prove that\\n\\nG = {a+ b\\n√\\n2 : a, b ∈ Q and a and b are not both zero}\\n\\nis a subgroup of R∗ under the group operation of multiplication.\\n\\n42. Let G be the group of 2× 2 matrices under addition and\\n\\nH =\\n\\n{(\\na b\\n\\nc d\\n\\n)\\n: a+ d = 0\\n\\n}\\n.\\n\\nProve that H is a subgroup of G.\\n\\n43. Prove or disprove: SL2(Z), the set of 2×2 matrices with integer entries and determinant\\none, is a subgroup of SL2(R).\\n\\n44. List the subgroups of the quaternion group, Q8.\\n\\n45. Prove that the intersection of two subgroups of a group G is also a subgroup of G.\\n\\n\\n\\n50 CHAPTER 3. GROUPS\\n\\n46. Prove or disprove: If H and K are subgroups of a group G, then H ∪K is a subgroup\\nof G.\\n\\n47. Prove or disprove: If H and K are subgroups of a group G, then HK = {hk : h ∈\\nH and k ∈ K} is a subgroup of G. What if G is abelian?\\n\\n48. Let G be a group and g ∈ G. Show that\\n\\nZ(G) = {x ∈ G : gx = xg for all g ∈ G}\\n\\nis a subgroup of G. This subgroup is called the center of G.\\n\\n49. Let a and b be elements of a group G. If a4b = ba and a3 = e, prove that ab = ba.\\n\\n50. Give an example of an infinite group in which every nontrivial subgroup is infinite.\\n\\n51. If xy = x−1y−1 for all x and y in G, prove that G must be abelian.\\n\\n52. Prove or disprove: Every proper subgroup of an nonabelian group is nonabelian.\\n\\n53. Let H be a subgroup of G and\\n\\nC(H) = {g ∈ G : gh = hg for all h ∈ H}.\\n\\nProve C(H) is a subgroup of G. This subgroup is called the centralizer of H in G.\\n\\n54. Let H be a subgroup of G. If g ∈ G, show that gHg−1 = {ghg−1 : h ∈ H} is also a\\nsubgroup of G.\\n\\n3.5 Additional Exercises: Detecting Errors\\n\\n1. (UPC Symbols) Universal Product Code (upc) symbols are found on most products in\\ngrocery and retail stores. The upc symbol is a 12-digit code identifying the manufacturer\\nof a product and the product itself (Figure 3.32). The first 11 digits contain information\\nabout the product; the twelfth digit is used for error detection. If d1d2 · · · d12 is a valid upc\\nnumber, then\\n\\n3 · d1 + 1 · d2 + 3 · d3 + · · ·+ 3 · d11 + 1 · d12 ≡ 0 (mod 10).\\n\\n(a) Show that the upc number 0-50000-30042-6, which appears in Figure 3.32, is a valid\\nupc number.\\n\\n(b) Show that the number 0-50000-30043-6 is not a valid upc number.\\n\\n(c) Write a formula to calculate the check digit, d12, in the upc number.\\n\\n(d) The upc error detection scheme can detect most transposition errors; that is, it can\\ndetermine if two digits have been interchanged. Show that the transposition error\\n0-05000-30042-6 is not detected. Find a transposition error that is detected. Can you\\nfind a general rule for the types of transposition errors that can be detected?\\n\\n(e) Write a program that will determine whether or not a upc number is valid.\\n\\n\\n\\n3.5. ADDITIONAL EXERCISES: DETECTING ERRORS 51\\n\\nFigure 3.32: A upc code\\n\\n2. It is often useful to use an inner product notation for this type of error detection scheme;\\nhence, we will use the notion\\n\\n(d1, d2, . . . , dk) · (w1, w2, . . . , wk) ≡ 0 (mod n)\\n\\nto mean\\nd1w1 + d2w2 + · · ·+ dkwk ≡ 0 (mod n).\\n\\nSuppose that (d1, d2, . . . , dk) · (w1, w2, . . . , wk) ≡ 0 (mod n) is an error detection scheme for\\nthe k-digit identification number d1d2 · · · dk, where 0 ≤ di < n. Prove that all single-digit\\nerrors are detected if and only if gcd(wi, n) = 1 for 1 ≤ i ≤ k.\\n\\n3. Let (d1, d2, . . . , dk) · (w1, w2, . . . , wk) ≡ 0 (mod n) be an error detection scheme for the\\nk-digit identification number d1d2 · · · dk, where 0 ≤ di < n. Prove that all transposition\\nerrors of two digits di and dj are detected if and only if gcd(wi − wj , n) = 1 for i and j\\nbetween 1 and k.\\n\\n4. (ISBN Codes) Every book has an International Standard Book Number (isbn) code.\\nThis is a 10-digit code indicating the book’s publisher and title. The tenth digit is a check\\ndigit satisfying\\n\\n(d1, d2, . . . , d10) · (10, 9, . . . , 1) ≡ 0 (mod 11).\\n\\nOne problem is that d10 might have to be a 10 to make the inner product zero; in this case,\\n11 digits would be needed to make this scheme work. Therefore, the character X is used for\\nthe eleventh digit. So isbn 3-540-96035-X is a valid isbn code.\\n(a) Is isbn 0-534-91500-0 a valid isbn code? What about isbn 0-534-91700-0 and isbn\\n\\n0-534-19500-0?\\n(b) Does this method detect all single-digit errors? What about all transposition errors?\\n(c) How many different isbn codes are there?\\n(d) Write a computer program that will calculate the check digit for the first nine digits\\n\\nof an isbn code.\\n(e) A publisher has houses in Germany and the United States. Its German prefix is 3-540.\\n\\nIf its United States prefix will be 0-abc, find abc such that the rest of the isbn code\\nwill be the same for a book printed in Germany and in the United States. Under the\\nisbn coding method the first digit identifies the language; German is 3 and English is\\n\\n\\n\\n52 CHAPTER 3. GROUPS\\n\\n0. The next group of numbers identifies the publisher, and the last group identifies\\nthe specific book.\\n\\n3.6 References and Suggested Readings\\n[1] Burnside, W. Theory of Groups of Finite Order. 2nd ed. Cambridge University Press,\\n\\nCambridge, 1911; Dover, New York, 1953. A classic. Also available at books.google.com.\\n[2] Gallian, J. A. and Winters, S. “Modular Arithmetic in the Marketplace,” The Amer-\\n\\nican Mathematical Monthly 95 (1988): 548–51.\\n[3] Gallian, J. A. Contemporary Abstract Algebra. 7th ed. Brooks/Cole, Belmont, CA,\\n\\n2009.\\n[4] Hall, M. Theory of Groups. 2nd ed. American Mathematical Society, Providence,\\n\\n1959.\\n[5] Kurosh, A. E. The Theory of Groups, vols. I and II. American Mathematical Society,\\n\\nProvidence, 1979.\\n[6] Rotman, J. J. An Introduction to the Theory of Groups. 4th ed. Springer, New York,\\n\\n1995.\\n\\n3.7 Sage\\nMany of the groups discussed in this chapter are available for study in Sage. It is important\\nto understand that sets that form algebraic objects (groups in this chapter) are called\\n“parents” in Sage, and elements of these objects are called, well, “elements.” So every\\nelement belongs to a parent (in other words, is contained in some set). We can ask about\\nproperties of parents (finite? order? abelian?), and we can ask about properties of individual\\nelements (identity? inverse?). In the following we will show you how to create some of these\\ncommon groups and begin to explore their properties with Sage.\\n\\nIntegers mod n\\n\\nZ8 = Integers (8)\\nZ8\\n\\nRing of integers modulo 8\\n\\nZ8.list()\\n\\n[0, 1, 2, 3, 4, 5, 6, 7]\\n\\na = Z8.an_element (); a\\n\\n0\\n\\na.parent ()\\n\\nRing of integers modulo 8\\n\\n\\n\\n3.7. SAGE 53\\n\\nWe would like to work with elements of Z8. If you were to type a 6 into a compute cell\\nright now, what would you mean? The integer 6, the rational number 6\\n\\n1 , the real number\\n6.00000, or the complex number 6.00000 + 0.00000i? Or perhaps you really do want the\\ninteger 6 mod 8? Sage really has no idea what you mean or want. To make this clear, you\\ncan “coerce” 6 into Z8 with the syntax Z8(6). Without this, Sage will treat a input number\\nlike 6 as an integer, the simplest possible interpretation in some sense. Study the following\\ncarefully, where we first work with “normal” integers and then with integers mod 8.\\n\\na = 6\\na\\n\\n6\\n\\na.parent ()\\n\\nInteger Ring\\n\\nb = 7\\nc = a + b; c\\n\\n13\\n\\nd = Z8(6)\\nd\\n\\n6\\n\\nd.parent ()\\n\\nRing of integers modulo 8\\n\\ne = Z8(7)\\nf = d+e; f\\n\\n5\\n\\ng = Z8(85); g\\n\\n5\\n\\nf == g\\n\\nTrue\\n\\nZ8 is a bit unusual as a first example, since it has two operations defined, both addition\\nand multiplication, with addition forming a group, and multiplication not forming a group.\\nStill, we can work with the additive portion, here forming the Cayley table for the addition.\\n\\nZ8.addition_table(names= \' elements \' )\\n\\n\\n\\n54 CHAPTER 3. GROUPS\\n\\n+ 0 1 2 3 4 5 6 7\\n+----------------\\n\\n0| 0 1 2 3 4 5 6 7\\n1| 1 2 3 4 5 6 7 0\\n2| 2 3 4 5 6 7 0 1\\n3| 3 4 5 6 7 0 1 2\\n4| 4 5 6 7 0 1 2 3\\n5| 5 6 7 0 1 2 3 4\\n6| 6 7 0 1 2 3 4 5\\n7| 7 0 1 2 3 4 5 6\\n\\nWhen n is a prime number, the multipicative structure (excluding zero), will also form\\na group.\\n\\nThe integers mod n are very important, so Sage implements both addition and mul-\\ntiplication together. Groups of symmetries are a better example of how Sage implements\\ngroups, since there is just one operation present.\\n\\nGroups of symmetries\\n\\nThe symmetries of some geometric shapes are already defined in Sage, albeit with different\\nnames. They are implemented as “permutation groups” which we will begin to study\\ncarefully in Chapter 5.\\n\\nSage uses integers to label vertices, starting the count at 1, instead of letters. Elements\\nby default are printed using “cycle notation” which we will see described carefully in Chap-\\nter 5. Here is an example, with both the mathematics and Sage. For the Sage part, we\\ncreate the group of symmetries and then create the symmetry ρ2 with coercion, followed\\nby outputting the element in cycle notation. Then we create just the bottom row of the\\nnotation we are using for permutations.\\n\\nρ2 =\\n\\n(\\nA B C\\n\\nC A B\\n\\n)\\n=\\n\\n(\\n1 2 3\\n\\n3 1 2\\n\\n)\\n\\ntriangle = SymmetricGroup (3)\\nrho2 = triangle ([3,1 ,2])\\nrho2\\n\\n(1,3,2)\\n\\n[rho2(x) for x in triangle.domain ()]\\n\\n[3, 1, 2]\\n\\nThe final list comprehension deserves comment. The .domain() method gives a lait of\\nthe symbols used for the permutation group triangle and then rho2 is employed with syntax\\nlike it is a function (it is a function) to create the images that would occupy the bottom\\nrow.\\n\\nWith a double list comprehension we can list all six elements of the group in the “bottom\\nrow” format. A good exercise would be to pair up each element with its name as given in\\nFigure 3.6.\\n\\n[[a(x) for x in triangle.domain ()] for a in triangle]\\n\\n[[1, 2, 3], [2, 1, 3], [2, 3, 1], [3, 1, 2], [1, 3, 2], [3, 2, 1]]\\n\\n\\n\\n3.7. SAGE 55\\n\\nDifferent books, different authors, different software all have different ideas about the\\norder in which to write multiplication of functions. This textbook builds on the idea of\\ncomposition of functions, so that fg is the composition (fg)(x) = f(g(x)) and it is natural\\nto apply g first. Sage takes the opposite view and since we write fg, Sage will understand\\nthat we want to do f first. Neither approach is wrong, and neither is necessarily superior,\\nthey are just different and there are good arguments for either one. When you consult other\\nbooks that work with permutation groups, you want to first determine which approach it\\ntakes. (Be aware that this discussion of Sage function composition is limited to permutations\\nonly—“regular” functions in Sage compose in the order you might be familiar with from a\\ncalculus course.)\\n\\nThe translation here between the text and Sage will be worthwhile practice. Here we\\nwill reprise the discussion at the end of Section 3.1, but reverse the order on each product\\nto compute Sage-style and exactly mirror what the text does.\\n\\nmu1 = triangle ([1 ,3,2])\\nmu2 = triangle ([3 ,2,1])\\nmu3 = triangle ([2 ,1,3])\\nrho1 = triangle ([2,3 ,1])\\nproduct = rho1*mu1\\nproduct == mu2\\n\\nTrue\\n\\n[product(x) for x in triangle.domain ()]\\n\\n[3, 2, 1]\\n\\nrho1*mu1 == mu1*rho1\\n\\nFalse\\n\\nmu1*rho1 == mu3\\n\\nTrue\\n\\nNow that we understand that Sage does multiplication in reverse, we can compute the\\nCayley table for this group. Default behavior is to just name elements of a group as letters,\\na, b, c, … in the same order that the .list() command would produce the elements of\\nthe group. But you can also print the elements in the table as themselves (that uses cycle\\nnotation here), or you can give the elements names. We will use u as shorthand for µ and\\nr as shorthand for ρ.\\n\\ntriangle.cayley_table ()\\n\\n* a b c d e f\\n+------------\\n\\na| a b c d e f\\nb| b a f e d c\\nc| c e d a f b\\nd| d f a c b e\\ne| e c b f a d\\nf| f d e b c a\\n\\ntriangle.cayley_table(names= \' elements \' )\\n\\n\\n\\n56 CHAPTER 3. GROUPS\\n\\n* () (1,2) (1,2,3) (1,3,2) (2,3) (1,3)\\n+------------------------------------------------\\n\\n()| () (1,2) (1,2,3) (1,3,2) (2,3) (1,3)\\n(1,2)| (1,2) () (1,3) (2,3) (1,3,2) (1,2,3)\\n\\n(1,2,3)| (1,2,3) (2,3) (1,3,2) () (1,3) (1,2)\\n(1,3,2)| (1,3,2) (1,3) () (1,2,3) (1,2) (2,3)\\n\\n(2,3)| (2,3) (1,2,3) (1,2) (1,3) () (1,3,2)\\n(1,3)| (1,3) (1,3,2) (2,3) (1,2) (1,2,3) ()\\n\\ntriangle.cayley_table(names=[ \' id \' , \' u1 \' , \' u3 \' , \' r1 \' , \' r2 \' , \' u2 \' ])\\n\\n* id u1 u3 r1 r2 u2\\n+------------------\\n\\nid| id u1 u3 r1 r2 u2\\nu1| u1 id u2 r2 r1 u3\\nu3| u3 r2 r1 id u2 u1\\nr1| r1 u2 id u3 u1 r2\\nr2| r2 u3 u1 u2 id r1\\nu2| u2 r1 r2 u1 u3 id\\n\\nYou should verify that the table above is correct, just like Table 3.2 is correct. Remember\\nthat the convention is to multiply a row label times a column label, in that order. However,\\nto do a check across the two tables, you will need to recall the difference in ordering between\\nyour textbook and Sage.\\n\\nQuaternions\\nSage implements the quaternions, but the elements are not matrices, but rather are permu-\\ntations. Despite appearances the structure is identical. It should not matter which version\\nyou have in mind (matrices or permutations) if you build the Cayley table and use the\\ndefault behavior of using letters to name the elements. As permutations, or as letters, can\\nyou identify −1, I, J and K?\\n\\nQ = QuaternionGroup ()\\n[[a(x) for x in Q.domain ()] for a in Q]\\n\\n[[1, 2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 1, 6, 7, 8, 5],\\n[5, 8, 7, 6, 3, 2, 1, 4], [3, 4, 1, 2, 7, 8, 5, 6],\\n[6, 5, 8, 7, 4, 3, 2, 1], [8, 7, 6, 5, 2, 1, 4, 3],\\n[4, 1, 2, 3, 8, 5, 6, 7], [7, 6, 5, 8, 1, 4, 3, 2]]\\n\\nQ.cayley_table ()\\n\\n* a b c d e f g h\\n+----------------\\n\\na| a b c d e f g h\\nb| b d f g c h a e\\nc| c e d h g b f a\\nd| d g h a f e b c\\ne| e h b f d a c g\\nf| f c g e a d h b\\ng| g a e b h c d f\\nh| h f a c b g e d\\n\\nIt should be fairly obvious that a is the identity element of the group (1), either from\\nits behavior in the table, or from its “bottom row” representation in the list above. And if\\nyou prefer, you can ask Sage.\\n\\n\\n\\n3.7. SAGE 57\\n\\nid = Q.identity ()\\n[id(x) for x in Q.domain ()]\\n\\n[1, 2, 3, 4, 5, 6, 7, 8]\\n\\nNow −1 should have the property that −1 · −1 = 1. We see that the identity element a\\n\\nis on the diagonal of the Cayley table only when we compute c*c. We can verify this easily,\\nborrowing the third “bottom row” element from the list above. With this information, once\\nwe locate I, we can easily compute −I, and so on.\\n\\nminus_one = Q([3, 4, 1, 2, 7, 8, 5, 6])\\nminus_one*minus_one == Q.identity ()\\n\\nTrue\\n\\nSee if you can pair up the letters with all eight elements of the quaternions. Be a bit\\ncareful with your names, the symbol I is used by Sage for the imaginary number i (which\\nwe will use below), but Sage will silently let you redefine it to be anything you like. Same\\ngoes for lower-case i. So call your elements of the quaternions something like QI, QJ, QK to\\navoid confusion.\\n\\nAs we begin to work with groups it is instructive to work with the actual elements. But\\nmany properties of groups are totally independent of the order we use for multiplication, or\\nthe names or representations we use for the elements. Here are facts about the quaternions\\nwe can compute without any knowledge of just how the elements are written or multiplied.\\n\\nQ.is_finite ()\\n\\nTrue\\n\\nQ.order()\\n\\n8\\n\\nQ.is_abelian ()\\n\\nFalse\\n\\nSubgroups\\nThe best techniques for creating subgroups will come in future chapters, but we can create\\nsome groups that are naturally subgroups of other groups.\\n\\nElements of the quaternions were represented by certain permutations of the integers 1\\nthrough 8. We can also build the group of all permutations of these eight integers. It gets\\npretty big, so do not list it unless you want a lot of output! (I dare you.)\\n\\nS8 = SymmetricGroup (8)\\na = S8.random_element ()\\n[a(x) for x in S8.domain ()] # random\\n\\n[5, 2, 6, 4, 1, 8, 3, 7]\\n\\nS8.order ()\\n\\n40320\\n\\n\\n\\n58 CHAPTER 3. GROUPS\\n\\nThe quaternions, Q, is a subgroup of the full group of all permutations, the symmetric\\ngroup S8 or S8, and Sage regards this as a property of Q.\\n\\nQ.is_subgroup(S8)\\n\\nTrue\\n\\nIn Sage the complex numbers are known by the name CC. We can create a list of the\\nelements in the subgroup described in Example 3.16. Then we can verify that this set is a\\nsubgroup by examining the Cayley table, using multiplication as the operation.\\n\\nH = [CC(1), CC(-1), CC(I), CC(-I)]\\nCC.multiplication_table(elements=H,\\n\\nnames=[ \' 1 \' , \' -1 \' , \' i \' , \' -i \' ])\\n\\n* 1 -1 i -i\\n+------------\\n\\n1| 1 -1 i -i\\n-1| -1 1 -i i\\ni| i -i -1 1\\n\\n-i| -i i 1 -1\\n\\n3.8 Sage Exercises\\nThese exercises are about becoming comfortable working with groups in Sage.\\n1. Create the groups CyclicPermutationGroup(8) and DihedralGroup(4) and name these groups\\nC and D, respectively. We will understand these constructions better shortly, but for now\\njust understand that both objects you create are actually groups.\\n\\n2. Check that C and D have the same size by using the .order() method. Determine which\\ngroup is abelian, and which is not, by using the .is_abelian() method.\\n\\n3. Use the .cayley_table() method to create the Cayley table for each group.\\n\\n4. Write a nicely formatted discussion identifying differences between the two groups that\\nare discernible in properties of their Cayley tables. In other words, what is different about\\nthese two groups that you can “see” in the Cayley tables? (In the Sage notebook, a Shift-\\nclick on a blue bar will bring up a mini-word-processor, and you can use use dollar signs to\\nembed mathematics formatted using TEX syntax.)\\n\\n5. For C locate the one subgroup of order 4. The group D has three subgroups of order 4.\\nSelect one of the three subgroups of D that has a different structure than the subgroup you\\nobtained from C.\\nThe .subgroups() method will give you a list of all of the subgroups to help you get started.\\nA Cayley table will help you tell the difference between the two subgroups. What properties\\nof these tables did you use to determine the difference in the structure of the subgroups?\\n\\n6. The .subgroup(elt_list) method of a group will create the smallest subgroup containing\\nthe specified elements of the group, when given the elements as a list elt_list. Use this\\ncommand to discover the shortest list of elements necessary to recreate the subgroups you\\nfound in the previous exercise. The equality comparison, ==, can be used to test if two\\nsubgroups are equal.\\n\\n\\n\\n4\\n\\nCyclic Groups\\n\\nThe groups Z and Zn, which are among the most familiar and easily understood groups, are\\nboth examples of what are called cyclic groups. In this chapter we will study the properties\\nof cyclic groups and cyclic subgroups, which play a fundamental part in the classification\\nof all abelian groups.\\n\\n4.1 Cyclic Subgroups\\n\\nOften a subgroup will depend entirely on a single element of the group; that is, knowing\\nthat particular element will allow us to compute any other element in the subgroup.\\n\\nExample 4.1. Suppose that we consider 3 ∈ Z and look at all multiples (both positive and\\nnegative) of 3. As a set, this is\\n\\n3Z = {. . . ,−3, 0, 3, 6, . . .}.\\n\\nIt is easy to see that 3Z is a subgroup of the integers. This subgroup is completely deter-\\nmined by the element 3 since we can obtain all of the other elements of the group by taking\\nmultiples of 3. Every element in the subgroup is “generated” by 3.\\n\\nExample 4.2. If H = {2n : n ∈ Z}, then H is a subgroup of the multiplicative group of\\nnonzero rational numbers, Q∗. If a = 2m and b = 2n are in H, then ab−1 = 2m2−n = 2m−n\\n\\nis also in H. By Proposition 3.31, H is a subgroup of Q∗ determined by the element 2.\\n\\nTheorem 4.3. Let G be a group and a be any element in G. Then the set\\n\\n⟨a⟩ = {ak : k ∈ Z}\\n\\nis a subgroup of G. Furthermore, ⟨a⟩ is the smallest subgroup of G that contains a.\\n\\nProof. The identity is in ⟨a⟩ since a0 = e. If g and h are any two elements in ⟨a⟩, then\\nby the definition of ⟨a⟩ we can write g = am and h = an for some integers m and n. So\\ngh = aman = am+n is again in ⟨a⟩. Finally, if g = an in ⟨a⟩, then the inverse g−1 = a−n\\n\\nis also in ⟨a⟩. Clearly, any subgroup H of G containing a must contain all the powers of a\\nby closure; hence, H contains ⟨a⟩. Therefore, ⟨a⟩ is the smallest subgroup of G containing\\na.\\n\\nRemark 4.4. If we are using the “+” notation, as in the case of the integers under addition,\\nwe write ⟨a⟩ = {na : n ∈ Z}.\\n\\n59\\n\\n\\n\\n60 CHAPTER 4. CYCLIC GROUPS\\n\\nFor a ∈ G, we call ⟨a⟩ the cyclic subgroup generated by a. If G contains some element\\na such that G = ⟨a⟩, then G is a cyclic group. In this case a is a generator of G. If a\\nis an element of a group G, we define the order of a to be the smallest positive integer n\\nsuch that an = e, and we write |a| = n. If there is no such integer n, we say that the order\\nof a is infinite and write |a| = ∞ to denote the order of a.\\n\\nExample 4.5. Notice that a cyclic group can have more than a single generator. Both\\n1 and 5 generate Z6; hence, Z6 is a cyclic group. Not every element in a cyclic group\\nis necessarily a generator of the group. The order of 2 ∈ Z6 is 3. The cyclic subgroup\\ngenerated by 2 is ⟨2⟩ = {0, 2, 4}.\\n\\nThe groups Z and Zn are cyclic groups. The elements 1 and −1 are generators for Z.\\nWe can certainly generate Zn with 1 although there may be other generators of Zn, as in\\nthe case of Z6.\\n\\nExample 4.6. The group of units, U(9), in Z9 is a cyclic group. As a set, U(9) is\\n{1, 2, 4, 5, 7, 8}. The element 2 is a generator for U(9) since\\n\\n21 = 2 22 = 4\\n\\n23 = 8 24 = 7\\n\\n25 = 5 26 = 1.\\n\\nExample 4.7. Not every group is a cyclic group. Consider the symmetry group of an\\nequilateral triangle S3. The multiplication table for this group is Table 3.7. The subgroups\\nof S3 are shown in Figure 4.8. Notice that every subgroup is cyclic; however, no single\\nelement generates the entire group.\\n\\n{id, ρ1, ρ2} {id, µ1} {id, µ2} {id, µ3}\\n\\nS3\\n\\n{id}\\n\\nFigure 4.8: Subgroups of S3\\n\\nTheorem 4.9. Every cyclic group is abelian.\\n\\nProof. Let G be a cyclic group and a ∈ G be a generator for G. If g and h are in G, then\\nthey can be written as powers of a, say g = ar and h = as. Since\\n\\ngh = aras = ar+s = as+r = asar = hg,\\n\\nG is abelian.\\n\\nSubgroups of Cyclic Groups\\nWe can ask some interesting questions about cyclic subgroups of a group and subgroups of\\na cyclic group. If G is a group, which subgroups of G are cyclic? If G is a cyclic group,\\nwhat type of subgroups does G possess?\\n\\n\\n\\n4.1. CYCLIC SUBGROUPS 61\\n\\nTheorem 4.10. Every subgroup of a cyclic group is cyclic.\\nProof. The main tools used in this proof are the division algorithm and the Principle of\\nWell-Ordering. Let G be a cyclic group generated by a and suppose that H is a subgroup\\nof G. If H = {e}, then trivially H is cyclic. Suppose that H contains some other element\\ng distinct from the identity. Then g can be written as an for some integer n. Since H is a\\nsubgroup, g−1 = a−n must also be in H. Since either n or −n is postive, we can assume\\nthat H contains positve powers of a and n > 0. Let m be the smallest natural number such\\nthat am ∈ H. Such an m exists by the Principle of Well-Ordering.\\n\\nWe claim that h = am is a generator for H. We must show that every h′ ∈ H can be\\nwritten as a power of h. Since h′ ∈ H and H is a subgroup of G, h′ = ak for some integer\\nk. Using the division algorithm, we can find numbers q and r such that k = mq + r where\\n0 ≤ r < m; hence,\\n\\nak = amq+r = (am)qar = hqar.\\n\\nSo ar = akh−q. Since ak and h−q are in H, ar must also be in H. However, m was the\\nsmallest positive number such that am was in H; consequently, r = 0 and so k = mq.\\nTherefore,\\n\\nh′ = ak = amq = hq\\n\\nand H is generated by h.\\n\\nCorollary 4.11. The subgroups of Z are exactly nZ for n = 0, 1, 2, . . ..\\nProposition 4.12. Let G be a cyclic group of order n and suppose that a is a generator\\nfor G. Then ak = e if and only if n divides k.\\nProof. First suppose that ak = e. By the division algorithm, k = nq+ r where 0 ≤ r < n;\\nhence,\\n\\ne = ak = anq+r = anqar = ear = ar.\\n\\nSince the smallest positive integer m such that am = e is n, r = 0.\\nConversely, if n divides k, then k = ns for some integer s. Consequently,\\n\\nak = ans = (an)s = es = e.\\n\\nTheorem 4.13. Let G be a cyclic group of order n and suppose that a ∈ G is a generator\\nof the group. If b = ak, then the order of b is n/d, where d = gcd(k, n).\\nProof. We wish to find the smallest integer m such that e = bm = akm. By Proposi-\\ntion 4.12, this is the smallest integer m such that n divides km or, equivalently, n/d divides\\nm(k/d). Since d is the greatest common divisor of n and k, n/d and k/d are relatively\\nprime. Hence, for n/d to divide m(k/d) it must divide m. The smallest such m is n/d.\\n\\nCorollary 4.14. The generators of Zn are the integers r such that 1 ≤ r < n and gcd(r, n) =\\n1.\\nExample 4.15. Let us examine the group Z16. The numbers 1, 3, 5, 7, 9, 11, 13, and 15\\nare the elements of Z16 that are relatively prime to 16. Each of these elements generates\\nZ16. For example,\\n\\n1 · 9 = 9 2 · 9 = 2 3 · 9 = 11\\n\\n4 · 9 = 4 5 · 9 = 13 6 · 9 = 6\\n\\n7 · 9 = 15 8 · 9 = 8 9 · 9 = 1\\n\\n10 · 9 = 10 11 · 9 = 3 12 · 9 = 12\\n\\n13 · 9 = 5 14 · 9 = 14 15 · 9 = 7.\\n\\n\\n\\n62 CHAPTER 4. CYCLIC GROUPS\\n\\n4.2 Multiplicative Group of Complex Numbers\\nThe complex numbers are defined as\\n\\nC = {a+ bi : a, b ∈ R},\\n\\nwhere i2 = −1. If z = a+ bi, then a is the real part of z and b is the imaginary part of\\nz.\\n\\nTo add two complex numbers z = a+ bi and w = c+ di, we just add the corresponding\\nreal and imaginary parts:\\n\\nz + w = (a+ bi) + (c+ di) = (a+ c) + (b+ d)i.\\n\\nRemembering that i2 = −1, we multiply complex numbers just like polynomials. The\\nproduct of z and w is\\n\\n(a+ bi)(c+ di) = ac+ bdi2 + adi+ bci = (ac− bd) + (ad+ bc)i.\\n\\nEvery nonzero complex number z = a + bi has a multiplicative inverse; that is, there\\nexists a z−1 ∈ C∗ such that zz−1 = z−1z = 1. If z = a+ bi, then\\n\\nz−1 =\\na− bi\\n\\na2 + b2\\n.\\n\\nThe complex conjugate of a complex number z = a+ bi is defined to be z = a− bi. The\\nabsolute value or modulus of z = a+ bi is |z| =\\n\\n√\\na2 + b2.\\n\\nExample 4.16. Let z = 2 + 3i and w = 1− 2i. Then\\n\\nz + w = (2 + 3i) + (1− 2i) = 3 + i\\n\\nand\\nzw = (2 + 3i)(1− 2i) = 8− i.\\n\\nAlso,\\n\\nz−1 =\\n2\\n\\n13\\n− 3\\n\\n13\\ni\\n\\n|z| =\\n√\\n13\\n\\nz = 2− 3i.\\n\\ny\\n\\nx0\\n\\nz1 = 2 + 3i\\n\\nz3 = −3 + 2i\\n\\nz2 = 1− 2i\\n\\nFigure 4.17: Rectangular coordinates of a complex number\\n\\n\\n\\n4.2. MULTIPLICATIVE GROUP OF COMPLEX NUMBERS 63\\n\\nThere are several ways of graphically representing complex numbers. We can represent\\na complex number z = a+ bi as an ordered pair on the xy plane where a is the x (or real)\\ncoordinate and b is the y (or imaginary) coordinate. This is called the rectangular or\\nCartesian representation. The rectangular representations of z1 = 2+3i, z2 = 1− 2i, and\\nz3 = −3 + 2i are depicted in Figure 4.17.\\n\\ny\\n\\nx0\\n\\na+ bi\\n\\nr\\n\\nθ\\n\\nFigure 4.18: Polar coordinates of a complex number\\n\\nNonzero complex numbers can also be represented using polar coordinates. To specify\\nany nonzero point on the plane, it suffices to give an angle θ from the positive x axis in the\\ncounterclockwise direction and a distance r from the origin, as in Figure 4.18. We can see\\nthat\\n\\nz = a+ bi = r(cos θ + i sin θ).\\nHence,\\n\\nr = |z| =\\n√\\na2 + b2\\n\\nand\\n\\na = r cos θ\\nb = r sin θ.\\n\\nWe sometimes abbreviate r(cos θ + i sin θ) as r cis θ. To assure that the representation of z\\nis well-defined, we also require that 0◦ ≤ θ < 360◦. If the measurement is in radians, then\\n0 ≤ θ < 2π.\\n\\nExample 4.19. Suppose that z = 2 cis 60◦. Then\\n\\na = 2 cos 60◦ = 1\\n\\nand\\nb = 2 sin 60◦ =\\n\\n√\\n3.\\n\\nHence, the rectangular representation is z = 1 +\\n√\\n3 i.\\n\\nConversely, if we are given a rectangular representation of a complex number, it is often\\nuseful to know the number’s polar representation. If z = 3\\n\\n√\\n2− 3\\n\\n√\\n2 i, then\\n\\nr =\\n√\\na2 + b2 =\\n\\n√\\n36 = 6\\n\\nand\\nθ = arctan\\n\\n(\\nb\\n\\na\\n\\n)\\n= arctan(−1) = 315◦,\\n\\nso 3\\n√\\n2− 3\\n\\n√\\n2 i = 6 cis 315◦.\\n\\n\\n\\n64 CHAPTER 4. CYCLIC GROUPS\\n\\nThe polar representation of a complex number makes it easy to find products and powers\\nof complex numbers. The proof of the following proposition is straightforward and is left\\nas an exercise.\\n\\nProposition 4.20. Let z = r cis θ and w = s cisϕ be two nonzero complex numbers. Then\\n\\nzw = rs cis(θ + ϕ).\\n\\nExample 4.21. If z = 3 cis(π/3) and w = 2 cis(π/6), then zw = 6 cis(π/2) = 6i.\\n\\nTheorem 4.22 (DeMoivre). Let z = r cis θ be a nonzero complex number. Then\\n\\n[r cis θ]n = rn cis(nθ)\\n\\nfor n = 1, 2, . . ..\\n\\nProof. We will use induction on n. For n = 1 the theorem is trivial. Assume that the\\ntheorem is true for all k such that 1 ≤ k ≤ n. Then\\n\\nzn+1 = znz\\n\\n= rn(cosnθ + i sinnθ)r(cos θ + i sin θ)\\n= rn+1[(cosnθ cos θ − sinnθ sin θ) + i(sinnθ cos θ + cosnθ sin θ)]\\n= rn+1[cos(nθ + θ) + i sin(nθ + θ)]\\n\\n= rn+1[cos(n+ 1)θ + i sin(n+ 1)θ].\\n\\nExample 4.23. Suppose that z = 1+i and we wish to compute z10. Rather than computing\\n(1 + i)10 directly, it is much easier to switch to polar coordinates and calculate z10 using\\nDeMoivre’s Theorem:\\n\\nz10 = (1 + i)10\\n\\n=\\n(√\\n\\n2 cis\\n(π\\n4\\n\\n))10\\n= (\\n\\n√\\n2 )10 cis\\n\\n(\\n5π\\n\\n2\\n\\n)\\n= 32 cis\\n\\n(π\\n2\\n\\n)\\n= 32i.\\n\\nThe Circle Group and the Roots of Unity\\nThe multiplicative group of the complex numbers, C∗, possesses some interesting subgroups.\\nWhereas Q∗ and R∗ have no interesting subgroups of finite order, C∗ has many. We first\\nconsider the circle group,\\n\\nT = {z ∈ C : |z| = 1}.\\n\\nThe following proposition is a direct result of Proposition 4.20.\\n\\nProposition 4.24. The circle group is a subgroup of C∗.\\n\\nAlthough the circle group has infinite order, it has many interesting finite subgroups.\\nSuppose that H = {1,−1, i,−i}. Then H is a subgroup of the circle group. Also, 1, −1, i,\\nand −i are exactly those complex numbers that satisfy the equation z4 = 1. The complex\\nnumbers satisfying the equation zn = 1 are called the nth roots of unity.\\n\\n\\n\\n4.3. THE METHOD OF REPEATED SQUARES 65\\n\\nTheorem 4.25. If zn = 1, then the nth roots of unity are\\n\\nz = cis\\n(\\n2kπ\\n\\nn\\n\\n)\\n,\\n\\nwhere k = 0, 1, . . . , n − 1. Furthermore, the nth roots of unity form a cyclic subgroup of T\\nof order n\\n\\nProof. By DeMoivre’s Theorem,\\n\\nzn = cis\\n(\\nn\\n2kπ\\n\\nn\\n\\n)\\n= cis(2kπ) = 1.\\n\\nThe z’s are distinct since the numbers 2kπ/n are all distinct and are greater than or equal\\nto 0 but less than 2π. The fact that these are all of the roots of the equation zn = 1 follows\\nfrom from Corollary 17.9, which states that a polynomial of degree n can have at most n\\nroots. We will leave the proof that the nth roots of unity form a cyclic subgroup of T as an\\nexercise.\\n\\nA generator for the group of the nth roots of unity is called a primitive nth root of\\nunity.\\n\\nExample 4.26. The 8th roots of unity can be represented as eight equally spaced points\\non the unit circle (Figure 4.27). The primitive 8th roots of unity are\\n\\nω =\\n\\n√\\n2\\n\\n2\\n+\\n\\n√\\n2\\n\\n2\\ni\\n\\nω3 = −\\n√\\n2\\n\\n2\\n+\\n\\n√\\n2\\n\\n2\\ni\\n\\nω5 = −\\n√\\n2\\n\\n2\\n−\\n\\n√\\n2\\n\\n2\\ni\\n\\nω7 =\\n\\n√\\n2\\n\\n2\\n−\\n\\n√\\n2\\n\\n2\\ni.\\n\\ny\\n\\nx0 1\\n\\nω\\n\\ni\\n\\nω3\\n\\n−1\\n\\nω5\\n\\n−i\\n\\nω7\\n\\nFigure 4.27: 8th roots of unity\\n\\n4.3 The Method of Repeated Squares\\nComputing large powers can be very time-consuming. Just as anyone can compute 22 or\\n28, everyone knows how to compute\\n\\n22\\n1000000\\n\\n.\\n\\n\\n\\n66 CHAPTER 4. CYCLIC GROUPS\\n\\nHowever, such numbers are so large that we do not want to attempt the calculations;\\nmoreover, past a certain point the computations would not be feasible even if we had every\\ncomputer in the world at our disposal. Even writing down the decimal representation of a\\nvery large number may not be reasonable. It could be thousands or even millions of digits\\nlong. However, if we could compute something like\\n\\n237398332 (mod 46389),\\n\\nwe could very easily write the result down since it would be a number between 0 and 46,388.\\nIf we want to compute powers modulo n quickly and efficiently, we will have to be clever.1\\n\\nThe first thing to notice is that any number a can be written as the sum of distinct\\npowers of 2; that is, we can write\\n\\na = 2k1 + 2k2 + · · ·+ 2kn ,\\n\\nwhere k1 < k2 < · · · < kn. This is just the binary representation of a. For example, the\\nbinary representation of 57 is 111001, since we can write 57 = 20 + 23 + 24 + 25.\\n\\nThe laws of exponents still work in Zn; that is, if b ≡ ax (mod n) and c ≡ ay (mod n),\\nthen bc ≡ ax+y (mod n). We can compute a2k (mod n) in k multiplications by computing\\n\\na2\\n0\\n\\n(mod n)\\n\\na2\\n1\\n\\n(mod n)\\n\\n...\\n\\na2\\nk\\n\\n(mod n).\\n\\nEach step involves squaring the answer obtained in the previous step, dividing by n, and\\ntaking the remainder.\\n\\nExample 4.28. We will compute 271321 (mod 481). Notice that\\n\\n321 = 20 + 26 + 28;\\n\\nhence, computing 271321 (mod 481) is the same as computing\\n\\n2712\\n0+26+28 ≡ 2712\\n\\n0 · 27126 · 27128 (mod 481).\\n\\nSo it will suffice to compute 2712\\ni\\n(mod 481) where i = 0, 6, 8. It is very easy to see that\\n\\n2712\\n1\\n= 73,441 ≡ 329 (mod 481).\\n\\nWe can square this result to obtain a value for 2712\\n2\\n(mod 481):\\n\\n2712\\n2 ≡ (2712\\n\\n1\\n)2 (mod 481)\\n\\n≡ (329)2 (mod 481)\\n\\n≡ 108,241 (mod 481)\\n\\n≡ 16 (mod 481).\\n\\nWe are using the fact that (a2\\nn\\n)2 ≡ a2·2\\n\\nn ≡ a2\\nn+1\\n\\n(mod n). Continuing, we can calculate\\n\\n2712\\n6 ≡ 419 (mod 481)\\n\\n1The results in this section are needed only in Chapter 7\\n\\n\\n\\n4.4. EXERCISES 67\\n\\nand\\n2712\\n\\n8 ≡ 16 (mod 481).\\n\\nTherefore,\\n\\n271321 ≡ 2712\\n0+26+28 (mod 481)\\n\\n≡ 2712\\n0 · 27126 · 27128 (mod 481)\\n\\n≡ 271 · 419 · 16 (mod 481)\\n\\n≡ 1,816,784 (mod 481)\\n\\n≡ 47 (mod 481).\\n\\nThe method of repeated squares will prove to be a very useful tool when we explore rsa\\ncryptography in Chapter 7. To encode and decode messages in a reasonable manner under\\nthis scheme, it is necessary to be able to quickly compute large powers of integers mod n.\\n\\n4.4 Exercises\\n1. Prove or disprove each of the following statements.\\n(a) All of the generators of Z60 are prime.\\n(b) U(8) is cyclic.\\n(c) Q is cyclic.\\n(d) If every proper subgroup of a group G is cyclic, then G is a cyclic group.\\n(e) A group with a finite number of subgroups is finite.\\n\\n2. Find the order of each of the following elements.\\n\\n(a) 5 ∈ Z12\\n\\n(b)\\n√\\n3 ∈ R\\n\\n(c)\\n√\\n3 ∈ R∗\\n\\n(d) −i ∈ C∗\\n\\n(e) 72 in Z240\\n\\n(f) 312 in Z471\\n\\n3. List all of the elements in each of the following subgroups.\\n(a) The subgroup of Z generated by 7\\n(b) The subgroup of Z24 generated by 15\\n(c) All subgroups of Z12\\n\\n(d) All subgroups of Z60\\n\\n(e) All subgroups of Z13\\n\\n(f) All subgroups of Z48\\n\\n(g) The subgroup generated by 3 in U(20)\\n\\n(h) The subgroup generated by 5 in U(18)\\n\\n(i) The subgroup of R∗ generated by 7\\n(j) The subgroup of C∗ generated by i where i2 = −1\\n\\n(k) The subgroup of C∗ generated by 2i\\n\\n(l) The subgroup of C∗ generated by (1 + i)/\\n√\\n2\\n\\n(m) The subgroup of C∗ generated by (1 +\\n√\\n3 i)/2\\n\\n4. Find the subgroups of GL2(R) generated by each of the following matrices.\\n\\n\\n\\n68 CHAPTER 4. CYCLIC GROUPS\\n\\n(a)\\n(\\n\\n0 1\\n\\n−1 0\\n\\n)\\n(b)\\n\\n(\\n0 1/3\\n\\n3 0\\n\\n) (c)\\n(\\n1 −1\\n\\n1 0\\n\\n)\\n(d)\\n\\n(\\n1 −1\\n\\n0 1\\n\\n) (e)\\n(\\n\\n1 −1\\n\\n−1 0\\n\\n)\\n(f)\\n\\n(√\\n3/2 1/2\\n\\n−1/2\\n√\\n3/2\\n\\n)\\n\\n5. Find the order of every element in Z18.\\n\\n6. Find the order of every element in the symmetry group of the square, D4.\\n\\n7. What are all of the cyclic subgroups of the quaternion group, Q8?\\n\\n8. List all of the cyclic subgroups of U(30).\\n\\n9. List every generator of each subgroup of order 8 in Z32.\\n\\n10. Find all elements of finite order in each of the following groups. Here the “∗” indicates\\nthe set with zero removed.\\n\\n(a) Z (b) Q∗ (c) R∗\\n\\n11. If a24 = e in a group G, what are the possible orders of a?\\n\\n12. Find a cyclic group with exactly one generator. Can you find cyclic groups with exactly\\ntwo generators? Four generators? How about n generators?\\n\\n13. For n ≤ 20, which groups U(n) are cyclic? Make a conjecture as to what is true in\\ngeneral. Can you prove your conjecture?\\n\\n14. Let\\nA =\\n\\n(\\n0 1\\n\\n−1 0\\n\\n)\\nand B =\\n\\n(\\n0 −1\\n\\n1 −1\\n\\n)\\nbe elements in GL2(R). Show that A and B have finite orders but AB does not.\\n\\n15. Evaluate each of the following.\\n\\n(a) (3− 2i) + (5i− 6)\\n\\n(b) (4− 5i)− (4i− 4)\\n\\n(c) (5− 4i)(7 + 2i)\\n\\n(d) (9− i)(9− i)\\n\\n(e) i45\\n\\n(f) (1 + i) + (1 + i)\\n\\n16. Convert the following complex numbers to the form a+ bi.\\n\\n(a) 2 cis(π/6)\\n(b) 5 cis(9π/4)\\n\\n(c) 3 cis(π)\\n(d) cis(7π/4)/2\\n\\n17. Change the following complex numbers to polar representation.\\n\\n(a) 1− i\\n\\n(b) −5\\n\\n(c) 2 + 2i\\n\\n(d)\\n√\\n3 + i\\n\\n(e) −3i\\n\\n(f) 2i+ 2\\n√\\n3\\n\\n18. Calculate each of the following expressions.\\n\\n\\n\\n4.4. EXERCISES 69\\n\\n(a) (1 + i)−1\\n\\n(b) (1− i)6\\n\\n(c) (\\n√\\n3 + i)5\\n\\n(d) (−i)10\\n\\n(e) ((1− i)/2)4\\n\\n(f) (−\\n√\\n2−\\n\\n√\\n2 i)12\\n\\n(g) (−2 + 2i)−5\\n\\n19. Prove each of the following statements.\\n\\n(a) |z| = |z|\\n(b) zz = |z|2\\n\\n(c) z−1 = z/|z|2\\n\\n(d) |z + w| ≤ |z|+ |w|\\n(e) |z − w| ≥ ||z| − |w||\\n(f) |zw| = |z||w|\\n\\n20. List and graph the 6th roots of unity. What are the generators of this group? What\\nare the primitive 6th roots of unity?\\n\\n21. List and graph the 5th roots of unity. What are the generators of this group? What\\nare the primitive 5th roots of unity?\\n\\n22. Calculate each of the following.\\n\\n(a) 2923171 (mod 582)\\n\\n(b) 2557341 (mod 5681)\\n\\n(c) 20719521 (mod 4724)\\n\\n(d) 971321 (mod 765)\\n\\n23. Let a, b ∈ G. Prove the following statements.\\n(a) The order of a is the same as the order of a−1.\\n(b) For all g ∈ G, |a| = |g−1ag|.\\n(c) The order of ab is the same as the order of ba.\\n\\n24. Let p and q be distinct primes. How many generators does Zpq have?\\n\\n25. Let p be prime and r be a positive integer. How many generators does Zpr have?\\n\\n26. Prove that Zp has no nontrivial subgroups if p is prime.\\n\\n27. If g and h have orders 15 and 16 respectively in a group G, what is the order of ⟨g⟩∩⟨h⟩?\\n\\n28. Let a be an element in a group G. What is a generator for the subgroup ⟨am⟩ ∩ ⟨an⟩?\\n\\n29. Prove that Zn has an even number of generators for n > 2.\\n\\n30. Suppose that G is a group and let a, b ∈ G. Prove that if |a| = m and |b| = n with\\ngcd(m,n) = 1, then ⟨a⟩ ∩ ⟨b⟩ = {e}.\\n\\n31. Let G be an abelian group. Show that the elements of finite order in G form a subgroup.\\nThis subgroup is called the torsion subgroup of G.\\n\\n32. Let G be a finite cyclic group of order n generated by x. Show that if y = xk where\\ngcd(k, n) = 1, then y must be a generator of G.\\n\\n33. If G is an abelian group that contains a pair of cyclic subgroups of order 2, show that\\nG must contain a subgroup of order 4. Does this subgroup have to be cyclic?\\n\\n34. Let G be an abelian group of order pq where gcd(p, q) = 1. If G contains elements a\\nand b of order p and q respectively, then show that G is cyclic.\\n\\n\\n\\n70 CHAPTER 4. CYCLIC GROUPS\\n\\n35. Prove that the subgroups of Z are exactly nZ for n = 0, 1, 2, . . ..\\n\\n36. Prove that the generators of Zn are the integers r such that 1 ≤ r < n and gcd(r, n) = 1.\\n\\n37. Prove that if G has no proper nontrivial subgroups, then G is a cyclic group.\\n\\n38. Prove that the order of an element in a cyclic group G must divide the order of the\\ngroup.\\n\\n39. Prove that if G is a cyclic group of order m and d | m, then G must have a subgroup\\nof order d.\\n\\n40. For what integers n is −1 an nth root of unity?\\n\\n41. If z = r(cos θ + i sin θ) and w = s(cosϕ + i sinϕ) are two nonzero complex numbers,\\nshow that\\n\\nzw = rs[cos(θ + ϕ) + i sin(θ + ϕ)].\\n\\n42. Prove that the circle group is a subgroup of C∗.\\n\\n43. Prove that the nth roots of unity form a cyclic subgroup of T of order n.\\n\\n44. Let α ∈ T. Prove that αm = 1 and αn = 1 if and only if αd = 1 for d = gcd(m,n).\\n\\n45. Let z ∈ C∗. If |z| ≠ 1, prove that the order of z is infinite.\\n\\n46. Let z = cos θ + i sin θ be in T where θ ∈ Q. Prove that the order of z is infinite.\\n\\n4.5 Programming Exercises\\n1. Write a computer program that will write any decimal number as the sum of distinct\\npowers of 2. What is the largest integer that yourprogram will handle?\\n\\n2. Write a computer program to calculate ax (mod n) by the method of repeated squares.\\nWhat are the largest values of n and x that your program will accept?\\n\\n4.6 References and Suggested Readings\\n[1] Koblitz, N. A Course in Number Theory and Cryptography. 2nd ed. Springer, New\\n\\nYork, 1994.\\n[2] Pomerance, C. “Cryptology and Computational Number Theory—An Introduction,”\\n\\nin Cryptology and Computational Number Theory, Pomerance, C., ed. Proceedings of\\nSymposia in Applied Mathematics, vol. 42, American Mathematical Society, Provi-\\ndence, RI, 1990. Thisbook gives an excellent account of how the method of repeated\\nsquares is used in cryptography.\\n\\n4.7 Sage\\nCyclic groups are very important, so it is no surprise that they appear in many different\\nforms in Sage. Each is slightly different, and no one implementation is ideal for an intro-\\nduction, but together they can illustrate most of the important ideas. Here is a guide to\\nthe various ways to construct, and study, a cyclic group in Sage.\\n\\n\\n\\n4.7. SAGE 71\\n\\nInfinite Cyclic Groups\\nIn Sage, the integers Z are constructed with ZZ. To build the infinite cyclic group such as 3Z\\nfrom Example 4.1, simply use 3*ZZ. As an infinite set, there is not a whole lot you can do\\nwith this. You can test if integers are in this set, or not. You can also recall the generator\\nwith the .gen() command.\\n\\nG = 3*ZZ\\n-12 in G\\n\\nTrue\\n\\n37 in G\\n\\nFalse\\n\\nG.gen()\\n\\n3\\n\\nAdditive Cyclic Groups\\nThe additive cyclic group Zn can be built as a special case of a more general Sage construc-\\ntion. First we build Z14 and capture its generator. Throughout, pay close attention to the\\nuse of parentheses and square brackets for when you experiment on your own.\\n\\nG = AdditiveAbelianGroup ([14])\\nG.order()\\n\\n14\\n\\nG.list()\\n\\n[(0), (1), (2), (3), (4), (5), (6), (7),\\n(8), (9), (10), (11), (12), (13)]\\n\\na = G.gen (0)\\na\\n\\n(1)\\n\\nYou can compute in this group, by using the generator, or by using new elements formed\\nby coercing integers into the group, or by taking the result of operations on other elements.\\nAnd we can compute the order of elements in this group. Notice that we can perform\\nrepeated additions with the shortcut of taking integer multiples of an element.\\n\\na + a\\n\\n(2)\\n\\na + a + a + a\\n\\n(4)\\n\\n4*a\\n\\n\\n\\n72 CHAPTER 4. CYCLIC GROUPS\\n\\n(4)\\n\\n37*a\\n\\n(9)\\n\\nWe can create, and then compute with, new elements of the group by coercing an integer\\n(in a list of length 1) into the group. You may get a DeprecationWarning the first time you\\nuse this syntax to create a new element. The mysterious warning can be safely ignored.\\n\\nG([2])\\n\\ndoctest :...: DeprecationWarning: The default behaviour changed! If\\nyou\\n\\n*really* want a linear combination of smith generators , use\\n.linear_combination_of_smith_form_gens.\\nSee http :// trac.sagemath.org /16261 for details.\\n(2)\\n\\nb = G([2]); b\\n\\n(2)\\n\\nb + b\\n\\n(4)\\n\\n2*b == 4*a\\n\\nTrue\\n\\n7*b\\n\\n(0)\\n\\nb.order()\\n\\n7\\n\\nc = a - 6*b; c\\n\\n(3)\\n\\nc + c + c + c\\n\\n(12)\\n\\nc.order()\\n\\n14\\n\\nIt is possible to create cyclic subgroups, from an element designated to be the new\\ngenerator. Unfortunately, to do this requires the .submodule() method (which should be\\nrenamed in Sage).\\n\\n\\n\\n4.7. SAGE 73\\n\\nH = G.submodule ([b]); H\\n\\nAdditive abelian group isomorphic to Z/7\\n\\nH.list()\\n\\n[(0), (2), (4), (6), (8), (10), (12)]\\n\\nH.order()\\n\\n7\\n\\ne = H.gen (0); e\\n\\n(2)\\n\\n3*e\\n\\n(6)\\n\\ne.order()\\n\\n7\\n\\nThe cyclic subgroup H just created has more than one generator. We can test this by\\nbuilding a new subgroup and comparing the two subgroups.\\n\\nf = 12*a; f\\n\\n(12)\\n\\nf.order()\\n\\n7\\n\\nK = G.submodule ([f]); K\\n\\nAdditive abelian group isomorphic to Z/7\\n\\nK.order()\\n\\n7\\n\\nK.list()\\n\\n[(0), (2), (4), (6), (8), (10), (12)]\\n\\nK.gen (0)\\n\\n(2)\\n\\nH == K\\n\\nTrue\\n\\nCertainly the list of elements, and the common generator of (2) lead us to belive that H\\n\\nand K are the same, but the comparison in the last line leaves no doubt.\\nResults in this section, especially Theorem 4.13 and Corollary 4.14, can be investigated\\n\\nby creating generators of subgroups from a generator of one additive cyclic group, creating\\nthe subgroups, and computing the orders of both elements and orders of groups.\\n\\n\\n\\n74 CHAPTER 4. CYCLIC GROUPS\\n\\nAbstract Multiplicative Cyclic Groups\\nWe can create an abstract cyclic group in the style of Theorems 4.3, 4.9, 4.10. In the\\nsyntax below a is a name for the generator, and 14 is the order of the element. Notice that\\nthe notation is now multiplicative, so we multiply elements, and repeated products can be\\nwritten as powers.\\n\\nG.<a> = AbelianGroup ([14])\\nG.order()\\n\\n14\\n\\nG.list()\\n\\n(1, a, a^2, a^3, a^4, a^5, a^6, a^7, a^8, a^9, a^10, a^11, a^12,\\na^13)\\n\\na.order()\\n\\n14\\n\\nComputations in the group are similar to before, only with different notation. Now\\nproducts, with repeated products written as exponentiation.\\n\\nb = a^2\\nb.order()\\n\\n7\\n\\nb*b*b\\n\\na^6\\n\\nc = a^7\\nc.order()\\n\\n2\\n\\nc^2\\n\\n1\\n\\nb*c\\n\\na^9\\n\\nb^37*c^42\\n\\na^4\\n\\nSubgroups can be formed with a .subgroup() command. But do not try to list the\\ncontents of a subgroup, it’ll look strangely unfamiliar. Also, comparison of subgroups is not\\nimplemented.\\n\\nH = G.subgroup ([a^2])\\nH.order()\\n\\n\\n\\n4.7. SAGE 75\\n\\n7\\n\\nK = G.subgroup ([a^12])\\nK.order()\\n\\n7\\n\\nL = G.subgroup ([a^4])\\nH == L\\n\\nFalse\\n\\nOne advantage of this implementation is the possibility to create all possible subgroups.\\nHere we create the list of subgroups, extract one in particular (the third), and check its\\norder.\\n\\nallsg = G.subgroups (); allsg\\n\\n[Multiplicative Abelian subgroup isomorphic to C2 x C7 generated by\\n{a},\\n\\nMultiplicative Abelian subgroup isomorphic to C7 generated by {a^2},\\nMultiplicative Abelian subgroup isomorphic to C2 generated by {a^7},\\nTrivial Abelian subgroup]\\n\\nsub = allsg [2]\\nsub.order ()\\n\\n2\\n\\nCyclic Permutation Groups\\nWe will learn more about permutation groups in the next chapter. But we will mention\\nhere that it is easy to create cyclic groups as permutation groups, and a variety of methods\\nare available for working with them, even if the actual elements get a bit cumbersome to\\nwork with. As before, notice that the notation and syntax is multiplicative.\\n\\nG=CyclicPermutationGroup (14)\\na = G.gen (0); a\\n\\n(1,2,3,4,5,6,7,8,9,10,11,12,13,14)\\n\\nb = a^2\\nb = a^2; b\\n\\n(1,3,5,7,9,11,13)(2,4,6,8,10,12,14)\\n\\nb.order()\\n\\n7\\n\\na*a*b*b*b\\n\\n(1,9,3,11,5,13,7)(2,10,4,12,6,14,8)\\n\\nc = a^37*b^26; c\\n\\n\\n\\n76 CHAPTER 4. CYCLIC GROUPS\\n\\n(1,6,11,2,7,12,3,8,13,4,9,14,5,10)\\n\\nc.order()\\n\\n14\\n\\nWe can create subgroups, check their orders, and list their elements.\\nH = G.subgroup ([a^2])\\nH.order()\\n\\n7\\n\\nH.gen (0)\\n\\n(1,3,5,7,9,11,13)(2,4,6,8,10,12,14)\\n\\nH.list()\\n\\n[(),\\n(1,3,5,7,9,11,13)(2,4,6,8,10,12,14),\\n(1,5,9,13,3,7,11)(2,6,10,14,4,8,12),\\n(1,7,13,5,11,3,9)(2,8,14,6,12,4,10),\\n(1,9,3,11,5,13,7)(2,10,4,12,6,14,8),\\n(1,11,7,3,13,9,5)(2,12,8,4,14,10,6),\\n(1,13,11,9,7,5,3)(2,14,12,10,8,6,4)]\\n\\nIt could help to visualize this group, and the subgroup, as rotations of a regular 12-\\ngon with the vertices labeled with the integers 1 through 12. This is not the full group of\\nsymmetries, since it does not include reflections, just the 12 rotations.\\n\\nCayley Tables\\nAs groups, each of the examples above (groups and subgroups) have Cayley tables imple-\\nmented. Since the groups are cyclic, and their subgroups are therefore cyclic, the Cayley\\ntables should have a similar “cyclic” pattern. Note that the letters used in the default table\\nare generic, and are not related to the letters used above for specific elements — they just\\nmatch up with the group elements in the order given by .list().\\n\\nG.<a> = AbelianGroup ([14])\\nG.cayley_table ()\\n\\n* a b c d e f g h i j k l m n\\n+----------------------------\\n\\na| a b c d e f g h i j k l m n\\nb| b c d e f g h i j k l m n a\\nc| c d e f g h i j k l m n a b\\nd| d e f g h i j k l m n a b c\\ne| e f g h i j k l m n a b c d\\nf| f g h i j k l m n a b c d e\\ng| g h i j k l m n a b c d e f\\nh| h i j k l m n a b c d e f g\\ni| i j k l m n a b c d e f g h\\nj| j k l m n a b c d e f g h i\\nk| k l m n a b c d e f g h i j\\nl| l m n a b c d e f g h i j k\\nm| m n a b c d e f g h i j k l\\nn| n a b c d e f g h i j k l m\\n\\n\\n\\n4.7. SAGE 77\\n\\nIf the real names of the elements are not too complicated, the table could be more\\ninformative using these names.\\n\\nK.<b> = AbelianGroup ([10])\\nK.cayley_table(names= \' elements \' )\\n\\n* 1 b b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9\\n+----------------------------------------\\n\\n1| 1 b b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9\\nb| b b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9 1\\n\\nb^2| b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9 1 b\\nb^3| b^3 b^4 b^5 b^6 b^7 b^8 b^9 1 b b^2\\nb^4| b^4 b^5 b^6 b^7 b^8 b^9 1 b b^2 b^3\\nb^5| b^5 b^6 b^7 b^8 b^9 1 b b^2 b^3 b^4\\nb^6| b^6 b^7 b^8 b^9 1 b b^2 b^3 b^4 b^5\\nb^7| b^7 b^8 b^9 1 b b^2 b^3 b^4 b^5 b^6\\nb^8| b^8 b^9 1 b b^2 b^3 b^4 b^5 b^6 b^7\\nb^9| b^9 1 b b^2 b^3 b^4 b^5 b^6 b^7 b^8\\n\\nComplex Roots of Unity\\nThe finite cyclic subgroups of T, generated by a primitive nth root of unity are implemented\\nas a more general construction in Sage, known as a cyclotomic field. If you concentrate\\non just the multiplication of powers of a generator (and ignore the infinitely many other\\nelements) then this is a finite cyclic group. Since this is not implemented directly in Sage\\nas a group, per se, it is a bit harder to construct things like subgroups, but it is an excellent\\nexercise to try. It is a nice example since the complex numbers are a concrete and familiar\\nconstruction. Here are a few sample calculations to provide you with some exploratory\\ntools. See the notes following the computations.\\n\\nG = CyclotomicField (14)\\nw = G.gen (0); w\\n\\nzeta14\\n\\nwc = CDF(w)\\nwc.abs()\\n\\n1.0\\n\\nwc.arg()/N(2*pi/14)\\n\\n1.0\\n\\nb = w^2\\nb.multiplicative_order ()\\n\\n7\\n\\nbc = CDF(b); bc\\n\\n0.62348980185... + 0.781831482468...*I\\n\\nbc.abs()\\n\\n\\n\\n78 CHAPTER 4. CYCLIC GROUPS\\n\\n1.0\\n\\nbc.arg()/N(2*pi/14)\\n\\n2.0\\n\\nsg = [b^i for i in range (7)]; sg\\n\\n[1, zeta14^2, zeta14^4,\\nzeta14 ^5 - zeta14 ^4 + zeta14 ^3 - zeta14 ^2 + zeta14 - 1,\\n-zeta14 , -zeta14^3, -zeta14 ^5]\\n\\nc = sg[3]; d = sg[5]\\nc*d\\n\\nzeta14 ^2\\n\\nc = sg[3]; d = sg[6]\\nc*d in sg\\n\\nTrue\\n\\nc*d == sg[2]\\n\\nTrue\\n\\nsg[5]*sg[6] == sg[4]\\n\\nTrue\\n\\nG.multiplication_table(elements=sg)\\n\\n* a b c d e f g\\n+--------------\\n\\na| a b c d e f g\\nb| b c d e f g a\\nc| c d e f g a b\\nd| d e f g a b c\\ne| e f g a b c d\\nf| f g a b c d e\\ng| g a b c d e f\\n\\nNotes:\\n\\n1. zeta14 is the name of the generator used for the cyclotomic field, it is a primitive root\\nof unity (a 14th root of unity in this case). We have captured it as w.\\n\\n2. The syntax CDF(w) will convert the complex number w into the more familiar form\\nwith real and imaginary parts.\\n\\n3. The method .abs() will return the modulus of a complex number, r as described in\\nthe text. For elements of C∗ this should always equal 1.\\n\\n4. The method .arg() will return the argument of a complex number, θ as described in\\nthe text. Every element of the cyclic group in this example should have an argument\\nthat is an integer multiple of 2π\\n\\n14 . The N() syntax converts the symbolic value of pi to\\na numerical approximation.\\n\\n\\n\\n4.8. SAGE EXERCISES 79\\n\\n5. sg is a list of elements that form a cyclic subgroup of order 7, composed of the first 7\\npowers of b = w^2. So, for example, the last comparison multiplies the fifth power of\\nb with the sixth power of b, which would be the eleventh power of b. But since b has\\norder 7, this reduces to the fourth power.\\n\\n6. If you know a subset of an infinite group forms a subgroup, then you can produce its\\nCayley table by specifying the list of elements you want to use. Here we ask for a\\nmultiplication table, since that is the relevant operation.\\n\\n4.8 Sage Exercises\\nThis group of exercises is about the group of units mod n, U(n), which is sometimes cyclic,\\nsometimes not. There are some commands in Sage that will answer some of these questions\\nvery quickly, but instead of using those now, just use the basic techniques described. The\\nidea here is to just work with elements, and lists of elements, to discern the subgroup\\nstructure of these groups.\\n1. Execute the statement R = Integers(40) to create the set [0,1,2,...,39] This is a group\\nunder addition mod 40, which we will ignore. Instead we are interested in the subset\\nof elements which have an inverse under multiplication mod 40. Determine how big this\\nsubgroup is by executing the command R.unit_group_order(), and then obtain a list of these\\nelements with R.list_of_elements_of_multiplicative_group().\\n\\n2. You can create elements of this group by coercing regular integers into U, such as with\\nthe statement a = U(7). (Don’t confuse this with our mathematical notation U(40).) This\\nwill tell Sage that you want to view 7 as an element of U , subject to the corresponding\\noperations. Determine the elements of the cyclic subgroup of U generated by 7 with a list\\ncomprehension as follows:\\n\\nR = Integers (40)\\na = R(7)\\n[a^i for i in srange (16)]\\n\\nWhat is the order of 7 in U(40)?\\n\\n3. The group U(49) is cyclic. Using only the Sage commands described previously, use\\nSage to find a generator for this group. Now using only theorems about the structure of\\ncyclic groups, describe each of the subgroups of U(49) by specifying its order and by giving\\nan explicit generator. Do not repeat any of the subgroups — in other words, present each\\nsubgroup exactly once. You can use Sage to check your work on the subgroups, but your\\nanswer about the subgroups should rely only on theorems and be a nicely written paragraph\\nwith a table, etc.\\n\\n4. The group U(35) is not cyclic. Again, using only the Sage commands described pre-\\nviously, use computations to provide irrefutable evidence of this. How many of the 16\\ndifferent subgroups of U(35) can you list?\\n\\n5. Again, using only the Sage commands described previously, explore the structure of U(n)\\nfor various values of n and see if you can formulate an interesting conjecture about some\\nbasic property of this group. (Yes, this is a very open-ended question, but this is ultimately\\nthe real power of exploring mathematics with Sage.)\\n\\n\\n\\n5\\n\\nPermutation Groups\\n\\nPermutation groups are central to the study of geometric symmetries and to Galois the-\\nory, the study of finding solutions of polynomial equations. They also provide abundant\\nexamples of nonabelian groups.\\n\\nLet us recall for a moment the symmetries of the equilateral triangle △ABC from\\nChapter 3. The symmetries actually consist of permutations of the three vertices, where a\\npermutation of the set S = {A,B,C} is a one-to-one and onto map π : S → S. The three\\nvertices have the following six permutations.(\\n\\nA B C\\n\\nA B C\\n\\n) (\\nA B C\\n\\nC A B\\n\\n) (\\nA B C\\n\\nB C A\\n\\n)\\n(\\nA B C\\n\\nA C B\\n\\n) (\\nA B C\\n\\nC B A\\n\\n) (\\nA B C\\n\\nB A C\\n\\n)\\nWe have used the array (\\n\\nA B C\\n\\nB C A\\n\\n)\\nto denote the permutation that sends A to B, B to C, and C to A. That is,\\n\\nA 7→ B\\n\\nB 7→ C\\n\\nC 7→ A.\\n\\nThe symmetries of a triangle form a group. In this chapter we will study groups of this\\ntype.\\n\\n5.1 Definitions and Notation\\nIn general, the permutations of a set X form a group SX . If X is a finite set, we can assume\\nX = {1, 2, . . . , n}. In this case we write Sn instead of SX . The following theorem says that\\nSn is a group. We call this group the symmetric group on n letters.\\n\\nTheorem 5.1. The symmetric group on n letters, Sn, is a group with n! elements, where\\nthe binary operation is the composition of maps.\\n\\nProof. The identity of Sn is just the identity map that sends 1 to 1, 2 to 2, . . ., n to n. If\\nf : Sn → Sn is a permutation, then f−1 exists, since f is one-to-one and onto; hence, every\\npermutation has an inverse. Composition of maps is associative, which makes the group\\noperation associative. We leave the proof that |Sn| = n! as an exercise.\\n\\n80\\n\\n\\n\\n5.1. DEFINITIONS AND NOTATION 81\\n\\nA subgroup of Sn is called a permutation group.\\n\\nExample 5.2. Consider the subgroup G of S5 consisting of the identity permutation id\\nand the permutations\\n\\nσ =\\n\\n(\\n1 2 3 4 5\\n\\n1 2 3 5 4\\n\\n)\\nτ =\\n\\n(\\n1 2 3 4 5\\n\\n3 2 1 4 5\\n\\n)\\nµ =\\n\\n(\\n1 2 3 4 5\\n\\n3 2 1 5 4\\n\\n)\\n.\\n\\nThe following table tells us how to multiply elements in the permutation group G.\\n\\n◦ id σ τ µ\\n\\nid id σ τ µ\\n\\nσ σ id µ τ\\n\\nτ τ µ id σ\\n\\nµ µ τ σ id\\n\\nRemark 5.3. Though it is natural to multiply elements in a group from left to right,\\nfunctions are composed from right to left. Let σ and τ be permutations on a set X. To\\ncompose σ and τ as functions, we calculate (σ◦τ)(x) = σ(τ(x)). That is, we do τ first, then\\nσ. There are several ways to approach this inconsistency. We will adopt the convention of\\nmultiplying permutations right to left. To compute στ , do τ first and then σ. That is, by\\nστ(x) we mean σ(τ(x)). (Another way of solving this problem would be to write functions\\non the right; that is, instead of writing σ(x), we could write (x)σ. We could also multiply\\npermutations left to right to agree with the usual way of multiplying elements in a group.\\nCertainly all of these methods have been used.\\n\\nExample 5.4. Permutation multiplication is not usually commutative. Let\\n\\nσ =\\n\\n(\\n1 2 3 4\\n\\n4 1 2 3\\n\\n)\\nτ =\\n\\n(\\n1 2 3 4\\n\\n2 1 4 3\\n\\n)\\n.\\n\\nThen\\nστ =\\n\\n(\\n1 2 3 4\\n\\n1 4 3 2\\n\\n)\\n,\\n\\nbut\\nτσ =\\n\\n(\\n1 2 3 4\\n\\n3 2 1 4\\n\\n)\\n.\\n\\nCycle Notation\\nThe notation that we have used to represent permutations up to this point is cumbersome,\\nto say the least. To work effectively with permutation groups, we need a more streamlined\\nmethod of writing down and manipulating permutations.\\n\\n\\n\\n82 CHAPTER 5. PERMUTATION GROUPS\\n\\nA permutation σ ∈ SX is a cycle of length k if there exist elements a1, a2, . . . , ak ∈ X\\nsuch that\\n\\nσ(a1) = a2\\n\\nσ(a2) = a3\\n...\\n\\nσ(ak) = a1\\n\\nand σ(x) = x for all other elements x ∈ X. We will write (a1, a2, . . . , ak) to denote the\\ncycle σ. Cycles are the building blocks of all permutations.\\n\\nExample 5.5. The permutation\\n\\nσ =\\n\\n(\\n1 2 3 4 5 6 7\\n\\n6 3 5 1 4 2 7\\n\\n)\\n= (162354)\\n\\nis a cycle of length 6, whereas\\n\\nτ =\\n\\n(\\n1 2 3 4 5 6\\n\\n1 4 2 3 5 6\\n\\n)\\n= (243)\\n\\nis a cycle of length 3.\\nNot every permutation is a cycle. Consider the permutation(\\n\\n1 2 3 4 5 6\\n\\n2 4 1 3 6 5\\n\\n)\\n= (1243)(56).\\n\\nThis permutation actually contains a cycle of length 2 and a cycle of length 4.\\n\\nExample 5.6. It is very easy to compute products of cycles. Suppose that\\n\\nσ = (1352) and τ = (256).\\n\\nIf we think of σ as\\n1 7→ 3, 3 7→ 5, 5 7→ 2, 2 7→ 1,\\n\\nand τ as\\n2 7→ 5, 5 7→ 6, 6 7→ 2,\\n\\nthen for στ remembering that we apply τ first and then σ, it must be the case that\\n\\n1 7→ 3, 3 7→ 5, 5 7→ 6, 6 7→ 2 7→ 1,\\n\\nor στ = (1356). If µ = (1634), then σµ = (1652)(34).\\n\\nTwo cycles in SX , σ = (a1, a2, . . . , ak) and τ = (b1, b2, . . . , bl), are disjoint if ai ̸= bj for\\nall i and j.\\n\\nExample 5.7. The cycles (135) and (27) are disjoint; however, the cycles (135) and (347)\\nare not. Calculating their products, we find that\\n\\n(135)(27) = (135)(27)\\n\\n(135)(347) = (13475).\\n\\nThe product of two cycles that are not disjoint may reduce to something less complicated;\\nthe product of disjoint cycles cannot be simplified.\\n\\n\\n\\n5.1. DEFINITIONS AND NOTATION 83\\n\\nProposition 5.8. Let σ and τ be two disjoint cycles in SX . Then στ = τσ.\\n\\nProof. Let σ = (a1, a2, . . . , ak) and τ = (b1, b2, . . . , bl). We must show that στ(x) = τσ(x)\\nfor all x ∈ X. If x is neither in {a1, a2, . . . , ak} nor {b1, b2, . . . , bl}, then both σ and τ fix x.\\nThat is, σ(x) = x and τ(x) = x. Hence,\\n\\nστ(x) = σ(τ(x)) = σ(x) = x = τ(x) = τ(σ(x)) = τσ(x).\\n\\nDo not forget that we are multiplying permutations right to left, which is the opposite of the\\norder in which we usually multiply group elements. Now suppose that x ∈ {a1, a2, . . . , ak}.\\nThen σ(ai) = a(i mod k)+1; that is,\\n\\na1 7→ a2\\n\\na2 7→ a3\\n...\\n\\nak−1 7→ ak\\n\\nak 7→ a1.\\n\\nHowever, τ(ai) = ai since σ and τ are disjoint. Therefore,\\n\\nστ(ai) = σ(τ(ai))\\n\\n= σ(ai)\\n\\n= a(i mod k)+1\\n\\n= τ(a(i mod k)+1)\\n\\n= τ(σ(ai))\\n\\n= τσ(ai).\\n\\nSimilarly, if x ∈ {b1, b2, . . . , bl}, then σ and τ also commute.\\n\\nTheorem 5.9. Every permutation in Sn can be written as the product of disjoint cycles.\\n\\nProof. We can assume that X = {1, 2, . . . , n}. If σ ∈ Sn and we define X1 to be\\n{σ(1), σ2(1), . . .}, then the set X1 is finite since X is finite. Now let i be the first inte-\\nger in X that is not in X1 and define X2 by {σ(i), σ2(i), . . .}. Again, X2 is a finite set.\\nContinuing in this manner, we can define finite disjoint sets X3, X4, . . .. Since X is a finite\\nset, we are guaranteed that this process will end and there will be only a finite number of\\nthese sets, say r. If σi is the cycle defined by\\n\\nσi(x) =\\n\\n{\\nσ(x) x ∈ Xi\\n\\nx x /∈ Xi,\\n\\nthen σ = σ1σ2 · · ·σr. Since the sets X1, X2, . . . , Xr are disjoint, the cycles σ1, σ2, . . . , σr\\nmust also be disjoint.\\n\\nExample 5.10. Let\\n\\nσ =\\n\\n(\\n1 2 3 4 5 6\\n\\n6 4 3 1 5 2\\n\\n)\\nτ =\\n\\n(\\n1 2 3 4 5 6\\n\\n3 2 1 5 6 4\\n\\n)\\n.\\n\\n\\n\\n84 CHAPTER 5. PERMUTATION GROUPS\\n\\nUsing cycle notation, we can write\\n\\nσ = (1624)\\n\\nτ = (13)(456)\\n\\nστ = (136)(245)\\n\\nτσ = (143)(256).\\n\\nRemark 5.11. From this point forward we will find it convenient to use cycle notation to\\nrepresent permutations. When using cycle notation, we often denote the identity permuta-\\ntion by (1).\\n\\nTranspositions\\nThe simplest permutation is a cycle of length 2. Such cycles are called transpositions.\\nSince\\n\\n(a1, a2, . . . , an) = (a1an)(a1an−1) · · · (a1a3)(a1a2),\\n\\nany cycle can be written as the product of transpositions, leading to the following proposi-\\ntion.\\n\\nProposition 5.12. Any permutation of a finite set containing at least two elements can be\\nwritten as the product of transpositions.\\n\\nExample 5.13. Consider the permutation\\n\\n(16)(253) = (16)(23)(25) = (16)(45)(23)(45)(25).\\n\\nAs we can see, there is no unique way to represent permutation as the product of transposi-\\ntions. For instance, we can write the identity permutation as (12)(12), as (13)(24)(13)(24),\\nand in many other ways. However, as it turns out, no permutation can be written as the\\nproduct of both an even number of transpositions and an odd number of transpositions.\\nFor instance, we could represent the permutation (16) by\\n\\n(23)(16)(23)\\n\\nor by\\n(35)(16)(13)(16)(13)(35)(56),\\n\\nbut (16) will always be the product of an odd number of transpositions.\\n\\nLemma 5.14. If the identity is written as the product of r transpositions,\\n\\nid = τ1τ2 · · · τr,\\n\\nthen r is an even number.\\n\\nProof. We will employ induction on r. A transposition cannot be the identity; hence,\\nr > 1. If r = 2, then we are done. Suppose that r > 2. In this case the product of the last\\ntwo transpositions, τr−1τr, must be one of the following cases:\\n\\n(ab)(ab) = id\\n(bc)(ab) = (ac)(bc)\\n\\n(cd)(ab) = (ab)(cd)\\n\\n(ac)(ab) = (ab)(bc),\\n\\n\\n\\n5.1. DEFINITIONS AND NOTATION 85\\n\\nwhere a, b, c, and d are distinct.\\nThe first equation simply says that a transposition is its own inverse. If this case occurs,\\n\\ndelete τr−1τr from the product to obtain\\n\\nid = τ1τ2 · · · τr−3τr−2.\\n\\nBy induction r − 2 is even; hence, r must be even.\\nIn each of the other three cases, we can replace τr−1τr with the right-hand side of the\\n\\ncorresponding equation to obtain a new product of r transpositions for the identity. In this\\nnew product the last occurrence of a will be in the next-to-the-last transposition. We can\\ncontinue this process with τr−2τr−1 to obtain either a product of r − 2 transpositions or a\\nnew product of r transpositions where the last occurrence of a is in τr−2. If the identity is\\nthe product of r − 2 transpositions, then again we are done, by our induction hypothesis;\\notherwise, we will repeat the procedure with τr−3τr−2.\\n\\nAt some point either we will have two adjacent, identical transpositions canceling each\\nother out or a will be shuffled so that it will appear only in the first transposition. However,\\nthe latter case cannot occur, because the identity would not fix a in this instance. Therefore,\\nthe identity permutation must be the product of r − 2 transpositions and, again by our\\ninduction hypothesis, we are done.\\n\\nTheorem 5.15. If a permutation σ can be expressed as the product of an even number\\nof transpositions, then any other product of transpositions equaling σ must also contain an\\neven number of transpositions. Similarly, if σ can be expressed as the product of an odd\\nnumber of transpositions, then any other product of transpositions equaling σ must also\\ncontain an odd number of transpositions.\\n\\nProof. Suppose that\\nσ = σ1σ2 · · ·σm = τ1τ2 · · · τn,\\n\\nwhere m is even. We must show that n is also an even number. The inverse of σ is σm · · ·σ1.\\nSince\\n\\nid = σσm · · ·σ1 = τ1 · · · τnσm · · ·σ1,\\nn must be even by Lemma 5.14. The proof for the case in which σ can be expressed as an\\nodd number of transpositions is left as an exercise.\\n\\nIn light of Theorem 5.15, we define a permutation to be even if it can be expressed\\nas an even number of transpositions and odd if it can be expressed as an odd number of\\ntranspositions.\\n\\nThe Alternating Groups\\nOne of the most important subgroups of Sn is the set of all even permutations, An. The\\ngroup An is called the alternating group on n letters.\\n\\nTheorem 5.16. The set An is a subgroup of Sn.\\n\\nProof. Since the product of two even permutations must also be an even permutation,\\nAn is closed. The identity is an even permutation and therefore is in An. If σ is an even\\npermutation, then\\n\\nσ = σ1σ2 · · ·σr,\\nwhere σi is a transposition and r is even. Since the inverse of any transposition is itself,\\n\\nσ−1 = σrσr−1 · · ·σ1\\n\\nis also in An.\\n\\n\\n\\n86 CHAPTER 5. PERMUTATION GROUPS\\n\\nProposition 5.17. The number of even permutations in Sn, n ≥ 2, is equal to the number\\nof odd permutations; hence, the order of An is n!/2.\\n\\nProof. Let An be the set of even permutations in Sn and Bn be the set of odd permuta-\\ntions. If we can show that there is a bijection between these sets, they must contain the\\nsame number of elements. Fix a transposition σ in Sn. Since n ≥ 2, such a σ exists. Define\\n\\nλσ : An → Bn\\n\\nby\\nλσ(τ) = στ.\\n\\nSuppose that λσ(τ) = λσ(µ). Then στ = σµ and so\\n\\nτ = σ−1στ = σ−1σµ = µ.\\n\\nTherefore, λσ is one-to-one. We will leave the proof that λσ is surjective to the reader.\\n\\nExample 5.18. The group A4 is the subgroup of S4 consisting of even permutations. There\\nare twelve elements in A4:\\n\\n(1) (12)(34) (13)(24) (14)(23)\\n\\n(123) (132) (124) (142)\\n\\n(134) (143) (234) (243).\\n\\nOne of the end-of-chapter exercises will be to write down all the subgroups of A4. You will\\nfind that there is no subgroup of order 6. Does this surprise you?\\n\\nHistorical Note\\n\\nLagrange first thought of permutations as functions from a set to itself, but it was\\nCauchy who developed the basic theorems and notation for permutations. He was the first\\nto use cycle notation. Augustin-Louis Cauchy (1789–1857) was born in Paris at the height\\nof the French Revolution. His family soon left Paris for the village of Arcueil to escape\\nthe Reign of Terror. One of the family’s neighbors there was Pierre-Simon Laplace (1749–\\n1827), who encouraged him to seek a career in mathematics. Cauchy began his career as\\na mathematician by solving a problem in geometry given to him by Lagrange. Cauchy\\nwrote over 800 papers on such diverse topics as differential equations, finite groups, applied\\nmathematics, and complex analysis. He was one of the mathematicians responsible for\\nmaking calculus rigorous. Perhaps more theorems and concepts in mathematics have the\\nname Cauchy attached to them than that of any other mathematician.\\n\\n5.2 Dihedral Groups\\nAnother special type of permutation group is the dihedral group. Recall the symmetry\\ngroup of an equilateral triangle in Chapter 3. Such groups consist of the rigid motions of\\na regular n-sided polygon or n-gon. For n = 3, 4, . . ., we define the nth dihedral group\\nto be the group of rigid motions of a regular n-gon. We will denote this group by Dn. We\\ncan number the vertices of a regular n-gon by 1, 2, . . . , n (Figure 5.19). Notice that there\\nare exactly n choices to replace the first vertex. If we replace the first vertex by k, then the\\nsecond vertex must be replaced either by vertex k+1 or by vertex k−1; hence, there are 2n\\npossible rigid motions of the n-gon. We summarize these results in the following theorem.\\n\\n\\n\\n5.2. DIHEDRAL GROUPS 87\\n\\n1\\n\\nn− 1 3\\n\\n2n\\n\\n4\\n\\nFigure 5.19: A regular n-gon\\n\\nTheorem 5.20. The dihedral group, Dn, is a subgroup of Sn of order 2n.\\n\\n8\\n1\\n\\n2\\n\\n3\\n\\n4\\n5\\n\\n6\\n\\n7\\n\\n2\\n1\\n\\n8\\n\\n7\\n\\n6\\n5\\n\\n4\\n\\n3\\nreflection\\n\\n3\\n2\\n\\n1\\n\\n8\\n\\n7\\n6\\n\\n5\\n\\n4\\n\\n2\\n1\\n\\n8\\n\\n7\\n\\n6\\n5\\n\\n4\\n\\n3 rotation\\n\\nFigure 5.21: Rotations and reflections of a regular n-gon\\n\\n5\\n\\n1\\n\\n2\\n\\n3 4\\n\\n2\\n\\n1\\n\\n5\\n\\n4 3\\n\\n6\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n2\\n\\n1\\n\\n6\\n\\n5\\n\\n4\\n\\n3\\n\\nFigure 5.22: Types of reflections of a regular n-gon\\n\\n\\n\\n88 CHAPTER 5. PERMUTATION GROUPS\\n\\nTheorem 5.23. The group Dn, n ≥ 3, consists of all products of the two elements r and\\ns, satisfying the relations\\n\\nrn = 1\\n\\ns2 = 1\\n\\nsrs = r−1.\\n\\nProof. The possible motions of a regular n-gon are either reflections or rotations (Fig-\\nure 5.21). There are exactly n possible rotations:\\n\\nid, 360\\n◦\\n\\nn\\n, 2 · 360\\n\\n◦\\n\\nn\\n, . . . , (n− 1) · 360\\n\\n◦\\n\\nn\\n.\\n\\nWe will denote the rotation 360◦/n by r. The rotation r generates all of the other rotations.\\nThat is,\\n\\nrk = k · 360\\n◦\\n\\nn\\n.\\n\\nLabel the n reflections s1, s2, . . . , sn, where sk is the reflection that leaves vertex k fixed.\\nThere are two cases of reflection, depending on whether n is even or odd. If there are an\\neven number of vertices, then 2 vertices are left fixed by a reflection. If there are an odd\\nnumber of vertices, then only a single vertex is left fixed by a reflection (Figure 5.22).\\n\\nIn either case, the order of sk is two. Let s = s1. Then s2 = id and rn = id. Since\\nany rigid motion t of the n-gon replaces the first vertex by the vertex k, the second vertex\\nmust be replaced by either k+1 or by k− 1. If the second vertex is replaced by k+1, then\\nt = rk−1. If it is replaced by k − 1, then t = rk−1s. Hence, r and s generate Dn; that is,\\nDn consists of all finite products of r and s. We will leave the proof that srs = r−1 as an\\nexercise.\\n\\nExample 5.24. The group of rigid motions of a square, D4, consists of eight elements.\\nWith the vertices numbered 1, 2, 3, 4 (Figure 5.25), the rotations are\\n\\nr = (1234)\\n\\nr2 = (13)(24)\\n\\nr3 = (1432)\\n\\nr4 = (1)\\n\\nand the reflections are\\n\\ns1 = (24)\\n\\ns2 = (13).\\n\\nThe order of D4 is 8. The remaining two elements are\\n\\nrs1 = (12)(34)\\n\\nr3s1 = (14)(23).\\n\\n\\n\\n5.2. DIHEDRAL GROUPS 89\\n\\n21\\n\\n4 3\\n\\nFigure 5.25: The group D4\\n\\nThe Motion Group of a Cube\\n\\nWe can investigate the groups of rigid motions of geometric objects other than a regular\\nn-sided polygon to obtain interesting examples of permutation groups. Let us consider the\\ngroup of rigid motions of a cube. One of the first questions that we can ask about this group\\nis “what is its order?” A cube has 6 sides. If a particular side is facing upward, then there\\nare four possible rotations of the cube that will preserve the upward-facing side. Hence, the\\norder of the group is 6 · 4 = 24. We have just proved the following proposition.\\n\\n2\\n\\n2\\n4\\n\\n4\\n1\\n\\n1\\n3\\n\\n3\\n\\nFigure 5.26: The motion group of a cube\\n\\nProposition 5.27. The group of rigid motions of a cube contains 24 elements.\\n\\nTheorem 5.28. The group of rigid motions of a cube is S4.\\n\\nProof. From Proposition 5.27, we already know that the motion group of the cube has 24\\nelements, the same number of elements as there are in S4. There are exactly four diagonals\\nin the cube. If we label these diagonals 1, 2, 3, and 4, we must show that the motion group\\nof the cube will give us any permutation of the diagonals (Figure 5.26). If we can obtain\\nall of these permutations, then S4 and the group of rigid motions of the cube must be the\\nsame. To obtain a transposition we can rotate the cube 180◦ about the axis joining the\\nmidpoints of opposite edges (Figure 5.29). There are six such axes, giving all transpositions\\nin S4. Since every element in S4 is the product of a finite number of transpositions, the\\nmotion group of a cube must be S4.\\n\\n\\n\\n90 CHAPTER 5. PERMUTATION GROUPS\\n\\n2\\n\\n4 3\\n\\n1\\n\\n1 2\\n\\n43\\n1\\n\\n4 3\\n\\n2\\n\\n2 1\\n\\n43\\n\\nFigure 5.29: Transpositions in the motion group of a cube\\n\\n5.3 Exercises\\n1. Write the following permutations in cycle notation.\\n\\n(a) (\\n1 2 3 4 5\\n\\n2 4 1 5 3\\n\\n)\\n(b) (\\n\\n1 2 3 4 5\\n\\n4 2 5 1 3\\n\\n)\\n\\n(c) (\\n1 2 3 4 5\\n\\n3 5 1 4 2\\n\\n)\\n(d) (\\n\\n1 2 3 4 5\\n\\n1 4 3 2 5\\n\\n)\\n\\n2. Compute each of the following.\\n\\n(a) (1345)(234)\\n\\n(b) (12)(1253)\\n\\n(c) (143)(23)(24)\\n\\n(d) (1423)(34)(56)(1324)\\n\\n(e) (1254)(13)(25)\\n\\n(f) (1254)(13)(25)2\\n\\n(g) (1254)−1(123)(45)(1254)\\n\\n(h) (1254)2(123)(45)\\n\\n(i) (123)(45)(1254)−2\\n\\n(j) (1254)100\\n\\n(k) |(1254)|\\n(l) |(1254)2|\\n\\n(m) (12)−1\\n\\n(n) (12537)−1\\n\\n(o) [(12)(34)(12)(47)]−1\\n\\n(p) [(1235)(467)]−1\\n\\n3. Express the following permutations as products of transpositions and identify them as\\neven or odd.\\n\\n(a) (14356)\\n\\n(b) (156)(234)\\n\\n(c) (1426)(142)\\n\\n(d) (17254)(1423)(154632)\\n\\n(e) (142637)\\n\\n4. Find (a1, a2, . . . , an)\\n−1.\\n\\n5. List all of the subgroups of S4. Find each of the following sets.\\n(a) {σ ∈ S4 : σ(1) = 3}\\n(b) {σ ∈ S4 : σ(2) = 2}\\n\\n\\n\\n5.3. EXERCISES 91\\n\\n(c) {σ ∈ S4 : σ(1) = 3 and σ(2) = 2}\\nAre any of these sets subgroups of S4?\\n\\n6. Find all of the subgroups in A4. What is the order of each subgroup?\\n\\n7. Find all possible orders of elements in S7 and A7.\\n\\n8. Show that A10 contains an element of order 15.\\n\\n9. Does A8 contain an element of order 26?\\n\\n10. Find an element of largest order in Sn for n = 3, . . . , 10.\\n\\n11. What are the possible cycle structures of elements of A5? What about A6?\\n\\n12. Let σ ∈ Sn have order n. Show that for all integers i and j, σi = σj if and only if i ≡ j\\n(mod n).\\n\\n13. Let σ = σ1 · · ·σm ∈ Sn be the product of disjoint cycles. Prove that the order of σ is\\nthe least common multiple of the lengths of the cycles σ1, . . . , σm.\\n\\n14. Using cycle notation, list the elements in D5. What are r and s? Write every element\\nas a product of r and s.\\n\\n15. If the diagonals of a cube are labeled as Figure 5.26, to which motion of the cube does\\nthe permutation (12)(34) correspond? What about the other permutations of the diagonals?\\n\\n16. Find the group of rigid motions of a tetrahedron. Show that this is the same group as\\nA4.\\n\\n17. Prove that Sn is nonabelian for n ≥ 3.\\n\\n18. Show that An is nonabelian for n ≥ 4.\\n\\n19. Prove that Dn is nonabelian for n ≥ 3.\\n\\n20. Let σ ∈ Sn be a cycle. Prove that σ can be written as the product of at most n − 1\\ntranspositions.\\n\\n21. Let σ ∈ Sn. If σ is not a cycle, prove that σ can be written as the product of at most\\nn− 2 transpositions.\\n\\n22. If σ can be expressed as an odd number of transpositions, show that any other product\\nof transpositions equaling σ must also be odd.\\n\\n23. If σ is a cycle of odd length, prove that σ2 is also a cycle.\\n\\n24. Show that a 3-cycle is an even permutation.\\n\\n25. Prove that in An with n ≥ 3, any permutation is a product of cycles of length 3.\\n\\n26. Prove that any element in Sn can be written as a finite product of the following per-\\nmutations.\\n(a) (12), (13), . . . , (1n)\\n\\n(b) (12), (23), . . . , (n− 1, n)\\n\\n(c) (12), (12 . . . n)\\n\\n27. Let G be a group and define a map λg : G → G by λg(a) = ga. Prove that λg is a\\npermutation of G.\\n\\n\\n\\n92 CHAPTER 5. PERMUTATION GROUPS\\n\\n28. Prove that there exist n! permutations of a set containing n elements.\\n\\n29. Recall that the center of a group G is\\n\\nZ(G) = {g ∈ G : for all }.\\n\\nFind the center of D8. What about the center of D10? What is the center of Dn?\\n\\n30. Let τ = (a1, a2, . . . , ak) be a cycle of length k.\\n\\n(a) Prove that if σ is any permutation, then\\n\\nστσ−1 = (σ(a1), σ(a2), . . . , σ(ak))\\n\\nis a cycle of length k.\\n(b) Let µ be a cycle of length k. Prove that there is a permutation σ such that στσ−1 = µ.\\n\\n31. For α and β in Sn, define α ∼ β if there exists an σ ∈ Sn such that σασ−1 = β. Show\\nthat ∼ is an equivalence relation on Sn.\\n\\n32. Let σ ∈ SX . If σn(x) = y, we will say that x ∼ y.\\n\\n(a) Show that ∼ is an equivalence relation on X.\\n(b) If σ ∈ An and τ ∈ Sn, show that τ−1στ ∈ An.\\n(c) Define the orbit of x ∈ X under σ ∈ SX to be the set\\n\\nOx,σ = {y : x ∼ y}.\\n\\nCompute the orbits of each of the following elements in S5:\\n\\nα = (1254)\\n\\nβ = (123)(45)\\n\\nγ = (13)(25).\\n\\n(d) If Ox,σ ∩Oy,σ ̸= ∅, prove that Ox,σ = Oy,σ. The orbits under a permutation σ are the\\nequivalence classes corresponding to the equivalence relation ∼.\\n\\n(e) A subgroup H of SX is transitive if for every x, y ∈ X, there exists a σ ∈ H such\\nthat σ(x) = y. Prove that ⟨σ⟩ is transitive if and only if Ox,σ = X for some x ∈ X.\\n\\n33. Let α ∈ Sn for n ≥ 3. If αβ = βα for all β ∈ Sn, prove that α must be the identity\\npermutation; hence, the center of Sn is the trivial subgroup.\\n\\n34. If α is even, prove that α−1 is also even. Does a corresponding result hold if α is odd?\\n\\n35. Show that α−1β−1αβ is even for α, β ∈ Sn.\\n\\n36. Let r and s be the elements in Dn described in Theorem 5.10.\\n\\n(a) Show that srs = r−1.\\n(b) Show that rks = sr−k in Dn.\\n(c) Prove that the order of rk ∈ Dn is n/ gcd(k, n).\\n\\n\\n\\n5.4. SAGE 93\\n\\n5.4 Sage\\nA good portion of Sage’s support for group theory is based on routines from gap (Groups,\\nAlgorithms, and Programming) at www.gap-system.org, which is included in every copy of\\nSage. This is a mature open source package, dating back to 1986. (Forward reference here\\nto gap console, etc.)\\n\\nAs we have seen, groups can be described in many different ways, such as sets of matrices,\\nsets of complex numbers, or sets of symbols subject to defining relations. A very concrete\\nway to represent groups is via permutations (one-to-one and onto functions of the integers\\n1 through n), using function composition as the operation in the group, as described in this\\nchapter. Sage has many routines designed to work with groups of this type and they are\\nalso a good way for those learning group theory to gain experience with the basic ideas of\\ngroup theory. For both these reasons, we will concentrate on these types of groups.\\n\\nPermutation Groups and Elements\\nThe easiest way to work with permutation group elements in Sage is to write them in cycle\\nnotation. Since these are products of disjoint cycles (which commute), we do not need to\\nconcern ourselves with the actual order of the cycles. If we write (1,3)(2,4) we probably\\nunderstand it to be a permutation (the topic of this chapter!) and we know that it could\\nbe an element of S4, or perhaps a symmetric group on more symbols than just 4. Sage\\ncannot get started that easily and needs a bit of context, so we coerce a string of characters\\nwritten with cycle notation into a symmetric group to make group elements. Here are some\\nexamples and some sample computations. Remember that Sage and your text differ on how\\nto interpret the order of composing two permutations in a product.\\n\\nG = SymmetricGroup (5)\\nsigma = G(\\"(1,3)(2,5,4)\\")\\nsigma*sigma\\n\\n(2,4,5)\\n\\nrho = G(\\"(2,4)(1,5)\\")\\nrho^3\\n\\n(1,5)(2,4)\\n\\nIf the next three examples seem confusing, or “backwards”, then now would be an\\nexcellent time to review the Sage discussion about the order of permutation composition in\\nthe subsection “Groups of symmetries”.\\n\\nsigma*rho\\n\\n(1,3,5,2)\\n\\nrho*sigma\\n\\n(1,4,5,3)\\n\\nrho^-1* sigma*rho\\n\\n(1,2,4)(3,5)\\n\\nThere are alternate ways to create permutation group elements, which can be useful in\\nsome situations, but they are not quite as useful in everday use.\\n\\nhttp://www.gap-system.org/\\n\\n\\n94 CHAPTER 5. PERMUTATION GROUPS\\n\\nsigma1 = G(\\"(1,3)(2,5,4)\\")\\nsigma1\\n\\n(1,3)(2,5,4)\\n\\nsigma2 = G([(1 ,3) ,(2,5,4)])\\nsigma2\\n\\n(1,3)(2,5,4)\\n\\nsigma3 = G([3,5,1,2,4])\\nsigma3\\n\\n(1,3)(2,5,4)\\n\\nsigma1 == sigma2\\n\\nTrue\\n\\nsigma2 == sigma3\\n\\nTrue\\n\\nsigma2.cycle_tuples ()\\n\\n[(1, 3), (2, 5, 4)]\\n\\n[sigma3(x) for x in G.domain ()]\\n\\n[3, 5, 1, 2, 4]\\n\\nThe second version of σ is a list of “tuples”, which requires a lot of commas and these\\nmust be enclosed in a list. (A tuple of length one must be written like (4,) to distinguish it\\nfrom using parentheses for grouping, as in 5*(4).) The third version uses the “bottom-row”\\nof the more cumbersome two-row notation introduced at the beginning of the chapter — it\\nis an ordered list of the output values of the permutation when considered as a function.\\n\\nSo we then see that despite three different input procedures, all the versions of σ print\\nthe same way, and moreso they are actually equal to each other. (This is a subtle difference\\n— what an object is in Sage versus how an object displays itself.)\\n\\nWe can be even more careful about the nature of our elements. Notice that once we get\\nSage started, it can promote the product τσ into the larger permutation group. We can\\n“promote” elements into larger permutation groups, but it is an error to try to shoe-horn\\nan element into a too-small symmetric group.\\n\\nH = SymmetricGroup (4)\\nsigma = H(\\"(1,2,3,4)\\")\\nG = SymmetricGroup (6)\\ntau = G(\\"(1,2,3,4,5,6)\\")\\nrho = tau * sigma\\nrho\\n\\n(1,3)(2,4,5,6)\\n\\n\\n\\n5.4. SAGE 95\\n\\nsigma.parent ()\\n\\nSymmetric group of order 4! as a permutation group\\n\\ntau.parent ()\\n\\nSymmetric group of order 6! as a permutation group\\n\\nrho.parent ()\\n\\nSymmetric group of order 6! as a permutation group\\n\\ntau.parent () == rho.parent ()\\n\\nTrue\\n\\nsigmaG = G(sigma)\\nsigmaG.parent ()\\n\\nSymmetric group of order 6! as a permutation group\\n\\nIt is an error to try to coerce a permutation with too many symbols into a permutation\\ngroup employing too few symbols.\\n\\ntauH = H(tau)\\n\\nTraceback (most recent call last):\\n...\\nValueError: Invalid permutation vector: (1,2,3,4,5,6)\\n\\nBetter than working with just elements of the symmetric group, we can create a variety\\nof permutation groups in Sage. Here is a sampling for starters:\\n\\nSage Command Description\\nSymmetricGroup(n) Permutations on n symbols, n! elements\\nDihedralGroup(n) Symmetries of an n-gon, 2n elements.\\nCyclicPermutationGroup(n) Rotations of an n-gon (no flips), n elements\\nAlternatingGroup(n) Alternating group on n symbols, n!/2 elements\\nKleinFourGroup() A non-cyclic group of order 4\\n\\nTable 5.30: Some Sage permutation groups\\n\\nYou can also locate Sage permutation groups with the groups catalog. In the next cell\\nplace your cursor right after the final dot and hit the tab-key. You will get a list of methods\\nyou can use to create permutation groups. As always, place a question-mark after a method\\nand hit the tab-key to get online documentation of a method.\\n\\ngroups.permutation.\\n\\n\\n\\n96 CHAPTER 5. PERMUTATION GROUPS\\n\\nProperties of Permutation Elements\\nSometimes it is easier to grab an element out of a list of elements of a permutation group,\\nand then it is already attached to a parent and there is no need for any coercion. In the\\nfollowing, rotate and flip are automatically elements of G because of the way we procured\\nthem.\\n\\nD = DihedralGroup (5)\\nelements = D.list(); elements\\n\\n[(), (1,5)(2,4), (1,2,3,4,5), (1,4)(2,3), (1,3,5,2,4), (2,5)(3,4),\\n(1,3)(4,5), (1,5,4,3,2), (1,4,2,5,3), (1,2)(3,5)]\\n\\nrotate = elements [2]\\nflip = elements [3]\\nflip*rotate == rotate* flip\\n\\nFalse\\n\\nSo we see from this final statement that the group of symmetries of a pentagon is not\\nabelian. But there is an easier way.\\n\\nD = DihedralGroup (5)\\nD.is_abelian ()\\n\\nFalse\\n\\nThere are many more methods you can use for both permutation groups and their\\nindividual elements. Use the blank compute cell below to create a permutation group (any\\none you like) and an element of a permutation group (any one you like). Then use tab-\\ncompletion to see all the methods available for an element, or for a group (name, period,\\ntab-key). Some names you may recognize, some we will learn about in the coming chapters,\\nsome are highly-specialized research tools you can use when you write your Ph.D. thesis in\\ngroup theory. For any of these methods, remember that you can type the name, followed\\nby a question mark, to see documentation and examples. Experiment and explore — it is\\nreally hard to break anything.\\n\\nHere are some selected examples of various methods available.\\nA4 = AlternatingGroup (4)\\nA4.order ()\\n\\n12\\n\\nA4.is_finite ()\\n\\nTrue\\n\\nA4.is_abelian ()\\n\\nFalse\\n\\nA4.is_cyclic ()\\n\\nFalse\\n\\n\\n\\n5.4. SAGE 97\\n\\nsigma = A4(\\"(1,2,4)\\")\\nsigma^-1\\n\\n(1,4,2)\\n\\nsigma.order()\\n\\n3\\n\\nA very useful method when studying the alternating group is the permutation group\\nelement method .sign(). It will return 1 if a permutation is even and -1 if a permutation\\nis odd.\\n\\nG = SymmetricGroup (3)\\nsigma = G(\\"(1,2)\\")\\ntau = G(\\"(1,3)\\")\\nrho = sigma*tau\\nsigma.sign()\\n\\n-1\\n\\nrho.sign()\\n\\n1\\n\\nWe can create subgroups by giving the main group a list of “generators.” These elements\\nserve to “generate” a subgroup — imagine multiplying these elements (and their inverses)\\ntogether over and over, creating new elements that must also be in the subgroup and also\\nbecome involved in new products, until you see no new elements. Now that definition ends\\nwith a horribly imprecise statement, but it should suffice for now. A better definition is\\nthat the subgroup generated by the elements is the smallest subgroup of the main group\\nthat contains all the generators — which is fine if you know what all the subgroups might\\nbe.\\n\\nWith a single generator, the repeated products just become powers of the lone generator.\\nThe subgroup generated then is cyclic. With two (or more) generators, especially in a non-\\nabelian group, the situation can be much, much more complicated. So let us begin with\\njust a single generator. But do not forget to put it in a list anyway.\\n\\nA4 = AlternatingGroup (4)\\nsigma = A4(\\"(1,2,4)\\")\\nsg = A4.subgroup ([sigma])\\nsg\\n\\nSubgroup of (Alternating group of order 4!/2 as a permutation group)\\ngenerated by [(1,2,4)]\\n\\nsg.order ()\\n\\n3\\n\\nsg.list()\\n\\n[(), (1,2,4), (1,4,2)]\\n\\n\\n\\n98 CHAPTER 5. PERMUTATION GROUPS\\n\\nsg.is_abelian ()\\n\\nTrue\\n\\nsg.is_cyclic ()\\n\\nTrue\\n\\nsg.is_subgroup(A4)\\n\\nTrue\\n\\nWe can now redo the example from the very beginning of this chapter. We translate to\\nelements to cycle notation, construct the subgroup from two generators (the subgroup is\\nnot cyclic), and since the subgroup is abelian, we do not have to view Sage’s Cayley table\\nas a diagonal reflection of the table in the example.\\n\\nG = SymmetricGroup (5)\\nsigma = G(\\"(4,5)\\")\\ntau = G(\\"(1,3)\\")\\nH = G.subgroup ([sigma , tau])\\nH.list()\\n\\n[(), (1,3), (4,5), (1,3)(4,5)]\\n\\ntext_names = [ \' id \' , \' sigma \' , \' tau \' , \' mu \' ]\\nH.cayley_table(names=text_names)\\n\\n* id sigma tau mu\\n+------------------------\\n\\nid| id sigma tau mu\\nsigma| sigma id mu tau\\n\\ntau| tau mu id sigma\\nmu| mu tau sigma id\\n\\nMotion Group of a Cube\\nWe could mimic the example in the text and create elements of S4 as permutations of the\\ndiagonals. A more obvious, but less insightful, construction is to view the 8 corners of the\\ncube as the items being permuted. Then some obvious symmetries of the cube come from\\nrunning an axis through the center of a side, through to the center of the opposite side,\\nwith quarter-turns or half-turns about these axes forming symmetries. With three such\\naxes and four rotations per axis, we get 12 symmetries, except we have counted the identity\\npermutation two extra times.\\n\\nLabel the four corners of the square top with 1 through 4, placing 1 in the left-front\\ncorner, and following around clockwise when viewed from above. Use 5 through 8 for the\\nbottom square’s corner, so that 5 is directly below 1, 6 below 2, etc. We will use quarter-\\nturns, clockwise, around each axis, when viewed from above, the front, and the right.\\n\\nG = SymmetricGroup (8)\\nabove = G(\\"(1,2,3,4)(5,6,7,8)\\")\\nfront = G(\\"(1,4,8,5)(2,3,7,6)\\")\\nright = G(\\"(1,2,6,5)(3,7,8,4)\\")\\ncube = G.subgroup ([above , front , right])\\ncube.order()\\n\\n\\n\\n5.5. SAGE EXERCISES 99\\n\\n24\\n\\ncube.list()\\n\\n[(), (1,2,3,4)(5,6,7,8), (1,2,6,5)(3,7,8,4),\\n(1,4,8,5)(2,3,7,6), (1,6,8)(2,7,4), (2,4,5)(3,8,6),\\n(1,3,8)(2,7,5), (1,6)(2,5)(3,8)(4,7), (1,3,6)(4,7,5),\\n(1,3)(2,4)(5,7)(6,8), (1,8)(2,7)(3,6)(4,5), (1,7)(2,3)(4,6)(5,8),\\n(1,4)(2,8)(3,5)(6,7), (1,5,6,2)(3,4,8,7), (1,5,8,4)(2,6,7,3),\\n(1,7)(2,6)(3,5)(4,8), (1,7)(2,8)(3,4)(5,6), (1,4,3,2)(5,8,7,6),\\n(1,5)(2,8)(3,7)(4,6), (1,2)(3,5)(4,6)(7,8), (1,8,6)(2,4,7),\\n(2,5,4)(3,6,8), (1,6,3)(4,5,7), (1,8,3)(2,5,7)]\\n\\nSince we know from the discussion in the text that the symmetry group has 24 elements,\\nwe see that our three quarter-turns are sufficient to create every symmetry. This prompts\\nseveral questions which you can find in Exercise 5.5.4.\\n\\n5.5 Sage Exercises\\nThese exercises are designed to help you become familiar with permutation groups in Sage.\\n1. Create the full symmetric group S10 with the command G = SymmetricGroup(10).\\n\\n2. Create elements of G with the following (varying) syntax. Pay attention to commas,\\nquotes, brackets, parentheses. The first two use a string (characters) as input, mimicking\\nthe way we write permuations (but with commas). The second two use a list of tuples.\\n\\n• a = G(\\"(5,7,2,9,3,1,8)\\")\\n\\n• b = G(\\"(1,3)(4,5)\\")\\n\\n• c = G([(1,2),(3,4)])\\n\\n• d = G([(1,3),(2,5,8),(4,6,7,9,10)])\\n\\n(a) Compute a3, bc, ad−1b.\\n(b) Compute the orders of each of these four individual elements (a through d) using a\\n\\nsingle permutation group element method.\\n(c) Use the permutation group element method .sign() to determine if a, b, c, d are even\\n\\nor odd permutations.\\n(d) Create two cyclic subgroups of G with the commands:\\n\\n• H = G.subgroup([a])\\n\\n• K = G.subgroup([d])\\n\\nList, and study, the elements of each subgroup. Without using Sage, list the order of\\neach subgroup of K. Then use Sage to construct a subgroup of K with order 10.\\n\\n(e) More complicated subgroups can be formed by using two or more generators. Construct\\na subgroup L of G with the command L = G.subgroup([b,c]). Compute the order of\\nL and list all of the elements of L.\\n\\n3. Construct the group of symmetries of the tetrahedron (also the alternating group on\\n4 symbols, A4) with the command A=AlternatingGroup(4). Using tools such as orders of\\nelements, and generators of subgroups, see if you can find all of the subgroups of A4 (each\\none exactly once). Do this without using the .subgroups() method to justify the correctness\\nof your answer (though it might be a convenient way to check your work).\\n\\n\\n\\n100 CHAPTER 5. PERMUTATION GROUPS\\n\\nProvide a nice summary as your answer—not just piles of output. So use Sage as a tool,\\nas needed, but basically your answer will be a concise paragraph and/or table. This is the\\none part of this assignment without clear, precise directions, so spend some time on this\\nportion to get it right. Hint: no subgroup of A4 requires more than two generators.\\n\\n4. The subsection “Motion Group of a Cube” describes the 24 symmetries of a cube as a\\nsubgroup of the symmetric group S8 generated by three quarter-turns. Answer the following\\nquestions about this symmetry group.\\n(a) From the list of elements of the group, can you locate the ten rotations about axes?\\n\\n(Hint: the identity is easy, the other nine never send any symbol to itself.)\\n(b) Can you identify the six symmetries that are a transposition of diagonals? (Hint:\\n\\n[g for g in cube if g.order()== 2] is a good preliminary filter.)\\n(c) Verify that any two of the quarter-turns (above, front, right) are sufficient to generate\\n\\nthe whole group. How do you know each pair generates the entire group?\\n(d) Can you express one of the diagonal transpositions as a product of quarter-turns? This\\n\\ncan be a notoriously difficult problem, especially for software. It is known as the “word\\nproblem.”\\n\\n(e) Number the six faces of the cube with the numbers 1 through 6 (any way you like).\\nNow consider the same three symmetries we used before (quarter-turns about face-\\nto-face axes), but now view them as permutations of the six faces. In this way, we\\nconstruct each symmetry as an element of S6. Verify that the subgroup generated by\\nthese symmetries is the whole symmetry group of the cube. Again, rather than using\\nthree generators, try using just two.\\n\\n5. Save your work, and then see if you can crash your Sage session by building the subgroup\\nof S10 generated by the elements b and d of orders 2 and 30 from above. Do not submit the\\nlist of elements of N as part of your submitted worksheet.\\n\\nN = G.subgroup ([b,d])\\nN.list()\\n\\nWhat is the order of N?\\n\\n\\n\\n6\\n\\nCosets and Lagrange’s Theorem\\n\\nLagrange’s Theorem, one of the most important results in finite group theory, states that the\\norder of a subgroup must divide the order of the group. This theorem provides a powerful\\ntool for analyzing finite groups; it gives us an idea of exactly what type of subgroups we\\nmight expect a finite group to possess. Central to understanding Lagranges’s Theorem is\\nthe notion of a coset.\\n\\n6.1 Cosets\\nLet G be a group and H a subgroup of G. Define a left coset of H with representative\\ng ∈ G to be the set\\n\\ngH = {gh : h ∈ H}.\\n\\nRight cosets can be defined similarly by\\n\\nHg = {hg : h ∈ H}.\\n\\nIf left and right cosets coincide or if it is clear from the context to which type of coset that\\nwe are referring, we will use the word coset without specifying left or right.\\n\\nExample 6.1. Let H be the subgroup of Z6 consisting of the elements 0 and 3. The cosets\\nare\\n\\n0 +H = 3 +H = {0, 3}\\n1 +H = 4 +H = {1, 4}\\n2 +H = 5 +H = {2, 5}.\\n\\nWe will always write the cosets of subgroups of Z and Zn with the additive notation we have\\nused for cosets here. In a commutative group, left and right cosets are always identical.\\n\\nExample 6.2. Let H be the subgroup of S3 defined by the permutations {(1), (123), (132)}.\\nThe left cosets of H are\\n\\n(1)H = (123)H = (132)H = {(1), (123), (132)}\\n(12)H = (13)H = (23)H = {(12), (13), (23)}.\\n\\nThe right cosets of H are exactly the same as the left cosets:\\n\\nH(1) = H(123) = H(132) = {(1), (123), (132)}\\nH(12) = H(13) = H(23) = {(12), (13), (23)}.\\n\\n101\\n\\n\\n\\n102 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\\n\\nIt is not always the case that a left coset is the same as a right coset. Let K be the\\nsubgroup of S3 defined by the permutations {(1), (12)}. Then the left cosets of K are\\n\\n(1)K = (12)K = {(1), (12)}\\n(13)K = (123)K = {(13), (123)}\\n(23)K = (132)K = {(23), (132)};\\n\\nhowever, the right cosets of K are\\n\\nK(1) = K(12) = {(1), (12)}\\nK(13) = K(132) = {(13), (132)}\\nK(23) = K(123) = {(23), (123)}.\\n\\nThe following lemma is quite useful when dealing with cosets. (We leave its proof as an\\nexercise.)\\n\\nLemma 6.3. Let H be a subgroup of a group G and suppose that g1, g2 ∈ G. The following\\nconditions are equivalent.\\n\\n1. g1H = g2H;\\n\\n2. Hg−1\\n1 = Hg−1\\n\\n2 ;\\n\\n3. g1H ⊆ g2H;\\n\\n4. g2 ∈ g1H;\\n\\n5. g−1\\n1 g2 ∈ H.\\n\\nIn all of our examples the cosets of a subgroup H partition the larger group G. The\\nfollowing theorem proclaims that this will always be the case.\\n\\nTheorem 6.4. Let H be a subgroup of a group G. Then the left cosets of H in G partition\\nG. That is, the group G is the disjoint union of the left cosets of H in G.\\n\\nProof. Let g1H and g2H be two cosets of H in G. We must show that either g1H∩g2H = ∅\\nor g1H = g2H. Suppose that g1H ∩ g2H ̸= ∅ and a ∈ g1H ∩ g2H. Then by the definition\\nof a left coset, a = g1h1 = g2h2 for some elements h1 and h2 in H. Hence, g1 = g2h2h\\n\\n−1\\n1 or\\n\\ng1 ∈ g2H. By Lemma 6.3, g1H = g2H.\\n\\nRemark 6.5. There is nothing special in this theorem about left cosets. Right cosets also\\npartition G; the proof of this fact is exactly the same as the proof for left cosets except that\\nall group multiplications are done on the opposite side of H.\\n\\nLet G be a group and H be a subgroup of G. Define the index of H in G to be the\\nnumber of left cosets of H in G. We will denote the index by [G : H].\\n\\nExample 6.6. Let G = Z6 and H = {0, 3}. Then [G : H] = 3.\\n\\nExample 6.7. Suppose that G = S3, H = {(1), (123), (132)}, and K = {(1), (12)}. Then\\n[G : H] = 2 and [G : K] = 3.\\n\\nTheorem 6.8. Let H be a subgroup of a group G. The number of left cosets of H in G is\\nthe same as the number of right cosets of H in G.\\n\\n\\n\\n6.2. LAGRANGE’S THEOREM 103\\n\\nProof. Let LH and RH denote the set of left and right cosets of H in G, respectively. If\\nwe can define a bijective map ϕ : LH → RH , then the theorem will be proved. If gH ∈ LH ,\\nlet ϕ(gH) = Hg−1. By Lemma 6.3, the map ϕ is well-defined; that is, if g1H = g2H, then\\nHg−1\\n\\n1 = Hg−1\\n2 . To show that ϕ is one-to-one, suppose that\\n\\nHg−1\\n1 = ϕ(g1H) = ϕ(g2H) = Hg−1\\n\\n2 .\\n\\nAgain by Lemma 6.3, g1H = g2H. The map ϕ is onto since ϕ(g−1H) = Hg.\\n\\n6.2 Lagrange’s Theorem\\nProposition 6.9. Let H be a subgroup of G with g ∈ G and define a map ϕ : H → gH by\\nϕ(h) = gh. The map ϕ is bijective; hence, the number of elements in H is the same as the\\nnumber of elements in gH.\\n\\nProof. We first show that the map ϕ is one-to-one. Suppose that ϕ(h1) = ϕ(h2) for\\nelements h1, h2 ∈ H. We must show that h1 = h2, but ϕ(h1) = gh1 and ϕ(h2) = gh2. So\\ngh1 = gh2, and by left cancellation h1 = h2. To show that ϕ is onto is easy. By definition\\nevery element of gH is of the form gh for some h ∈ H and ϕ(h) = gh.\\n\\nTheorem 6.10 (Lagrange). Let G be a finite group and let H be a subgroup of G. Then\\n|G|/|H| = [G : H] is the number of distinct left cosets of H in G. In particular, the number\\nof elements in H must divide the number of elements in G.\\n\\nProof. The group G is partitioned into [G : H] distinct left cosets. Each left coset has\\n|H| elements; therefore, |G| = [G : H]|H|.\\n\\nCorollary 6.11. Suppose that G is a finite group and g ∈ G. Then the order of g must\\ndivide the number of elements in G.\\n\\nCorollary 6.12. Let |G| = p with p a prime number. Then G is cyclic and any g ∈ G such\\nthat g ̸= e is a generator.\\n\\nProof. Let g be in G such that g ̸= e. Then by Corollary 6.11, the order of g must divide\\nthe order of the group. Since |⟨g⟩| > 1, it must be p. Hence, g generates G.\\n\\nCorollary 6.12 suggests that groups of prime order p must somehow look like Zp.\\n\\nCorollary 6.13. Let H and K be subgroups of a finite group G such that G ⊃ H ⊃ K.\\nThen\\n\\n[G : K] = [G : H][H : K].\\n\\nProof. Observe that\\n\\n[G : K] =\\n|G|\\n|K|\\n\\n=\\n|G|\\n|H|\\n\\n· |H|\\n|K|\\n\\n= [G : H][H : K].\\n\\nRemark 6.14 (The converse of Lagrange’s Theorem is false). The group A4 has order 12;\\nhowever, it can be shown that it does not possess a subgroup of order 6. According to\\nLagrange’s Theorem, subgroups of a group of order 12 can have orders of either 1, 2, 3,\\n4, or 6. However, we are not guaranteed that subgroups of every possible order exist. To\\nprove that A4 has no subgroup of order 6, we will assume that it does have such a subgroup\\nH and show that a contradiction must occur. Since A4 contains eight 3-cycles, we know\\nthat H must contain a 3-cycle. We will show that if H contains one 3-cycle, then it must\\ncontain more than 6 elements.\\n\\n\\n\\n104 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\\n\\nProposition 6.15. The group A4 has no subgroup of order 6.\\n\\nProof. Since [A4 : H] = 2, there are only two cosets of H in A4. Inasmuch as one of the\\ncosets is H itself, right and left cosets must coincide; therefore, gH = Hg or gHg−1 = H\\nfor every g ∈ A4. Since there are eight 3-cycles in A4, at least one 3-cycle must be in H.\\nWithout loss of generality, assume that (123) is in H. Then (123)−1 = (132) must also be\\nin H. Since ghg−1 ∈ H for all g ∈ A4 and all h ∈ H and\\n\\n(124)(123)(124)−1 = (124)(123)(142) = (243)\\n\\n(243)(123)(243)−1 = (243)(123)(234) = (142)\\n\\nwe can conclude that H must have at least seven elements\\n\\n(1), (123), (132), (243), (243)−1 = (234), (142), (142)−1 = (124).\\n\\nTherefore, A4 has no subgroup of order 6.\\n\\nIn fact, we can say more about when two cycles have the same length.\\n\\nTheorem 6.16. Two cycles τ and µ in Sn have the same length if and only if there exists\\na σ ∈ Sn such that µ = στσ−1.\\n\\nProof. Suppose that\\n\\nτ = (a1, a2, . . . , ak)\\n\\nµ = (b1, b2, . . . , bk).\\n\\nDefine σ to be the permutation\\n\\nσ(a1) = b1\\n\\nσ(a2) = b2\\n...\\n\\nσ(ak) = bk.\\n\\nThen µ = στσ−1.\\nConversely, suppose that τ = (a1, a2, . . . , ak) is a k-cycle and σ ∈ Sn. If σ(ai) = b and\\n\\nσ(a(i mod k)+1) = b′, then µ(b) = b′. Hence,\\n\\nµ = (σ(a1), σ(a2), . . . , σ(ak)).\\n\\nSince σ is one-to-one and onto, µ is a cycle of the same length as τ .\\n\\n6.3 Fermat’s and Euler’s Theorems\\nThe Euler ϕ-function is the map ϕ : N → N defined by ϕ(n) = 1 for n = 1, and, for n > 1,\\nϕ(n) is the number of positive integers m with 1 ≤ m < n and gcd(m,n) = 1.\\n\\nFrom Proposition 3.4, we know that the order of U(n), the group of units in Zn, is ϕ(n).\\nFor example, |U(12)| = ϕ(12) = 4 since the numbers that are relatively prime to 12 are 1,\\n5, 7, and 11. For any prime p, ϕ(p) = p−1. We state these results in the following theorem.\\n\\nTheorem 6.17. Let U(n) be the group of units in Zn. Then |U(n)| = ϕ(n).\\n\\nThe following theorem is an important result in number theory, due to Leonhard Euler.\\n\\n\\n\\n6.4. EXERCISES 105\\n\\nTheorem 6.18 (Euler’s Theorem). Let a and n be integers such that n > 0 and gcd(a, n) =\\n1. Then aϕ(n) ≡ 1 (mod n).\\n\\nProof. By Theorem 6.17 the order of U(n) is ϕ(n). Consequently, aϕ(n) = 1 for all\\na ∈ U(n); or aϕ(n) − 1 is divisible by n. Therefore, aϕ(n) ≡ 1 (mod n).\\n\\nIf we consider the special case of Euler’s Theorem in which n = p is prime and recall\\nthat ϕ(p) = p− 1, we obtain the following result, due to Pierre de Fermat.\\n\\nTheorem 6.19 (Fermat’s Little Theorem). Let p be any prime number and suppose that\\np ̸ |a. Then\\n\\nap−1 ≡ 1 (mod p).\\n\\nFurthermore, for any integer b, bp ≡ b (mod p).\\n\\nHistorical Note\\n\\nJoseph-Louis Lagrange (1736–1813), born in Turin, Italy, was of French and Italian\\ndescent. His talent for mathematics became apparent at an early age. Leonhard Euler\\nrecognized Lagrange’s abilities when Lagrange, who was only 19, communicated to Euler\\nsome work that he had done in the calculus of variations. That year he was also named\\na professor at the Royal Artillery School in Turin. At the age of 23 he joined the Berlin\\nAcademy. Frederick the Great had written to Lagrange proclaiming that the “greatest king\\nin Europe” should have the “greatest mathematician in Europe” at his court. For 20 years\\nLagrange held the position vacated by his mentor, Euler. His works include contributions to\\nnumber theory, group theory, physics and mechanics, the calculus of variations, the theory\\nof equations, and differential equations. Along with Laplace and Lavoisier, Lagrange was\\none of the people responsible for designing the metric system. During his life Lagrange\\nprofoundly influenced the development of mathematics, leaving much to the next generation\\nof mathematicians in the form of examples and new problems to be solved.\\n\\n6.4 Exercises\\n1. Suppose that G is a finite group with an element g of order 5 and an element h of order\\n7. Why must |G| ≥ 35?\\n\\n2. Suppose that G is a finite group with 60 elements. What are the orders of possible\\nsubgroups of G?\\n\\n3. Prove or disprove: Every subgroup of the integers has finite index.\\n\\n4. Prove or disprove: Every subgroup of the integers has finite order.\\n\\n5. List the left and right cosets of the subgroups in each of the following.\\n\\n(a) ⟨8⟩ in Z24\\n\\n(b) ⟨3⟩ in U(8)\\n\\n(c) 3Z in Z\\n(d) A4 in S4\\n\\n(e) An in Sn\\n\\n(f) D4 in S4\\n\\n(g) T in C∗\\n\\n(h) H = {(1), (123), (132)} in S4\\n\\n6. Describe the left cosets of SL2(R) in GL2(R). What is the index of SL2(R) in GL2(R)?\\n\\n\\n\\n106 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\\n\\n7. Verify Euler’s Theorem for n = 15 and a = 4.\\n\\n8. Use Fermat’s Little Theorem to show that if p = 4n+3 is prime, there is no solution to\\nthe equation x2 ≡ −1 (mod p).\\n\\n9. Show that the integers have infinite index in the additive group of rational numbers.\\n\\n10. Show that the additive group of real numbers has infinite index in the additive group\\nof the complex numbers.\\n\\n11. Let H be a subgroup of a group G and suppose that g1, g2 ∈ G. Prove that the following\\nconditions are equivalent.\\n(a) g1H = g2H\\n\\n(b) Hg−1\\n1 = Hg−1\\n\\n2\\n\\n(c) g1H ⊆ g2H\\n\\n(d) g2 ∈ g1H\\n\\n(e) g−1\\n1 g2 ∈ H\\n\\n12. If ghg−1 ∈ H for all g ∈ G and h ∈ H, show that right cosets are identical to left\\ncosets. That is, show that gH = Hg for all g ∈ G.\\n\\n13. What fails in the proof of Theorem 6.8 if ϕ : LH → RH is defined by ϕ(gH) = Hg?\\n\\n14. Suppose that gn = e. Show that the order of g divides n.\\n\\n15. Show that any two permutations α, β ∈ Sn have the same cycle structure if and only\\nif there exists a permutation γ such that β = γαγ−1. If β = γαγ−1 for some γ ∈ Sn, then\\nα and β are conjugate.\\n\\n16. If |G| = 2n, prove that the number of elements of order 2 is odd. Use this result to\\nshow that G must contain a subgroup of order 2.\\n\\n17. Suppose that [G : H] = 2. If a and b are not in H, show that ab ∈ H.\\n\\n18. If [G : H] = 2, prove that gH = Hg.\\n\\n19. Let H and K be subgroups of a group G. Prove that gH ∩ gK is a coset of H ∩K in\\nG.\\n\\n20. Let H and K be subgroups of a group G. Define a relation ∼ on G by a ∼ b if there\\nexists an h ∈ H and a k ∈ K such that hak = b. Show that this relation is an equivalence\\nrelation. The corresponding equivalence classes are called double cosets. Compute the\\ndouble cosets of H = {(1), (123), (132)} in A4.\\n\\n21. Let G be a cyclic group of order n. Show that there are exactly ϕ(n) generators for G.\\n\\n22. Let n = pe11 p\\ne2\\n2 · · · pekk , where p1, p2, . . . , pk are distinct primes. Prove that\\n\\nϕ(n) = n\\n\\n(\\n1− 1\\n\\np1\\n\\n)(\\n1− 1\\n\\np2\\n\\n)\\n· · ·\\n(\\n1− 1\\n\\npk\\n\\n)\\n.\\n\\n23. Show that\\nn =\\n\\n∑\\nd|n\\n\\nϕ(d)\\n\\nfor all positive integers n.\\n\\n\\n\\n6.5. SAGE 107\\n\\n6.5 Sage\\nSage can create all of the cosets of a subgroup, and all of the subgroups of a group. While\\nthese methods can be somewhat slow, they are in many, many ways much better than ex-\\nperimenting with pencil and paper, and can greatly assist us in understanding the structure\\nof finite groups.\\n\\nCosets\\nSage will create all the right (or left) cosets of a subgroup. Written mathematically, cosets\\nare sets, and the order of the elements within the set is irrelevant. With Sage, lists are more\\nnatural, and here it is to our advantage.\\n\\nSage creates the cosets of a subgroup as a list of lists. Each inner list is a single coset.\\nThe first coset is always the coset that is the subgroup itself, and the first element of this\\ncoset is the identity. Each of the other cosets can be construed to have their first element\\nas their representative, and if you use this element as the representative, the elements of\\nthe coset are in the same order they would be created by multiplying this representative by\\nthe elements of the first coset (the subgroup).\\n\\nThe keyword side can be \'right\' or \'left\', and if not given, then the default is right\\ncosets. The options refer to which side of the product has the representative. Notice\\nthat now Sage’s results will be “backwards” compared with the text. Here is Example 6.2\\nreprised, but in a slightly different order.\\n\\nG = SymmetricGroup (3)\\na = G(\\"(1,2)\\")\\nH = G.subgroup ([a])\\nrc = G.cosets(H, side= \' right \' ); rc\\n\\n[[(), (1,2)], [(2,3), (1,3,2)], [(1,2,3), (1,3)]]\\n\\nlc = G.cosets(H, side= \' left \' ); lc\\n\\n[[(), (1,2)], [(2,3), (1,2,3)], [(1,3,2), (1,3)]]\\n\\nSo if we work our way through the brackets carefully we can see the difference between\\nthe right cosets and the left cosets. Compare these cosets with the ones in the text and see\\nthat left and right are reversed. Shouldn’t be a problem — just keep it in mind.\\n\\nG = SymmetricGroup (3)\\nb = G(\\"(1,2,3)\\")\\nH = G.subgroup ([b])\\nrc = G.cosets(H, side= \' right \' ); rc\\n\\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,3), (1,2)]]\\n\\nlc = G.cosets(H, side= \' left \' ); lc\\n\\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,2), (1,3)]]\\n\\nIf we study the bracketing, we can see that the left and right cosets are equal. Let’s see\\nwhat Sage thinks:\\n\\nrc == lc\\n\\nFalse\\n\\n\\n\\n108 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\\n\\nMathematically, we need sets, but Sage is working with ordered lists, and the order\\nmatters. However, if we know our lists do not have duplicates (the .cosets() method will\\nnever produce duplicates) then we can sort the lists and a test for equality will perform as\\nexpected. The elements of a permutation group have an ordering defined for them — it is\\nnot so important what this is, just that some ordering is defined. The sorted() function\\nwill take any list and return a sorted version. So for each list of cosets, we will sort the\\nindividual cosets and then sort the list of sorted cosets. This is a typical maneuver, though\\na bit complicated with the nested lists.\\n\\nrc_sorted = sorted ([ sorted(coset) for coset in rc])\\nrc_sorted\\n\\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,2), (1,3)]]\\n\\nlc_sorted = sorted ([ sorted(coset) for coset in lc])\\nlc_sorted\\n\\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,2), (1,3)]]\\n\\nrc_sorted == lc_sorted\\n\\nTrue\\n\\nThe list of all cosets can be quite long (it will include every element of the group) and\\ncan take a few seconds to complete, even for small groups. There are more sophisticated,\\nand faster, ways to study cosets (such as just using their representatives), but to understand\\nthese techniques you also need to understand more theory.\\n\\nSubgroups\\nSage can compute all of the subgroups of a group. This can produce even more output\\nthan the coset method and can sometimes take much longer, depending on the structure\\nof the group. The list is in order of the size of the subgroups, with smallest first. As a\\ndemonstration we will first compute and list all of the subgroups of a small group, and then\\nextract just one of these subgroups from the list for some futher study.\\n\\nG = SymmetricGroup (3)\\nsg = G.subgroups (); sg\\n\\n[Subgroup of (Symmetric group of order 3! as a permutation group)\\ngenerated by [()],\\n\\nSubgroup of (Symmetric group of order 3! as a permutation group)\\ngenerated by [(2,3)],\\n\\nSubgroup of (Symmetric group of order 3! as a permutation group)\\ngenerated by [(1,2)],\\n\\nSubgroup of (Symmetric group of order 3! as a permutation group)\\ngenerated by [(1,3)],\\n\\nSubgroup of (Symmetric group of order 3! as a permutation group)\\ngenerated by [(1,2,3)],\\n\\nSubgroup of (Symmetric group of order 3! as a permutation group)\\ngenerated by [(2,3), (1,2,3)]]\\n\\nH = sg[4]; H\\n\\n\\n\\n6.5. SAGE 109\\n\\nSubgroup of (Symmetric group of order 3! as a permutation group)\\ngenerated by [(1,2,3)]\\n\\nH.order()\\n\\n3\\n\\nH.list()\\n\\n[(), (1,2,3), (1,3,2)]\\n\\nH.is_cyclic ()\\n\\nTrue\\n\\nThe output of the .subgroups() method can be voluminous, so sometimes we are inter-\\nested in properties of specific subgroups (as in the previous example) or broader questions\\nof the group’s “subgroup structure.” Here we expand on Corollary 6.15. Notice that just\\nbecause Sage does not compute a subgroup of order 6 in A4, this is no substitute whatsoever\\nfor a proof such as given for the corollary. But the computational result emboldens us to\\nsearch for the theoretical result with confidence.\\n\\nG = AlternatingGroup (4)\\nsg = G.subgroups ()\\n[H.order () for H in sg]\\n\\n[1, 2, 2, 2, 3, 3, 3, 3, 4, 12]\\n\\nSo we see no subgroup of order 6 in the list of subgroups of A4. Notice how Lagrange’s\\nTheorem (Theorem 6.10) is in evidence — all the subgroup orders divide 12, the order of\\nA4. Be patient, the next subgroup computation may take a while.\\n\\nG = SymmetricGroup (4)\\nsg = G.subgroups ()\\n[H.order () for H in sg]\\n\\n[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,\\n6, 6, 6, 6, 8, 8, 8, 12, 24]\\n\\nAgain, note Lagrange’s Theorem in action. But more interestingly, S4 has a subgroup\\nof order 6. Four of them, to be precise. These four subgroups of order 6 are similar to each\\nother, can you describe them simply (before digging into the sg list for more information)?\\nIf you were curious how many subgroups S4 has, you could simply count the number of\\nsubgroups in the sg list. The len() function does this for any list and is often an easy way\\nto count things.\\n\\nlen(sg)\\n\\n30\\n\\nSubgroups of Cyclic Groups\\nNow that we are more familiar with permutation groups, and know about the .subgroups()\\n\\nmethod, we can revisit an idea from Chapter 4. The subgroups of a cyclic group are always\\ncyclic, but how many are there and what are their orders?\\n\\n\\n\\n110 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\\n\\nG = CyclicPermutationGroup (20)\\n[H.order () for H in G.subgroups ()]\\n\\n[1, 2, 4, 5, 10, 20]\\n\\nG = CyclicPermutationGroup (19)\\n[H.order () for H in G.subgroups ()]\\n\\n[1, 19]\\n\\nWe could do this all day, but you have Sage at your disposal, so vary the order of G by\\nchanging n and study the output across many runs. Maybe try a cyclic group of order 24\\nand compare with the symmetric group S4 (above) which also has order 24. Do you feel a\\nconjecture coming on?\\n\\nn = 8\\nG = CyclicPermutationGroup(n)\\n[H.order () for H in G.subgroups ()]\\n\\n[1, 2, 4, 8]\\n\\nEuler Phi Function\\nTo add to our number-theoretic functions from Chapter 2, we note that Sage makes the\\nEuler ϕ-function available as the function euler_phi().\\n\\neuler_phi (345)\\n\\n176\\n\\nHere’s an interesting experiment that you can try running several times.\\nm = random_prime (10000)\\nn = random_prime (10000)\\nm, n, euler_phi(m*n) == euler_phi(m)*euler_phi(n)\\n\\n(5881, 1277, True)\\n\\nFeel another conjecture coming on? Can you generalize this result?\\n\\n6.6 Sage Exercises\\nThe following exercises are less about cosets and subgroups, and more about using Sage\\nas an experimental tool. They are designed to help you become both more efficient, and\\nmore expressive, as you write commands in Sage. We will have many opportunities to work\\nwith cosets and subgroups in the coming chapters. These exercises do not contain much\\nguidance, and get more challenging as they go. They are designed to explore, or confirm,\\nresults presented in this chapter or earlier chapters.\\n\\nImportant: You should answer each of the last three problems with a single (com-\\nplicated) line of Sage that concludes by outputting True. A “single line” means you will\\nhave several Sage commands packaged up together in complicated ways. It does not mean\\nseveral Sage commands seperated by semi-colons and typed in on a single line. Be sure\\ninclude some intermediate steps used in building up your solution, but using smaller ranges\\n\\n\\n\\n6.6. SAGE EXERCISES 111\\n\\nof values so as to not overwhelm the reader with lots of output. This will help you, and the\\ngrader of your work, have some confidence that the final version is correct.\\n\\nWhen you check integers below for divisibility, remember that range() produces plain\\nintegers, which are quite simple in their functionality. The srange() command produces Sage\\nintegers, which have many more capabilities. (See the last exercise for an example.) And\\nremember that a list comprehension is a very compact way to examine many possibilities\\nat once.\\n1. Use .subgroups() to find an example of a group G and an integer m, so that (a) m\\ndivides the order of G, and (b) G has no subgroup of order m. (Do not use the group A4\\n\\nfor G, since this is in the text.) Provide a single line of Sage code that has all the logic to\\nproduce the desired m as its output. (You can give your group a simple name on a prior\\nline and then just reference the group by name.) Here is a very simple example that might\\nhelp you structure your answer.\\n\\na = 5\\nb = 10\\nc = 6\\nd = 13\\na.divides(b)\\n\\nTrue\\n\\nnot (b in [c,d])\\n\\nTrue\\n\\na.divides(b) and not (b in [c,d])\\n\\nTrue\\n\\n2. Verify the truth of Fermat’s Little Theorem (either variant) using the composite number\\n391 = 17 · 23 as the choice of the base (either a or b), and for p assuming the value of every\\nprime number between 100 and 1000.\\nBuild up a solution slowly — make a list of powers (start with just a few primes), then\\nmake a list of powers reduced by modular arithmetic, then a list of comparisons with the\\npredicted value, then a check on all these logical values resulting from the comparisons.\\nThis is a useful strategy for many similar problems. Eventually you will write a single line\\nthat performs the verification by eventually printing out True. Here are some more hints\\nabout useful functions.\\n\\na = 20\\nb = 6\\na.mod(b)\\n\\n2\\n\\nprime_range (50, 100)\\n\\n[53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\\n\\nall([True , True , True , True])\\n\\nTrue\\n\\n\\n\\n112 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\\n\\nall([True , True , False , True])\\n\\nFalse\\n\\n3. Verify that the group of units mod n has order n−1 when n is prime, again for all primes\\nbetween 100 and 1000. As before, your output should be simply True, just once, indicating\\nthat the statement about the order is true for all the primes examined. As before, build\\nup your solution slowly, and with a smaller range of primes in the beginning. Express your\\nanswer as a single line of Sage code.\\n\\n4. Verify Euler’s Theorem for all values of 0 < n < 100 and for 1 ≤ a ≤ n. This will require\\nnested for statements with a conditional. Again, here is a small example that might be\\nhelpful for constructing your one line of Sage code. Note the use of srange() in this example.\\n\\n[a/b for a in srange (9) for b in srange(1,a) if gcd(a,b)==1]\\n\\n[2, 3, 3/2, 4, 4/3, 5, 5/2, 5/3, 5/4, 6, 6/5,\\n7, 7/2, 7/3, 7/4, 7/5, 7/6, 8, 8/3, 8/5, 8/7]\\n\\n5. The symmetric group on 7 symbols, S7, has 7! = 5040 elements. Consider the following\\nquestions without employing Sage, based on what we know about orders of elements of\\npermutation groups (Exercise 5.3.13).\\n\\n• What is the maximum possible order?\\n• How many elements are there of order 10?\\n• How many elements are there of order 1?\\n• How many elements are there of order 2?\\n• What is the smallest positive integer for which there is no element with that order?\\n\\nThese questions will be easier if you are familiar with using binomial coefficients for counting\\nin similarly complex situations. But either way, give some serious thought to each question\\n(and maybe a few of your own) before firing up Sage.\\nNow, compute how many elements there are of each order using the .order() method, and\\nthen embed this into a list comprehension which creates a single list of these counts. You\\ncan check your work (or check Sage) by wrapping this list in sum() and hopefully getting\\n5040.\\nComment on the process of studying these questions first without any computational aid,\\nand then again with Sage. For which values of n do you think Sage would be too slow and\\nyour mind quicker?\\n\\n\\n\\n7\\n\\nIntroduction to Cryptography\\n\\nCryptography is the study of sending and receiving secret messages. The aim of cryptogra-\\nphy is to send messages across a channel so that only the intended recipient of the message\\ncan read it. In addition, when a message is received, the recipient usually requires some\\nassurance that the message is authentic; that is, that it has not been sent by someone who\\nis trying to deceive the recipient. Modern cryptography is heavily dependent on abstract\\nalgebra and number theory.\\n\\nThe message to be sent is called the plaintext message. The disguised message is called\\nthe ciphertext. The plaintext and the ciphertext are both written in an alphabet, con-\\nsisting of letters or characters. Characters can include not only the familiar alphabetic\\ncharacters A, . . ., Z and a, . . ., z but also digits, punctuation marks, and blanks. A cryp-\\ntosystem, or cipher, has two parts: encryption, the process of transforming a plaintext\\nmessage to a ciphertext message, and decryption, the reverse transformation of changing\\na ciphertext message into a plaintext message.\\n\\nThere are many different families of cryptosystems, each distinguished by a particular\\nencryption algorithm. Cryptosystems in a specified cryptographic family are distinguished\\nfrom one another by a parameter to the encryption function called a key. A classical\\ncryptosystem has a single key, which must be kept secret, known only to the sender and\\nthe receiver of the message. If person A wishes to send secret messages to two different\\npeople B and C, and does not wish to have B understand C’s messages or vice versa, A\\nmust use two separate keys, so one cryptosystem is used for exchanging messages with B,\\nand another is used for exchanging messages with C.\\n\\nSystems that use two separate keys, one for encoding and another for decoding, are\\ncalled public key cryptosystems. Since knowledge of the encoding key does not allow\\nanyone to guess at the decoding key, the encoding key can be made public. A public key\\ncryptosystem allows A and B to send messages to C using the same encoding key. Anyone\\nis capable of encoding a message to be sent to C, but only C knows how to decode such a\\nmessage.\\n\\n7.1 Private Key Cryptography\\n\\nIn single or private key cryptosystems the same key is used for both encrypting and\\ndecrypting messages. To encrypt a plaintext message, we apply to the message some func-\\ntion which is kept secret, say f . This function will yield an encrypted message. Given\\nthe encrypted form of the message, we can recover the original message by applying the\\ninverse transformation f−1. The transformation f must be relatively easy to compute, as\\nmust f−1; however, f must be extremely difficult to guess from available examples of coded\\nmessages.\\n\\n113\\n\\n\\n\\n114 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\\n\\nExample 7.1. One of the first and most famous private key cryptosystems was the shift\\ncode used by Julius Caesar. We first digitize the alphabet by letting A = 00,B = 01, . . . ,Z =\\n25. The encoding function will be\\n\\nf(p) = p+ 3 mod 26;\\n\\nthat is, A 7→ D,B 7→ E, . . . , Z 7→ C. The decoding function is then\\n\\nf−1(p) = p− 3 mod 26 = p+ 23 mod 26.\\n\\nSuppose we receive the encoded message DOJHEUD. To decode this message, we first\\ndigitize it:\\n\\n3, 14, 9, 7, 4, 20, 3.\\n\\nNext we apply the inverse transformation to get\\n\\n0, 11, 6, 4, 1, 17, 0,\\n\\nor ALGEBRA. Notice here that there is nothing special about either of the numbers 3 or\\n26. We could have used a larger alphabet or a different shift.\\n\\nCryptanalysis is concerned with deciphering a received or intercepted message. Meth-\\nods from probability and statistics are great aids in deciphering an intercepted message;\\nfor example, the frequency analysis of the characters appearing in the intercepted message\\noften makes its decryption possible.\\n\\nExample 7.2. Suppose we receive a message that we know was encrypted by using a shift\\ntransformation on single letters of the 26-letter alphabet. To find out exactly what the shift\\ntransformation was, we must compute b in the equation f(p) = p + b mod 26. We can do\\nthis using frequency analysis. The letter E = 04 is the most commonly occurring letter\\nin the English language. Suppose that S = 18 is the most commonly occurring letter in\\nthe ciphertext. Then we have good reason to suspect that 18 = 4 + b mod 26, or b = 14.\\nTherefore, the most likely encrypting function is\\n\\nf(p) = p+ 14 mod 26.\\n\\nThe corresponding decrypting function is\\n\\nf−1(p) = p+ 12 mod 26.\\n\\nIt is now easy to determine whether or not our guess is correct.\\n\\nSimple shift codes are examples of monoalphabetic cryptosystems. In these ciphers a\\ncharacter in the enciphered message represents exactly one character in the original message.\\nSuch cryptosystems are not very sophisticated and are quite easy to break. In fact, in a\\nsimple shift as described in Example 7.1, there are only 26 possible keys. It would be quite\\neasy to try them all rather than to use frequency analysis.\\n\\nLet us investigate a slightly more sophisticated cryptosystem. Suppose that the encoding\\nfunction is given by\\n\\nf(p) = ap+ b mod 26.\\n\\nWe first need to find out when a decoding function f−1 exists. Such a decoding function\\nexists when we can solve the equation\\n\\nc = ap+ b mod 26\\n\\n\\n\\n7.2. PUBLIC KEY CRYPTOGRAPHY 115\\n\\nfor p. By Proposition 3.4, this is possible exactly when a has an inverse or, equivalently,\\nwhen gcd(a, 26) = 1. In this case\\n\\nf−1(p) = a−1p− a−1b mod 26.\\n\\nSuch a cryptosystem is called an affine cryptosystem.\\n\\nExample 7.3. Let us consider the affine cryptosystem f(p) = ap + b mod 26. For this\\ncryptosystem to work we must choose an a ∈ Z26 that is invertible. This is only possible if\\ngcd(a, 26) = 1. Recognizing this fact, we will let a = 5 since gcd(5, 26) = 1. It is easy to see\\nthat a−1 = 21. Therefore, we can take our encryption function to be f(p) = 5p+3 mod 26.\\nThus, ALGEBRA is encoded as 3, 6, 7, 23, 8, 10, 3, or DGHXIKD. The decryption function\\nwill be\\n\\nf−1(p) = 21p− 21 · 3 mod 26 = 21p+ 15 mod 26.\\n\\nA cryptosystem would be more secure if a ciphertext letter could represent more than one\\nplaintext letter. To give an example of this type of cryptosystem, called a polyalphabetic\\ncryptosystem, we will generalize affine codes by using matrices. The idea works roughly\\nthe same as before; however, instead of encrypting one letter at a time we will encrypt pairs\\nof letters. We can store a pair of letters p1 and p2 in a vector\\n\\np =\\n\\n(\\np1\\np2\\n\\n)\\n.\\n\\nLet A be a 2× 2 invertible matrix with entries in Z26. We can define an encoding function\\nby\\n\\nf(p) = Ap + b,\\n\\nwhere b is a fixed column vector and matrix operations are performed in Z26. The decoding\\nfunction must be\\n\\nf−1(p) = A−1p −A−1b.\\n\\nExample 7.4. Suppose that we wish to encode the word HELP. The corresponding digit\\nstring is 7, 4, 11, 15. If\\n\\nA =\\n\\n(\\n3 5\\n\\n1 2\\n\\n)\\n,\\n\\nthen\\nA−1 =\\n\\n(\\n2 21\\n\\n25 3\\n\\n)\\n.\\n\\nIf b = (2, 2)t, then our message is encrypted as RRGR. The encrypted letter R represents\\nmore than one plaintext letter.\\n\\nFrequency analysis can still be performed on a polyalphabetic cryptosystem, because we\\nhave a good understanding of how pairs of letters appear in the English language. The pair\\nth appears quite often; the pair qz never appears. To avoid decryption by a third party, we\\nmust use a larger matrix than the one we used in Example 7.4.\\n\\n7.2 Public Key Cryptography\\nIf traditional cryptosystems are used, anyone who knows enough to encode a message will\\nalso know enough to decode an intercepted message. In 1976, W. Diffie and M. Hellman\\nproposed public key cryptography, which is based on the observation that the encryption and\\n\\n\\n\\n116 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\\n\\ndecryption procedures need not have the same key. This removes the requirement that the\\nencoding key be kept secret. The encoding function f must be relatively easy to compute,\\nbut f−1 must be extremely difficult to compute without some additional information, so\\nthat someone who knows only the encrypting key cannot find the decrypting key without\\nprohibitive computation. It is interesting to note that to date, no system has been proposed\\nthat has been proven to be “one-way;” that is, for any existing public key cryptosystem,\\nit has never been shown to be computationally prohibitive to decode messages with only\\nknowledge of the encoding key.\\n\\nThe RSA Cryptosystem\\nThe rsa cryptosystem introduced by R. Rivest, A. Shamir, and L. Adleman in 1978, is\\nbased on the difficulty of factoring large numbers. Though it is not a difficult task to find\\ntwo large random primes and multiply them together, factoring a 150-digit number that is\\nthe product of two large primes would take 100 million computers operating at 10 million\\ninstructions per second about 50 million years under the fastest algorithms available in the\\nearly 1990s. Although the algorithms have improved, factoring a number that is a product\\nof two large primes is still computationally prohibative.\\n\\nThe rsa cryptosystem works as follows. Suppose that we choose two random 150-\\ndigit prime numbers p and q. Next, we compute the product n = pq and also compute\\nϕ(n) = m = (p− 1)(q− 1), where ϕ is the Euler ϕ-function. Now we start choosing random\\nintegers E until we find one that is relatively prime to m; that is, we choose E such that\\ngcd(E,m) = 1. Using the Euclidean algorithm, we can find a number D such that DE ≡ 1\\n(mod m). The numbers n and E are now made public.\\n\\nSuppose now that person B (Bob) wishes to send person A (Alice) a message over a\\npublic line. Since E and n are known to everyone, anyone can encode messages. Bob\\nfirst digitizes the message according to some scheme, say A = 00,B = 02, . . . ,Z = 25. If\\nnecessary, he will break the message into pieces such that each piece is a positive integer\\nless than n. Suppose x is one of the pieces. Bob forms the number y = xE mod n and\\nsends y to Alice. For Alice to recover x, she need only compute x = yD mod n. Only Alice\\nknows D.\\nExample 7.5. Before exploring the theory behind the rsa cryptosystem or attempting to\\nuse large integers, we will use some small integers just to see that the system does indeed\\nwork. Suppose that we wish to send some message, which when digitized is 25. Let p = 23\\nand q = 29. Then\\n\\nn = pq = 667\\n\\nand\\nϕ(n) = m = (p− 1)(q − 1) = 616.\\n\\nWe can let E = 487, since gcd(616, 487) = 1. The encoded message is computed to be\\n\\n25487 mod 667 = 169.\\n\\nThis computation can be reasonably done by using the method of repeated squares as\\ndescribed in Chapter 4. Using the Euclidean algorithm, we determine that 191E = 1+151m;\\ntherefore, the decrypting key is (n,D) = (667, 191). We can recover the original message\\nby calculating\\n\\n169191 mod 667 = 25.\\n\\nNow let us examine why the rsa cryptosystem works. We know that DE ≡ 1 (mod m);\\nhence, there exists a k such that\\n\\nDE = km+ 1 = kϕ(n) + 1.\\n\\n\\n\\n7.2. PUBLIC KEY CRYPTOGRAPHY 117\\n\\nThere are two cases to consider. In the first case assume that gcd(x, n) = 1. Then by\\nTheorem 6.18,\\n\\nyD = (xE)D = xDE = xkm+1 = (xϕ(n))kx = (1)kx = x mod n.\\n\\nSo we see that Alice recovers the original message x when she computes yD mod n.\\nFor the other case, assume that gcd(x, n) ̸= 1. Since n = pq and x < n, we know x is\\n\\na multiple of p or a multiple of q, but not both. We will describe the first possibility only,\\nsince the second is entirely similar. There is then an integer r, with r < q and x = rp. Note\\nthat we have gcd(x, q) = 1 and that m = ϕ(n) = (p − 1)(q − 1) = ϕ(p)ϕ(q). Then, using\\nTheorem 6.18, but now mod q,\\n\\nxkm = xkϕ(p)ϕ(q) = (xϕ(q))kϕ(p) = (1)kϕ(p) = 1 mod q.\\n\\nSo there is an integer t such that xkm = 1 + tq. Thus, Alice also recovers the message in\\nthis case,\\n\\nyD = xkm+1 = xkmx = (1 + tq)x = x+ tq(rp) = x+ trn = x mod n.\\n\\nWe can now ask how one would go about breaking the rsa cryptosystem. To find D\\ngiven n and E, we simply need to factor n and solve for D by using the Euclidean algorithm.\\nIf we had known that 667 = 23 · 29 in Example 7.5, we could have recovered D.\\n\\nMessage Verification\\nThere is a problem of message verification in public key cryptosystems. Since the encoding\\nkey is public knowledge, anyone has the ability to send an encoded message. If Alice\\nreceives a message from Bob, she would like to be able to verify that it was Bob who\\nactually sent the message. Suppose that Bob’s encrypting key is (n′, E′) and his decrypting\\nkey is (n′, D′). Also, suppose that Alice’s encrypting key is (n,E) and her decrypting key is\\n(n,D). Since encryption keys are public information, they can exchange coded messages at\\ntheir convenience. Bob wishes to assure Alice that the message he is sending is authentic.\\nBefore Bob sends the message x to Alice, he decrypts x with his own key:\\n\\nx′ = xD\\n′ mod n′.\\n\\nAnyone can change x′ back to x just by encryption, but only Bob has the ability to form\\nx′. Now Bob encrypts x′ with Alice’s encryption key to form\\n\\ny′ = x′\\nE mod n,\\n\\na message that only Alice can decode. Alice decodes the message and then encodes the\\nresult with Bob’s key to read the original message, a message that could have only been\\nsent by Bob.\\n\\nHistorical Note\\n\\nEncrypting secret messages goes as far back as ancient Greece and Rome. As we know,\\nJulius Caesar used a simple shift code to send and receive messages. However, the formal\\nstudy of encoding and decoding messages probably began with the Arabs in the 1400s. In\\nthe fifteenth and sixteenth centuries mathematicians such as Alberti and Viete discovered\\nthat monoalphabetic cryptosystems offered no real security. In the 1800s, F. W. Kasiski\\nestablished methods for breaking ciphers in which a ciphertext letter can represent more\\n\\n\\n\\n118 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\\n\\nthan one plaintext letter, if the same key was used several times. This discovery led to the\\nuse of cryptosystems with keys that were used only a single time. Cryptography was placed\\non firm mathematical foundations by such people as W. Friedman and L. Hill in the early\\npart of the twentieth century.\\n\\nThe period after World War I saw the development of special-purpose machines for\\nencrypting and decrypting messages, and mathematicians were very active in cryptography\\nduring World War II. Efforts to penetrate the cryptosystems of the Axis nations were\\norganized in England and in the United States by such notable mathematicians as Alan\\nTuring and A. A. Albert. The Allies gained a tremendous advantage in World War II by\\nbreaking the ciphers produced by the German Enigma machine and the Japanese Purple\\nciphers.\\n\\nBy the 1970s, interest in commercial cryptography had begun to take hold. There was\\na growing need to protect banking transactions, computer data, and electronic mail. In\\nthe early 1970s, ibm developed and implemented luzifer, the forerunner of the National\\nBureau of Standards’ Data Encryption Standard (DES).\\n\\nThe concept of a public key cryptosystem, due to Diffie and Hellman, is very recent\\n(1976). It was further developed by Rivest, Shamir, and Adleman with the rsa cryptosys-\\ntem (1978). It is not known how secure any of these systems are. The trapdoor knapsack\\ncryptosystem, developed by Merkle and Hellman, has been broken. It is still an open ques-\\ntion whether or not the rsa system can be broken. In 1991, rsa Laboratories published a\\nlist of semiprimes (numbers with exactly two prime factors) with a cash prize for whoever\\nwas able to provide a factorization (http://www.emc.com/emc-plus/rsa-labs/historical/the-\\nrsa-challenge-numbers.htm). Although the challenge ended in 2007, many of these numbers\\nhave not yet been factored.\\n\\nThere been a great deal of controversy about research in cryptography and cryptography\\nitself. In 1929, when Henry Stimson, Secretary of State under Herbert Hoover, dismissed the\\nBlack Chamber (the State Department’s cryptography division) on the ethical grounds that\\n“gentlemen do not read each other’s mail.” During the last two decades of the twentieth\\ncentury, the National Security Agency wanted to keep information about cryptography\\nsecret, whereas the academic community fought for the right to publish basic research.\\nCurrently, research in mathematical cryptography and computational number theory is\\nvery active, and mathematicians are free to publish their results in these areas.\\n\\n7.3 Exercises\\n1. Encode IXLOVEXMATH using the cryptosystem in Example 1.\\n\\n2. Decode ZLOOA WKLVA EHARQ WKHA ILQDO, which was encoded using the cryptosystem in\\nExample 1.\\n\\n3. Assuming that monoalphabetic code was used to encode the following secret message,\\nwhat was the original message?\\n\\nAPHUO EGEHP PEXOV FKEUH CKVUE CHKVE APHUO\\nEGEHU EXOVL EXDKT VGEFT EHFKE UHCKF TZEXO\\nVEZDT TVKUE XOVKV ENOHK ZFTEH TEHKQ LEROF\\nPVEHP PEXOV ERYKP GERYT GVKEG XDRTE RGAGA\\n\\nWhat is the significance of this message in the history of cryptography?\\n\\n4. What is the total number of possible monoalphabetic cryptosystems? How secure are\\nsuch cryptosystems?\\n\\nhttp://www.emc.com/emc-plus/rsa-labs/historical/the-rsa-challenge-numbers.htm\\nhttp://www.emc.com/emc-plus/rsa-labs/historical/the-rsa-challenge-numbers.htm\\n\\n\\n7.4. ADDITIONAL EXERCISES: PRIMALITY AND FACTORING 119\\n\\n5. Prove that a 2×2 matrix A with entries in Z26 is invertible if and only if gcd(det(A), 26) =\\n1.\\n\\n6. Given the matrix\\nA =\\n\\n(\\n3 4\\n\\n2 3\\n\\n)\\n,\\n\\nuse the encryption function f(p) = Ap + b to encode the message CRYPTOLOGY, where b =\\n(2, 5)t. What is the decoding function?\\n\\n7. Encrypt each of the following rsa messages x so that x is divided into blocks of integers\\nof length 2; that is, if x = 142528, encode 14, 25, and 28 separately.\\n(a) n = 3551, E = 629, x = 31\\n\\n(b) n = 2257, E = 47, x = 23\\n\\n(c) n = 120979, E = 13251, x = 142371\\n\\n(d) n = 45629, E = 781, x = 231561\\n\\n8. Compute the decoding key D for each of the encoding keys in Exercise 7.\\n\\n9. Decrypt each of the following rsa messages y.\\n(a) n = 3551, D = 1997, y = 2791\\n\\n(b) n = 5893, D = 81, y = 34\\n\\n(c) n = 120979, D = 27331, y = 112135\\n\\n(d) n = 79403, D = 671, y = 129381\\n\\n10. For each of the following encryption keys (n,E) in the rsa cryptosystem, compute D.\\n(a) (n,E) = (451, 231)\\n\\n(b) (n,E) = (3053, 1921)\\n\\n(c) (n,E) = (37986733, 12371)\\n\\n(d) (n,E) = (16394854313, 34578451)\\n\\n11. Encrypted messages are often divided into blocks of n letters. A message such as\\nTHE WORLD WONDERS WHY might be encrypted as JIW OCFRJ LPOEVYQ IOC but sent as JIW OCF\\n\\nRJL POE VYQ IOC. What are the advantages of using blocks of n letters?\\n\\n12. Find integers n, E, and X such that\\n\\nXE ≡ X (mod n).\\n\\nIs this a potential problem in the rsa cryptosystem?\\n\\n13. Every person in the class should construct an rsa cryptosystem using primes that are\\n10 to 15 digits long. Hand in (n,E) and an encoded message. Keep D secret. See if you\\ncan break one another’s codes.\\n\\n7.4 Additional Exercises: Primality and Factoring\\nIn the rsa cryptosystem it is important to be able to find large prime numbers easily. Also,\\nthis cryptosystem is not secure if we can factor a composite number that is the product\\nof two large primes. The solutions to both of these problems are quite easy. To find out\\n\\n\\n\\n120 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\\n\\nif a number n is prime or to factor n, we can use trial division. We simply divide n by\\nd = 2, 3, . . . ,\\n\\n√\\nn. Either a factorization will be obtained, or n is prime if no d divides n.\\n\\nThe problem is that such a computation is prohibitively time-consuming if n is very large.\\n1. A better algorithm for factoring odd positive integers is Fermat’s factorization al-\\ngorithm.\\n(a) Let n = ab be an odd composite number. Prove that n can be written as the difference\\n\\nof two perfect squares:\\nn = x2 − y2 = (x− y)(x+ y).\\n\\nConsequently, a positive odd integer can be factored exactly when we can find integers\\nx and y such that n = x2 − y2.\\n\\n(b) Write a program to implement the following factorization algorithm based on the\\nobservation in part (a). The expression ceiling(sqrt(n)) means the smallest integer\\ngreater than or equal to the square root of n. Write another program to do factorization\\nusing trial division and compare the speed of the two algorithms. Which algorithm is\\nfaster and why?\\n\\nx := ceiling(sqrt(n))\\ny := 1\\n\\n1 : while x^2 - y^2 > n do\\ny := y + 1\\n\\nif x^2 - y^2 < n then\\nx := x + 1\\ny := 1\\ngoto 1\\n\\nelse if x^2 - y^2 = 0 then\\na := x - y\\nb := x + y\\nwrite n = a * b\\n\\n2. (Primality Testing) Recall Fermat’s Little Theorem from Chapter 6. Let p be prime\\nwith gcd(a, p) = 1. Then ap−1 ≡ 1 (mod p). We can use Fermat’s Little Theorem as a\\nscreening test for primes. For example, 15 cannot be prime since\\n\\n215−1 ≡ 214 ≡ 4 (mod 15).\\n\\nHowever, 17 is a potential prime since\\n\\n217−1 ≡ 216 ≡ 1 (mod 17).\\n\\nWe say that an odd composite number n is a pseudoprime if\\n\\n2n−1 ≡ 1 (mod n).\\n\\nWhich of the following numbers are primes and which are pseudoprimes?\\n\\n(a) 342\\n(b) 811\\n\\n(c) 601\\n(d) 561\\n\\n(e) 771\\n(f) 631\\n\\n\\n\\n7.5. REFERENCES AND SUGGESTED READINGS 121\\n\\n3. Let n be an odd composite number and b be a positive integer such that gcd(b, n) = 1.\\nIf bn−1 ≡ 1 (mod n), then n is a pseudoprime base b. Show that 341 is a pseudoprime\\nbase 2 but not a pseudoprime base 3.\\n\\n4. Write a program to determine all primes less than 2000 using trial division. Write a\\nsecond program that will determine all numbers less than 2000 that are either primes or\\npseudoprimes. Compare the speed of the two programs. How many pseudoprimes are there\\nbelow 2000?\\nThere exist composite numbers that are pseudoprimes for all bases to which they are rel-\\natively prime. These numbers are called Carmichael numbers. The first Carmichael\\nnumber is 561 = 3 · 11 · 17. In 1992, Alford, Granville, and Pomerance proved that there\\nare an infinite number of Carmichael numbers [4]. However, Carmichael numbers are very\\nrare. There are only 2163 Carmichael numbers less than 25× 109. For more sophisticated\\nprimality tests, see [1], [6], or [7].\\n\\n7.5 References and Suggested Readings\\n[1] Bressoud, D. M. Factorization and Primality Testing. Springer-Verlag, New York,\\n\\n1989.\\n[2] Diffie, W. and Hellman, M. E. “New Directions in Cryptography,” IEEE Trans. In-\\n\\nform. Theory 22 (1976), 644–54.\\n[3] Gardner, M. “Mathematical games: A new kind of cipher that would take millions of\\n\\nyears to break,” Scientific American 237 (1977), 120–24.\\n[4] Granville, A. “Primality Testing and Carmichael Numbers,” Notices of the American\\n\\nMathematical Society 39(1992), 696–700.\\n[5] Hellman, M. E. “The Mathematics of Public Key Cryptography,” Scientific American\\n\\n241(1979), 130–39.\\n[6] Koblitz, N. A Course in Number Theory and Cryptography. 2nd ed. Springer, New\\n\\nYork, 1994.\\n[7] Pomerance, C., ed. “Cryptology and Computational Number Theory”, Proceedings\\n\\nof Symposia in Applied Mathematics 42(1990) American Mathematical Society, Prov-\\nidence, RI.\\n\\n[8] Rivest, R. L., Shamir, A., and Adleman, L., “A Method for Obtaining Signatures and\\nPublic-key Cryptosystems,” Comm. ACM 21(1978), 120–26.\\n\\n7.6 Sage\\nSince Sage began as software to support research in number theory, we can quickly and easily\\ndemonstrate the internal workings of the rsa algorithm. Recognize that, in practice, many\\nother details such as encoding between letters and integers, or protecting one’s private key,\\nare equally important for the security of communications. So rsa itself is just the theoretical\\nfoundation.\\n\\nConstructing Keys\\nWe will suppose that Alice wants to send a secret message to Bob, along with message\\nverification (also known as a message with a digital signature). So we begin with the\\nconstruction of key pairs (private and public) for both Alice and Bob. We first need two\\n\\n\\n\\n122 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\\n\\nlarge primes for both individuals, and their product. In practice, values of n would have\\nhundreds of digits, rather than just 21 as we have done here.\\n\\np_a = next_prime (10^10)\\nq_a = next_prime(p_a)\\np_b = next_prime ((3/2) *10^10)\\nq_b = next_prime(p_b)\\nn_a = p_a * q_a\\nn_b = p_b * q_b\\nn_a , n_b\\n\\n(100000000520000000627 , 225000000300000000091)\\n\\nComputationally, the value of the Euler ϕ-function for a product of primes pq can be\\nobtained from (p− 1)(q − 1), but we could use Sage’s built-in function just as well.\\n\\nm_a = euler_phi(n_a)\\nm_b = euler_phi(n_b)\\nm_a , m_b\\n\\n(100000000500000000576 , 225000000270000000072)\\n\\nNow we can create the encryption and decryption exponents. We choose the encryption\\nexponent as a (small) number relatively prime to the value of m. With Sage we can factor m\\nquickly to help us choose this value. In practice we would not want to do this computation\\nfor large values of m, so we might more easily choose “random” values and check for the first\\nvalue which is relatively prime to m. The decryption exponent is the multiplicative inverse,\\nmod m, of the encryption exponent. If you construct an improper encryption exponent (not\\nrelatively prime to m), the computation of the multiplicative inverse will fail (and Sage will\\ntell you so). We do this twice —- for both Alice and Bob.\\n\\nfactor(m_a)\\n\\n2^6 * 3 * 11 * 17 * 131 * 521 * 73259 * 557041\\n\\nE_a = 5*23\\nD_a = inverse_mod(E_a , m_a)\\nD_a\\n\\n20869565321739130555\\n\\nfactor(m_b)\\n\\n2^3 * 3^4 * 107 * 1298027 * 2500000001\\n\\nE_b = 7*29\\nD_b = inverse_mod(E_b , m_b)\\nD_b\\n\\n24384236482463054195\\n\\nAt this stage, each individual would publish their values of n and E, while keeping D\\nvery private and secure. In practice D should be protected on the user’s hard disk by a\\npassword only the owner knows. For even greater security a person might only have two\\ncopies of their private key, one on a usb memory stick they always carry with them, and a\\nbackup in their sage deposit box. Every time the person uses D they would need to provide\\nthe password. The value of m can be discarded. For the record, here are all the keys:\\n\\n\\n\\n7.6. SAGE 123\\n\\nprint \\"Alice \' s␣public␣key ,␣n:\\", n_a , \\"E:\\", E_a\\n\\nAlice \' s␣public␣key ,␣n:␣100000000520000000627␣E:␣115\\n\\nprint \\"Alice \' s␣private␣key ,␣D:\\", D_a\\n\\nAlice \' s␣private␣key ,␣D:␣20869565321739130555\\n\\nprint \\"Bob \' s␣public␣key ,␣n:\\", n_b , \\"E:\\", E_b\\n\\nBob \' s␣public␣key ,␣n:␣225000000300000000091␣E:␣203\\n\\nprint \\"Bob \' s␣private␣key ,␣D:\\", D_b\\n\\nBob \' s␣private␣key ,␣D:␣24384236482463054195\\n\\nSigning and Encoding a Message\\nAlice is going to construct a message as an English word with four letters. From these four\\nletters we will construct a single number to represent the message in a form we can use in\\nthe rsa algorithm. The function ord() will convert a single letter to its ascii code value, a\\nnumber between 0 and 127. If we use these numbers as “digits” mod 128, we can be sure\\nthat Alice’s four-letter word will encode to an integer less than 1284 = 268, 435, 456. The\\nparticular maximum value is not important, so long as it is smaller than our value of n since\\nall of our subsequent arithmetic is mod n. We choose a popular four-letter word, convert\\nto ascii “digits” with a list comprehension, and then construct the integer from the digits\\nwith the right base. Notice how we can treat the word as a list and that the first digit in\\nthe list is in the “ones” place (we say the list is in “little-endian” order).\\n\\nword = \' Sage \'\\ndigits = [ord(letter) for letter in word]\\ndigits\\n\\n[83, 97, 103, 101]\\n\\nmessage = ZZ(digits , 128)\\nmessage\\n\\n213512403\\n\\nFirst, Alice will sign her message to provide message verification. She uses her private\\nkey for this, since this is an act that only she should be able to perform.\\n\\nsigned = power_mod(message , D_a , n_a)\\nsigned\\n\\n47838774644892618423\\n\\nThen Alice encrypts her message so that only Bob can read it. To do this, she uses\\nBob’s public key. Notice how she does not have to even know Bob — for example, she could\\nhave obtained Bob’s public key off his web site or maybe Bob announced his public key in\\nan advertisement in the New York Times.\\n\\n\\n\\n124 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\\n\\nencrypted = power_mod(signed , E_b , n_b)\\nencrypted\\n\\n111866209291209840488\\n\\nAlice’s communication is now ready to travel on any communications network, no matter\\nhow insecure the network may be, and no matter how many snoops may be monitoring the\\nnetwork.\\n\\nDecoding and Verifying a Message\\nNow assume that the value of encrypted has reached Bob. Realize that Bob may not know\\nAlice, and realize that Bob does not even necessarily believe what he has received has\\ngenuinely originated from Alice. An adversary could be trying to confuse Bob by sending\\nmessages that claim to be from Alice. First, Bob must unwrap the encyption Alice has\\nprovided. This is an act only Bob, as the intended recipient, should be able to do. And he\\ndoes it by using his private key, which only he knows, and which he has kept secure.\\n\\ndecrypted = power_mod(encrypted , D_b , n_b)\\ndecrypted\\n\\n47838774644892618423\\n\\nRight now, this means very little to Bob. Anybody could have sent him an encoded\\nmessage. However, this was a message Alice signed. Lets unwrap the message signing.\\nNotice that this uses Alice’s public key. Bob does not need to know Alice — for example,\\nhe could obtain Alice’s key off her web site or maybe Alice announced her public key in an\\nadvertisement in the New York Times.\\n\\nreceived = power_mod(decrypted , E_a , n_a)\\nreceived\\n\\n213512403\\n\\nBob needs to transform this integer representation back to a word with letters. The\\nchr() function converts ascii code values to letters, and we use a list comprehension to do\\nthis repeatedly.\\n\\ndigits = received.digits(base =128)\\nletters = [chr(ascii) for ascii in digits]\\nletters\\n\\n[ \' S \' , \' a \' , \' g \' , \' e \' ]\\n\\nIf we would like a slightly more recognizable result, we can combine the letters into a\\nstring.\\n\\n\' \' .join(letters)\\n\\n\' Sage \'\\n\\nBob is pleased to obtain such an informative message from Alice. What would have\\nhappened if an imposter had sent a message ostensibly from Alice, or what if an adversary\\nhad intercepted Alice’s original message and replaced it with a tampered message? (The\\nlatter is known as a “man in the middle” attack.)\\n\\n\\n\\n7.7. SAGE EXERCISES 125\\n\\nIn either case, the rogue party would not be able to duplicate Alice’s first action —\\nsigning her message. If an adversary somehow signs the message, or tampers with it, the\\nstep where Bob unwraps the signing will lead to total garbage. (Try it!) Because Bob\\nreceived a legitimate word, properly capitalized, he has confidence that the message he\\nunsigned is the same as the message Alice signed. In practice, if Alice sent several hundred\\nwords as her message, the odds that it will unsign as cohrent text are astronomically small.\\n\\nWhat have we demonstrated?\\n\\n1. Alice can send messages that only Bob can read.\\n\\n2. Bob can receive secret messages from anybody.\\n\\n3. Alice can sign messages, so that then Bob (or anybody else)knows they are genuinely\\nfrom Alice.\\n\\nOf course, without making new keys, you can reverse the roles of Alice and Bob. And if\\nCarol makes a key pair, she can communicate with both Alice and Bob in the same fashion.\\n\\nIf you want to use rsa public-key encryption seriously, investigate the open source\\nsoftware GNU Privacy Guard, aka GPG, which is freely available at www.gnupg.org/. Notice\\nthat it only makes sense to use encryption programs that allow you to look at the source\\ncode.\\n\\n7.7 Sage Exercises\\n1. Construct a keypair for Alice using the first two primes greater than 1012. For your\\nchoice of E, use a single prime number and use the smallest possible choice.\\nOutput the values of n, E, and D for Alice. Then use Sage commands to verify that Alice’s\\nencryption and decryption keys are multiplicative inverses.\\n\\n2. Construct a keypair for Bob using the first two primes greater than 2 · 1012. For your\\nchoice of E, use a single prime number and use the smallest possible choice. Output the\\nvalues of n, E, and D for Alice.\\nEncode the word Math using ascii values in the same manner as described in this section\\n(keep the capitalization as shown). Create a signed message of this word for communication\\nfrom Alice to Bob. Output the three integers: the message, the signed message and the\\nsigned, encrypted message.\\n\\n3. Demonstrate how Bob converts the message received from Alice back into the word Math.\\nOutput the value of the intermediate computations and the final human-readable message.\\n\\n4. Create a new signed message from Alice to Bob. Simulate the message being tampered\\nwith by adding 1 to the integer Bob receives, before he decrypts it. What result does Bob\\nget for the letters of the message when he decrypts and unsigns the tampered message?\\n\\n5. (Classroom Exercise) Organize a class into several small groups. Have each group con-\\nstruct key pairs with some minimum size (digits in n). Each group should keep their private\\nkey to themselves, but make their public key available to everybody in the room. It could\\nbe written on the board (error-prone) or maybe pasted in a public site like pastebin.com.\\nThen each group can send a signed message to another group, where the groups could be\\narranged logically in a circular fashion for this purpose. Of course, messages should be\\nposted publicly as well. Expect a success rate somewhere between 50% and 100%.\\nIf you do not do this in class, grab a study buddy and send each other messages in the same\\nmanner. Expect a success rate of 0%, 50% or 100%.\\n\\nhttps://www.gnupg.org/\\nhttp://pastebin.com/\\n\\n\\n8\\n\\nAlgebraic Coding Theory\\n\\nCoding theory is an application of algebra that has become increasingly important over the\\nlast several decades. When we transmit data, we are concerned about sending a message\\nover a channel that could be affected by “noise.” We wish to be able to encode and decode the\\ninformation in a manner that will allow the detection, and possibly the correction, of errors\\ncaused by noise. This situation arises in many areas of communications, including radio,\\ntelephone, television, computer communications, and digital media technology. Probability,\\ncombinatorics, group theory, linear algebra, and polynomial rings over finite fields all play\\nimportant roles in coding theory.\\n\\n8.1 Error-Detecting and Correcting Codes\\nLet us examine a simple model of a communications system for transmitting and receiving\\ncoded messages (Figure 8.1).\\n\\nm-digit message\\n\\nEncoder\\n\\nn-digit code word\\n\\nTransmitter\\n\\nNoise\\n\\nReceiver\\n\\nn-digit received word\\n\\nDecoder\\n\\nm-digit received message or error\\n\\nFigure 8.1: Encoding and decoding messages\\n\\nUncoded messages may be composed of letters or characters, but typically they consist\\nof binary m-tuples. These messages are encoded into codewords, consisting of binary n-\\ntuples, by a device called an encoder. The message is transmitted and then decoded. We\\n\\n126\\n\\n\\n\\n8.1. ERROR-DETECTING AND CORRECTING CODES 127\\n\\nwill consider the occurrence of errors during transmission. An error occurs if there is a\\nchange in one or more bits in the codeword. A decoding scheme is a method that either\\nconverts an arbitrarily received n-tuple into a meaningful decoded message or gives an error\\nmessage for that n-tuple. If the received message is a codeword (one of the special n-tuples\\nallowed to be transmitted), then the decoded message must be the unique message that\\nwas encoded into the codeword. For received non-codewords, the decoding scheme will give\\nan error indication, or, if we are more clever, will actually try to correct the error and\\nreconstruct the original message. Our goal is to transmit error-free messages as cheaply\\nand quickly as possible.\\n\\nExample 8.2. One possible coding scheme would be to send a message several times and\\nto compare the received copies with one another. Suppose that the message to be encoded is\\na binary n-tuple (x1, x2, . . . , xn). The message is encoded into a binary 3n-tuple by simply\\nrepeating the message three times:\\n\\n(x1, x2, . . . , xn) 7→ (x1, x2, . . . , xn, x1, x2, . . . , xn, x1, x2, . . . , xn).\\n\\nTo decode the message, we choose as the ith digit the one that appears in the ith place\\nin at least two of the three transmissions. For example, if the original message is (0110),\\nthen the transmitted message will be (0110 0110 0110). If there is a transmission error in\\nthe fifth digit, then the received codeword will be (0110 1110 0110), which will be correctly\\ndecoded as (0110).1 This triple-repetition method will automatically detect and correct all\\nsingle errors, but it is slow and inefficient: to send a message consisting of n bits, 2n extra\\nbits are required, and we can only detect and correct single errors. We will see that it is\\npossible to find an encoding scheme that will encode a message of n bits into m bits with\\nm much smaller than 3n.\\n\\nExample 8.3. Even parity, a commonly used coding scheme, is much more efficient\\nthan the simple repetition scheme. The ascii (American Standard Code for Information\\nInterchange) coding system uses binary 8-tuples, yielding 28 = 256 possible 8-tuples. How-\\never, only seven bits are needed since there are only 27 = 128 ascii characters. What\\ncan or should be done with the extra bit? Using the full eight bits, we can detect single\\ntransmission errors. For example, the ascii codes for A, B, and C are\\n\\nA = 6510 = 010000012,\\n\\nB = 6610 = 010000102,\\n\\nC = 6710 = 010000112.\\n\\nNotice that the leftmost bit is always set to 0; that is, the 128 ascii characters have codes\\n\\n000000002 = 010,\\n\\n...\\n011111112 = 12710.\\n\\nThe bit can be used for error checking on the other seven bits. It is set to either 0 or 1\\nso that the total number of 1 bits in the representation of a character is even. Using even\\nparity, the codes for A, B, and C now become\\n\\nA = 010000012,\\n\\nB = 010000102,\\n\\nC = 110000112.\\n\\n1We will adopt the convention that bits are numbered left to right in binary n-tuples.\\n\\n\\n\\n128 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nSuppose an A is sent and a transmission error in the sixth bit is caused by noise over the\\ncommunication channel so that (0100 0101) is received. We know an error has occurred since\\nthe received word has an odd number of 1s, and we can now request that the codeword be\\ntransmitted again. When used for error checking, the leftmost bit is called a parity check\\nbit.\\n\\nBy far the most common error-detecting codes used in computers are based on the\\naddition of a parity bit. Typically, a computer stores information in m-tuples called words.\\nCommon word lengths are 8, 16, and 32 bits. One bit in the word is set aside as the parity\\ncheck bit, and is not used to store information. This bit is set to either 0 or 1, depending\\non the number of 1s in the word.\\n\\nAdding a parity check bit allows the detection of all single errors because changing a\\nsingle bit either increases or decreases the number of 1s by one, and in either case the parity\\nhas been changed from even to odd, so the new word is not a codeword. (We could also\\nconstruct an error detection scheme based on odd parity; that is, we could set the parity\\ncheck bit so that a codeword always has an odd number of 1s.)\\n\\nThe even parity system is easy to implement, but has two drawbacks. First, multiple\\nerrors are not detectable. Suppose an A is sent and the first and seventh bits are changed\\nfrom 0 to 1. The received word is a codeword, but will be decoded into a C instead of an A.\\nSecond, we do not have the ability to correct errors. If the 8-tuple (1001 1000) is received,\\nwe know that an error has occurred, but we have no idea which bit has been changed.\\nWe will now investigate a coding scheme that will not only allow us to detect transmission\\nerrors but will actually correct the errors.\\n\\nTransmitted Received Word\\nCodeword 000 001 010 011 100 101 110 111\\n\\n000 0 1 1 2 1 2 2 3\\n111 3 2 2 1 2 1 1 0\\n\\nTable 8.4: A repetition code\\n\\nExample 8.5. Suppose that our original message is either a 0 or a 1, and that 0 encodes\\nto (000) and 1 encodes to (111). If only a single error occurs during transmission, we can\\ndetect and correct the error. For example, if a 101 is received, then the second bit must\\nhave been changed from a 1 to a 0. The originally transmitted codeword must have been\\n(111). This method will detect and correct all single errors.\\n\\nIn Table 8.4, we present all possible words that might be received for the transmitted\\ncodewords (000) and (111). Table 8.4 also shows the number of bits by which each received\\n3-tuple differs from each original codeword.\\n\\nMaximum-Likelihood Decoding\\nThe coding scheme presented in Example 8.5 is not a complete solution to the problem\\nbecause it does not account for the possibility of multiple errors. For example, either a\\n(000) or a (111) could be sent and a (001) received. We have no means of deciding from the\\nreceived word whether there was a single error in the third bit or two errors, one in the first\\nbit and one in the second. No matter what coding scheme is used, an incorrect message\\ncould be received. We could transmit a (000), have errors in all three bits, and receive\\nthe codeword (111). It is important to make explicit assumptions about the likelihood and\\ndistribution of transmission errors so that, in a particular application, it will be known\\nwhether a given error detection scheme is appropriate. We will assume that transmission\\n\\n\\n\\n8.1. ERROR-DETECTING AND CORRECTING CODES 129\\n\\nerrors are rare, and, that when they do occur, they occur independently in each bit; that is,\\nif p is the probability of an error in one bit and q is the probability of an error in a different\\nbit, then the probability of errors occurring in both of these bits at the same time is pq.\\nWe will also assume that a received n-tuple is decoded into a codeword that is closest to it;\\nthat is, we assume that the receiver uses maximum-likelihood decoding.2\\n\\np1 1\\n\\np\\n0 0\\n\\nq\\n\\nq\\n\\nFigure 8.6: Binary symmetric channel\\n\\nA binary symmetric channel is a model that consists of a transmitter capable of\\nsending a binary signal, either a 0 or a 1, together with a receiver. Let p be the probability\\nthat the signal is correctly received. Then q = 1 − p is the probability of an incorrect\\nreception. If a 1 is sent, then the probability that a 1 is received is p and the probability\\nthat a 0 is received is q (Figure 8.6). The probability that no errors occur during the\\ntransmission of a binary codeword of length n is pn. For example, if p = 0.999 and a\\nmessage consisting of 10,000 bits is sent, then the probability of a perfect transmission is\\n\\n(0.999)10,000 ≈ 0.00005.\\n\\nTheorem 8.7. If a binary n-tuple (x1, . . . , xn) is transmitted across a binary symmetric\\nchannel with probability p that no error will occur in each coordinate, then the probability\\nthat there are errors in exactly k coordinates is(\\n\\nn\\n\\nk\\n\\n)\\nqkpn−k.\\n\\nProof. Fix k different coordinates. We first compute the probability that an error has\\noccurred in this fixed set of coordinates. The probability of an error occurring in a particular\\none of these k coordinates is q; the probability that an error will not occur in any of the\\nremaining n− k coordinates is p. The probability of each of these n independent events is\\nqkpn−k. The number of possible error patterns with exactly k errors occurring is equal to(\\n\\nn\\n\\nk\\n\\n)\\n=\\n\\nn!\\n\\nk!(n− k)!\\n,\\n\\nthe number of combinations of n things taken k at a time. Each of these error patterns has\\nprobability qkpn−k of occurring; hence, the probability of all of these error patterns is(\\n\\nn\\n\\nk\\n\\n)\\nqkpn−k.\\n\\nExample 8.8. Suppose that p = 0.995 and a 500-bit message is sent. The probability that\\nthe message was sent error-free is\\n\\npn = (0.995)500 ≈ 0.082.\\n\\n2This section requires a knowledge of probability, but can be skipped without loss of continuity.\\n\\n\\n\\n130 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nThe probability of exactly one error occurring is(\\nn\\n\\n1\\n\\n)\\nqpn−1 = 500(0.005)(0.995)499 ≈ 0.204.\\n\\nThe probability of exactly two errors is(\\nn\\n\\n2\\n\\n)\\nq2pn−2 =\\n\\n500 · 499\\n2\\n\\n(0.005)2(0.995)498 ≈ 0.257.\\n\\nThe probability of more than two errors is approximately\\n\\n1− 0.082− 0.204− 0.257 = 0.457.\\n\\nBlock Codes\\nIf we are to develop efficient error-detecting and error-correcting codes, we will need more\\nsophisticated mathematical tools. Group theory will allow faster methods of encoding and\\ndecoding messages. A code is an (n,m)-block code if the information that is to be coded\\ncan be divided into blocks of m binary digits, each of which can be encoded into n binary\\ndigits. More specifically, an (n,m)-block code consists of an encoding function\\n\\nE : Zm\\n2 → Zn\\n\\n2\\n\\nand a decoding function\\nD : Zn\\n\\n2 → Zm\\n2 .\\n\\nA codeword is any element in the image of E. We also require that E be one-to-one so\\nthat two information blocks will not be encoded into the same codeword. If our code is to\\nbe error-correcting, then D must be onto.\\n\\nExample 8.9. The even-parity coding system developed to detect single errors in ascii\\ncharacters is an (8, 7)-block code. The encoding function is\\n\\nE(x7, x6, . . . , x1) = (x8, x7, . . . , x1),\\n\\nwhere x8 = x7 + x6 + · · ·+ x1 with addition in Z2.\\n\\nLet x = (x1, . . . , xn) and y = (y1, . . . , yn) be binary n-tuples. The Hamming distance\\nor distance, d(x,y), between x and y is the number of bits in which x and y differ. The\\ndistance between two codewords is the minimum number of transmission errors required\\nto change one codeword into the other. The minimum distance for a code, dmin, is the\\nminimum of all distances d(x,y), where x and y are distinct codewords. The weight,\\nw(x), of a binary codeword x is the number of 1s in x. Clearly, w(x) = d(x,0), where\\n0 = (00 · · · 0).\\n\\nExample 8.10. Let x = (10101), y = (11010), and z = (00011) be all of the codewords in\\nsome code C. Then we have the following Hamming distances:\\n\\nd(x,y) = 4, d(x, z) = 3, d(y, z) = 3.\\n\\nThe minimum distance for this code is 3. We also have the following weights:\\n\\nw(x) = 3, w(y) = 3, w(z) = 2.\\n\\nThe following proposition lists some basic properties about the weight of a codeword\\nand the distance between two codewords. The proof is left as an exercise.\\n\\n\\n\\n8.1. ERROR-DETECTING AND CORRECTING CODES 131\\n\\nProposition 8.11. Let x, y, and z be binary n-tuples. Then\\n\\n1. w(x) = d(x,0);\\n\\n2. d(x,y) ≥ 0;\\n\\n3. d(x,y) = 0 exactly when x = y;\\n\\n4. d(x,y) = d(y,x);\\n\\n5. d(x,y) ≤ d(x, z) + d(z,y).\\n\\nThe weights in a particular code are usually much easier to compute than the Hamming\\ndistances between all codewords in the code. If a code is set up carefully, we can use this\\nfact to our advantage.\\n\\nSuppose that x = (1101) and y = (1100) are codewords in some code. If we transmit\\n(1101) and an error occurs in the rightmost bit, then (1100) will be received. Since (1100) is\\na codeword, the decoder will decode (1100) as the transmitted message. This code is clearly\\nnot very appropriate for error detection. The problem is that d(x,y) = 1. If x = (1100) and\\ny = (1010) are codewords, then d(x,y) = 2. If x is transmitted and a single error occurs,\\nthen y can never be received. Table 8.12 gives the distances between all 4-bit codewords\\nin which the first three bits carry information and the fourth is an even parity check bit.\\nWe can see that the minimum distance here is 2; hence, the code is suitable as a single\\nerror-correcting code.\\n\\n0000 0011 0101 0110 1001 1010 1100 1111\\n0000 0 2 2 2 2 2 2 4\\n0011 2 0 2 2 2 2 4 2\\n0101 2 2 0 2 2 4 2 2\\n0110 2 2 2 0 4 2 2 2\\n1001 2 2 2 4 0 2 2 2\\n1010 2 2 4 2 2 0 2 2\\n1100 2 4 2 2 2 2 0 2\\n1111 4 2 2 2 2 2 2 0\\n\\nTable 8.12: Distances between 4-bit codewords\\n\\nTo determine exactly what the error-detecting and error-correcting capabilities for a code\\nare, we need to analyze the minimum distance for the code. Let x and y be codewords. If\\nd(x,y) = 1 and an error occurs where x and y differ, then x is changed to y. The received\\ncodeword is y and no error message is given. Now suppose d(x,y) = 2. Then a single error\\ncannot change x to y. Therefore, if dmin = 2, we have the ability to detect single errors.\\nHowever, suppose that d(x,y) = 2, y is sent, and a noncodeword z is received such that\\n\\nd(x, z) = d(y, z) = 1.\\n\\nThen the decoder cannot decide between x and y. Even though we are aware that an error\\nhas occurred, we do not know what the error is.\\n\\nSuppose dmin ≥ 3. Then the maximum-likelihood decoding scheme corrects all single\\nerrors. Starting with a codeword x, an error in the transmission of a single bit gives y\\nwith d(x,y) = 1, but d(z,y) ≥ 2 for any other codeword z ̸= x. If we do not require the\\ncorrection of errors, then we can detect multiple errors when a code has a minimum distance\\nthat is greater than or equal to 3.\\n\\n\\n\\n132 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nTheorem 8.13. Let C be a code with dmin = 2n + 1. Then C can correct any n or fewer\\nerrors. Furthermore, any 2n or fewer errors can be detected in C.\\n\\nProof. Suppose that a codeword x is sent and the word y is received with at most n\\nerrors. Then d(x,y) ≤ n. If z is any codeword other than x, then\\n\\n2n+ 1 ≤ d(x, z) ≤ d(x,y) + d(y, z) ≤ n+ d(y, z).\\n\\nHence, d(y, z) ≥ n + 1 and y will be correctly decoded as x. Now suppose that x is\\ntransmitted and y is received and that at least one error has occurred, but not more than\\n2n errors. Then 1 ≤ d(x,y) ≤ 2n. Since the minimum distance between codewords is 2n+1,\\ny cannot be a codeword. Consequently, the code can detect between 1 and 2n errors.\\n\\nExample 8.14. In Table 8.15, the codewords c1 = (00000), c2 = (00111), c3 = (11100),\\nand c4 = (11011) determine a single error-correcting code.\\n\\n00000 00111 11100 11011\\n00000 0 3 3 4\\n00111 3 0 4 3\\n11100 3 4 0 3\\n11011 4 3 3 0\\n\\nTable 8.15: Hamming distances for an error-correcting code\\n\\nHistorical Note\\n\\nModern coding theory began in 1948 with C. Shannon’s paper, “A Mathematical Theory\\nof Information” [7]. This paper offered an example of an algebraic code, and Shannon’s\\nTheorem proclaimed exactly how good codes could be expected to be. Richard Hamming\\nbegan working with linear codes at Bell Labs in the late 1940s and early 1950s after becoming\\nfrustrated because the programs that he was running could not recover from simple errors\\ngenerated by noise. Coding theory has grown tremendously in the past several decades.\\nThe Theory of Error-Correcting Codes, by MacWilliams and Sloane [5], published in 1977,\\nalready contained over 1500 references. Linear codes (Reed-Muller (32, 6)-block codes) were\\nused on NASA’s Mariner space probes. More recent space probes such as Voyager have used\\nwhat are called convolution codes. Currently, very active research is being done with Goppa\\ncodes, which are heavily dependent on algebraic geometry.\\n\\n8.2 Linear Codes\\nTo gain more knowledge of a particular code and develop more efficient techniques of en-\\ncoding, decoding, and error detection, we need to add additional structure to our codes.\\nOne way to accomplish this is to require that the code also be a group. A group code is a\\ncode that is also a subgroup of Zn\\n\\n2 .\\nTo check that a code is a group code, we need only verify one thing. If we add any\\n\\ntwo elements in the code, the result must be an n-tuple that is again in the code. It is not\\nnecessary to check that the inverse of the n-tuple is in the code, since every codeword is its\\nown inverse, nor is it necessary to check that 0 is a codeword. For instance,\\n\\n(11000101) + (11000101) = (00000000).\\n\\n\\n\\n8.2. LINEAR CODES 133\\n\\nExample 8.16. Suppose that we have a code that consists of the following 7-tuples:\\n\\n(0000000) (0001111) (0010101) (0011010)\\n\\n(0100110) (0101001) (0110011) (0111100)\\n\\n(1000011) (1001100) (1010110) (1011001)\\n\\n(1100101) (1101010) (1110000) (1111111).\\n\\nIt is a straightforward though tedious task to verify that this code is also a subgroup of Z7\\n2\\n\\nand, therefore, a group code. This code is a single error-detecting and single error-correcting\\ncode, but it is a long and tedious process to compute all of the distances between pairs of\\ncodewords to determine that dmin = 3. It is much easier to see that the minimum weight\\nof all the nonzero codewords is 3. As we will soon see, this is no coincidence. However, the\\nrelationship between weights and distances in a particular code is heavily dependent on the\\nfact that the code is a group.\\n\\nLemma 8.17. Let x and y be binary n-tuples. Then w(x + y) = d(x,y).\\n\\nProof. Suppose that x and y are binary n-tuples. Then the distance between x and y is\\nexactly the number of places in which x and y differ. But x and y differ in a particular\\ncoordinate exactly when the sum in the coordinate is 1, since\\n\\n1 + 1 = 0\\n\\n0 + 0 = 0\\n\\n1 + 0 = 1\\n\\n0 + 1 = 1.\\n\\nConsequently, the weight of the sum must be the distance between the two codewords.\\n\\nTheorem 8.18. Let dmin be the minimum distance for a group code C. Then dmin is the\\nminimum of all the nonzero weights of the nonzero codewords in C. That is,\\n\\ndmin = min{w(x) : x ̸= 0}.\\n\\nProof. Observe that\\n\\ndmin = min{d(x,y) : x ̸= y}\\n= min{d(x,y) : x + y ̸= 0}\\n= min{w(x + y) : x + y ̸= 0}\\n= min{w(z) : z ̸= 0}.\\n\\nLinear Codes\\nFrom Example 8.16, it is now easy to check that the minimum nonzero weight is 3; hence,\\nthe code does indeed detect and correct all single errors. We have now reduced the problem\\nof finding “good” codes to that of generating group codes. One easy way to generate group\\ncodes is to employ a bit of matrix theory.\\n\\nDefine the inner product of two binary n-tuples to be\\n\\nx · y = x1y1 + · · ·+ xnyn,\\n\\n\\n\\n134 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nwhere x = (x1, x2, . . . , xn)\\nt and y = (y1, y2, . . . , yn)\\n\\nt are column vectors.3 For example, if\\nx = (011001)t and y = (110101)t, then x · y = 0. We can also look at an inner product as\\nthe product of a row matrix with a column matrix; that is,\\n\\nx · y = xty\\n\\n=\\n(\\nx1 x2 · · · xn\\n\\n)\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\ny1\\ny2\\n...\\nyn\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n= x1y1 + x2y2 + · · ·+ xnyn.\\n\\nExample 8.19. Suppose that the words to be encoded consist of all binary 3-tuples and\\nthat our encoding scheme is even-parity. To encode an arbitrary 3-tuple, we add a fourth bit\\nto obtain an even number of 1s. Notice that an arbitrary n-tuple x = (x1, x2, . . . , xn)\\n\\nt has an\\neven number of 1s exactly when x1+x2+ · · ·+xn = 0; hence, a 4-tuple x = (x1, x2, x3, x4)\\n\\nt\\n\\nhas an even number of 1s if x1 + x2 + x3 + x4 = 0, or\\n\\nx · 1 = xt1 =\\n(\\nx1 x2 x3 x4\\n\\n)\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 = 0.\\n\\nThis example leads us to hope that there is a connection between matrices and coding\\ntheory.\\n\\nLet Mm×n(Z2) denote the set of all m × n matrices with entries in Z2. We do matrix\\noperations as usual except that all our addition and multiplication operations occur in Z2.\\nDefine the null space of a matrix H ∈ Mm×n(Z2) to be the set of all binary n-tuples x\\nsuch that Hx = 0. We denote the null space of a matrix H by Null(H).\\n\\nExample 8.20. Suppose that\\n\\nH =\\n\\n\uf8eb\uf8ed0 1 0 1 0\\n\\n1 1 1 1 0\\n\\n0 0 1 1 1\\n\\n\uf8f6\uf8f8 .\\n\\nFor a 5-tuple x = (x1, x2, x3, x4, x5)\\nt to be in the null space of H, Hx = 0. Equivalently,\\n\\nthe following system of equations must be satisfied:\\n\\nx2 + x4 = 0\\n\\nx1 + x2 + x3 + x4 = 0\\n\\nx3 + x4 + x5 = 0.\\n\\nThe set of binary 5-tuples satisfying these equations is\\n\\n(00000) (11110) (10101) (01011).\\n\\nThis code is easily determined to be a group code.\\n\\nTheorem 8.21. Let H be in Mm×n(Z2). Then the null space of H is a group code.\\n3Since we will be working with matrices, we will write binary n-tuples as column vectors for the remainder\\n\\nof this chapter.\\n\\n\\n\\n8.3. PARITY-CHECK AND GENERATOR MATRICES 135\\n\\nProof. Since each element of Zn\\n2 is its own inverse, the only thing that really needs to be\\n\\nchecked here is closure. Let x,y ∈ Null(H) for some matrix H in Mm×n(Z2). Then Hx = 0\\nand Hy = 0. So\\n\\nH(x + y) = Hx +Hy = 0 + 0 = 0.\\nHence, x + y is in the null space of H and therefore must be a codeword.\\n\\nA code is a linear code if it is determined by the null space of some matrix H ∈\\nMm×n(Z2).\\nExample 8.22. Let C be the code given by the matrix\\n\\nH =\\n\\n\uf8eb\uf8ed0 0 0 1 1 1\\n\\n0 1 1 0 1 1\\n\\n1 0 1 0 0 1\\n\\n\uf8f6\uf8f8 .\\n\\nSuppose that the 6-tuple x = (010011)t is received. It is a simple matter of matrix multi-\\nplication to determine whether or not x is a codeword. Since\\n\\nHx =\\n\\n\uf8eb\uf8ed0\\n\\n1\\n\\n1\\n\\n\uf8f6\uf8f8 ,\\n\\nthe received word is not a codeword. We must either attempt to correct the word or request\\nthat it be transmitted again.\\n\\n8.3 Parity-Check and Generator Matrices\\nWe need to find a systematic way of generating linear codes as well as fast methods of\\ndecoding. By examining the properties of a matrix H and by carefully choosing H, it is\\npossible to develop very efficient methods of encoding and decoding messages. To this end,\\nwe will introduce standard generator and canonical parity-check matrices.\\n\\nSuppose that H is an m × n matrix with entries in Z2 and n > m. If the last m\\ncolumns of the matrix form the m×m identity matrix, Im, then the matrix is a canonical\\nparity-check matrix. More specifically, H = (A | Im), where A is the m× (n−m) matrix\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n\\na11 a12 · · · a1,n−m\\n\\na21 a22 · · · a2,n−m\\n...\\n\\n... . . . ...\\nam1 am2 · · · am,n−m\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\nand Im is the m×m identity matrix\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 0 · · · 0\\n\\n0 1 · · · 0\\n...\\n\\n... . . . ...\\n0 0 · · · 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\nWith each canonical parity-check matrix we can associate an n× (n−m) standard gen-\\nerator matrix\\n\\nG =\\n\\n(\\nIn−m\\n\\nA\\n\\n)\\n.\\n\\nOur goal will be to show that an x satisfying Gx = y exists if and only if Hy = 0. Given a\\nmessage block x to be encoded, the matrix G will allow us to quickly encode it into a linear\\ncodeword y.\\n\\n\\n\\n136 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nExample 8.23. Suppose that we have the following eight words to be encoded:\\n\\n(000), (001), (010), . . . , (111).\\n\\nFor\\n\\nA =\\n\\n\uf8eb\uf8ed0 1 1\\n\\n1 1 0\\n\\n1 0 1\\n\\n\uf8f6\uf8f8 ,\\n\\nthe associated standard generator and canonical parity-check matrices are\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 0 0\\n\\n0 1 0\\n\\n0 0 1\\n\\n0 1 1\\n\\n1 1 0\\n\\n1 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\nand\\n\\nH =\\n\\n\uf8eb\uf8ed0 1 1 1 0 0\\n\\n1 1 0 0 1 0\\n\\n1 0 1 0 0 1\\n\\n\uf8f6\uf8f8 ,\\n\\nrespectively.\\nObserve that the rows in H represent the parity checks on certain bit positions in a\\n\\n6-tuple. The 1s in the identity matrix serve as parity checks for the 1s in the same row. If\\nx = (x1, x2, x3, x4, x5, x6), then\\n\\n0 = Hx =\\n\\n\uf8eb\uf8edx2 + x3 + x4\\nx1 + x2 + x5\\nx1 + x3 + x6\\n\\n\uf8f6\uf8f8 ,\\n\\nwhich yields a system of equations:\\n\\nx2 + x3 + x4 = 0\\n\\nx1 + x2 + x5 = 0\\n\\nx1 + x3 + x6 = 0.\\n\\nHere x4 serves as a check bit for x2 and x3; x5 is a check bit for x1 and x2; and x6 is a\\ncheck bit for x1 and x3. The identity matrix keeps x4, x5, and x6 from having to check on\\neach other. Hence, x1, x2, and x3 can be arbitrary but x4, x5, and x6 must be chosen to\\nensure parity. The null space of H is easily computed to be\\n\\n(000000) (001101) (010110) (011011)\\n\\n(100011) (101110) (110101) (111000).\\n\\nAn even easier way to compute the null space is with the generator matrix G (Table 8.24).\\n\\n\\n\\n8.3. PARITY-CHECK AND GENERATOR MATRICES 137\\n\\nMessage Word x Codeword Gx\\n000 000000\\n001 001101\\n010 010110\\n011 011011\\n100 100011\\n101 101110\\n110 110101\\n111 111000\\n\\nTable 8.24: A matrix-generated code\\n\\nTheorem 8.25. If H ∈ Mm×n(Z2) is a canonical parity-check matrix, then Null(H) consists\\nof all x ∈ Zn\\n\\n2 whose first n −m bits are arbitrary but whose last m bits are determined by\\nHx = 0. Each of the last m bits serves as an even parity check bit for some of the first\\nn−m bits. Hence, H gives rise to an (n, n−m)-block code.\\n\\nWe leave the proof of this theorem as an exercise. In light of the theorem, the first\\nn−m bits in x are called information bits and the last m bits are called check bits. In\\nExample 8.23, the first three bits are the information bits and the last three are the check\\nbits.\\nTheorem 8.26. Suppose that G is an n × k standard generator matrix. Then C ={\\n\\ny : Gx = y for x ∈ Zk\\n2\\n\\n}\\nis an (n, k)-block code. More specifically, C is a group code.\\n\\nProof. Let Gx1 = y1 and Gx2 = y2 be two codewords. Then y1 + y2 is in C since\\nG(x1 + x2) = Gx1 +Gx2 = y1 + y2.\\n\\nWe must also show that two message blocks cannot be encoded into the same codeword.\\nThat is, we must show that if Gx = Gy, then x = y. Suppose that Gx = Gy. Then\\n\\nGx −Gy = G(x − y) = 0.\\nHowever, the first k coordinates in G(x − y) are exactly x1 − y1, . . . , xk − yk, since they\\nare determined by the identity matrix, Ik, part of G. Hence, G(x − y) = 0 exactly when\\nx = y.\\n\\nBefore we can prove the relationship between canonical parity-check matrices and stan-\\ndard generating matrices, we need to prove a lemma.\\n\\nLemma 8.27. Let H = (A | Im) be an m×n canonical parity-check matrix and G =\\n(\\nIn−m\\n\\nA\\n\\n)\\nbe the corresponding n× (n−m) standard generator matrix. Then HG = 0.\\nProof. Let C = HG. The ijth entry in C is\\n\\ncij =\\n\\nn∑\\nk=1\\n\\nhikgkj\\n\\n=\\nn−m∑\\nk=1\\n\\nhikgkj +\\nn∑\\n\\nk=n−m+1\\n\\nhikgkj\\n\\n=\\nn−m∑\\nk=1\\n\\naikδkj +\\nn∑\\n\\nk=n−m+1\\n\\nδi−(m−n),kakj\\n\\n= aij + aij\\n\\n= 0,\\n\\n\\n\\n138 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nwhere\\n\\nδij =\\n\\n{\\n1, i = j\\n\\n0, i ̸= j\\n\\nis the Kronecker delta.\\n\\nTheorem 8.28. Let H = (A | Im) be an m × n canonical parity-check matrix and let\\nG =\\n\\n(\\nIn−m\\n\\nA\\n\\n)\\nbe the n× (n−m) standard generator matrix associated with H. Let C be the\\n\\ncode generated by G. Then y is in C if and only if Hy = 0. In particular, C is a linear\\ncode with canonical parity-check matrix H.\\n\\nProof. First suppose that y ∈ C. Then Gx = y for some x ∈ Zm\\n2 . By Lemma 8.27,\\n\\nHy = HGx = 0.\\nConversely, suppose that y = (y1, . . . , yn)\\n\\nt is in the null space of H. We need to find\\nan x in Zn−m\\n\\n2 such that Gxt = y. Since Hy = 0, the following set of equations must be\\nsatisfied:\\n\\na11y1 + a12y2 + · · ·+ a1,n−myn−m + yn−m+1 = 0\\n\\na21y1 + a22y2 + · · ·+ a2,n−myn−m + yn−m+1 = 0\\n\\n...\\nam1y1 + am2y2 + · · ·+ am,n−myn−m + yn−m+1 = 0.\\n\\nEquivalently, yn−m+1, . . . , yn are determined by y1, . . . , yn−m:\\n\\nyn−m+1 = a11y1 + a12y2 + · · ·+ a1,n−myn−m\\n\\nyn−m+1 = a21y1 + a22y2 + · · ·+ a2,n−myn−m\\n\\n...\\nyn−m+1 = am1y1 + am2y2 + · · ·+ am,n−myn−m.\\n\\nConsequently, we can let xi = yi for i = 1, . . . , n−m.\\n\\nIt would be helpful if we could compute the minimum distance of a linear code directly\\nfrom its matrix H in order to determine the error-detecting and error-correcting capabilities\\nof the code. Suppose that\\n\\ne1 = (100 · · · 00)t\\n\\ne2 = (010 · · · 00)t\\n\\n...\\nen = (000 · · · 01)t\\n\\nare the n-tuples in Zn\\n2 of weight 1. For an m × n binary matrix H, Hei is exactly the ith\\n\\ncolumn of the matrix H.\\n\\nExample 8.29. Observe that\\n\\n\uf8eb\uf8ed1 1 1 0 0\\n\\n1 0 0 1 0\\n\\n1 1 0 0 1\\n\\n\uf8f6\uf8f8\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 =\\n\\n\uf8eb\uf8ed1\\n\\n0\\n\\n1\\n\\n\uf8f6\uf8f8 .\\n\\n\\n\\n8.3. PARITY-CHECK AND GENERATOR MATRICES 139\\n\\nWe state this result in the following proposition and leave the proof as an exercise.\\nProposition 8.30. Let ei be the binary n-tuple with a 1 in the ith coordinate and 0’s\\nelsewhere and suppose that H ∈ Mm×n(Z2). Then Hei is the ith column of the matrix H.\\nTheorem 8.31. Let H be an m × n binary matrix. Then the null space of H is a single\\nerror-detecting code if and only if no column of H consists entirely of zeros.\\nProof. Suppose that Null(H) is a single error-detecting code. Then the minimum distance\\nof the code must be at least 2. Since the null space is a group code, it is sufficient to require\\nthat the code contain no codewords of less than weight 2 other than the zero codeword.\\nThat is, ei must not be a codeword for i = 1, . . . , n. Since Hei is the ith column of H, the\\nonly way in which ei could be in the null space of H would be if the ith column were all\\nzeros, which is impossible; hence, the code must have the capability to detect at least single\\nerrors.\\n\\nConversely, suppose that no column of H is the zero column. By Proposition 8.30,\\nHei ̸= 0.\\n\\nExample 8.32. If we consider the matrices\\n\\nH1 =\\n\\n\uf8eb\uf8ed1 1 1 0 0\\n\\n1 0 0 1 0\\n\\n1 1 0 0 1\\n\\n\uf8f6\uf8f8\\nand\\n\\nH2 =\\n\\n\uf8eb\uf8ed1 1 1 0 0\\n\\n1 0 0 0 0\\n\\n1 1 0 0 1\\n\\n\uf8f6\uf8f8 ,\\n\\nthen the null space of H1 is a single error-detecting code and the null space of H2 is not.\\nWe can even do better than Theorem 8.31. This theorem gives us conditions on a matrix\\n\\nH that tell us when the minimum weight of the code formed by the null space of H is 2.\\nWe can also determine when the minimum distance of a linear code is 3 by examining the\\ncorresponding matrix.\\nExample 8.33. If we let\\n\\nH =\\n\\n\uf8eb\uf8ed1 1 1 0\\n\\n1 0 0 1\\n\\n1 1 0 0\\n\\n\uf8f6\uf8f8\\nand want to determine whether or not H is the canonical parity-check matrix for an error-\\ncorrecting code, it is necessary to make certain that Null(H) does not contain any 4-tuples\\nof weight 2. That is, (1100), (1010), (1001), (0110), (0101), and (0011) must not be in\\nNull(H). The next theorem states that we can indeed determine that the code generated\\nby H is error-correcting by examining the columns of H. Notice in this example that not\\nonly does H have no zero columns, but also that no two columns are the same.\\nTheorem 8.34. Let H be a binary matrix. The null space of H is a single error-correcting\\ncode if and only if H does not contain any zero columns and no two columns of H are\\nidentical.\\nProof. The n-tuple ei + ej has 1s in the ith and jth entries and 0s elsewhere, and w(ei +\\nej) = 2 for i ̸= j. Since\\n\\n0 = H(ei + ej) = Hei +Hej\\n\\ncan only occur if the ith and jth columns are identical, the null space of H is a single\\nerror-correcting code.\\n\\n\\n\\n140 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nSuppose now that we have a canonical parity-check matrix H with three rows. Then\\nwe might ask how many more columns we can add to the matrix and still have a null space\\nthat is a single error-detecting and single error-correcting code. Since each column has three\\nentries, there are 23 = 8 possible distinct columns. We cannot add the columns\uf8eb\uf8ed0\\n\\n0\\n\\n0\\n\\n\uf8f6\uf8f8 ,\\n\\n\uf8eb\uf8ed1\\n\\n0\\n\\n0\\n\\n\uf8f6\uf8f8 ,\\n\\n\uf8eb\uf8ed0\\n\\n1\\n\\n0\\n\\n\uf8f6\uf8f8 ,\\n\\n\uf8eb\uf8ed0\\n\\n0\\n\\n1\\n\\n\uf8f6\uf8f8 .\\n\\nSo we can add as many as four columns and still maintain a minimum distance of 3.\\nIn general, if H is an m×n canonical parity-check matrix, then there are n−m informa-\\n\\ntion positions in each codeword. Each column has m bits, so there are 2m possible distinct\\ncolumns. It is necessary that the columns 0, e1, . . . , em be excluded, leaving 2m − (1 +m)\\nremaining columns for information if we are still to maintain the ability not only to detect\\nbut also to correct single errors.\\n\\n8.4 Efficient Decoding\\nWe are now at the stage where we are able to generate linear codes that detect and correct\\nerrors fairly easily, but it is still a time-consuming process to decode a received n-tuple and\\ndetermine which is the closest codeword, because the received n-tuple must be compared to\\neach possible codeword to determine the proper decoding. This can be a serious impediment\\nif the code is very large.\\n\\nExample 8.35. Given the binary matrix\\n\\nH =\\n\\n\uf8eb\uf8ed1 1 1 0 0\\n\\n0 1 0 1 0\\n\\n1 0 0 0 1\\n\\n\uf8f6\uf8f8\\nand the 5-tuples x = (11011)t and y = (01011)t, we can compute\\n\\nHx =\\n\\n\uf8eb\uf8ed0\\n\\n0\\n\\n0\\n\\n\uf8f6\uf8f8 and Hy =\\n\\n\uf8eb\uf8ed1\\n\\n0\\n\\n1\\n\\n\uf8f6\uf8f8 .\\n\\nHence, x is a codeword and y is not, since x is in the null space and y is not. Notice that\\nHy is identical to the first column of H. In fact, this is where the error occurred. If we flip\\nthe first bit in y from 0 to 1, then we obtain x.\\n\\nIf H is an m× n matrix and x ∈ Zn\\n2 , then we say that the syndrome of x is Hx. The\\n\\nfollowing proposition allows the quick detection and correction of errors.\\n\\nProposition 8.36. Let the m × n binary matrix H determine a linear code and let x be\\nthe received n-tuple. Write x as x = c+ e, where c is the transmitted codeword and e is the\\ntransmission error. Then the syndrome Hx of the received codeword x is also the syndrome\\nof the error e.\\n\\nProof. The proof follows from the fact that\\n\\nHx = H(c + e) = Hc +He = 0 +He = He.\\n\\n\\n\\n8.4. EFFICIENT DECODING 141\\n\\nThis proposition tells us that the syndrome of a received word depends solely on the\\nerror and not on the transmitted codeword. The proof of the following theorem follows\\nimmediately from Proposition 8.36 and from the fact that He is the ith column of the\\nmatrix H.\\n\\nTheorem 8.37. Let H ∈ Mm×n(Z2) and suppose that the linear code corresponding to H\\nis single error-correcting. Let r be a received n-tuple that was transmitted with at most one\\nerror. If the syndrome of r is 0, then no error has occurred; otherwise, if the syndrome of\\nr is equal to some column of H, say the ith column, then the error has occurred in the ith\\nbit.\\n\\nExample 8.38. Consider the matrix\\n\\nH =\\n\\n\uf8eb\uf8ed1 0 1 1 0 0\\n\\n0 1 1 0 1 0\\n\\n1 1 1 0 0 1\\n\\n\uf8f6\uf8f8\\nand suppose that the 6-tuples x = (111110)t, y = (111111)t, and z = (010111)t have been\\nreceived. Then\\n\\nHx =\\n\\n\uf8eb\uf8ed1\\n\\n1\\n\\n1\\n\\n\uf8f6\uf8f8 ,Hy =\\n\\n\uf8eb\uf8ed1\\n\\n1\\n\\n0\\n\\n\uf8f6\uf8f8 ,Hz =\\n\\n\uf8eb\uf8ed1\\n\\n0\\n\\n0\\n\\n\uf8f6\uf8f8 .\\n\\nHence, x has an error in the third bit and z has an error in the fourth bit. The transmitted\\ncodewords for x and z must have been (110110) and (010011), respectively. The syndrome\\nof y does not occur in any of the columns of the matrix H, so multiple errors must have\\noccurred to produce y.\\n\\nCoset Decoding\\n\\nWe can use group theory to obtain another way of decoding messages. A linear code C is\\na subgroup of Zn\\n\\n2 . Coset or standard decoding uses the cosets of C in Zn\\n2 to implement\\n\\nmaximum-likelihood decoding. Suppose that C is an (n,m)-linear code. A coset of C in\\nZn\\n2 is written in the form x + C, where x ∈ Zn\\n\\n2 . By Lagrange’s Theorem (Theorem 6.10),\\nthere are 2n−m distinct cosets of C in Zn\\n\\n2 .\\n\\nExample 8.39. Let C be the (5, 3)-linear code given by the parity-check matrix\\n\\nH =\\n\\n\uf8eb\uf8ed0 1 1 0 0\\n\\n1 0 0 1 0\\n\\n1 1 0 0 1\\n\\n\uf8f6\uf8f8 .\\n\\nThe code consists of the codewords\\n\\n(00000) (01101) (10011) (11110).\\n\\nThere are 25−2 = 23 cosets of C in Z5\\n2, each with order 22 = 4. These cosets are listed in\\n\\nTable 8.40.\\n\\n\\n\\n142 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nCoset Coset\\nRepresentative\\n\\nC (00000) (01101) (10011) (11110)\\n(10000) + C (10000) (11101) (00011) (01110)\\n(01000) + C (01000) (00101) (11011) (10110)\\n(00100) + C (00100) (01001) (10111) (11010)\\n(00010) + C (00010) (01111) (10001) (11100)\\n(00001) + C (00001) (01100) (10010) (11111)\\n(10100) + C (00111) (01010) (10100) (11001)\\n(00110) + C (00110) (01011) (10101) (11000)\\n\\nTable 8.40: Cosets of C\\n\\nOur task is to find out how knowing the cosets might help us to decode a message.\\nSuppose that x was the original codeword sent and that r is the n-tuple received. If e is\\nthe transmission error, then r = e + x or, equivalently, x = e + r. However, this is exactly\\nthe statement that r is an element in the coset e + C. In maximum-likelihood decoding\\nwe expect the error e to be as small as possible; that is, e will have the least weight. An\\nn-tuple of least weight in a coset is called a coset leader. Once we have determined a coset\\nleader for each coset, the decoding process becomes a task of calculating r + e to obtain x.\\n\\nExample 8.41. In Table 8.40, notice that we have chosen a representative of the least\\npossible weight for each coset. These representatives are coset leaders. Now suppose that\\nr = (01111) is the received word. To decode r, we find that it is in the coset (00010) + C;\\nhence, the originally transmitted codeword must have been (01101) = (01111) + (00010).\\n\\nA potential problem with this method of decoding is that we might have to examine every\\ncoset for the received codeword. The following proposition gives a method of implementing\\ncoset decoding. It states that we can associate a syndrome with each coset; hence, we can\\nmake a table that designates a coset leader corresponding to each syndrome. Such a list is\\ncalled a decoding table.\\n\\nSyndrome Coset Leader\\n(000) (00000)\\n(001) (00001)\\n(010) (00010)\\n(011) (10000)\\n(100) (00100)\\n(101) (01000)\\n(110) (00110)\\n(111) (10100)\\n\\nTable 8.42: Syndromes for each coset\\n\\nProposition 8.43. Let C be an (n, k)-linear code given by the matrix H and suppose that\\nx and y are in Zn\\n\\n2 . Then x and y are in the same coset of C if and only if Hx = Hy. That\\nis, two n-tuples are in the same coset if and only if their syndromes are the same.\\n\\nProof. Two n-tuples x and y are in the same coset of C exactly when x−y ∈ C; however,\\nthis is equivalent to H(x − y) = 0 or Hx = Hy.\\n\\n\\n\\n8.5. EXERCISES 143\\n\\nExample 8.44. Table 8.42 is a decoding table for the code C given in Example 8.39. If\\nx = (01111) is received, then its syndrome can be computed to be\\n\\nHx =\\n\\n\uf8eb\uf8ed0\\n\\n1\\n\\n1\\n\\n\uf8f6\uf8f8 .\\n\\nExamining the decoding table, we determine that the coset leader is (00010). It is now easy\\nto decode the received codeword.\\n\\nGiven an (n, k)-block code, the question arises of whether or not coset decoding is a\\nmanageable scheme. A decoding table requires a list of cosets and syndromes, one for each\\nof the 2n−k cosets of C. Suppose that we have a (32, 24)-block code. We have a huge\\nnumber of codewords, 224, yet there are only 232−24 = 28 = 256 cosets.\\n\\n8.5 Exercises\\n1. Why is the following encoding scheme not acceptable?\\n\\nInformation 0 1 2 3 4 5 6 7 8\\nCodeword 000 001 010 011 101 110 111 000 001\\n\\n2. Without doing any addition, explain why the following set of 4-tuples in Z4\\n2 cannot be\\n\\na group code.\\n(0110) (1001) (1010) (1100)\\n\\n3. Compute the Hamming distances between the following pairs of n-tuples.\\n\\n(a) (011010), (011100)\\n\\n(b) (11110101), (01010100)\\n\\n(c) (00110), (01111)\\n\\n(d) (1001), (0111)\\n\\n4. Compute the weights of the following n-tuples.\\n\\n(a) (011010)\\n\\n(b) (11110101)\\n\\n(c) (01111)\\n\\n(d) (1011)\\n\\n5. Suppose that a linear code C has a minimum weight of 7. What are the error-detection\\nand error-correction capabilities of C?\\n\\n6. In each of the following codes, what is the minimum distance for the code? What is the\\nbest situation we might hope for in connection with error detection and error correction?\\n(a) (011010) (011100) (110111) (110000)\\n\\n(b) (011100) (011011) (111011) (100011) (000000) (010101) (110100) (110011)\\n\\n(c) (000000) (011100) (110101) (110001)\\n\\n(d) (0110110) (0111100) (1110000) (1111111) (1001001) (1000011) (0001111) (0000000)\\n\\n7. Compute the null space of each of the following matrices. What type of (n, k)-block\\ncodes are the null spaces? Can you find a matrix (not necessarily a standard generator\\nmatrix) that generates each code? Are your generator matrices unique?\\n\\n\\n\\n144 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\n(a) \uf8eb\uf8ed0 1 0 0 0\\n\\n1 0 1 0 1\\n\\n1 0 0 1 0\\n\\n\uf8f6\uf8f8\\n(b) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 0 1 0 0 0\\n\\n1 1 0 1 0 0\\n\\n0 1 0 0 1 0\\n\\n1 1 0 0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n\\n(c) (\\n1 0 0 1 1\\n\\n0 1 0 1 1\\n\\n)\\n\\n(d) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n0 0 0 1 1 1 1\\n\\n0 1 1 0 0 1 1\\n\\n1 0 1 0 1 0 1\\n\\n0 1 1 0 0 1 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n8. Construct a (5, 2)-block code. Discuss both the error-detection and error-correction\\ncapabilities of your code.\\n\\n9. Let C be the code obtained from the null space of the matrix\\n\\nH =\\n\\n\uf8eb\uf8ed0 1 0 0 1\\n\\n1 0 1 0 1\\n\\n0 0 1 1 1\\n\\n\uf8f6\uf8f8 .\\n\\nDecode the message\\n01111 10101 01110 00011\\n\\nif possible.\\n\\n10. Suppose that a 1000-bit binary message is transmitted. Assume that the probability\\nof a single error is p and that the errors occurring in different bits are independent of one\\nanother. If p = 0.01, what is the probability of more than one error occurring? What is the\\nprobability of exactly two errors occurring? Repeat this problem for p = 0.0001.\\n\\n11. Which matrices are canonical parity-check matrices? For those matrices that are canon-\\nical parity-check matrices, what are the corresponding standard generator matrices? What\\nare the error-detection and error-correction capabilities of the code generated by each of\\nthese matrices?\\n\\n(a) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n1 1 0 0 0\\n\\n0 0 1 0 0\\n\\n0 0 0 1 0\\n\\n1 0 0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n(b) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n0 1 1 0 0 0\\n\\n1 1 0 1 0 0\\n\\n0 1 0 0 1 0\\n\\n1 1 0 0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n\\n(c) (\\n1 1 1 0\\n\\n1 0 0 1\\n\\n)\\n\\n(d) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n0 0 0 1 0 0 0\\n\\n0 1 1 0 1 0 0\\n\\n1 0 1 0 0 1 0\\n\\n0 1 1 0 0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n12. List all possible syndromes for the codes generated by each of the matrices in Exer-\\ncise 8.5.11.\\n\\n13. Let\\n\\nH =\\n\\n\uf8eb\uf8ed0 1 1 1 1\\n\\n0 0 0 1 1\\n\\n1 0 1 0 1\\n\\n\uf8f6\uf8f8 .\\n\\n\\n\\n8.5. EXERCISES 145\\n\\nCompute the syndrome caused by each of the following transmission errors.\\n(a) An error in the first bit.\\n(b) An error in the third bit.\\n(c) An error in the last bit.\\n(d) Errors in the third and fourth bits.\\n\\n14. Let C be the group code in Z3\\n2 defined by the codewords (000) and (111). Compute the\\n\\ncosets of H in Z3\\n2. Why was there no need to specify right or left cosets? Give the single\\n\\ntransmission error, if any, to which each coset corresponds.\\n\\n15. For each of the following matrices, find the cosets of the corresponding code C. Give a\\ndecoding table for each code if possible.\\n\\n(a) \uf8eb\uf8ed0 1 0 0 0\\n\\n1 0 1 0 1\\n\\n1 0 0 1 0\\n\\n\uf8f6\uf8f8\\n(b) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n0 0 1 0 0\\n\\n1 1 0 1 0\\n\\n0 1 0 1 0\\n\\n1 1 0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n\\n(c) (\\n1 0 0 1 1\\n\\n0 1 0 1 1\\n\\n)\\n\\n(d) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n1 0 0 1 1 1 1\\n\\n1 1 1 0 0 1 1\\n\\n1 0 1 0 1 0 1\\n\\n1 1 1 0 0 1 0\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\n16. Let x, y, and z be binary n-tuples. Prove each of the following statements.\\n(a) w(x) = d(x,0)\\n(b) d(x,y) = d(x + z,y + z)\\n(c) d(x,y) = w(x − y)\\n\\n17. A metric on a set X is a map d : X ×X → R satisfying the following conditions.\\n(a) d(x,y) ≥ 0 for all x,y ∈ X;\\n(b) d(x,y) = 0 exactly when x = y;\\n(c) d(x,y) = d(y,x);\\n(d) d(x,y) ≤ d(x, z) + d(z,y).\\n\\nIn other words, a metric is simply a generalization of the notion of distance. Prove that\\nHamming distance is a metric on Zn\\n\\n2 . Decoding a message actually reduces to deciding\\nwhich is the closest codeword in terms of distance.\\n\\n18. Let C be a linear code. Show that either the ith coordinates in the codewords of C are\\nall zeros or exactly half of them are zeros.\\n\\n19. Let C be a linear code. Show that either every codeword has even weight or exactly\\nhalf of the codewords have even weight.\\n\\n20. Show that the codewords of even weight in a linear code C are also a linear code.\\n\\n21. If we are to use an error-correcting linear code to transmit the 128 ascii characters,\\nwhat size matrix must be used? What size matrix must be used to transmit the extended\\nascii character set of 256 characters? What if we require only error detection in both cases?\\n\\n\\n\\n146 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\n22. Find the canonical parity-check matrix that gives the even parity check bit code with\\nthree information positions. What is the matrix for seven information positions? What are\\nthe corresponding standard generator matrices?\\n\\n23. How many check positions are needed for a single error-correcting code with 20 infor-\\nmation positions? With 32 information positions?\\n\\n24. Let ei be the binary n-tuple with a 1 in the ith coordinate and 0’s elsewhere and\\nsuppose that H ∈ Mm×n(Z2). Show that Hei is the ith column of the matrix H.\\n\\n25. Let C be an (n, k)-linear code. Define the dual or orthogonal code of C to be\\n\\nC⊥ = {x ∈ Zn\\n2 : x · y = 0 for all y ∈ C}.\\n\\n(a) Find the dual code of the linear code C where C is given by the matrix\uf8eb\uf8ed1 1 1 0 0\\n\\n0 0 1 0 1\\n\\n1 0 0 1 0\\n\\n\uf8f6\uf8f8 .\\n\\n(b) Show that C⊥ is an (n, n− k)-linear code.\\n(c) Find the standard generator and parity-check matrices of C and C⊥. What happens\\n\\nin general? Prove your conjecture.\\n\\n26. Let H be an m × n matrix over Z2, where the ith column is the number i written in\\nbinary with m bits. The null space of such a matrix is called a Hamming code.\\n(a) Show that the matrix\\n\\nH =\\n\\n\uf8eb\uf8ed0 0 0 1 1 1\\n\\n0 1 1 0 0 1\\n\\n1 0 1 0 1 0\\n\\n\uf8f6\uf8f8\\ngenerates a Hamming code. What are the error-correcting properties of a Hamming\\ncode?\\n\\n(b) The column corresponding to the syndrome also marks the bit that was in error; that\\nis, the ith column of the matrix is i written as a binary number, and the syndrome\\nimmediately tells us which bit is in error. If the received word is (101011), compute\\nthe syndrome. In which bit did the error occur in this case, and what codeword was\\noriginally transmitted?\\n\\n(c) Give a binary matrix H for the Hamming code with six information positions and four\\ncheck positions. What are the check positions and what are the information positions?\\nEncode the messages (101101) and (001001). Decode the received words (0010000101)\\nand (0000101100). What are the possible syndromes for this code?\\n\\n(d) What is the number of check bits and the number of information bits in an (m,n)-\\nblock Hamming code? Give both an upper and a lower bound on the number of\\ninformation bits in terms of the number of check bits. Hamming codes having the\\nmaximum possible number of information bits with k check bits are called perfect.\\nEvery possible syndrome except 0 occurs as a column. If the number of information\\nbits is less than the maximum, then the code is called shortened. In this case, give\\nan example showing that some syndromes can represent multiple errors.\\n\\n\\n\\n8.6. PROGRAMMING EXERCISES 147\\n\\n8.6 Programming Exercises\\n1. Write a program to implement a (16, 12)-linear code. Your program should be able to\\nencode and decode messages using coset decoding. Once your program is written, write\\na program to simulate a binary symmetric channel with transmission noise. Compare the\\nresults of your simulation with the theoretically predicted error probability.\\n\\n8.7 References and Suggested Readings\\n[1] Blake, I. F. “Codes and Designs,” Mathematics Magazine 52(1979), 81–95.\\n[2] Hill, R. A First Course in Coding Theory. Oxford University Press, Oxford, 1990.\\n[3] Levinson, N. “Coding Theory: A Counterexample to G. H. Hardy’s Conception of\\n\\nApplied Mathematics,” American Mathematical Monthly 77(1970), 249–58.\\n[4] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\\n[5] MacWilliams, F. J. and Sloane, N. J. A. The Theory of Error-Correcting Codes.\\n\\nNorth-Holland Mathematical Library, 16, Elsevier, Amsterdam, 1983.\\n[6] Roman, S. Coding and Information Theory. Springer-Verlag, New York, 1992.\\n[7] Shannon, C. E. “A Mathematical Theory of Communication,” Bell System Technical\\n\\nJournal 27(1948), 379–423, 623–56.\\n[8] Thompson, T. M. From Error-Correcting Codes through Sphere Packing to Simple\\n\\nGroups. Carus Monograph Series, No. 21. Mathematical Association of America,\\nWashington, DC, 1983.\\n\\n[9] van Lint, J. H. Introduction to Coding Theory. Springer, New York, 1999.\\n\\n8.8 Sage\\nSage has a full suite of linear codes and a variety of methods that may be used to investigate\\nthem.\\n\\nConstructing Linear Codes\\nThe codes object can be used to get a concise listing of the available implemented codes.\\nType codes. and press the Tab key and most interfaces to Sage will give you a list. You\\ncan then use a question mark at the end of a method name to learn about the various\\nparameters.\\n\\ncodes.\\n\\nWe will use the classic binary Hamming (7, 4) code as an illustration. “Binary” means\\nwe have vectors with just 0’s and 1’s, the 7 is the length and means the vectors have 7\\ncoordinates, and the 4 is the dimension, meaning this code has 24 = 16 vectors comprising\\nthe code. The documentation assumes we know a few things from later in the course. We\\nuse GF(2) to specify that our code is binary — this will make more sense at the end of the\\ncourse. A second parameter is r and we can see from the formulas in the documenation\\nthat setting r=3 will give length 7.\\n\\nH = codes.HammingCode(GF(2), 3); H\\n\\n[7, 4] Hamming Code over Finite Field of size 2\\n\\n\\n\\n148 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nProperties of Linear Codes\\n\\nWe can examine the Hamming code we just built. First the dimension.\\n\\nH.dimension ()\\n\\n4\\n\\nThe code is small enough that we can list all the codewords.\\n\\nH.list()\\n\\n[(0, 0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 1, 1), (0, 1, 0, 0, 1, 0, 1),\\n(1, 1, 0, 0, 1, 1, 0), (0, 0, 1, 0, 1, 1, 0), (1, 0, 1, 0, 1, 0, 1),\\n(0, 1, 1, 0, 0, 1, 1), (1, 1, 1, 0, 0, 0, 0), (0, 0, 0, 1, 1, 1, 1),\\n(1, 0, 0, 1, 1, 0, 0), (0, 1, 0, 1, 0, 1, 0), (1, 1, 0, 1, 0, 0, 1),\\n(0, 0, 1, 1, 0, 0, 1), (1, 0, 1, 1, 0, 1, 0), (0, 1, 1, 1, 1, 0, 0),\\n(1, 1, 1, 1, 1, 1, 1)]\\n\\nThe minimum distance is perhaps one of the most important properties. Hamming\\ncodes always have minimum distance d = 3, so they are always single error-correcting.\\n\\nH.minimum_distance ()\\n\\n3\\n\\nWe know that the parity-check matrix and the generator matrix are useful for the\\nconstruction, description and analysis of linear codes. The Sage method names are just a\\nbit cryptic. Sage has extensive routines for analyzing matrices with elements from different\\nfields, so we perform much of the subsequent analysis of these matrices within Sage.\\n\\nC = H.parity_check_matrix (); C\\n\\n[1 0 1 0 1 0 1]\\n[0 1 1 0 0 1 1]\\n[0 0 0 1 1 1 1]\\n\\nThe generator matrix here in the text has columns that are codewords, and linear\\ncombinations of the columns (the column space of the matrix) are codewords. In Sage the\\ngenerator matrix has rows that are codewords and the row space of the matrix is the code.\\nSo here is another place where we need to mentally translate between a choice made in the\\ntext and a choice made by the Sage developers.\\n\\nG = H.generator_matrix (); G\\n\\n[1 0 0 0 0 1 1]\\n[0 1 0 0 1 0 1]\\n[0 0 1 0 1 1 0]\\n[0 0 0 1 1 1 1]\\n\\nHere is a partial test that these two matrices are correct, exercising Lemma 8.27. Notice\\nthat we need to use the transpose of the generator matrix, for reasons described above.\\n\\nC*G.transpose () == zero_matrix (3, 4)\\n\\nTrue\\n\\n\\n\\n8.8. SAGE 149\\n\\nNote that the parity-check may not be canonical and the generator matrix may not be\\nstandard. Sage can produce a generator matrix that has a set of columns that forms an\\nidentity matrix, though no guarantee is made that these columns are the first columns.\\n(Columns, not rows.) Such a matrix is said to be systematic, and the Sage method is\\n.generator_matrix_systematic().\\n\\nH.generator_matrix_systematic ()\\n\\n[1 0 0 0 0 1 1]\\n[0 1 0 0 1 0 1]\\n[0 0 1 0 1 1 0]\\n[0 0 0 1 1 1 1]\\n\\nDecoding with a Linear Code\\nWe can decode received messages originating from a linear code. Suppose we receive the\\nlength 7 binary vector r.\\n\\nr = vector(GF(2), [1, 1, 1, 1, 0, 0, 1]); r\\n\\n(1, 1, 1, 1, 0, 0, 1)\\n\\nWe can recognize that one or more errors has occured, since r is not in the code, as the\\nnext computation does not yield the zero vector.\\n\\nC*r\\n\\n(1, 1, 0)\\n\\nA linear code has a .decode method. You may choose from several different algorithms,\\nwhile the Hamming codes have their own custom algorithm. The default algorithm is\\nsyndrome decoding.\\n\\nH.decode_to_code(r)\\n\\n(1, 1, 0, 1, 0, 0, 1)\\n\\nSo if we are willing to assume that only one error occured (which we might, if the\\nprobability of an indivual entry of the vector being in error is very low), then we see that\\nan error occured in the third position.\\n\\nRemember that it could happen that there was more than just one error. For example,\\nsuppose the message was the same as before and errors occurred in the third, fifth and sixth\\nlocations.\\n\\nmessage = vector(GF(2), [1, 1, 0, 1, 0, 0, 1])\\nerrors = vector(GF(2), [0, 0, 1, 0, 1, 1, 0])\\nreceived = message + errors\\nreceived\\n\\n(1, 1, 1, 1, 1, 1, 1)\\n\\nIt then appears that we have received a codeword, so we assume no errors at all, and\\ndecode incorrectly.\\n\\nH.decode_to_code(received) == message\\n\\nFalse\\n\\n\\n\\n150 CHAPTER 8. ALGEBRAIC CODING THEORY\\n\\nH.decode_to_code(received) == received\\n\\nTrue\\n\\n8.9 Sage Exercises\\n1. Create the (binary) Golay code with the codes.BinaryGolayCode() constructor.\\n(a) Use Sage methods to compute the length, dimension and minimum distance of the\\n\\ncode.\\n(b) How many errors can this code detect? How many can it correct?\\n(c) Find a nonzero codeword and introduce three errors by adding a vector with three 1’s\\n\\n(your choice) to create a received message. Show that the message is decoded properly.\\n(d) Recycle your choices from the previous part, but now add one more error. Does the\\n\\nnew received message get decoded properly?\\n\\n2. One technique for improving the characteristics of a code is to add an overall parity-\\ncheck bit, much like the lone parity-check bit of the ascii code described in Example 8.3.\\nSuch codes are referred to as the extended version of the original.\\n(a) Construct the (binary) Golay code and obtain the parity-check matrix. Use Sage com-\\n\\nmands to enlarge this matrix to create a new parity check matrix that has an additional\\noverall parity-check bit. You may find the matrix methods .augment() and .stack()\\n\\nuseful, as well as the constructors zero_vector() and ones_matrix() (remembering that\\nwe specify the binary entries as being from the field GF(2).)\\nCreate the extended code by supplying your enlarged parity-check matrix to the\\ncodes.LinearCodeFromCheckMatrix() constructor and compute the length, dimension and\\nminimum distance of the extended code.\\n\\n(b) How are the properties of this new code better? At what cost?\\n(c) Now create the extended (binary) Golay code with the Sage constructor codes.ExtendedBinaryGolayCode().\\n\\nWith luck, the sorted lists of your codewords and Sage’s codewords will be equal. If\\nnot, the linear code method .is_permutation_equivalent() should return True to indi-\\ncate that your code and Sage’s are just rearrangements of each other.\\n\\n3. Note: This problem is on holiday (as of Sage 6.7), while some buggy Sage code for the\\nminimum distance of a Hamming code gets sorted out. The r = 2 case produces an error\\nmessage and for r > 5 the computation of the minimum distance has become intolerably\\nslow. So it is a bit harder to make a reasonable conjecture from just 3 cases.\\nThe dual of an (n, k) block code is formed as all the set of all binary vectors which are\\northogonal to every vector of the original code. Exercise 8.5.25 describes this construction\\nand asks about some of its properties.\\nYou can construct the dual of a code in Sage with the .dual_code() method. Construct\\nthe binary Hamming codes, and their duals, with the parameter r ranging from 2 to 5,\\ninclusive. Build a table with six columns (perhaps employing the html.table() function)\\nthat lists r, the length of the codes, the dimensions of the original and the dual, and the\\nminimum distances of the orginal and the dual.\\nConjecture formulas for the dimension and minimum distance of the dual of the Hamming\\ncode as expressions in the parameter r.\\n\\n\\n\\n8.9. SAGE EXERCISES 151\\n\\n4. A code with minimum distance d is called perfect if every possible vector is within\\nHamming distance (d − 1)/2 of some codeword. If we expand our notion of geometry to\\naccount for the Hamming distance as the metric, then we can speak of a sphere of radius r\\naround a vector (or codeword. For a code of length n, such a sphere will contain\\n\\n1 +\\n\\n(\\nn\\n\\n1\\n\\n)\\n+\\n\\n(\\nn\\n\\n2\\n\\n)\\n+ · · ·+\\n\\n(\\nn\\n\\nr\\n\\n)\\nvectors within in it. For a perfect code, the spheres of radius d centered at the codewords of\\nthe code will exactly partition the entire set of all possible vectors. (This is the connection\\nthat means that coding theory meshes with sphere packing problems.)\\nA consequence of a code of dimension k being perfect is that\\n\\n2k\\n((\\n\\nn\\n\\n0\\n\\n)\\n+\\n\\n(\\nn\\n\\n1\\n\\n)\\n+\\n\\n(\\nn\\n\\n2\\n\\n)\\n+ · · ·+\\n\\n(\\nn\\n\\nd−1\\n2\\n\\n))\\n= 2n\\n\\nConversely, if a code has minimum distance d and the condition above is true, then the\\ncode is perfect.\\nWrite a Python function, named is_perfect() which accepts a linear code as input and\\nreturns True or False. Demonstrate your function by checking that the (binary) Golay code\\nis perfect, and then use a loop to verify that the (binary) Hamming codes are perfect for\\nall lengths below 32.\\n\\n\\n\\n9\\n\\nIsomorphisms\\n\\nMany groups may appear to be different at first glance, but can be shown to be the same\\nby a simple renaming of the group elements. For example, Z4 and the subgroup of the\\ncircle group T generated by i can be shown to be the same by demonstrating a one-to-one\\ncorrespondence between the elements of the two groups and between the group operations.\\nIn such a case we say that the groups are isomorphic.\\n\\n9.1 Definition and Examples\\nTwo groups (G, ·) and (H, ◦) are isomorphic if there exists a one-to-one and onto map\\nϕ : G→ H such that the group operation is preserved; that is,\\n\\nϕ(a · b) = ϕ(a) ◦ ϕ(b)\\n\\nfor all a and b in G. If G is isomorphic to H, we write G ∼= H. The map ϕ is called an\\nisomorphism.\\n\\nExample 9.1. To show that Z4\\n∼= ⟨i⟩, define a map ϕ : Z4 → ⟨i⟩ by ϕ(n) = in. We must\\n\\nshow that ϕ is bijective and preserves the group operation. The map ϕ is one-to-one and\\nonto because\\n\\nϕ(0) = 1\\n\\nϕ(1) = i\\n\\nϕ(2) = −1\\n\\nϕ(3) = −i.\\n\\nSince\\nϕ(m+ n) = im+n = imin = ϕ(m)ϕ(n),\\n\\nthe group operation is preserved.\\n\\nExample 9.2. We can define an isomorphism ϕ from the additive group of real numbers\\n(R,+) to the multiplicative group of positive real numbers (R+, ·) with the exponential\\nmap; that is,\\n\\nϕ(x+ y) = ex+y = exey = ϕ(x)ϕ(y).\\n\\nOf course, we must still show that ϕ is one-to-one and onto, but this can be determined\\nusing calculus.\\n\\nExample 9.3. The integers are isomorphic to the subgroup of Q∗ consisting of elements of\\nthe form 2n. Define a map ϕ : Z → Q∗ by ϕ(n) = 2n. Then\\n\\nϕ(m+ n) = 2m+n = 2m2n = ϕ(m)ϕ(n).\\n\\n152\\n\\n\\n\\n9.1. DEFINITION AND EXAMPLES 153\\n\\nBy definition the map ϕ is onto the subset {2n : n ∈ Z} of Q∗. To show that the map is\\ninjective, assume that m ̸= n. If we can show that ϕ(m) ̸= ϕ(n), then we are done. Suppose\\nthat m > n and assume that ϕ(m) = ϕ(n). Then 2m = 2n or 2m−n = 1, which is impossible\\nsince m− n > 0.\\n\\nExample 9.4. The groups Z8 and Z12 cannot be isomorphic since they have different\\norders; however, it is true that U(8) ∼= U(12). We know that\\n\\nU(8) = {1, 3, 5, 7}\\nU(12) = {1, 5, 7, 11}.\\n\\nAn isomorphism ϕ : U(8) → U(12) is then given by\\n\\n1 7→ 1\\n\\n3 7→ 5\\n\\n5 7→ 7\\n\\n7 7→ 11.\\n\\nThe map ϕ is not the only possible isomorphism between these two groups. We could define\\nanother isomorphism ψ by ψ(1) = 1, ψ(3) = 11, ψ(5) = 5, ψ(7) = 7. In fact, both of these\\ngroups are isomorphic to Z2 × Z2 (see Example 3.28 in Chapter 3).\\n\\nExample 9.5. Even though S3 and Z6 possess the same number of elements, we would\\nsuspect that they are not isomorphic, because Z6 is abelian and S3 is nonabelian. To\\ndemonstrate that this is indeed the case, suppose that ϕ : Z6 → S3 is an isomorphism. Let\\na, b ∈ S3 be two elements such that ab ̸= ba. Since ϕ is an isomorphism, there exist elements\\nm and n in Z6 such that\\n\\nϕ(m) = a and ϕ(n) = b.\\n\\nHowever,\\nab = ϕ(m)ϕ(n) = ϕ(m+ n) = ϕ(n+m) = ϕ(n)ϕ(m) = ba,\\n\\nwhich contradicts the fact that a and b do not commute.\\n\\nTheorem 9.6. Let ϕ : G → H be an isomorphism of two groups. Then the following\\nstatements are true.\\n\\n1. ϕ−1 : H → G is an isomorphism.\\n\\n2. |G| = |H|.\\n\\n3. If G is abelian, then H is abelian.\\n\\n4. If G is cyclic, then H is cyclic.\\n\\n5. If G has a subgroup of order n, then H has a subgroup of order n.\\n\\nProof. Assertions (1) and (2) follow from the fact that ϕ is a bijection. We will prove (3)\\nhere and leave the remainder of the theorem to be proved in the exercises.\\n\\n(3) Suppose that h1 and h2 are elements of H. Since ϕ is onto, there exist elements\\ng1, g2 ∈ G such that ϕ(g1) = h1 and ϕ(g2) = h2. Therefore,\\n\\nh1h2 = ϕ(g1)ϕ(g2) = ϕ(g1g2) = ϕ(g2g1) = ϕ(g2)ϕ(g1) = h2h1.\\n\\n\\n\\n154 CHAPTER 9. ISOMORPHISMS\\n\\nWe are now in a position to characterize all cyclic groups.\\n\\nTheorem 9.7. All cyclic groups of infinite order are isomorphic to Z.\\n\\nProof. Let G be a cyclic group with infinite order and suppose that a is a generator of G.\\nDefine a map ϕ : Z → G by ϕ : n 7→ an. Then\\n\\nϕ(m+ n) = am+n = aman = ϕ(m)ϕ(n).\\n\\nTo show that ϕ is injective, suppose that m and n are two elements in Z, where m ̸= n.\\nWe can assume that m > n. We must show that am ̸= an. Let us suppose the contrary;\\nthat is, am = an. In this case am−n = e, where m− n > 0, which contradicts the fact that\\na has infinite order. Our map is onto since any element in G can be written as an for some\\ninteger n and ϕ(n) = an.\\n\\nTheorem 9.8. If G is a cyclic group of order n, then G is isomorphic to Zn.\\n\\nProof. Let G be a cyclic group of order n generated by a and define a map ϕ : Zn → G\\nby ϕ : k 7→ ak, where 0 ≤ k < n. The proof that ϕ is an isomorphism is one of the\\nend-of-chapter exercises.\\n\\nCorollary 9.9. If G is a group of order p, where p is a prime number, then G is isomorphic\\nto Zp.\\n\\nProof. The proof is a direct result of Corollary 6.12.\\n\\nThe main goal in group theory is to classify all groups; however, it makes sense to\\nconsider two groups to be the same if they are isomorphic. We state this result in the\\nfollowing theorem, whose proof is left as an exercise.\\n\\nTheorem 9.10. The isomorphism of groups determines an equivalence relation on the class\\nof all groups.\\n\\nHence, we can modify our goal of classifying all groups to classifying all groups up to\\nisomorphism; that is, we will consider two groups to be the same if they are isomorphic.\\n\\nCayley’s Theorem\\n\\nCayley proved that if G is a group, it is isomorphic to a group of permutations on some set;\\nhence, every group is a permutation group. Cayley’s Theorem is what we call a represen-\\ntation theorem. The aim of representation theory is to find an isomorphism of some group\\nG that we wish to study into a group that we know a great deal about, such as a group of\\npermutations or matrices.\\n\\nExample 9.11. Consider the group Z3. The Cayley table for Z3 is as follows.\\n\\n+ 0 1 2\\n\\n0 0 1 2\\n\\n1 1 2 0\\n\\n2 2 0 1\\n\\n\\n\\n9.1. DEFINITION AND EXAMPLES 155\\n\\nThe addition table of Z3 suggests that it is the same as the permutation group G =\\n{(0), (012), (021)}. The isomorphism here is\\n\\n0 7→\\n(\\n0 1 2\\n\\n0 1 2\\n\\n)\\n= (0)\\n\\n1 7→\\n(\\n0 1 2\\n\\n1 2 0\\n\\n)\\n= (012)\\n\\n2 7→\\n(\\n0 1 2\\n\\n2 0 1\\n\\n)\\n= (021).\\n\\nTheorem 9.12 (Cayley). Every group is isomorphic to a group of permutations.\\nProof. Let G be a group. We must find a group of permutations G that is isomorphic to\\nG. For any g ∈ G, define a function λg : G → G by λg(a) = ga. We claim that λg is a\\npermutation of G. To show that λg is one-to-one, suppose that λg(a) = λg(b). Then\\n\\nga = λg(a) = λg(b) = gb.\\n\\nHence, a = b. To show that λg is onto, we must prove that for each a ∈ G, there is a b such\\nthat λg(b) = a. Let b = g−1a.\\n\\nNow we are ready to define our group G. Let\\nG = {λg : g ∈ G}.\\n\\nWe must show that G is a group under composition of functions and find an isomorphism\\nbetween G and G. We have closure under composition of functions since\\n\\n(λg ◦ λh)(a) = λg(ha) = gha = λgh(a).\\n\\nAlso,\\nλe(a) = ea = a\\n\\nand\\n(λg−1 ◦ λg)(a) = λg−1(ga) = g−1ga = a = λe(a).\\n\\nWe can define an isomorphism from G to G by ϕ : g 7→ λg. The group operation is\\npreserved since\\n\\nϕ(gh) = λgh = λgλh = ϕ(g)ϕ(h).\\n\\nIt is also one-to-one, because if ϕ(g)(a) = ϕ(h)(a), then\\nga = λga = λha = ha.\\n\\nHence, g = h. That ϕ is onto follows from the fact that ϕ(g) = λg for any λg ∈ G.\\n\\nThe isomorphism g 7→ λg is known as the left regular representation of G.\\n\\nHistorical Note\\n\\nArthur Cayley was born in England in 1821, though he spent much of the first part of\\nhis life in Russia, where his father was a merchant. Cayley was educated at Cambridge,\\nwhere he took the first Smith’s Prize in mathematics. A lawyer for much of his adult life,\\nhe wrote several papers in his early twenties before entering the legal profession at the age\\nof 25. While practicing law he continued his mathematical research, writing more than 300\\npapers during this period of his life. These included some of his best work. In 1863 he left\\nlaw to become a professor at Cambridge. Cayley wrote more than 900 papers in fields such\\nas group theory, geometry, and linear algebra. His legal knowledge was very valuable to\\nCambridge; he participated in the writing of many of the university’s statutes. Cayley was\\nalso one of the people responsible for the admission of women to Cambridge.\\n\\n\\n\\n156 CHAPTER 9. ISOMORPHISMS\\n\\n9.2 Direct Products\\nGiven two groups G and H, it is possible to construct a new group from the Cartesian\\nproduct of G and H, G ×H. Conversely, given a large group, it is sometimes possible to\\ndecompose the group; that is, a group is sometimes isomorphic to the direct product of\\ntwo smaller groups. Rather than studying a large group G, it is often easier to study the\\ncomponent groups of G.\\n\\nExternal Direct Products\\nIf (G, ·) and (H, ◦) are groups, then we can make the Cartesian product of G and H into a\\nnew group. As a set, our group is just the ordered pairs (g, h) ∈ G ×H where g ∈ G and\\nh ∈ H. We can define a binary operation on G×H by\\n\\n(g1, h1)(g2, h2) = (g1 · g2, h1 ◦ h2);\\n\\nthat is, we just multiply elements in the first coordinate as we do in G and elements in the\\nsecond coordinate as we do in H. We have specified the particular operations · and ◦ in\\neach group here for the sake of clarity; we usually just write (g1, h1)(g2, h2) = (g1g2, h1h2).\\n\\nProposition 9.13. Let G and H be groups. The set G×H is a group under the operation\\n(g1, h1)(g2, h2) = (g1g2, h1h2) where g1, g2 ∈ G and h1, h2 ∈ H.\\n\\nProof. Clearly the binary operation defined above is closed. If eG and eH are the identities\\nof the groups G and H respectively, then (eG, eH) is the identity of G×H. The inverse of\\n(g, h) ∈ G×H is (g−1, h−1). The fact that the operation is associative follows directly from\\nthe associativity of G and H.\\n\\nExample 9.14. Let R be the group of real numbers under addition. The Cartesian product\\nof R with itself, R× R = R2, is also a group, in which the group operation is just addition\\nin each coordinate; that is, (a, b) + (c, d) = (a + c, b + d). The identity is (0, 0) and the\\ninverse of (a, b) is (−a,−b).\\n\\nExample 9.15. Consider\\n\\nZ2 × Z2 = {(0, 0), (0, 1), (1, 0), (1, 1)}.\\n\\nAlthough Z2 × Z2 and Z4 both contain four elements, they are not isomorphic. Every\\nelement (a, b) in Z2 × Z2 has order 2, since (a, b) + (a, b) = (0, 0); however, Z4 is cyclic.\\n\\nThe group G×H is called the external direct product of G and H. Notice that there\\nis nothing special about the fact that we have used only two groups to build a new group.\\nThe direct product\\n\\nn∏\\ni=1\\n\\nGi = G1 ×G2 × · · · ×Gn\\n\\nof the groups G1, G2, . . . , Gn is defined in exactly the same manner. If G = G1 = G2 =\\n· · · = Gn, we often write Gn instead of G1 ×G2 × · · · ×Gn.\\n\\nExample 9.16. The group Zn\\n2 , considered as a set, is just the set of all binary n-tuples.\\n\\nThe group operation is the “exclusive or” of two binary n-tuples. For example,\\n\\n(01011101) + (01001011) = (00010110).\\n\\nThis group is important in coding theory, in cryptography, and in many areas of computer\\nscience.\\n\\n\\n\\n9.2. DIRECT PRODUCTS 157\\n\\nTheorem 9.17. Let (g, h) ∈ G×H. If g and h have finite orders r and s respectively, then\\nthe order of (g, h) in G×H is the least common multiple of r and s.\\nProof. Suppose that m is the least common multiple of r and s and let n = |(g, h)|. Then\\n\\n(g, h)m = (gm, hm) = (eG, eH)\\n\\n(gn, hn) = (g, h)n = (eG, eH).\\n\\nHence, n must divide m, and n ≤ m. However, by the second equation, both r and s must\\ndivide n; therefore, n is a common multiple of r and s. Since m is the least common multiple\\nof r and s, m ≤ n. Consequently, m must be equal to n.\\n\\nCorollary 9.18. Let (g1, . . . , gn) ∈\\n∏\\nGi. If gi has finite order ri in Gi, then the order of\\n\\n(g1, . . . , gn) in\\n∏\\nGi is the least common multiple of r1, . . . , rn.\\n\\nExample 9.19. Let (8, 56) ∈ Z12 ×Z60. Since gcd(8, 12) = 4, the order of 8 is 12/4 = 3 in\\nZ12. Similarly, the order of 56 in Z60 is 15. The least common multiple of 3 and 15 is 15;\\nhence, (8, 56) has order 15 in Z12 × Z60.\\nExample 9.20. The group Z2 × Z3 consists of the pairs\\n\\n(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2).\\n\\nIn this case, unlike that of Z2×Z2 and Z4, it is true that Z2×Z3\\n∼= Z6. We need only show\\n\\nthat Z2 × Z3 is cyclic. It is easy to see that (1, 1) is a generator for Z2 × Z3.\\nThe next theorem tells us exactly when the direct product of two cyclic groups is cyclic.\\n\\nTheorem 9.21. The group Zm × Zn is isomorphic to Zmn if and only if gcd(m,n) = 1.\\nProof. We will first show that if Zm × Zn\\n\\n∼= Zmn, then gcd(m,n) = 1. We will prove the\\ncontrapositive; that is, we will show that if gcd(m,n) = d > 1, then Zm×Zn cannot be cyclic.\\nNotice that mn/d is divisible by both m and n; hence, for any element (a, b) ∈ Zm × Zn,\\n\\n(a, b) + (a, b) + · · ·+ (a, b)︸ ︷︷ ︸\\nmn/d times\\n\\n= (0, 0).\\n\\nTherefore, no (a, b) can generate all of Zm × Zn.\\nThe converse follows directly from Theorem 9.17 since lcm(m,n) = mn if and only if\\n\\ngcd(m,n) = 1.\\n\\nCorollary 9.22. Let n1, . . . , nk be positive integers. Then\\nk∏\\n\\ni=1\\n\\nZni\\n∼= Zn1···nk\\n\\nif and only if gcd(ni, nj) = 1 for i ̸= j.\\nCorollary 9.23. If\\n\\nm = pe11 · · · pekk ,\\nwhere the pis are distinct primes, then\\n\\nZm\\n∼= Zp\\n\\ne1\\n1\\n\\n× · · · × Zp\\nek\\nk\\n.\\n\\nProof. Since the greatest common divisor of peii and p\\nej\\nj is 1 for i ̸= j, the proof follows\\n\\nfrom Corollary 9.22.\\n\\nIn Chapter 13, we will prove that all finite abelian groups are isomorphic to direct\\nproducts of the form\\n\\nZp\\ne1\\n1\\n\\n× · · · × Zp\\nek\\nk\\n\\nwhere p1, . . . , pk are (not necessarily distinct) primes.\\n\\n\\n\\n158 CHAPTER 9. ISOMORPHISMS\\n\\nInternal Direct Products\\nThe external direct product of two groups builds a large group out of two smaller groups.\\nWe would like to be able to reverse this process and conveniently break down a group into\\nits direct product components; that is, we would like to be able to say when a group is\\nisomorphic to the direct product of two of its subgroups.\\n\\nLet G be a group with subgroups H and K satisfying the following conditions.\\n\\n• G = HK = {hk : h ∈ H, k ∈ K};\\n\\n• H ∩K = {e};\\n\\n• hk = kh for all k ∈ K and h ∈ H.\\n\\nThen G is the internal direct product of H and K.\\n\\nExample 9.24. The group U(8) is the internal direct product of\\n\\nH = {1, 3} and K = {1, 5}.\\n\\nExample 9.25. The dihedral group D6 is an internal direct product of its two subgroups\\n\\nH = {id, r3} and K = {id, r2, r4, s, r2s, r4s}.\\n\\nIt can easily be shown that K ∼= S3; consequently, D6\\n∼= Z2 × S3.\\n\\nExample 9.26. Not every group can be written as the internal direct product of two of its\\nproper subgroups. If the group S3 were an internal direct product of its proper subgroups\\nH and K, then one of the subgroups, say H, would have to have order 3. In this case H is\\nthe subgroup {(1), (123), (132)}. The subgroup K must have order 2, but no matter which\\nsubgroup we choose for K, the condition that hk = kh will never be satisfied for h ∈ H and\\nk ∈ K.\\n\\nTheorem 9.27. Let G be the internal direct product of subgroups H and K. Then G is\\nisomorphic to H ×K.\\n\\nProof. Since G is an internal direct product, we can write any element g ∈ G as g = hk\\nfor some h ∈ H and some k ∈ K. Define a map ϕ : G→ H ×K by ϕ(g) = (h, k).\\n\\nThe first problem that we must face is to show that ϕ is a well-defined map; that is, we\\nmust show that h and k are uniquely determined by g. Suppose that g = hk = h′k′. Then\\nh−1h′ = k(k′)−1 is in both H and K, so it must be the identity. Therefore, h = h′ and\\nk = k′, which proves that ϕ is, indeed, well-defined.\\n\\nTo show that ϕ preserves the group operation, let g1 = h1k1 and g2 = h2k2 and observe\\nthat\\n\\nϕ(g1g2) = ϕ(h1k1h2k2)\\n\\n= ϕ(h1h2k1k2)\\n\\n= (h1h2, k1k2)\\n\\n= (h1, k1)(h2, k2)\\n\\n= ϕ(g1)ϕ(g2).\\n\\nWe will leave the proof that ϕ is one-to-one and onto as an exercise.\\n\\nExample 9.28. The group Z6 is an internal direct product isomorphic to {0, 2, 4}×{0, 3}.\\n\\n\\n\\n9.3. EXERCISES 159\\n\\nWe can extend the definition of an internal direct product of G to a collection of sub-\\ngroups H1,H2, . . . , Hn of G, by requiring that\\n\\n• G = H1H2 · · ·Hn = {h1h2 · · ·hn : hi ∈ Hi};\\n\\n• Hi ∩ ⟨∪j ̸=iHj⟩ = {e};\\n\\n• hihj = hjhi for all hi ∈ Hi and hj ∈ Hj .\\n\\nWe will leave the proof of the following theorem as an exercise.\\n\\nTheorem 9.29. Let G be the internal direct product of subgroups Hi, where i = 1, 2, . . . , n.\\nThen G is isomorphic to\\n\\n∏\\niHi.\\n\\n9.3 Exercises\\n1. Prove that Z ∼= nZ for n ̸= 0.\\n\\n2. Prove that C∗ is isomorphic to the subgroup of GL2(R) consisting of matrices of the\\nform (\\n\\na b\\n\\n−b a\\n\\n)\\n.\\n\\n3. Prove or disprove: U(8) ∼= Z4.\\n\\n4. Prove that U(8) is isomorphic to the group of matrices(\\n1 0\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n1 0\\n\\n0 −1\\n\\n)\\n,\\n\\n(\\n−1 0\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n−1 0\\n\\n0 −1\\n\\n)\\n.\\n\\n5. Show that U(5) is isomorphic to U(10), but U(12) is not.\\n\\n6. Show that the nth roots of unity are isomorphic to Zn.\\n\\n7. Show that any cyclic group of order n is isomorphic to Zn.\\n\\n8. Prove that Q is not isomorphic to Z.\\n\\n9. Let G = R \\\\ {−1} and define a binary operation on G by\\n\\na ∗ b = a+ b+ ab.\\n\\nProve that G is a group under this operation. Show that (G, ∗) is isomorphic to the\\nmultiplicative group of nonzero real numbers.\\n\\n10. Show that the matrices\uf8eb\uf8ed1 0 0\\n\\n0 1 0\\n\\n0 0 1\\n\\n\uf8f6\uf8f8 \uf8eb\uf8ed1 0 0\\n\\n0 0 1\\n\\n0 1 0\\n\\n\uf8f6\uf8f8 \uf8eb\uf8ed0 1 0\\n\\n1 0 0\\n\\n0 0 1\\n\\n\uf8f6\uf8f8\\n\uf8eb\uf8ed0 0 1\\n\\n1 0 0\\n\\n0 1 0\\n\\n\uf8f6\uf8f8 \uf8eb\uf8ed0 0 1\\n\\n0 1 0\\n\\n1 0 0\\n\\n\uf8f6\uf8f8 \uf8eb\uf8ed0 1 0\\n\\n0 0 1\\n\\n1 0 0\\n\\n\uf8f6\uf8f8\\nform a group. Find an isomorphism of G with a more familiar group of order 6.\\n\\n\\n\\n160 CHAPTER 9. ISOMORPHISMS\\n\\n11. Find five non-isomorphic groups of order 8.\\n\\n12. Prove S4 is not isomorphic to D12.\\n\\n13. Let ω = cis(2π/n) be a primitive nth root of unity. Prove that the matrices\\n\\nA =\\n\\n(\\nω 0\\n\\n0 ω−1\\n\\n)\\nand B =\\n\\n(\\n0 1\\n\\n1 0\\n\\n)\\ngenerate a multiplicative group isomorphic to Dn.\\n\\n14. Show that the set of all matrices of the form(\\n±1 k\\n\\n0 1\\n\\n)\\n,\\n\\nis a group isomorphic to Dn, where all entries in the matrix are in Zn.\\n\\n15. List all of the elements of Z4 × Z2.\\n\\n16. Find the order of each of the following elements.\\n(a) (3, 4) in Z4 × Z6\\n\\n(b) (6, 15, 4) in Z30 × Z45 × Z24\\n\\n(c) (5, 10, 15) in Z25 × Z25 × Z25\\n\\n(d) (8, 8, 8) in Z10 × Z24 × Z80\\n\\n17. Prove that D4 cannot be the internal direct product of two of its proper subgroups.\\n\\n18. Prove that the subgroup of Q∗ consisting of elements of the form 2m3n for m,n ∈ Z is\\nan internal direct product isomorphic to Z× Z.\\n\\n19. Prove that S3×Z2 is isomorphic to D6. Can you make a conjecture about D2n? Prove\\nyour conjecture.\\n\\n20. Prove or disprove: Every abelian group of order divisible by 3 contains a subgroup of\\norder 3.\\n\\n21. Prove or disprove: Every nonabelian group of order divisible by 6 contains a subgroup\\nof order 6.\\n\\n22. Let G be a group of order 20. If G has subgroups H and K of orders 4 and 5 respectively\\nsuch that hk = kh for all h ∈ H and k ∈ K, prove that G is the internal direct product of\\nH and K.\\n\\n23. Prove or disprove the following assertion. Let G, H, and K be groups. If G × K ∼=\\nH ×K, then G ∼= H.\\n\\n24. Prove or disprove: There is a noncyclic abelian group of order 51.\\n\\n25. Prove or disprove: There is a noncyclic abelian group of order 52.\\n\\n26. Let ϕ : G → H be a group isomorphism. Show that ϕ(x) = eH if and only if x = eG,\\nwhere eG and eH are the identities of G and H, respectively.\\n\\n27. Let G ∼= H. Show that if G is cyclic, then so is H.\\n\\n28. Prove that any group G of order p, p prime, must be isomorphic to Zp.\\n\\n\\n\\n9.3. EXERCISES 161\\n\\n29. Show that Sn is isomorphic to a subgroup of An+2.\\n\\n30. Prove that Dn is isomorphic to a subgroup of Sn.\\n\\n31. Let ϕ : G1 → G2 and ψ : G2 → G3 be isomorphisms. Show that ϕ−1 and ψ ◦ ϕ are\\nboth isomorphisms. Using these results, show that the isomorphism of groups determines\\nan equivalence relation on the class of all groups.\\n\\n32. Prove U(5) ∼= Z4. Can you generalize this result for U(p), where p is prime?\\n\\n33. Write out the permutations associated with each element of S3 in the proof of Cayley’s\\nTheorem.\\n\\n34. An automorphism of a group G is an isomorphism with itself. Prove that complex\\nconjugation is an automorphism of the additive group of complex numbers; that is, show\\nthat the map ϕ(a+ bi) = a− bi is an isomorphism from C to C.\\n\\n35. Prove that a+ ib 7→ a− ib is an automorphism of C∗.\\n\\n36. Prove that A 7→ B−1AB is an automorphism of SL2(R) for all B in GL2(R).\\n\\n37. We will denote the set of all automorphisms of G by Aut(G). Prove that Aut(G) is a\\nsubgroup of SG, the group of permutations of G.\\n\\n38. Find Aut(Z6).\\n\\n39. Find Aut(Z).\\n\\n40. Find two nonisomorphic groups G and H such that Aut(G) ∼= Aut(H).\\n\\n41. Let G be a group and g ∈ G. Define a map ig : G→ G by ig(x) = gxg−1. Prove that ig\\ndefines an automorphism of G. Such an automorphism is called an inner automorphism.\\nThe set of all inner automorphisms is denoted by Inn(G).\\n\\n42. Prove that Inn(G) is a subgroup of Aut(G).\\n\\n43. What are the inner automorphisms of the quaternion group Q8? Is Inn(G) = Aut(G)\\nin this case?\\n\\n44. Let G be a group and g ∈ G. Define maps λg : G→ G and ρg : G→ G by λg(x) = gx\\nand ρg(x) = xg−1. Show that ig = ρg ◦ λg is an automorphism of G. The isomorphism\\ng 7→ ρg is called the right regular representation of G.\\n\\n45. Let G be the internal direct product of subgroups H and K. Show that the map\\nϕ : G→ H ×K defined by ϕ(g) = (h, k) for g = hk, where h ∈ H and k ∈ K, is one-to-one\\nand onto.\\n\\n46. Let G and H be isomorphic groups. If G has a subgroup of order n, prove that H must\\nalso have a subgroup of order n.\\n\\n47. If G ∼= G and H ∼= H, show that G×H ∼= G×H.\\n\\n48. Prove that G×H is isomorphic to H ×G.\\n\\n49. Let n1, . . . , nk be positive integers. Show that\\nk∏\\n\\ni=1\\n\\nZni\\n∼= Zn1···nk\\n\\nif and only if gcd(ni, nj) = 1 for i ̸= j.\\n\\n\\n\\n162 CHAPTER 9. ISOMORPHISMS\\n\\n50. Prove that A×B is abelian if and only if A and B are abelian.\\n\\n51. If G is the internal direct product of H1,H2, . . . , Hn, prove that G is isomorphic to∏\\niHi.\\n\\n52. Let H1 and H2 be subgroups of G1 and G2, respectively. Prove that H1 × H2 is a\\nsubgroup of G1 ×G2.\\n\\n53. Let m,n ∈ Z. Prove that ⟨m,n⟩ = ⟨d⟩ if and only if d = gcd(m,n).\\n\\n54. Let m,n ∈ Z. Prove that ⟨m⟩ ∩ ⟨n⟩ = ⟨l⟩ if and only if l = lcm(m,n).\\n\\n55. (Groups of order 2p) In this series of exercises we will classify all groups of order 2p,\\nwhere p is an odd prime.\\n(a) Assume G is a group of order 2p, where p is an odd prime. If a ∈ G, show that a must\\n\\nhave order 1, 2, p, or 2p.\\n(b) Suppose that G has an element of order 2p. Prove that G is isomorphic to Z2p. Hence,\\n\\nG is cyclic.\\n(c) Suppose that G does not contain an element of order 2p. Show that G must contain\\n\\nan element of order p. Hint: Assume that G does not contain an element of order p.\\n(d) Suppose that G does not contain an element of order 2p. Show that G must contain\\n\\nan element of order 2.\\n(e) Let P be a subgroup of G with order p and y ∈ G have order 2. Show that yP = Py.\\n(f) Suppose that G does not contain an element of order 2p and P = ⟨z⟩ is a subgroup of\\n\\norder p generated by z. If y is an element of order 2, then yz = zky for some 2 ≤ k < p.\\n(g) Suppose that G does not contain an element of order 2p. Prove that G is not abelian.\\n(h) Suppose that G does not contain an element of order 2p and P = ⟨z⟩ is a subgroup\\n\\nof order p generated by z and y is an element of order 2. Show that we can list the\\nelements of G as {ziyj | 0 ≤ i < p, 0 ≤ j < 2}.\\n\\n(i) Suppose that G does not contain an element of order 2p and P = ⟨z⟩ is a subgroup\\nof order p generated by z and y is an element of order 2. Prove that the product\\n(ziyj)(zrys) can be expressed as a uniquely as zmyn for some non negative integers\\nm,n. Thus, conclude that there is only one possibility for a non-abelian group of order\\n2p, it must therefore be the one we have seen already, the dihedral group.\\n\\n9.4 Sage\\nSage has limited support for actually creating isomorphisms, though it is possible. However,\\nthere is excellent support for determining if two permutation groups are isomorphic. This\\nwill allow us to begin a little project to locate all of the groups of order less than 16 in\\nSage’s permutation groups.\\n\\nIsomorphism Testing\\nIf G and H are two permutation groups, then the command G.is_isomorphic(H) will return\\nTrue or False as the two groups are, or are not, isomorphic. Since “isomorpic to” is an\\nequivalence relation by Theorem 9.10, it does not matter which group plays the role of G\\n\\nand which plays the role of H.\\nSo we have a few more examples to work with, let us introduce the Sage command\\n\\nthat creates an external direct product. If G and H are two permutation groups, then the\\n\\n\\n\\n9.4. SAGE 163\\n\\ncommand direct_product_permgroups([G,H]) will return the external direct product as a new\\npermutation group. Notice that this is a function (not a method) and the input is a list.\\nRather than just combining two groups in the list, any number of groups can be supplied.\\nWe illustrate isomorphism testing and direct products in the context of Theorem 9.21,\\nwhich is an equivalence, so tells us exactly when we have isomorphic groups. We use cyclic\\npermutation groups as stand-ins for Zn by Theorem 9.8.\\n\\nFirst, two isomorphic groups.\\nm = 12\\nn = 7\\ngcd(m, n)\\n\\n1\\n\\nG = CyclicPermutationGroup(m)\\nH = CyclicPermutationGroup(n)\\ndp = direct_product_permgroups ([G, H])\\nK = CyclicPermutationGroup(m*n)\\nK.is_isomorphic(dp)\\n\\nTrue\\n\\nNow, two non-isomorphic groups.\\nm = 15\\nn = 21\\ngcd(m, n)\\n\\n3\\n\\nG = CyclicPermutationGroup(m)\\nH = CyclicPermutationGroup(n)\\ndp = direct_product_permgroups ([G, H])\\nK = CyclicPermutationGroup(m*n)\\nK.is_isomorphic(dp)\\n\\nFalse\\n\\nNotice how the simple computation of a greatest common divisor predicts the incredibly\\ncomplicated computation of determining if two groups are isomorphic. This is a nice illus-\\ntration of the power of mathematics, replacing a difficult problem (group isomorphism) by\\na simple one (factoring and divisibility of integers). Let us build one more direct product\\nof cyclic groups, but with three groups, each with orders that are pairwise relatively prime.\\n\\nIf you try the following with larger parameters you may get an error (database_gap).\\nm = 6\\nn = 5\\nr = 7\\nG = CyclicPermutationGroup(m)\\nH = CyclicPermutationGroup(n)\\nL = CyclicPermutationGroup(r)\\ndp = direct_product_permgroups ([G, H, L])\\nK = CyclicPermutationGroup(m*n*r)\\nK.is_isomorphic(dp)\\n\\nTrue\\n\\n\\n\\n164 CHAPTER 9. ISOMORPHISMS\\n\\nClassifying Finite Groups\\nOnce we understand isomorphic groups as being the “same”, or “fundamentally no differ-\\nent,” or “structurally identical,” then it is natural to ask how many “really different” finite\\ngroups there are. Corollary 9.9 gives a partial answer: for each prime there is just one finite\\ngroup, with Zp as a concrete manifestation.\\n\\nLet us embark on a quest to find all the groups of order less than 16 in Sage as permu-\\ntation groups. For prime orders 1, 2, 3, 5, 7, 11 and 13 we know there is really just one group\\neach, and we can realize them all:\\n\\n[CyclicPermutationGroup(p) for p in [1, 2, 3, 5, 7, 11, 13]]\\n\\n[Cyclic group of order 1 as a permutation group ,\\nCyclic group of order 2 as a permutation group ,\\nCyclic group of order 3 as a permutation group ,\\nCyclic group of order 5 as a permutation group ,\\nCyclic group of order 7 as a permutation group ,\\nCyclic group of order 11 as a permutation group ,\\nCyclic group of order 13 as a permutation group]\\n\\nSo now our smallest unknown case is order 4. Sage knows at least three such groups,\\nand we can use Sage to check if any pair is isomorphic. Notice that since “isomorphic to” is\\nan equivalence relation, and hence a transitive relation, the two tests below are sufficient.\\n\\nG = CyclicPermutationGroup (4)\\nH = KleinFourGroup ()\\nT1 = CyclicPermutationGroup (2)\\nT2 = CyclicPermutationGroup (2)\\nK = direct_product_permgroups ([T1, T2])\\nG.is_isomorphic(H)\\n\\nFalse\\n\\nH.is_isomorphic(K)\\n\\nTrue\\n\\nSo we have at least two different groups: Z4 and Z2 × Z2, with the latter also known\\nas the Klein 4-group. Sage will not be able to tell us if we have a complete list — this will\\nalways require theoretical results like Theorem 9.10. We will shortly have a more general\\nresult that handles the case of order 4, but right now, a careful analysis (by hand) of the\\npossibilities for the Cayley table of a group of order 4 should lead you to the two possibilities\\nabove as the only possibilities. Try to deduce what the Cayley table of an order 4 group\\nshould look like, since you know about identity elements, inverses and cancellation.\\n\\nWe have seen at least two groups of order 6 (next on our list of non-prime orders). One\\nis abelian and one is not, so we do not need Sage to tell us they are structurally different.\\nBut let us do it anyway.\\n\\nG = CyclicPermutationGroup (6)\\nH = SymmetricGroup (3)\\nG.is_isomorphic(H)\\n\\nFalse\\n\\nIs that all? There is Z3 × Z2, but that is just Z6 since 2 and 3 are relatively prime.\\nThe dihedral group, D3, all symmetries of a triangle, is just S3, the symmetric group on 3\\nsymbols.\\n\\n\\n\\n9.4. SAGE 165\\n\\nG = DihedralGroup (3)\\nH = SymmetricGroup (3)\\nG.is_isomorphic(H)\\n\\nTrue\\n\\nExercise 9.3.55 from this section classifies all groups of order 2p, where p is a prime.\\nSuch a group is either cyclic or a dihedral group. So the two groups above, Z6 and D3, are\\nthe complete list of groups of order 6.\\n\\nBy this general result, in addition to order 6, we also know the complete lists of groups\\nof orders 10 and 14. To Be Continued.\\n\\nInternal Direct Products\\nAn internal direct product is a statement about subgroups of a single group, together with\\na theorem that links them to an external direct product. We will work an example here\\nthat will illustrate the nature of an internal direct product.\\n\\nGiven an integer n, the set of positive integers less than n, and relatively prime to n\\nforms a group under multiplication mod n. We will work in the set Integers(n) where we\\ncan add and multiply, but we want to stay strictly with multiplication only.\\n\\nFirst we build the subgroup itself. Notice how we must convert x into an integer (an\\nelement of ZZ) so that the greatest common divisor computation performs correctly.\\n\\nZ36 = Integers (36)\\nU = [x for x in Z36 if gcd(ZZ(x), 36) == 1]\\nU\\n\\n[1, 5, 7, 11, 13, 17, 19, 23, 25, 29, 31, 35]\\n\\nSo we have a group of order 12. We are going to try to find a subgroup of order 6 and\\na subgroup of order 2 to form the internal direct product, and we will restrict our search\\ninitially to cyclic subgroups of order 6. Sage has a method that will give the order of each\\nof these elements, relative to multiplication, so let us examine those next.\\n\\n[x.multiplicative_order () for x in U]\\n\\n[1, 6, 6, 6, 3, 2, 2, 6, 3, 6, 6, 2]\\n\\nWe have many choices for generators of a cyclic subgroup of order 6 and for a cyclic\\nsubgroup of order 2. Of course, some of the choices for a generator of the subgroup of order\\n6 will generate the same subgroup. Can you tell, just by counting, how many subgroups of\\norder 6 there are? We are going to pick the first element of order 6, and the last element\\nof order 2, for no particular reason. After your work through this once, we encourage you\\nto try other choices to understand why some choices lead to an internal direct product and\\nsome do not. Notice that we choose the elements from the list U so that they are sure to be\\nelements of Z36 and behave properly when multiplied.\\n\\na = U[1]\\nA = [a^i for i in srange (6)]\\nA\\n\\n[1, 5, 25, 17, 13, 29]\\n\\nb = U[11]\\nB = [b^i for i in srange (2)]\\nB\\n\\n\\n\\n166 CHAPTER 9. ISOMORPHISMS\\n\\n[1, 35]\\n\\nSo A and B are two cyclic subgroups. Notice that their intersection is the identity element,\\none of our requirements for an internal direct product. So this is a good start.\\n\\n[x for x in A if x in B]\\n\\n[1]\\n\\nZ36 is an abelian group, thus the condition on all products commuting will hold, but we\\nillustrate the Sage commands that will check this in a non-abelian situation.\\n\\nall([x*y == y*x for x in A for y in B])\\n\\nTrue\\n\\nFinally, we need to check that by forming products with elements from A and B we create\\nthe entire group. Sorting the resulting list will make a check easier for us visually, and is\\nrequired if we want Sage to do the check.\\n\\nT = sorted ([x*y for x in A for y in B])\\nT\\n\\n[1, 5, 7, 11, 13, 17, 19, 23, 25, 29, 31, 35]\\n\\nT == U\\n\\nTrue\\n\\nThat’s it. We now condense all this information into the statement that “U is the internal\\ndirect product of A and B.” By Theorem 9.27, we see that U is isomorphic to a product of a\\ncyclic group of order 6 and a cyclic group of order 2. So in a very real sense, U is no more or\\nless complicated than Z6 × Z2, which is in turn isomorphic to Z3 × Z2 × Z2. So we totally\\nunderstand the “structure” of U. For example, we can see that U is not cyclic, since when\\nwritten as a product of cyclic groups, the two orders are not relatively prime. The final\\nexpression of U suggests you could find three cyclic subgroups of U, with orders 3, 2 and 2,\\nso that U is an internal direct product of the three subgroups.\\n\\n9.5 Sage Exercises\\n1. This exercise is about putting Cayley’s Theorem into practice. First, read and study\\nthe theorem. Realize that this result by itself is primarily of theoretical interest, but with\\nsome more theory we could get into some subtler aspects of this (a subject known as\\n“representation theory”).\\nYou should create these representations mostly with pencil-and-paper work, using Sage as\\na fancy calculator and assistant. You do not need to include all these computations in your\\nworksheet. Build the requested group representations and then include enough verifications\\nin Sage to prove that that your representation correctly represents the group.\\nBegin by building a permutation representation of the quaternions, Q. There are eight\\nelements in Q (±1,±I,±J,±K), so you will be constructing a subgroup of S8. For each\\ng ∈ Q form the function Tg, defined as Tg(x) = xg. Notice that this definition is the\\n“reverse” of that given in the text. This is because Sage composes permutations left-to-\\nright, while your text composes right-to-left. To create the permutations Tg, the two-line\\nversion of writing permutations could be very useful as an intermediate step. You will\\nprobably want to “code” each element of Q with an integer in {1, 2, . . . , 8}.\\n\\n\\n\\n9.5. SAGE EXERCISES 167\\n\\nOne such representation is included in Sage as QuaternionGroup() — your answer should\\nlook very similar, but perhaps not identical. Do not submit your answer for a r4presenation\\nof the quaternions, but I strongly suggest working this particular group representation until\\nyou are sure you have it right — the problems below might be very difficult otherwise.\\nYou can use Sage’s .is_isomorphic() method to check if your representations are correct.\\nHowever, do not use this as a substitute for the part of each question that asks you to\\ninvestigate properties of your representation towards this end.\\n(a) Build the permutation representation of Z2 ×Z4 described in Cayley’s Theorem. (Re-\\n\\nmember that this group is additive, while the theorem uses multiplicative notation.)\\nInclude the representation of each of the 8 elements in your submitted work. Then\\nconstruct the permutation group as a subgroup of a full symmetric group that is gen-\\nerated by exactly two of the eight elements you have already constructed. Hint: which\\ntwo elements of Z2 × Z4 might you use to generate all of Z2 × Z4? Use commands\\nin Sage to investigate various properties of your permutation group, other than just\\n.list(), to provide evidence that your subgroup is correct — include these in your\\nsubmitted worksheet.\\n\\n(b) Build a permutation representation of U(24), the group of units mod 24. Again, list\\na representation of each element in your submitted work. Then construct the group\\nas a subgroup of a full symmetric group created with three generators. To determine\\nthese three generators, you will likely need to understand U(24) as an internal direct\\nproduct. Use commands in Sage to investigate various properties of your group, other\\nthan just .list(), to provide evidence that your subgroup is correct — include these\\nin your submitted worksheet.\\n\\n2. Consider the symmetries of a 10-gon, D10 in your text, DihedralGroup(10) in Sage. Pre-\\nsume that the vertices pf the 10-gon have been labeled 1 through 10 in order. Identify the\\npermutation that is a 180 degree rotation and use it to generate a subgroup R of order 2.\\nThen identify the permutation that is a 72 degree rotation, and any one of the ten permu-\\ntations that are a reflection of the 10-gon about a line. Use these latter two permutations\\nto generate a subgroup S of order 10. Use Sage to verify that the full dihedral group is\\nthe internal direct product of the subgroups R and S by checking the conditions in the\\ndefinition of an internal direct product.\\nWe have a theorem which says that if a group is an internal direct product, then it is\\nisomorphic to some external direct product. Understand that this does not mean that you\\ncan use the converse in this problem. In other words, establishing an isomorphism of G\\nwith an external direct product does not prove that G is an internal direct product.\\n\\n\\n\\n10\\n\\nNormal Subgroups and Factor\\nGroups\\n\\nIf H is a subgroup of a group G, then right cosets are not always the same as left cosets;\\nthat is, it is not always the case that gH = Hg for all g ∈ G. The subgroups for which this\\nproperty holds play a critical role in group theory—they allow for the construction of a new\\nclass of groups, called factor or quotient groups. Factor groups may be studied directly or\\nby using homomorphisms, a generalization of isomorphisms. We will study homomorphisms\\nin Chapter 11.\\n\\n10.1 Factor Groups and Normal Subgroups\\n\\nNormal Subgroups\\n\\nA subgroup H of a group G is normal in G if gH = Hg for all g ∈ G. That is, a normal\\nsubgroup of a group G is one in which the right and left cosets are precisely the same.\\n\\nExample 10.1. Let G be an abelian group. Every subgroup H of G is a normal subgroup.\\nSince gh = hg for all g ∈ G and h ∈ H, it will always be the case that gH = Hg.\\n\\nExample 10.2. Let H be the subgroup of S3 consisting of elements (1) and (12). Since\\n\\n(123)H = {(123), (13)} and H(123) = {(123), (23)},\\n\\nH cannot be a normal subgroup of S3. However, the subgroup N , consisting of the permu-\\ntations (1), (123), and (132), is normal since the cosets of N are\\n\\nN = {(1), (123), (132)}\\n(12)N = N(12) = {(12), (13), (23)}.\\n\\nThe following theorem is fundamental to our understanding of normal subgroups.\\n\\nTheorem 10.3. Let G be a group and N be a subgroup of G. Then the following statements\\nare equivalent.\\n\\n1. The subgroup N is normal in G.\\n\\n2. For all g ∈ G, gNg−1 ⊂ N .\\n\\n3. For all g ∈ G, gNg−1 = N .\\n\\n168\\n\\n\\n\\n10.1. FACTOR GROUPS AND NORMAL SUBGROUPS 169\\n\\nProof. (1) ⇒ (2). Since N is normal in G, gN = Ng for all g ∈ G. Hence, for a given\\ng ∈ G and n ∈ N , there exists an n′ in N such that gn = n′g. Therefore, gng−1 = n′ ∈ N\\nor gNg−1 ⊂ N .\\n\\n(2) ⇒ (3). Let g ∈ G. Since gNg−1 ⊂ N , we need only show N ⊂ gNg−1. For n ∈ N ,\\ng−1ng = g−1n(g−1)−1 ∈ N . Hence, g−1ng = n′ for some n′ ∈ N . Therefore, n = gn′g−1 is\\nin gNg−1.\\n\\n(3) ⇒ (1). Suppose that gNg−1 = N for all g ∈ G. Then for any n ∈ N there\\nexists an n′ ∈ N such that gng−1 = n′. Consequently, gn = n′g or gN ⊂ Ng. Similarly,\\nNg ⊂ gN .\\n\\nFactor Groups\\nIf N is a normal subgroup of a group G, then the cosets of N in G form a group G/N under\\nthe operation (aN)(bN) = abN . This group is called the factor or quotient group of G\\nand N . Our first task is to prove that G/N is indeed a group.\\n\\nTheorem 10.4. Let N be a normal subgroup of a group G. The cosets of N in G form a\\ngroup G/N of order [G : N ].\\n\\nProof. The group operation on G/N is (aN)(bN) = abN . This operation must be shown\\nto be well-defined; that is, group multiplication must be independent of the choice of coset\\nrepresentative. Let aN = bN and cN = dN . We must show that\\n\\n(aN)(cN) = acN = bdN = (bN)(dN).\\n\\nThen a = bn1 and c = dn2 for some n1 and n2 in N . Hence,\\n\\nacN = bn1dn2N\\n\\n= bn1dN\\n\\n= bn1Nd\\n\\n= bNd\\n\\n= bdN.\\n\\nThe remainder of the theorem is easy: eN = N is the identity and g−1N is the inverse of\\ngN . The order of G/N is, of course, the number of cosets of N in G.\\n\\nIt is very important to remember that the elements in a factor group are sets of elements\\nin the original group.\\n\\nExample 10.5. Consider the normal subgroup of S3, N = {(1), (123), (132)}. The cosets\\nof N in S3 are N and (12)N . The factor group S3/N has the following multiplication table.\\n\\nN (12)N\\n\\nN N (12)N\\n\\n(12)N (12)N N\\n\\nThis group is isomorphic to Z2. At first, multiplying cosets seems both complicated and\\nstrange; however, notice that S3/N is a smaller group. The factor group displays a certain\\namount of information about S3. Actually, N = A3, the group of even permutations, and\\n(12)N = {(12), (13), (23)} is the set of odd permutations. The information captured in\\n\\n\\n\\n170 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\\n\\nG/N is parity; that is, multiplying two even or two odd permutations results in an even\\npermutation, whereas multiplying an odd permutation by an even permutation yields an\\nodd permutation.\\n\\nExample 10.6. Consider the normal subgroup 3Z of Z. The cosets of 3Z in Z are\\n\\n0 + 3Z = {. . . ,−3, 0, 3, 6, . . .}\\n1 + 3Z = {. . . ,−2, 1, 4, 7, . . .}\\n2 + 3Z = {. . . ,−1, 2, 5, 8, . . .}.\\n\\nThe group Z/3Z is given by the multiplication table below.\\n\\n+ 0 + 3Z 1 + 3Z 2 + 3Z\\n0 + 3Z 0 + 3Z 1 + 3Z 2 + 3Z\\n1 + 3Z 1 + 3Z 2 + 3Z 0 + 3Z\\n2 + 3Z 2 + 3Z 0 + 3Z 1 + 3Z\\n\\nIn general, the subgroup nZ of Z is normal. The cosets of Z/nZ are\\n\\nnZ\\n1 + nZ\\n2 + nZ\\n\\n...\\n(n− 1) + nZ.\\n\\nThe sum of the cosets k + Z and l+ Z is k + l+ Z. Notice that we have written our cosets\\nadditively, because the group operation is integer addition.\\n\\nExample 10.7. Consider the dihedral group Dn, generated by the two elements r and s,\\nsatisfying the relations\\n\\nrn = id\\ns2 = id\\nsrs = r−1.\\n\\nThe element r actually generates the cyclic subgroup of rotations, Rn, of Dn. Since srs−1 =\\nsrs = r−1 ∈ Rn, the group of rotations is a normal subgroup of Dn; therefore, Dn/Rn is a\\ngroup. Since there are exactly two elements in this group, it must be isomorphic to Z2.\\n\\n10.2 The Simplicity of the Alternating Group\\nOf special interest are groups with no nontrivial normal subgroups. Such groups are called\\nsimple groups. Of course, we already have a whole class of examples of simple groups, Zp,\\nwhere p is prime. These groups are trivially simple since they have no proper subgroups\\nother than the subgroup consisting solely of the identity. Other examples of simple groups\\nare not so easily found. We can, however, show that the alternating group, An, is simple\\nfor n ≥ 5. The proof of this result requires several lemmas.\\n\\nLemma 10.8. The alternating group An is generated by 3-cycles for n ≥ 3.\\n\\n\\n\\n10.2. THE SIMPLICITY OF THE ALTERNATING GROUP 171\\n\\nProof. To show that the 3-cycles generate An, we need only show that any pair of trans-\\npositions can be written as the product of 3-cycles. Since (ab) = (ba), every pair of trans-\\npositions must be one of the following:\\n\\n(ab)(ab) = id\\n(ab)(cd) = (acb)(acd)\\n\\n(ab)(ac) = (acb).\\n\\nLemma 10.9. Let N be a normal subgroup of An, where n ≥ 3. If N contains a 3-cycle,\\nthen N = An.\\n\\nProof. We will first show that An is generated by 3-cycles of the specific form (ijk), where\\ni and j are fixed in {1, 2, . . . , n} and we let k vary. Every 3-cycle is the product of 3-cycles\\nof this form, since\\n\\n(iaj) = (ija)2\\n\\n(iab) = (ijb)(ija)2\\n\\n(jab) = (ijb)2(ija)\\n\\n(abc) = (ija)2(ijc)(ijb)2(ija).\\n\\nNow suppose that N is a nontrivial normal subgroup of An for n ≥ 3 such that N contains\\na 3-cycle of the form (ija). Using the normality of N , we see that\\n\\n[(ij)(ak)](ija)2[(ij)(ak)]−1 = (ijk)\\n\\nis in N . Hence, N must contain all of the 3-cycles (ijk) for 1 ≤ k ≤ n. By Lemma 10.8,\\nthese 3-cycles generate An; hence, N = An.\\n\\nLemma 10.10. For n ≥ 5, every nontrivial normal subgroup N of An contains a 3-cycle.\\n\\nProof. Let σ be an arbitrary element in a normal subgroup N . There are several possible\\ncycle structures for σ.\\n\\n• σ is a 3-cycle.\\n\\n• σ is the product of disjoint cycles, σ = τ(a1a2 · · · ar) ∈ N , where r > 3.\\n\\n• σ is the product of disjoint cycles, σ = τ(a1a2a3)(a4a5a6).\\n\\n• σ = τ(a1a2a3), where τ is the product of disjoint 2-cycles.\\n\\n• σ = τ(a1a2)(a3a4), where τ is the product of an even number of disjoint 2-cycles.\\n\\nIf σ is a 3-cycle, then we are done. If N contains a product of disjoint cycles, σ, and at\\nleast one of these cycles has length greater than 3, say σ = τ(a1a2 · · · ar), then\\n\\n(a1a2a3)σ(a1a2a3)\\n−1\\n\\nis in N since N is normal; hence,\\n\\nσ−1(a1a2a3)σ(a1a2a3)\\n−1\\n\\n\\n\\n172 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\\n\\nis also in N . Since\\n\\nσ−1(a1a2a3)σ(a1a2a3)\\n−1 = σ−1(a1a2a3)σ(a1a3a2)\\n\\n= (a1a2 · · · ar)−1τ−1(a1a2a3)τ(a1a2 · · · ar)(a1a3a2)\\n= (a1arar−1 · · · a2)(a1a2a3)(a1a2 · · · ar)(a1a3a2)\\n= (a1a3ar),\\n\\nN must contain a 3-cycle; hence, N = An.\\nNow suppose that N contains a disjoint product of the form\\n\\nσ = τ(a1a2a3)(a4a5a6).\\n\\nThen\\nσ−1(a1a2a4)σ(a1a2a4)\\n\\n−1 ∈ N\\n\\nsince\\n(a1a2a4)σ(a1a2a4)\\n\\n−1 ∈ N.\\n\\nSo\\n\\nσ−1(a1a2a4)σ(a1a2a4)\\n−1 = [τ(a1a2a3)(a4a5a6)]\\n\\n−1(a1a2a4)τ(a1a2a3)(a4a5a6)(a1a2a4)\\n−1\\n\\n= (a4a6a5)(a1a3a2)τ\\n−1(a1a2a4)τ(a1a2a3)(a4a5a6)(a1a4a2)\\n\\n= (a4a6a5)(a1a3a2)(a1a2a4)(a1a2a3)(a4a5a6)(a1a4a2)\\n\\n= (a1a4a2a6a3).\\n\\nSo N contains a disjoint cycle of length greater than 3, and we can apply the previous case.\\nSuppose N contains a disjoint product of the form σ = τ(a1a2a3), where τ is the product\\n\\nof disjoint 2-cycles. Since σ ∈ N , σ2 ∈ N , and\\n\\nσ2 = τ(a1a2a3)τ(a1a2a3)\\n\\n= (a1a3a2).\\n\\nSo N contains a 3-cycle.\\nThe only remaining possible case is a disjoint product of the form\\n\\nσ = τ(a1a2)(a3a4),\\n\\nwhere τ is the product of an even number of disjoint 2-cycles. But\\n\\nσ−1(a1a2a3)σ(a1a2a3)\\n−1\\n\\nis in N since (a1a2a3)σ(a1a2a3)\\n−1 is in N ; and so\\n\\nσ−1(a1a2a3)σ(a1a2a3)\\n−1 = τ−1(a1a2)(a3a4)(a1a2a3)τ(a1a2)(a3a4)(a1a2a3)\\n\\n−1\\n\\n= (a1a3)(a2a4).\\n\\nSince n ≥ 5, we can find b ∈ {1, 2, . . . , n} such that b ̸= a1, a2, a3, a4. Let µ = (a1a3b). Then\\n\\nµ−1(a1a3)(a2a4)µ(a1a3)(a2a4) ∈ N\\n\\nand\\n\\nµ−1(a1a3)(a2a4)µ(a1a3)(a2a4) = (a1ba3)(a1a3)(a2a4)(a1a3b)(a1a3)(a2a4)\\n\\n= (a1a3b).\\n\\nTherefore, N contains a 3-cycle. This completes the proof of the lemma.\\n\\n\\n\\n10.3. EXERCISES 173\\n\\nTheorem 10.11. The alternating group, An, is simple for n ≥ 5.\\n\\nProof. Let N be a normal subgroup of An. By Lemma 10.10, N contains a 3-cycle. By\\nLemma 10.9, N = An; therefore, An contains no proper nontrivial normal subgroups for\\nn ≥ 5.\\n\\nHistorical Note\\n\\nOne of the foremost problems of group theory has been to classify all simple finite\\ngroups. This problem is over a century old and has been solved only in the last few decades\\nof the twentieth century. In a sense, finite simple groups are the building blocks of all\\nfinite groups. The first nonabelian simple groups to be discovered were the alternating\\ngroups. Galois was the first to prove that A5 was simple. Later, mathematicians such as C.\\nJordan and L. E. Dickson found several infinite families of matrix groups that were simple.\\nOther families of simple groups were discovered in the 1950s. At the turn of the century,\\nWilliam Burnside conjectured that all nonabelian simple groups must have even order. In\\n1963, W. Feit and J. Thompson proved Burnside’s conjecture and published their results\\nin the paper “Solvability of Groups of Odd Order,” which appeared in the Pacific Journal\\nof Mathematics. Their proof, running over 250 pages, gave impetus to a program in the\\n1960s and 1970s to classify all finite simple groups. Daniel Gorenstein was the organizer of\\nthis remarkable effort. One of the last simple groups was the “Monster,” discovered by R.\\nGreiss. The Monster, a 196,833×196,833 matrix group, is one of the 26 sporadic, or special,\\nsimple groups. These sporadic simple groups are groups that fit into no infinite family of\\nsimple groups. Some of the sporadic groups play an important role in physics.\\n\\n10.3 Exercises\\n1. For each of the following groups G, determine whether H is a normal subgroup of G. If\\nH is a normal subgroup, write out a Cayley table for the factor group G/H.\\n(a) G = S4 and H = A4\\n\\n(b) G = A5 and H = {(1), (123), (132)}\\n(c) G = S4 and H = D4\\n\\n(d) G = Q8 and H = {1,−1, I,−I}\\n(e) G = Z and H = 5Z\\n\\n2. Find all the subgroups of D4. Which subgroups are normal? What are all the factor\\ngroups of D4 up to isomorphism?\\n\\n3. Find all the subgroups of the quaternion group, Q8. Which subgroups are normal? What\\nare all the factor groups of Q8 up to isomorphism?\\n\\n4. Let T be the group of nonsingular upper triangular 2 × 2 matrices with entries in R;\\nthat is, matrices of the form (\\n\\na b\\n\\n0 c\\n\\n)\\n,\\n\\nwhere a, b, c ∈ R and ac ̸= 0. Let U consist of matrices of the form(\\n1 x\\n\\n0 1\\n\\n)\\n,\\n\\nwhere x ∈ R.\\n\\n\\n\\n174 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\\n\\n(a) Show that U is a subgroup of T .\\n(b) Prove that U is abelian.\\n(c) Prove that U is normal in T .\\n(d) Show that T/U is abelian.\\n(e) Is T normal in GL2(R)?\\n\\n5. Show that the intersection of two normal subgroups is a normal subgroup.\\n\\n6. If G is abelian, prove that G/H must also be abelian.\\n\\n7. Prove or disprove: If H is a normal subgroup of G such that H and G/H are abelian,\\nthen G is abelian.\\n\\n8. If G is cyclic, prove that G/H must also be cyclic.\\n\\n9. Prove or disprove: If H and G/H are cyclic, then G is cyclic.\\n\\n10. Let H be a subgroup of index 2 of a group G. Prove that H must be a normal subgroup\\nof G. Conclude that Sn is not simple for n ≥ 3.\\n\\n11. If a group G has exactly one subgroup H of order k, prove that H is normal in G.\\n\\n12. Define the centralizer of an element g in a group G to be the set\\n\\nC(g) = {x ∈ G : xg = gx}.\\n\\nShow that C(g) is a subgroup of G. If g generates a normal subgroup of G, prove that C(g)\\nis normal in G.\\n\\n13. Recall that the center of a group G is the set\\n\\nZ(G) = {x ∈ G : xg = gx for all }.\\n\\n(a) Calculate the center of S3.\\n(b) Calculate the center of GL2(R).\\n(c) Show that the center of any group G is a normal subgroup of G.\\n(d) If G/Z(G) is cyclic, show that G is abelian.\\n\\n14. Let G be a group and let G′ = ⟨aba−1b−1⟩; that is, G′ is the subgroup of all finite\\nproducts of elements in G of the form aba−1b−1. The subgroup G′ is called the commutator\\nsubgroup of G.\\n\\n(a) Show that G′ is a normal subgroup of G.\\n(b) Let N be a normal subgroup of G. Prove that G/N is abelian if and only if N contains\\n\\nthe commutator subgroup of G.\\n\\n10.4 Sage\\nSage has several convenient functions that will allow us to investigate quickly if a subgroup\\nis normal, and if so, the nature of the resulting quotient group. But for an initial under-\\nstanding, we can also work with the raw cosets. Let us get our hands dirty first, then learn\\nabout the easy way.\\n\\n\\n\\n10.4. SAGE 175\\n\\nMultiplying Cosets\\nThe definiton of a factor group requires a normal subgroup, and then we define a way to\\n“multiply” two cosets of the subgroup to produce another coset. It is important to realize\\nthat we can interpret the definition of a normal subgroup to be exactly the condition we\\nneed for our new multiplication to be workable. We will do two examples — first with a\\nnormal subgroup, then with a subgroup that is not normal.\\n\\nConsider the dihedral group D8 that is the symmetry group of an 8-gon. If we take the\\nelement that creates a quarter-turn, we can use it generate a cyclic subgroup of order 4.\\nThis will be a normal subgroup (trust us for the moment on this). First, build the (right)\\ncosets (notice there is no output):\\n\\nG = DihedralGroup (8)\\nquarter_turn = G( \' (1,3,5,7)(2,4,6,8) \' )\\nS = G.subgroup ([ quarter_turn ])\\nC = G.cosets(S)\\n\\nSo C is a list of lists, with every element of the group G occuring exactly once somewhere.\\nYou could ask Sage to print out C for you if you like, but we will try to avoid that here. We\\nwant to multiply two cosets (lists) together. How do we do this? Take any element out of\\nthe first list, and any element out of the second list and multiply them together (which we\\nknow how to do since they are elements of G). Now we have an element of G. What do we\\ndo with this element, since we really want a coset as the result of the product of two cosets?\\nSimple — we see which coset the product is in. Let us give it a try. We will multiply coset 1\\nwith coset 3 (there are 4 cosets by Lagrange’s Theorem). Study the following code carefully\\nto see if you can understand what it is doing, and then read the explanation that follows.\\n\\np = C[1][0]*C[3][0]\\n[i for i in srange(len(C)) if p in C[i]]\\n\\n[2]\\n\\nWhat have we accomplished? In the first line we create p as the product of two group\\nelements, one from coset 1 and one from coset 3 (C[1], C[3]). Since we can choose any\\nelement from each coset, we choose the first element of each (C[ ][0]). Then we count our\\nway through all the cosets, selecting only cosets that contain p. Since p will only be in one\\ncoset, we expect a list with just one element. Here, our one-element list contains only 2. So\\nwe say the product of coset 1 and coset 3 is coset 2.\\n\\nThe point here is that this result (coset 1 times coset 3 is coset 2) should always be\\nthe same, no matter which elements we pick from the two cosets to form p. So let us do it\\nagain, but this time we will not simply choose the first element from each of coset 1 and\\ncoset 3, instead we will choose the third element of coset 1 and the second element of coset\\n3 (remember, we are counting from zero!).\\n\\np = C[1][2]*C[3][1]\\n[i for i in srange(len(C)) if p in C[i]]\\n\\n[2]\\n\\nGood. We have the same result. If you are still trusting us on S being a normal subgroup\\nof G, then this is the result that the theory predicts. Make a copy of the above compute cell\\nand try other choices for the representatives of each coset. Then try the product of other\\ncosets, with varying representatives.\\n\\nNow is a good time to introduce a way to extend Sage and add new functions. We\\nwill design a coset-multiplication function. Read the following carefully and then see the\\nsubsequent explanation.\\n\\n\\n\\n176 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\\n\\ndef coset_product(i, j, C):\\np = C[i][0]*C[j][0]\\nc = [k for k in srange(len(C)) if p in C[k]]\\nreturn c[0]\\n\\nThe first line creates a new Sage function named coset_product. This is accomplished\\nwith the word def, and note the colon ending the line. The inputs to the function are the\\nnumbers of the cosets we want to multiply and the complete list of the cosets. The middle\\ntwo lines should look familiar from above. We know c is a one-element list, so c[0] will\\nextract this one coset number, and return is what determines that this is the output of\\nthe function. Notice that the indentation above must be exactly as shown. We could have\\nwritten all this computation on a single line without making a new function, but that begins\\nto get unwieldly. You need to execute the code block above to actually define the function,\\nand there will be no output if successful. Now we can use our new function to repeat our\\nwork above:\\n\\ncoset_product (1, 3, C)\\n\\n2\\n\\nNow you know the basics of how to add onto Sage and do much more than it was\\ndesigned for. And with some practice, you could suggest and contribute new functions to\\nSage, since it is an open source project. Nice.\\n\\nNow let us examine a situation where the subgroup is not normal. So we will see\\nthat our definition of coset multiplication is insufficient in this case. And realize that our\\nnew coset_product function is also useless since it assumes the cosets come from a normal\\nsubgroup.\\n\\nConsider the alternating group A4 which we can interpet as the symmetry group of a\\ntetrahedron. For a subgroup, take an element that fixes one vertex and rotates the opposite\\nface — this will generate a cyclic subgroup of order 3, and by Lagrange’s Theorem we will\\nget four cosets. We compute them here. (Again, no output is requested.)\\n\\nG = AlternatingGroup (4)\\nface_turn = G(\\"(1,2,3)\\")\\nS = G.subgroup ([ face_turn ])\\nC = G.cosets(S)\\n\\nAgain, let’s consider the product of coset 1 and coset 3:\\np = C[1][0]*C[3][0]\\n[i for i in srange(len(C)) if p in C[i]]\\n\\n[0]\\n\\nAgain, but now for coset 3, choose the second element of the coset to produce the\\nproduct p:\\n\\np = C[1][0]*C[3][1]\\n[i for i in srange(len(C)) if p in C[i]]\\n\\n[2]\\n\\nSo, is the product of coset 1 and coset 3 equal to coset 0 or coset 2? We cannot say!\\nSo there is no way to construct a quotient group for this subgroup. You can experiment\\nsome more with this subgroup, but in some sense, we are done with this example — there\\nis nothing left to say.\\n\\n\\n\\n10.4. SAGE 177\\n\\nSage Methods for Normal Subgroups\\nYou can easily ask Sage if a subgroup is normal or not. This is viewed as a property of the\\nsubgroup, but you must tell Sage what the “supergroup” is, since the answer can change\\ndepending on this value. (For example H.is_normal(H) will always be True.) Here are our\\ntwo examples from above.\\n\\nG = DihedralGroup (8)\\nquarter_turn = G( \' (1,3,5,7)(2,4,6,8) \' )\\nS = G.subgroup ([ quarter_turn ])\\nS.is_normal(G)\\n\\nTrue\\n\\nG = AlternatingGroup (4)\\nface_turn = G(\\"(1,2,3)\\")\\nS = G.subgroup ([ face_turn ])\\nS.is_normal(G)\\n\\nFalse\\n\\nThe text proves in Section 10.2 that A5 is simple, i.e. A5 has no normal subgroups.\\nWe could build every subgroup of A5 and ask if it is normal in A5 using the .is_normal()\\n\\nmethod. But Sage has this covered for us already.\\nG = AlternatingGroup (5)\\nG.is_simple ()\\n\\nTrue\\n\\nWe can also build a quotient group when we have a normal subgroup.\\nG = DihedralGroup (8)\\nquarter_turn = G( \' (1,3,5,7)(2,4,6,8) \' )\\nS = G.subgroup ([ quarter_turn ])\\nQ = G.quotient(S)\\nQ\\n\\nPermutation Group with generators [(1,2)(3,4), (1,3)(2,4)]\\n\\nThis is useful, but also a bit unsettling. We have the quotient group, but any notion of\\ncosets has been lost, since Q is returned as a new permutation group on a different set of\\nsymbols. We cannot presume that the numbers used for the new permutation group Q bear\\nany resemblance to the cosets we get from the .cosets() method. But we can see that the\\nquotient group is described as a group generated by two elements of order two. We could\\nask for the order of the group, or by Lagrange’s Theorem we know the quotient has order\\n4. We can say now that there are only two groups of order four, the cyclic group of order\\n4 and a non-cyclic group of order 4, known to us as the Klein 4-group or Z2 × Z2. This\\nquotient group looks like the non-cyclic one since the cyclic group of order 4 has just one\\nelement of order 2. Let us see what Sage says.\\n\\nQ.is_isomorphic(KleinFourGroup ())\\n\\nTrue\\n\\nYes, that’s it.\\nFinally, Sage can build us a list of all of the normal subgroups of a group. The list of\\n\\ngroups themselves, as we have seen before, is sometimes an overwhelming amount of infor-\\nmation. We will demonstrate by just listing the orders of the normal subgroups produced.\\n\\n\\n\\n178 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\\n\\nG = DihedralGroup (8)\\nN = G.normal_subgroups ()\\n[H.order () for H in N]\\n\\n[1, 2, 4, 8, 8, 8, 16]\\n\\nSo, in particular, we see that our “quarter-turn” subgroup is the only normal subgroup\\nof order 4 in this group.\\n\\n10.5 Sage Exercises\\n1. Build every subgroup of the alternating group on 5 symbols, A5, and check that each\\nis not a normal subgroup (except for the two trivial cases). This command might take a\\ncouple seconds to run. Compare this with the time needed to run the .is_simple() method\\nand realize that there is a significant amount of theory and cleverness brought to bear in\\nspeeding up commands like this. (It is possible that your Sage installation lacks gap’s\\n“Table of Marks” library and you will be unable to compute the list of subgroups.)\\n\\n2. Consider the quotient group of the group of symmetries of an 8-gon, formed with the\\ncyclic subgroup of order 4 generated by a quarter-turn. Use the coset_product function\\nto determine the Cayley table for this quotient group. Use the number of each coset, as\\nproduced by the .cosets() method as names for the elements of the quotient group. You\\nwill need to build the table “by hand” as there is no easy way to have Sage’s Cayley table\\ncommand do this one for you. You can build a table in the Sage Notebook pop-up editor\\n(shift-click on a blue line) or you might read the documentation of the html.table() method.\\n\\n3. Consider the cyclic subgroup of order 4 in the symmetries of an 8-gon. Verify that\\nthe subgroup is normal by first building the raw left and right cosets (without using the\\n.cosets() method) and then checking their equality in Sage, all with a single command that\\nemploys sorting with the sorted() command.\\n\\n4. Again, use the same cyclic subgroup of order 4 in the group of symmetries of an 8-gon.\\nCheck that the subgroup is normal by using part (2) of Theorem 10.3. Construct a one-line\\ncommand that does the complete check and returns True. Maybe sort the elements of the\\nsubgroup S first, then slowly build up the necessary lists, commands, and conditions in\\nsteps. Notice that this check does not require ever building the cosets.\\n\\n5. Repeat the demonstration from the previous subsection that for the symmetries of a\\ntetrahedron, a cyclic subgroup of order 3 results in an undefined coset multiplication. Above,\\nthe default setting for the .cosets() method builds right cosets — but in this problem, work\\ninstead with left cosets. You need to choose two cosets to multiply, and then demonstrate\\ntwo choices for representatives that lead to different results for the product of the cosets.\\n\\n6. Construct some dihedral groups of order 2n (i.e. symmetries of an n-gon, Dn in the text,\\nDihedralGroup(n) in Sage). Maybe all of them for 3 ≤ n ≤ 100. For each dihedral group,\\nconstruct a list of the orders of each of the normal subgroups (so use .normal_subgroups()).\\nYou may need to wait ten or twenty seconds for this to finish - be patient. Observe enough\\nexamples to hypothesize a pattern to your observations, check your hypothesis against each\\nof your examples and then state your hypothesis clearly.\\nCan you predict how many normal subgroups there are in the dihedral groupD470448 without\\nusing Sage to build all the normal subgroups? Can you describe all of the normal subgroups\\nof a dihedral group in a way that would let us predict all of the normal subgroups of D470448\\n\\nwithout using Sage?\\n\\n\\n\\n11\\n\\nHomomorphisms\\n\\nOne of the basic ideas of algebra is the concept of a homomorphism, a natural generalization\\nof an isomorphism. If we relax the requirement that an isomorphism of groups be bijective,\\nwe have a homomorphism.\\n\\n11.1 Group Homomorphisms\\nA homomorphism between groups (G, ·) and (H, ◦) is a map ϕ : G→ H such that\\n\\nϕ(g1 · g2) = ϕ(g1) ◦ ϕ(g2)\\n\\nfor g1, g2 ∈ G. The range of ϕ in H is called the homomorphic image of ϕ.\\nTwo groups are related in the strongest possible way if they are isomorphic; however, a\\n\\nweaker relationship may exist between two groups. For example, the symmetric group Sn\\nand the group Z2 are related by the fact that Sn can be divided into even and odd permu-\\ntations that exhibit a group structure like that Z2, as shown in the following multiplication\\ntable.\\n\\neven odd\\neven even odd\\nodd odd even\\n\\nWe use homomorphisms to study relationships such as the one we have just described.\\n\\nExample 11.1. Let G be a group and g ∈ G. Define a map ϕ : Z → G by ϕ(n) = gn.\\nThen ϕ is a group homomorphism, since\\n\\nϕ(m+ n) = gm+n = gmgn = ϕ(m)ϕ(n).\\n\\nThis homomorphism maps Z onto the cyclic subgroup of G generated by g.\\n\\nExample 11.2. Let G = GL2(R). If\\n\\nA =\\n\\n(\\na b\\n\\nc d\\n\\n)\\nis in G, then the determinant is nonzero; that is, det(A) = ad − bc ̸= 0. Also, for any two\\nelements A and B in G, det(AB) = det(A) det(B). Using the determinant, we can define a\\nhomomorphism ϕ : GL2(R) → R∗ by A 7→ det(A).\\n\\n179\\n\\n\\n\\n180 CHAPTER 11. HOMOMORPHISMS\\n\\nExample 11.3. Recall that the circle group T consists of all complex numbers z such that\\n|z| = 1. We can define a homomorphism ϕ from the additive group of real numbers R to T\\nby ϕ : θ 7→ cos θ + i sin θ. Indeed,\\n\\nϕ(α+ β) = cos(α+ β) + i sin(α+ β)\\n\\n= (cosα cosβ − sinα sinβ) + i(sinα cosβ + cosα sinβ)\\n= (cosα+ i sinα)(cosβ + i sinβ)\\n= ϕ(α)ϕ(β).\\n\\nGeometrically, we are simply wrapping the real line around the circle in a group-theoretic\\nfashion.\\n\\nThe following proposition lists some basic properties of group homomorphisms.\\n\\nProposition 11.4. Let ϕ : G1 → G2 be a homomorphism of groups. Then\\n\\n1. If e is the identity of G1, then ϕ(e) is the identity of G2;\\n\\n2. For any element g ∈ G1, ϕ(g−1) = [ϕ(g)]−1;\\n\\n3. If H1 is a subgroup of G1, then ϕ(H1) is a subgroup of G2;\\n\\n4. If H2 is a subgroup of G2, then ϕ−1(H2) = {g ∈ G1 : ϕ(g) ∈ H2} is a subgroup of G1.\\nFurthermore, if H2 is normal in G2, then ϕ−1(H2) is normal in G1.\\n\\nProof. (1) Suppose that e and e′ are the identities of G1 and G2, respectively; then\\n\\ne′ϕ(e) = ϕ(e) = ϕ(ee) = ϕ(e)ϕ(e).\\n\\nBy cancellation, ϕ(e) = e′.\\n(2) This statement follows from the fact that\\n\\nϕ(g−1)ϕ(g) = ϕ(g−1g) = ϕ(e) = e′.\\n\\n(3) The set ϕ(H1) is nonempty since the identity of G2 is in ϕ(H1). Suppose that H1\\n\\nis a subgroup of G1 and let x and y be in ϕ(H1). There exist elements a, b ∈ H1 such that\\nϕ(a) = x and ϕ(b) = y. Since\\n\\nxy−1 = ϕ(a)[ϕ(b)]−1 = ϕ(ab−1) ∈ ϕ(H1),\\n\\nϕ(H1) is a subgroup of G2 by Proposition 3.31.\\n(4) Let H2 be a subgroup of G2 and define H1 to be ϕ−1(H2); that is, H1 is the set of\\n\\nall g ∈ G1 such that ϕ(g) ∈ H2. The identity is in H1 since ϕ(e) = e′. If a and b are in H1,\\nthen ϕ(ab−1) = ϕ(a)[ϕ(b)]−1 is in H2 since H2 is a subgroup of G2. Therefore, ab−1 ∈ H1\\n\\nand H1 is a subgroup of G1. If H2 is normal in G2, we must show that g−1hg ∈ H1 for\\nh ∈ H1 andvg ∈ G1. But\\n\\nϕ(g−1hg) = [ϕ(g)]−1ϕ(h)ϕ(g) ∈ H2,\\n\\nsince H2 is a normal subgroup of G2. Therefore, g−1hg ∈ H1.\\n\\nLet ϕ : G→ H be a group homomorphism and suppose that e is the identity of H. By\\nProposition 11.4, ϕ−1({e}) is a subgroup of G. This subgroup is called the kernel of ϕ and\\nwill be denoted by kerϕ. In fact, this subgroup is a normal subgroup of G since the trivial\\nsubgroup is normal in H. We state this result in the following theorem, which says that\\nwith every homomorphism of groups we can naturally associate a normal subgroup.\\n\\n\\n\\n11.2. THE ISOMORPHISM THEOREMS 181\\n\\nTheorem 11.5. Let ϕ : G → H be a group homomorphism. Then the kernel of ϕ is a\\nnormal subgroup of G.\\n\\nExample 11.6. Let us examine the homomorphism ϕ : GL2(R) → R∗ defined by A 7→\\ndet(A). Since 1 is the identity of R∗, the kernel of this homomorphism is all 2× 2 matrices\\nhaving determinant one. That is, kerϕ = SL2(R).\\n\\nExample 11.7. The kernel of the group homomorphism ϕ : R → C∗ defined by ϕ(θ) =\\ncos θ + i sin θ is {2πn : n ∈ Z}. Notice that kerϕ ∼= Z.\\n\\nExample 11.8. Suppose that we wish to determine all possible homomorphisms ϕ from\\nZ7 to Z12. Since the kernel of ϕ must be a subgroup of Z7, there are only two possible\\nkernels, {0} and all of Z7. The image of a subgroup of Z7 must be a subgroup of Z12.\\nHence, there is no injective homomorphism; otherwise, Z12 would have a subgroup of order\\n7, which is impossible. Consequently, the only possible homomorphism from Z7 to Z12 is\\nthe one mapping all elements to zero.\\n\\nExample 11.9. Let G be a group. Suppose that g ∈ G and ϕ is the homomorphism from Z\\nto G given by ϕ(n) = gn. If the order of g is infinite, then the kernel of this homomorphism\\nis {0} since ϕ maps Z onto the cyclic subgroup of G generated by g. However, if the order\\nof g is finite, say n, then the kernel of ϕ is nZ.\\n\\n11.2 The Isomorphism Theorems\\nAlthough it is not evident at first, factor groups correspond exactly to homomorphic images,\\nand we can use factor groups to study homomorphisms. We already know that with every\\ngroup homomorphism ϕ : G→ H we can associate a normal subgroup of G, kerϕ. The con-\\nverse is also true; that is, every normal subgroup of a group G gives rise to homomorphism\\nof groups.\\n\\nLet H be a normal subgroup of G. Define the natural or canonical homomorphism\\n\\nϕ : G→ G/H\\n\\nby\\nϕ(g) = gH.\\n\\nThis is indeed a homomorphism, since\\n\\nϕ(g1g2) = g1g2H = g1Hg2H = ϕ(g1)ϕ(g2).\\n\\nThe kernel of this homomorphism is H. The following theorems describe the relationships\\nbetween group homomorphisms, normal subgroups, and factor groups.\\n\\nTheorem 11.10 (First Isomorphism Theorem). If ψ : G → H is a group homomorphism\\nwith K = kerψ, then K is normal in G. Let ϕ : G→ G/K be the canonical homomorphism.\\nThen there exists a unique isomorphism η : G/K → ψ(G) such that ψ = ηϕ.\\n\\nProof. We already know thatK is normal inG. Define η : G/K → ψ(G) by η(gK) = ψ(g).\\nWe first show that η is a well-defined map. If g1K = g2K, then for some k ∈ K, g1k = g2;\\nconsequently,\\n\\nη(g1K) = ψ(g1) = ψ(g1)ψ(k) = ψ(g1k) = ψ(g2) = η(g2K).\\n\\n\\n\\n182 CHAPTER 11. HOMOMORPHISMS\\n\\nThus, η does not depend on the choice of coset representatives and the map η : G/K → ψ(G)\\nis uniquely defined since ψ = ηϕ. We must also show that η is a homomorphism, but\\n\\nη(g1Kg2K) = η(g1g2K)\\n\\n= ψ(g1g2)\\n\\n= ψ(g1)ψ(g2)\\n\\n= η(g1K)η(g2K).\\n\\nClearly, η is onto ψ(G). To show that η is one-to-one, suppose that η(g1K) = η(g2K).\\nThen ψ(g1) = ψ(g2). This implies that ψ(g−1\\n\\n1 g2) = e, or g−1\\n1 g2 is in the kernel of ψ; hence,\\n\\ng−1\\n1 g2K = K; that is, g1K = g2K.\\n\\nMathematicians often use diagrams called commutative diagrams to describe such\\ntheorems. The following diagram “commutes” since ψ = ηϕ.\\n\\nψ\\n\\nϕ η\\n\\nG H\\n\\nG/K\\n\\nExample 11.11. Let G be a cyclic group with generator g. Define a map ϕ : Z → G by\\nn 7→ gn. This map is a surjective homomorphism since\\n\\nϕ(m+ n) = gm+n = gmgn = ϕ(m)ϕ(n).\\n\\nClearly ϕ is onto. If |g| = m, then gm = e. Hence, kerϕ = mZ and Z/ kerϕ = Z/mZ ∼= G.\\nOn the other hand, if the order of g is infinite, then kerϕ = 0 and ϕ is an isomorphism of\\nG and Z. Hence, two cyclic groups are isomorphic exactly when they have the same order.\\nUp to isomorphism, the only cyclic groups are Z and Zn.\\n\\nTheorem 11.12 (Second Isomorphism Theorem). Let H be a subgroup of a group G (not\\nnecessarily normal in G) and N a normal subgroup of G. Then HN is a subgroup of G,\\nH ∩N is a normal subgroup of H, and\\n\\nH/H ∩N ∼= HN/N.\\n\\nProof. We will first show that HN = {hn : h ∈ H,n ∈ N} is a subgroup of G. Suppose\\nthat h1n1, h2n2 ∈ HN . Since N is normal, (h2)−1n1h2 ∈ N . So\\n\\n(h1n1)(h2n2) = h1h2((h2)\\n−1n1h2)n2\\n\\nis in HN . The inverse of hn ∈ HN is in HN since\\n\\n(hn)−1 = n−1h−1 = h−1(hn−1h−1).\\n\\nNext, we prove that H ∩ N is normal in H. Let h ∈ H and n ∈ H ∩ N . Then\\nh−1nh ∈ H since each element is in H. Also, h−1nh ∈ N since N is normal in G; therefore,\\nh−1nh ∈ H ∩N .\\n\\nNow define a map ϕ from H to HN/N by h 7→ hN . The map ϕ is onto, since any coset\\nhnN = hN is the image of h in H. We also know that ϕ is a homomorphism because\\n\\nϕ(hh′) = hh′N = hNh′N = ϕ(h)ϕ(h′).\\n\\n\\n\\n11.2. THE ISOMORPHISM THEOREMS 183\\n\\nBy the First Isomorphism Theorem, the image of ϕ is isomorphic to H/ kerϕ; that is,\\n\\nHN/N = ϕ(H) ∼= H/ kerϕ.\\n\\nSince\\nkerϕ = {h ∈ H : h ∈ N} = H ∩N,\\n\\nHN/N = ϕ(H) ∼= H/H ∩N .\\n\\nTheorem 11.13 (Correspondence Theorem). Let N be a normal subgroup of a group G.\\nThen H 7→ H/N is a one-to-one correspondence between the set of subgroups H containing\\nN and the set of subgroups of G/N . Furthermore, the normal subgroups of G containing N\\ncorrespond to normal subgroups of G/N .\\n\\nProof. Let H be a subgroup of G containing N . Since N is normal in H, H/N makes\\nsense. Let aN and bN be elements of H/N . Then (aN)(b−1N) = ab−1N ∈ H/N ; hence,\\nH/N is a subgroup ofG/N .\\n\\nLet S be a subgroup of G/N . This subgroup is a set of cosets of N . If H = {g ∈ G :\\ngN ∈ S}, then for h1, h2 ∈ H, we have that (h1N)(h2N) = h1h2N ∈ S and h−1\\n\\n1 N ∈ S.\\nTherefore, H must be a subgroup of G. Clearly, H contains N . Therefore, S = H/N .\\nConsequently, the map H 7→ H/N is onto.\\n\\nSuppose that H1 and H2 are subgroups of G containing N such that H1/N = H2/N .\\nIf h1 ∈ H1, then h1N ∈ H1/N . Hence, h1N = h2N ⊂ H2 for some h2 in H2. However,\\nsince N is contained in H2, we know that h1 ∈ H2 or H1 ⊂ H2. Similarly, H2 ⊂ H1. Since\\nH1 = H2, the map H 7→ H/N is one-to-one.\\n\\nSuppose that H is normal in G and N is a subgroup of H. Then it is easy to verify\\nthat the map G/N → G/H defined by gN 7→ gH is a homomorphism. The kernel of this\\nhomomorphism is H/N , which proves that H/N is normal in G/N .\\n\\nConversely, suppose that H/N is normal in G/N . The homomorphism given by\\n\\nG→ G/N → G/N\\n\\nH/N\\n\\nhas kernel H. Hence, H must be normal in G.\\n\\nNotice that in the course of the proof of Theorem 11.13, we have also proved the following\\ntheorem.\\n\\nTheorem 11.14 (Third Isomorphism Theorem). Let G be a group and N and H be normal\\nsubgroups of G with N ⊂ H. Then\\n\\nG/H ∼=\\nG/N\\n\\nH/N\\n.\\n\\nExample 11.15. By the Third Isomorphism Theorem,\\n\\nZ/mZ ∼= (Z/mnZ)/(mZ/mnZ).\\n\\nSince |Z/mnZ| = mn and |Z/mZ| = m, we have |mZ/mnZ| = n.\\n\\n\\n\\n184 CHAPTER 11. HOMOMORPHISMS\\n\\n11.3 Exercises\\n1. Prove that det(AB) = det(A)det(B) for A,B ∈ GL2(R). This shows that the determi-\\nnant is a homomorphism from GL2(R) to R∗.\\n\\n2. Which of the following maps are homomorphisms? If the map is a homomorphism, what\\nis the kernel?\\n(a) ϕ : R∗ → GL2(R) defined by\\n\\nϕ(a) =\\n\\n(\\n1 0\\n\\n0 a\\n\\n)\\n(b) ϕ : R → GL2(R) defined by\\n\\nϕ(a) =\\n\\n(\\n1 0\\n\\na 1\\n\\n)\\n(c) ϕ : GL2(R) → R defined by\\n\\nϕ\\n\\n((\\na b\\n\\nc d\\n\\n))\\n= a+ d\\n\\n(d) ϕ : GL2(R) → R∗ defined by\\n\\nϕ\\n\\n((\\na b\\n\\nc d\\n\\n))\\n= ad− bc\\n\\n(e) ϕ : M2(R) → R defined by\\n\\nϕ\\n\\n((\\na b\\n\\nc d\\n\\n))\\n= b,\\n\\nwhere M2(R) is the additive group of 2× 2 matrices with entries in R.\\n\\n3. Let A be an m× n matrix. Show that matrix multiplication, x 7→ Ax, defines a homo-\\nmorphism ϕ : Rn → Rm.\\n\\n4. Let ϕ : Z → Z be given by ϕ(n) = 7n. Prove that ϕ is a group homomorphism. Find\\nthe kernel and the image of ϕ.\\n\\n5. Describe all of the homomorphisms from Z24 to Z18.\\n\\n6. Describe all of the homomorphisms from Z to Z12.\\n\\n7. In the group Z24, let H = ⟨4⟩ and N = ⟨6⟩.\\n(a) List the elements in HN (we usually write H+N for these additive groups) and H∩N .\\n(b) List the cosets in HN/N , showing the elements in each coset.\\n(c) List the cosets in H/(H ∩N), showing the elements in each coset.\\n(d) Give the correspondence between HN/N and H/(H ∩ N) described in the proof of\\n\\nthe Second Isomorphism Theorem.\\n\\n8. If G is an abelian group and n ∈ N, show that ϕ : G→ G defined by g 7→ gn is a group\\nhomomorphism.\\n\\n9. If ϕ : G→ H is a group homomorphism and G is abelian, prove that ϕ(G) is also abelian.\\n\\n10. If ϕ : G→ H is a group homomorphism and G is cyclic, prove that ϕ(G) is also cyclic.\\n\\n\\n\\n11.4. ADDITIONAL EXERCISES: AUTOMORPHISMS 185\\n\\n11. Show that a homomorphism defined on a cyclic group is completely determined by its\\naction on the generator of the group.\\n\\n12. If a group G has exactly one subgroup H of order k, prove that H is normal in G.\\n\\n13. Prove or disprove: Q/Z ∼= Q.\\n\\n14. Let G be a finite group and N a normal subgroup of G. If H is a subgroup of G/N ,\\nprove that ϕ−1(H) is a subgroup in G of order |H|·|N |, where ϕ : G→ G/N is the canonical\\nhomomorphism.\\n\\n15. Let G1 and G2 be groups, and let H1 and H2 be normal subgroups of G1 and G2\\n\\nrespectively. Let ϕ : G1 → G2 be a homomorphism. Show that ϕ induces a natural\\nhomomorphism ϕ : (G1/H1) → (G2/H2) if ϕ(H1) ⊆ H2.\\n\\n16. If H and K are normal subgroups of G and H ∩K = {e}, prove that G is isomorphic\\nto a subgroup of G/H ×G/K.\\n\\n17. Let ϕ : G1 → G2 be a surjective group homomorphism. Let H1 be a normal subgroup\\nof G1 and suppose that ϕ(H1) = H2. Prove or disprove that G1/H1\\n\\n∼= G2/H2.\\n\\n18. Let ϕ : G → H be a group homomorphism. Show that ϕ is one-to-one if and only if\\nϕ−1(e) = {e}.\\n\\n19. Given a homomorphism ϕ : G→ H define a relation ∼ on G by a ∼ b if ϕ(a) = ϕ(b) for\\na, b ∈ G. Show this relation is an equivalence relation and describe the equivalence classes.\\n\\n11.4 Additional Exercises: Automorphisms\\n1. Let Aut(G) be the set of all automorphisms of G; that is, isomorphisms from G to itself.\\nProve this set forms a group and is a subgroup of the group of permutations of G; that is,\\nAut(G) ≤ SG.\\n\\n2. An inner automorphism of G,\\n\\nig : G→ G,\\n\\nis defined by the map\\nig(x) = gxg−1,\\n\\nfor g ∈ G. Show that ig ∈ Aut(G).\\n\\n3. The set of all inner automorphisms is denoted by Inn(G). Show that Inn(G) is a subgroup\\nof Aut(G).\\n\\n4. Find an automorphism of a group G that is not an inner automorphism.\\n\\n5. Let G be a group and ig be an inner automorphism of G, and define a map\\n\\nG→ Aut(G)\\n\\nby\\ng 7→ ig.\\n\\nProve that this map is a homomorphism with image Inn(G) and kernel Z(G). Use this\\nresult to conclude that\\n\\nG/Z(G) ∼= Inn(G).\\n\\n\\n\\n186 CHAPTER 11. HOMOMORPHISMS\\n\\n6. Compute Aut(S3) and Inn(S3). Do the same thing for D4.\\n\\n7. Find all of the homomorphisms ϕ : Z → Z. What is Aut(Z)?\\n\\n8. Find all of the automorphisms of Z8. Prove that Aut(Z8) ∼= U(8).\\n\\n9. For k ∈ Zn, define a map ϕk : Zn → Zn by a 7→ ka. Prove that ϕk is a homomorphism.\\n\\n10. Prove that ϕk is an isomorphism if and only if k is a generator of Zn.\\n\\n11. Show that every automorphism of Zn is of the form ϕk, where k is a generator of Zn.\\n\\n12. Prove that ψ : U(n) → Aut(Zn) is an isomorphism, where ψ : k 7→ ϕk.\\n\\n11.5 Sage\\nSage is able to create homomorphisms (and by extension, isomorphisms and automorphisms)\\nbetween finite permutation groups. There is a limited supply of commands then available\\nto manipulate these functions, but we can still illustrate many of the ideas in this chapter.\\n\\nHomomorphisms\\nThe principal device for creating a homomorphism is to specify the specific images of the\\nset of generators for the domain. Consider cyclic groups of order 12 and 20:\\n\\nG = {ai|a12 = e} H = {xi|x20 = e}\\n\\nand define a homomorphism by just defining the image of the generator of G, and define\\nthe rest of the mapping by extending the mapping via the operation-preserving property of\\na homomorphism.\\n\\nϕ : G→ H, ϕ(a) = x5\\n\\n⇒ ϕ(ai) = ϕ(a)i = (x5)i = x5i\\n\\nThe constructor PermutationGroupMorphism requires the two groups, then a list of images for\\neach generator (in order!), and then will create the homomorphism. Note that we can then\\nuse the result as a function. In the example below, we first verify that C12 has a single\\ngenerator (no surprise there), which we then send to a particular element of order 4 in\\nthe codomain. Sage then constructs the unique homomorphism that is consistent with this\\nrequirement.\\n\\nC12 = CyclicPermutationGroup (12)\\nC20 = CyclicPermutationGroup (20)\\ndomain_gens = C12.gens()\\n[g.order () for g in domain_gens]\\n\\n[12]\\n\\nx = C20.gen (0)\\ny = x^5\\ny.order()\\n\\n4\\n\\nphi = PermutationGroupMorphism(C12 , C20 , [y])\\nphi\\n\\n\\n\\n11.5. SAGE 187\\n\\nPermutation group morphism:\\nFrom: Cyclic group of order 12 as a permutation group\\nTo: Cyclic group of order 20 as a permutation group\\nDefn: [(1,2,3,4,5,6,7,8,9,10,11,12)] ->\\n\\n[(1 ,6,11,16)(2,7,12,17)(3,8,13,18)(4,9,14,19)(5,10,15,20)]\\n\\na = C12(\\"(1,6,11,4,9,2,7,12,5,10,3,8)\\")\\nphi(a)\\n\\n(1,6,11,16)(2,7,12,17)(3,8,13,18)(4,9,14,19)(5,10,15,20)\\n\\nb = C12(\\"(1,3,5,7,9,11)(2,4,6,8,10,12)\\")\\nphi(b)\\n\\n(1,11)(2 ,12)(3 ,13) (4,14)(5,15)(6,16)(7,17)(8,18)(9,19) (10 ,20)\\n\\nc = C12(\\"(1,9,5)(2,10,6)(3,11,7)(4,12,8)\\")\\nphi(c)\\n\\n()\\n\\nNote that the element c must therefore be in the kernel of phi.\\nWe can then compute the subgroup of the domain that is the kernel, in this case a cyclic\\n\\ngroup of order 3 inside the cyclic group of order 12. We can compute the image of any\\nsubgroup, but here we will build the whole homomorphic image by supplying the whole\\ndomain to the .image() method. Here the image is a cyclic subgroup of order 4 inside the\\ncyclic group of order 20. Then we can verify the First Isomorphism Theorem.\\n\\nK = phi.kernel (); K\\n\\nSubgroup of (Cyclic group of order 12 as a permutation group)\\ngenerated by [(1,5,9)(2,6,10)(3,7,11)(4,8,12)]\\n\\nIm = phi.image(C12); Im\\n\\nSubgroup of (Cyclic group of order 20 as a permutation group)\\ngenerated by\\n\\n[(1 ,6,11 ,16)(2,7,12,17)(3,8,13,18)(4,9,14,19)(5,10,15,20)]\\n\\nIm.is_isomorphic(C12.quotient(K))\\n\\nTrue\\n\\nHere is a slightly more complicated example. The dihedral group D20 is the symmetry\\ngroup of a 20-gon. Inside this group is a subgroup that is isomorphic to the symmetry\\ngroup of a 5-gon (pentagon). Is this a surprise, or is this obvious? Here is a way to make\\nprecise the statement “D20 contains a copy of D5.”\\n\\nWe build the domain and find its generators, so we know how many images to supply in\\nthe definition of the homomorphism. Then we construct the codomain, from which we will\\nconstruct images. Our choice here is to send a reflection to a reflection, and a rotation to a\\nrotation. But the rotations will both have order 5, and both are a rotation by 72 degrees.\\n\\nG = DihedralGroup (5)\\nH = DihedralGroup (20)\\nG.gens()\\n\\n\\n\\n188 CHAPTER 11. HOMOMORPHISMS\\n\\n[(1,2,3,4,5), (1,5)(2,4)]\\n\\nH.gens()\\n\\n[(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20),\\n(1,20) (2,19) (3,18)(4,17)(5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11)]\\n\\nx = H.gen (0)^4\\ny = H.gen (1)\\nrho = PermutationGroupMorphism(G, H, [x, y])\\nrho.kernel ()\\n\\nSubgroup of (Dihedral group of order 10 as a permutation group)\\ngenerated by [()]\\n\\nSince the kernel is trivial, rho is a one-to-one function (see Exercise 11.3.18). But more\\nimportantly, by the First Isomorphishm Theorem, G is isomorphic to the image of the\\nhomomorphism. We compute the image and check the claim.\\n\\nIm = rho.image(G); Im\\n\\nSubgroup of (Dihedral group of order 40 as a permutation group)\\ngenerated by\\n[(1,5,9,13,17)(2,6,10,14,18)(3,7,11,15,19)(4,8,12,16,20),\\n(1,20)(2 ,19)(3 ,18) (4,17)(5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11)]\\n\\nIm.is_subgroup(H)\\n\\nTrue\\n\\nIm.is_isomorphic(G)\\n\\nTrue\\n\\nJust providing a list of images for the generators of the domain is no guarantee that\\nthe function will extend to a homomorphism. For starters, the order of each image must\\ndivide the order of the corresponding preimage. (Can you prove this?) And similarly,\\nif the domain is abelian, then the image must also be abelian, so in this case the list of\\nimages should not generate a non-abelian subgroup. Here is an example. There are no\\nhomomorphisms from a cyclic group of order 7 to a cyclic group of order 4 (other than the\\ntrivial function that takes every element to the identity). To see this, consider the possible\\norders of the kernel, and of the two possibilities, see that one is impossible and the other\\narises with the trivial homomorphism. Unfortunately, Sage acts as if nothing is wrong in\\ncreating a homomorphism between these groups, but what Sage builds is useless and raises\\nerrors when you try to use it.\\n\\nG = CyclicPermutationGroup (7)\\nH = CyclicPermutationGroup (4)\\ntau = PermutationGroupMorphism_im_gens(G, H, H.gens())\\ntau\\n\\nPermutation group morphism:\\nFrom: Cyclic group of order 7 as a permutation group\\nTo: Cyclic group of order 4 as a permutation group\\nDefn: [(1,2,3,4,5,6,7)] -> [(1,2,3,4)]\\n\\n\\n\\n11.5. SAGE 189\\n\\ntau.kernel ()\\n\\nTraceback (most recent call last):\\n...\\nRuntimeError: Gap produced error output\\n...\\n\\nRather than creating homomorphisms ourselves, in certain situations Sage knows of the\\nexistence of natural homomorphisms and will create them for you. One such case is a direct\\nproduct construction. Given a group G, the method .direct_product(H) will create the direct\\nproduct G×H. (This is not the same command as the function direct_product_permgroups()\\n\\nfrom before.) Not only does this command create the direct product, but it also builds four\\nhomomorphisms, one with domain G, one with domain H and two with domain G×H. So\\nthe output consists of five objects, the first being the actual group, and the remainder are\\nhomomorphisms. We will demonstrate the call here, and leave a more thorough investigation\\nfor the exercises.\\n\\nG = CyclicPermutationGroup (3)\\nH = DihedralGroup (4)\\nresults = G.direct_product(H)\\nresults [0]\\n\\nPermutation Group with generators [(4,5,6,7), (4,7)(5,6), (1,2,3)]\\n\\nresults [1]\\n\\nPermutation group morphism:\\nFrom: Cyclic group of order 3 as a permutation group\\nTo: Permutation Group with generators\\n\\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\\nDefn: Embedding( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 1 )\\n\\nresults [2]\\n\\nPermutation group morphism:\\nFrom: Dihedral group of order 8 as a permutation group\\nTo: Permutation Group with generators\\n\\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\\nDefn: Embedding( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 2 )\\n\\nresults [3]\\n\\nPermutation group morphism:\\nFrom: Permutation Group with generators\\n\\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\\nTo: Cyclic group of order 3 as a permutation group\\nDefn: Projection( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 1 )\\n\\nresults [4]\\n\\nPermutation group morphism:\\nFrom: Permutation Group with generators\\n\\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\\nTo: Dihedral group of order 8 as a permutation group\\nDefn: Projection( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 2 )\\n\\n\\n\\n190 CHAPTER 11. HOMOMORPHISMS\\n\\n11.6 Sage Exercises\\n1. An automorphism is an isomorphism between a group and itself. The identity function\\n(x 7→ x) is always an isomorphism, which we consider trivial. Use Sage to construct a\\nnontrivial automorphism of the cyclic group of order 12. Check that the mapping is both\\nonto and one-to-one by computing the image and kernel and performing the proper tests\\non these subgroups. Now construct all of the possible automorphisms of the cyclic group of\\norder 12 without any duplicates.\\n\\n2. The four homomorphisms created by the direct product construction are each an example\\nof a more general construction of homomorphisms involving groups G, H and G × H.\\nBy using the same groups as in the example in the previous subsection, see if you can\\ndiscover and describe these constructions with exact definitions of the four homomorphisms\\nin general.\\nYour tools for investigating a Sage group homomorphism are limited, you might take each\\ngenerator of the domain and see what its image is. Here is an example of the type of\\ncomputation you might do repeatedly. We’ll investigate the second homomorphism. The\\ndomain is the dihedral group, and we will compute the image of the first generator.\\n\\nG = CyclicPermutationGroup (3)\\nH = DihedralGroup (4)\\nresults = G.direct_product(H)\\nphi = results [2]\\nH.gens()\\n\\n[(1,2,3,4), (1,4)(2,3)]\\n\\na = H.gen (0); a\\n\\n(1,2,3,4)\\n\\nphi(a)\\n\\n(4,5,6,7)\\n\\n3. Consider two permutation groups. The first is the subgroup of S7 generated by (1, 2, 3)\\nand (4, 5, 6, 7). The second is a subgroup of S12 generated by (1, 2, 3)(4, 5, 6)(7, 8, 9)(10, 11, 12)\\nand (1, 10, 7, 4)(2, 11, 8, 5)(3, 12, 9, 6). Build these two groups and use the proper Sage com-\\nmand to see that they are isomorphic. Then construct a homomorphism between these two\\ngroups that is an isomorphism and include enough details to verify that the mapping is\\nreally an isomorphism.\\n\\n4. The second paragraph of this chapter informally describes a homomorphism from Sn to\\nZ2, where the even permutations all map to one of the elements and the odd permutations\\nall map to the other element. Replace Sn by S6 and replace Z2 by the permutation version\\nof the cyclic subgroup of order 2, and construct a nontrivial homomorphism between these\\ntwo groups. Evaluate your homomorphism with enough even and odd permutations to be\\nconvinced that it is correct. Then construct the kernel and verify that it is the group you\\nexpect.\\nHints: First, decide which elements of the group of order 2 will be associated with even\\npermutations and which will be associated with odd permutations. Then examine the\\ngenerators of S6 to help decide just how to build the homomorphism.\\n\\n\\n\\n11.6. SAGE EXERCISES 191\\n\\n5. The dihedral group D20 has several normal subgroups, as seen below. Each of these is\\nthe kernel of a homomorphism with D20 as the domain. For each normal subgroup of D20\\n\\nconstruct a homomorphism from D20 to D20 that has the normal subgroup as the kernel.\\nInclude in your work verifications that you are creating the desired kernels. There is a\\npattern to many of these, but the three of order 20 will be a challenge.\\n\\nG = DihedralGroup (20)\\n[H.order () for H in G.normal_subgroups ()]\\n\\n[1, 2, 4, 5, 10, 20, 20, 20, 40]\\n\\n\\n\\n12\\n\\nMatrix Groups and Symmetry\\n\\nWhen Felix Klein (1849–1925) accepted a chair at the University of Erlangen, he outlined in\\nhis inaugural address a program to classify different geometries. Central to Klein’s program\\nwas the theory of groups: he considered geometry to be the study of properties that are\\nleft invariant under transformation groups. Groups, especially matrix groups, have now\\nbecome important in the study of symmetry and have found applications in such disciplines\\nas chemistry and physics. In the first part of this chapter, we will examine some of the\\nclassical matrix groups, such as the general linear group, the special linear group, and the\\northogonal group. We will then use these matrix groups to investigate some of the ideas\\nbehind geometric symmetry.\\n\\n12.1 Matrix Groups\\nSome Facts from Linear Algebra\\nBefore we study matrix groups, we must recall some basic facts from linear algebra. One of\\nthe most fundamental ideas of linear algebra is that of a linear transformation. A linear\\ntransformation or linear map T : Rn → Rm is a map that preserves vector addition and\\nscalar multiplication; that is, for vectors x and y in Rn and a scalar α ∈ R,\\n\\nT (x + y) = T (x) + T (y)\\nT (αy) = αT (y).\\n\\nAn m × n matrix with entries in R represents a linear transformation from Rn to Rm. If\\nwe write vectors x = (x1, . . . , xn)\\n\\nt and y = (y1, . . . , yn)\\nt in Rn as column matrices, then an\\n\\nm× n matrix\\n\\nA =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\na11 a12 · · · a1n\\na21 a22 · · · a2n\\n...\\n\\n... . . . ...\\nam1 am2 · · · amn\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\nmaps the vectors to Rm linearly by matrix multiplication. Observe that if α is a real number,\\n\\nA(x + y) = Ax +Ay and αAx = A(αx),\\n\\nwhere\\n\\nx =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\nx1\\nx2\\n...\\nxn\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\n192\\n\\n\\n\\n12.1. MATRIX GROUPS 193\\n\\nWe will often abbreviate the matrix A by writing (aij).\\nConversely, if T : Rn → Rm is a linear map, we can associate a matrix A with T by\\n\\nconsidering what T does to the vectors\\n\\ne1 = (1, 0, . . . , 0)t\\n\\ne2 = (0, 1, . . . , 0)t\\n\\n...\\nen = (0, 0, . . . , 1)t.\\n\\nWe can write any vector x = (x1, . . . , xn)\\nt as\\n\\nx1e1 + x2e2 + · · ·+ xnen.\\n\\nConsequently, if\\n\\nT (e1) = (a11, a21, . . . , am1)\\nt,\\n\\nT (e2) = (a12, a22, . . . , am2)\\nt,\\n\\n...\\nT (en) = (a1n, a2n, . . . , amn)\\n\\nt,\\n\\nthen\\n\\nT (x) = T (x1e1 + x2e2 + · · ·+ xnen)\\n\\n= x1T (e1) + x2T (e2) + · · ·+ xnT (en)\\n\\n=\\n\\n(\\nn∑\\n\\nk=1\\n\\na1kxk, . . . ,\\nn∑\\n\\nk=1\\n\\namkxk\\n\\n)t\\n\\n= Ax.\\n\\nExample 12.1. If we let T : R2 → R2 be the map given by\\n\\nT (x1, x2) = (2x1 + 5x2,−4x1 + 3x2),\\n\\nthe axioms that T must satisfy to be a linear transformation are easily verified. The column\\nvectors Te1 = (2,−4)t and Te2 = (5, 3)t tell us that T is given by the matrix\\n\\nA =\\n\\n(\\n2 5\\n\\n−4 3\\n\\n)\\n.\\n\\nSince we are interested in groups of matrices, we need to know which matrices have\\nmultiplicative inverses. Recall that an n × n matrix A is invertible exactly when there\\nexists another matrix A−1 such that AA−1 = A−1A = I, where\\n\\nI =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n1 0 · · · 0\\n\\n0 1 · · · 0\\n...\\n\\n... . . . ...\\n0 0 · · · 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\nis the n×n identity matrix. From linear algebra we know that A is invertible if and only if\\nthe determinant of A is nonzero. Sometimes an invertible matrix is said to be nonsingular.\\n\\n\\n\\n194 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\\n\\nExample 12.2. If A is the matrix (\\n2 1\\n\\n5 3\\n\\n)\\n,\\n\\nthen the inverse of A is\\n\\nA−1 =\\n\\n(\\n3 −1\\n\\n−5 2\\n\\n)\\n.\\n\\nWe are guaranteed that A−1 exists, since det(A) = 2 · 3− 5 · 1 = 1 is nonzero.\\n\\nSome other facts about determinants will also prove useful in the course of this chapter.\\nLet A and B be n × n matrices. From linear algebra we have the following properties of\\ndeterminants.\\n\\n• The determinant is a homomorphism into the multiplicative group of real numbers;\\nthat is, det(AB) = (detA)(detB).\\n\\n• If A is an invertible matrix, then det(A−1) = 1/detA.\\n\\n• If we define the transpose of a matrix A = (aij) to be At = (aji), then det(At) = detA.\\n\\n• Let T be the linear transformation associated with an n × n matrix A. Then T\\nmultiplies volumes by a factor of |detA|. In the case of R2, this means that T\\nmultiplies areas by |detA|.\\n\\nLinear maps, matrices, and determinants are covered in any elementary linear algebra\\ntext; however, if you have not had a course in linear algebra, it is a straightforward process\\nto verify these properties directly for 2 × 2 matrices, the case with which we are most\\nconcerned.\\n\\nThe General and Special Linear Groups\\n\\nThe set of all n × n invertible matrices forms a group called the general linear group.\\nWe will denote this group by GLn(R). The general linear group has several important\\nsubgroups. The multiplicative properties of the determinant imply that the set of matrices\\nwith determinant one is a subgroup of the general linear group. Stated another way, suppose\\nthat det(A) = 1 and det(B) = 1. Then det(AB) = det(A) det(B) = 1 and det(A−1) =\\n1/ detA = 1. This subgroup is called the special linear group and is denoted by SLn(R).\\n\\nExample 12.3. Given a 2× 2 matrix\\n\\nA =\\n\\n(\\na b\\n\\nc d\\n\\n)\\n,\\n\\nthe determinant of A is ad− bc. The group GL2(R) consists of those matrices in which\\nad− bc ̸= 0. The inverse of A is\\n\\nA−1 =\\n1\\n\\nad− bc\\n\\n(\\nd −b\\n−c a\\n\\n)\\n.\\n\\nIf A is in SL2(R), then\\n\\nA−1 =\\n\\n(\\nd −b\\n−c a\\n\\n)\\n.\\n\\n\\n\\n12.1. MATRIX GROUPS 195\\n\\nGeometrically, SL2(R) is the group that preserves the areas of parallelograms. Let\\n\\nA =\\n\\n(\\n1 1\\n\\n0 1\\n\\n)\\nbe in SL2(R). In Figure 12.4, the unit square corresponding to the vectors x = (1, 0)t\\n\\nand y = (0, 1)t is taken by A to the parallelogram with sides (1, 0)t and (1, 1)t; that is,\\nAx = (1, 0)t and Ay = (1, 1)t. Notice that these two parallelograms have the same area.\\n\\ny\\n\\nx\\n\\n(0, 1)\\n\\n(1, 0)\\n\\ny\\n\\nx\\n\\n(1, 1)\\n\\n(1, 0)\\n\\nFigure 12.4: SL2(R) acting on the unit square\\n\\nThe Orthogonal Group O(n)\\n\\nAnother subgroup of GLn(R) is the orthogonal group. A matrix A is orthogonal if A−1 =\\nAt. The orthogonal group consists of the set of all orthogonal matrices. We write O(n)\\nfor the n× n orthogonal group. We leave as an exercise the proof that O(n) is a subgroup\\nof GLn(R).\\n\\nExample 12.5. The following matrices are orthogonal:\\n\\n(\\n3/5 −4/5\\n\\n4/5 3/5\\n\\n)\\n,\\n\\n(\\n1/2 −\\n\\n√\\n3/2√\\n\\n3/2 1/2\\n\\n)\\n,\\n\\n\uf8eb\uf8ed−1/\\n√\\n2 0 1/\\n\\n√\\n2\\n\\n1/\\n√\\n6 −2/\\n\\n√\\n6 1/\\n\\n√\\n6\\n\\n1/\\n√\\n3 1/\\n\\n√\\n3 1/\\n\\n√\\n3\\n\\n\uf8f6\uf8f8 .\\n\\nThere is a more geometric way of viewing the group O(n). The orthogonal matrices\\nare exactly those matrices that preserve the length of vectors. We can define the length\\nof a vector using the Euclidean inner product, or dot product, of two vectors. The\\nEuclidean inner product of two vectors x = (x1, . . . , xn)\\n\\nt and y = (y1, . . . , yn)\\nt is\\n\\n⟨x,y⟩ = xty = (x1, x2, . . . , xn)\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\ny1\\ny2\\n...\\nyn\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 = x1y1 + · · ·+ xnyn.\\n\\nWe define the length of a vector x = (x1, . . . , xn)\\nt to be\\n\\n∥x∥ =\\n√\\n\\n⟨x,x⟩ =\\n√\\nx21 + · · ·+ x2n.\\n\\nAssociated with the notion of the length of a vector is the idea of the distance between two\\nvectors. We define the distance between two vectors x and y to be ∥x − y∥. We leave as\\nan exercise the proof of the following proposition about the properties of Euclidean inner\\nproducts.\\n\\n\\n\\n196 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\\n\\nProposition 12.6. Let x, y, and w be vectors in Rn and α ∈ R. Then\\n\\n1. ⟨x,y⟩ = ⟨y,x⟩.\\n\\n2. ⟨x,y + w⟩ = ⟨x,y⟩+ ⟨x,w⟩.\\n\\n3. ⟨αx,y⟩ = ⟨x, αy⟩ = α⟨x,y⟩.\\n\\n4. ⟨x,x⟩ ≥ 0 with equality exactly when x = 0.\\n\\n5. If ⟨x,y⟩ = 0 for all x in Rn, then y = 0.\\n\\nExample 12.7. The vector x = (3, 4)t has length\\n√\\n32 + 42 = 5. We can also see that the\\n\\northogonal matrix\\n\\nA =\\n\\n(\\n3/5 −4/5\\n\\n4/5 3/5\\n\\n)\\npreserves the length of this vector. The vector Ax = (−7/5, 24/5)t also has length 5.\\n\\nSince det(AAt) = det(I) = 1 and det(A) = det(At), the determinant of any orthogonal\\nmatrix is either 1 or −1. Consider the column vectors\\n\\naj =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\na1j\\na2j\\n...\\nanj\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\nof the orthogonal matrix A = (aij). Since AAt = I, ⟨ar,as⟩ = δrs, where\\n\\nδrs =\\n\\n{\\n1 r = s\\n\\n0 r ̸= s\\n\\nis the Kronecker delta. Accordingly, column vectors of an orthogonal matrix all have length\\n1; and the Euclidean inner product of distinct column vectors is zero. Any set of vectors\\nsatisfying these properties is called an orthonormal set. Conversely, given an n×n matrix\\nA whose columns form an orthonormal set, it follows that A−1 = At.\\n\\nWe say that a matrixA is distance-preserving, length-preserving, or inner product-\\npreserving when ∥Tx − Ty∥ = ∥x − y∥∥Tx∥ = ∥x∥, or ⟨Tx, Ty⟩ = ⟨x,y⟩, respectively.\\nThe following theorem, which characterizes the orthogonal group, says that these notions\\nare the same.\\n\\nTheorem 12.8. Let A be an n× n matrix. The following statements are equivalent.\\n\\n1. The columns of the matrix A form an orthonormal set.\\n\\n2. A−1 = At.\\n\\n3. For vectors x and y, ⟨Ax, Ay⟩ = ⟨x,y⟩.\\n\\n4. For vectors x and y, ∥Ax −Ay∥ = ∥x − y∥.\\n\\n5. For any vector x, ∥Ax∥ = ∥x∥.\\n\\n\\n\\n12.1. MATRIX GROUPS 197\\n\\nProof. We have already shown (1) and (2) to be equivalent.\\n(2) ⇒ (3).\\n\\n⟨Ax, Ay⟩ = (Ax)tAy\\n= xtAtAy\\n= xty\\n= ⟨x,y⟩.\\n\\n(3) ⇒ (2). Since\\n⟨x,x⟩ = ⟨Ax, Ax⟩\\n\\n= xtAtAx\\n= ⟨x, AtAx⟩,\\n\\nwe know that ⟨x, (AtA− I)x⟩ = 0 for all x. Therefore, AtA− I = 0 or A−1 = At.\\n(3) ⇒ (4). If A is inner product-preserving, then A is distance-preserving, since\\n\\n∥Ax −Ay∥2 = ∥A(x − y)∥2\\n\\n= ⟨A(x − y), A(x − y)⟩\\n= ⟨x − y,x − y⟩\\n= ∥x − y∥2.\\n\\n(4) ⇒ (5). If A is distance-preserving, then A is length-preserving. Letting y = 0, we\\nhave\\n\\n∥Ax∥ = ∥Ax −Ay∥ = ∥x − y∥ = ∥x∥.\\n(5) ⇒ (3). We use the following identity to show that length-preserving implies inner\\n\\nproduct-preserving:\\n⟨x,y⟩ = 1\\n\\n2\\n\\n[\\n∥x + y∥2 − ∥x∥2 − ∥y∥2\\n\\n]\\n.\\n\\nObserve that\\n\\n⟨Ax, Ay⟩ = 1\\n\\n2\\n\\n[\\n∥Ax +Ay∥2 − ∥Ax∥2 − ∥Ay∥2\\n\\n]\\n=\\n\\n1\\n\\n2\\n\\n[\\n∥A(x + y)∥2 − ∥Ax∥2 − ∥Ay∥2\\n\\n]\\n=\\n\\n1\\n\\n2\\n\\n[\\n∥x + y∥2 − ∥x∥2 − ∥y∥2\\n\\n]\\n= ⟨x,y⟩.\\n\\ny\\n\\nx\\n\\n(a, b)\\n\\n(a,−b)\\n\\ny\\n\\nx\\n\\n(cos θ, sin θ)\\n\\n(sin θ,− cos θ)\\n\\nθ\\n\\nFigure 12.9: O(2) acting on R2\\n\\n\\n\\n198 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\\n\\nExample 12.10. Let us examine the orthogonal group on R2 a bit more closely. An element\\nT ∈ O(2) is determined by its action on e1 = (1, 0)t and e2 = (0, 1)t. If T (e1) = (a, b)t,\\nthen a2 + b2 = 1 and T (e2) = (−b, a)t. Hence, T can be represented by\\n\\nA =\\n\\n(\\na −b\\nb a\\n\\n)\\n=\\n\\n(\\ncos θ − sin θ\\nsin θ cos θ\\n\\n)\\n,\\n\\nwhere 0 ≤ θ < 2π. A matrix T in O(2) either reflects or rotates a vector in R2 (Figure 12.9).\\nA reflection about the horizontal axis is given by the matrix(\\n\\n1 0\\n\\n0 −1\\n\\n)\\n,\\n\\nwhereas a rotation by an angle θ in a counterclockwise direction must come from a matrix\\nof the form (\\n\\ncos θ sin θ\\nsin θ − cos θ\\n\\n)\\n.\\n\\nA reflection about a line ℓ is simply a reflection about the horizontal axis followed by a\\nrotation. If detA = −1, then A gives a reflection.\\n\\nTwo of the other matrix or matrix-related groups that we will consider are the special\\northogonal group and the group of Euclidean motions. The special orthogonal group,\\nSO(n), is just the intersection of O(n) and SLn(R); that is, those elements in O(n) with\\ndeterminant one. The Euclidean group, E(n), can be written as ordered pairs (A,x),\\nwhere A is in O(n) and x is in Rn. We define multiplication by\\n\\n(A,x)(B,y) = (AB,Ay + x).\\nThe identity of the group is (I,0); the inverse of (A,x) is (A−1,−A−1x). In Exercise 12.3.6,\\nyou are asked to check that E(n) is indeed a group under this operation.\\n\\ny\\n\\nx\\n\\nx\\n\\ny\\n\\nx\\n\\nx + y\\n\\nFigure 12.11: Translations in R2\\n\\n12.2 Symmetry\\nAn isometry or rigid motion in Rn is a distance-preserving function f from Rn to Rn.\\nThis means that f must satisfy\\n\\n∥f(x)− f(y)∥ = ∥x − y∥\\n\\nfor all x,y ∈ Rn. It is not difficult to show that f must be a one-to-one map. By Theo-\\nrem 12.8, any element in O(n) is an isometry on Rn; however, O(n) does not include all\\n\\n\\n\\n12.2. SYMMETRY 199\\n\\npossible isometries on Rn. Translation by a vector x, Ty(x) = x + y is also an isometry\\n(Figure 12.11); however, T cannot be in O(n) since it is not a linear map.\\n\\nWe are mostly interested in isometries in R2. In fact, the only isometries in R2 are\\nrotations and reflections about the origin, translations, and combinations of the two. For\\nexample, a glide reflection is a translation followed by a reflection (Figure 12.12). In Rn\\n\\nall isometries are given in the same manner. The proof is very easy to generalize.\\n\\ny\\n\\nx\\n\\nx\\n\\ny\\n\\nx\\n\\nT (x)\\n\\nFigure 12.12: Glide reflections\\n\\nLemma 12.13. An isometry f that fixes the origin in R2 is a linear transformation. In\\nparticular, f is given by an element in O(2).\\n\\nProof. Let f be an isometry in R2 fixing the origin. We will first show that f preserves\\ninner products. Since f(0) = 0, ∥f(x)∥ = ∥x∥; therefore,\\n\\n∥x∥2 − 2⟨f(x), f(y)⟩+ ∥y∥2 = ∥f(x)∥2 − 2⟨f(x), f(y)⟩+ ∥f(y)∥2\\n\\n= ⟨f(x)− f(y), f(x)− f(y)⟩\\n= ∥f(x)− f(y)∥2\\n\\n= ∥x − y∥2\\n\\n= ⟨x − y,x − y⟩\\n= ∥x∥2 − 2⟨x,y⟩+ ∥y∥2.\\n\\nConsequently,\\n⟨f(x), f(y)⟩ = ⟨x,y⟩.\\n\\nNow let e1 and e2 be (1, 0)t and (0, 1)t, respectively. If\\n\\nx = (x1, x2) = x1e1 + x2e2,\\n\\nthen\\nf(x) = ⟨f(x), f(e1)⟩f(e1) + ⟨f(x), f(e2)⟩f(e2) = x1f(e1) + x2f(e2).\\n\\nThe linearity of f easily follows.\\n\\nFor any arbitrary isometry, f , Txf will fix the origin for some vector x in R2; hence,\\nTxf(y) = Ay for some matrix A ∈ O(2). Consequently, f(y) = Ay+x. Given the isometries\\n\\nf(y) = Ay + x1\\n\\ng(y) = By + x2,\\n\\n\\n\\n200 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\\n\\ntheir composition is\\n\\nf(g(y)) = f(By + x2) = ABy +Ax2 + x1.\\n\\nThis last computation allows us to identify the group of isometries on R2 with E(2).\\n\\nTheorem 12.14. The group of isometries on R2 is the Euclidean group, E(2).\\n\\nA symmetry group in Rn is a subgroup of the group of isometries on Rn that fixes a\\nset of points X ⊂ R2. It is important to realize that the symmetry group of X depends\\nboth on Rn and on X. For example, the symmetry group of the origin in R1 is Z2, but the\\nsymmetry group of the origin in R2 is O(2).\\n\\nTheorem 12.15. The only finite symmetry groups in R2 are Zn and Dn.\\n\\nProof. Let G = {f1, f2, . . . , fn} be a finite symmetry group that fixes a set of points in\\nX ⊂ R2. Choose a point x ∈ X. This point may not be a fixed point—it could be moved\\nby G to another point in X. Define a set S = {y1,y2, . . .yn}, where yi = fi(x). Now, let\\n\\nz =\\n1\\n\\nn\\n\\nn∑\\ni=1\\n\\nxi.\\n\\nWhile the point z is not necessarily in the set X, it is fixed by every element in the symetry\\ngroup. Without loss of generality, we may now assume that z is the origin.\\n\\nAny finite symmetry group G in R2 that fixes the origin must be a finite subgroup\\nof O(2), since translations and glide reflections have infinite order. By Example 12.10,\\nelements in O(2) are either rotations of the form\\n\\nRθ =\\n\\n(\\ncos θ − sin θ\\nsin θ cos θ\\n\\n)\\nor reflections of the form\\n\\nTϕ =\\n\\n(\\ncosϕ − sinϕ\\nsinϕ cosϕ\\n\\n)(\\n1 0\\n\\n0 −1\\n\\n)\\n=\\n\\n(\\ncosϕ sinϕ\\nsinϕ − cosϕ\\n\\n)\\n.\\n\\nNotice that det(Rθ) = 1, det(Tϕ) = −1, and T 2\\nϕ = I. We can divide the proof up into two\\n\\ncases. In the first case, all of the elements in G have determinant one. In the second case,\\nthere exists at least one element in G with determinant −1.\\n\\nCase 1. The determinant of every element in G is one. In this case every element in\\nG must be a rotation. Since G is finite, there is a smallest angle, say θ0, such that the\\ncorresponding element Rθ0 is the smallest rotation in the positive direction. We claim that\\nRθ0 generates G. If not, then for some positive integer n there is an angle θ1 between nθ0\\nand (n + 1)θ0. If so, then (n + 1)θ0 − θ1 corresponds to a rotation smaller than θ0, which\\ncontradicts the minimality of θ0.\\n\\nCase 2. The group G contains a reflection T . The kernel of the homomorphism ϕ :\\nG→ {−1, 1} given by A 7→ det(A) consists of elements whose determinant is 1. Therefore,\\n|G/ kerϕ| = 2. We know that the kernel is cyclic by the first case and is a subgroup of G\\nof, say, order n. Hence, |G| = 2n. The elements of G are\\n\\nRθ, . . . , R\\nn−1\\nθ , TRθ, . . . , TR\\n\\nn−1\\nθ .\\n\\nThese elements satisfy the relation\\n\\nTRθT = R−1\\nθ .\\n\\nConsequently, G must be isomorphic to Dn in this case.\\n\\n\\n\\n12.2. SYMMETRY 201\\n\\nThe Wallpaper Groups\\n\\nSuppose that we wish to study wallpaper patterns in the plane or crystals in three dimen-\\nsions. Wallpaper patterns are simply repeating patterns in the plane (Figure 12.16). The\\nanalogs of wallpaper patterns in R3 are crystals, which we can think of as repeating pat-\\nterns of molecules in three dimensions (Figure 12.17). The mathematical equivalent of a\\nwallpaper or crystal pattern is called a lattice.\\n\\nFigure 12.16: A wallpaper pattern in R2\\n\\nFigure 12.17: A crystal structure in R3\\n\\nLet us examine wallpaper patterns in the plane a little more closely. Suppose that x\\nand y are linearly independent vectors in R2; that is, one vector cannot be a scalar multiple\\nof the other. A lattice of x and y is the set of all linear combinations mx + ny, where m\\nand n are integers. The vectors x and y are said to be a basis for the lattice.\\n\\nNotice that a lattice can have several bases. For example, the vectors (1, 1)t and (2, 0)t\\n\\nhave the same lattice as the vectors (−1, 1)t and (−1,−1)t (Figure 12.18). However, any\\nlattice is completely determined by a basis. Given two bases for the same lattice, say\\n{x1,x2} and {y1,y2}, we can write\\n\\ny1 = α1x1 + α2x2\\n\\ny2 = β1x1 + β2x2,\\n\\nwhere α1, α2, β1, and β2 are integers. The matrix corresponding to this transformation is\\n\\nU =\\n\\n(\\nα1 α2\\n\\nβ1 β2\\n\\n)\\n.\\n\\n\\n\\n202 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\\n\\nIf we wish to give x1 and x2 in terms of y1 and y2, we need only calculate U−1; that is,\\n\\nU−1\\n\\n(\\ny1\\n\\ny2\\n\\n)\\n=\\n\\n(\\nx1\\n\\nx2\\n\\n)\\n.\\n\\nSince U has integer entries, U−1 must also have integer entries; hence the determinants of\\nboth U and U−1 must be integers. Because UU−1 = I,\\n\\ndet(UU−1) = det(U)det(U−1) = 1;\\n\\nconsequently, det(U) = ±1. A matrix with determinant ±1 and integer entries is called\\nunimodular. For example, the matrix (\\n\\n3 1\\n\\n5 2\\n\\n)\\nis unimodular. It should be clear that there is a minimum length for vectors in a lattice.\\n\\n(2, 0)\\n\\n(1, 1)(−1, 1)\\n\\n(−1,−1)\\n\\nFigure 12.18: A lattice in R2\\n\\nWe can classify lattices by studying their symmetry groups. The symmetry group of a\\nlattice is the subgroup of E(2) that maps the lattice to itself. We consider two lattices in R2\\n\\nto be equivalent if they have the same symmetry group. Similarly, classification of crystals\\nin R3 is accomplished by associating a symmetry group, called a space group, with each\\ntype of crystal. Two lattices are considered different if their space groups are not the same.\\nThe natural question that now arises is how many space groups exist.\\n\\nA space group is composed of two parts: a translation subgroup and a point. The\\ntranslation subgroup is an infinite abelian subgroup of the space group made up of the\\ntranslational symmetries of the crystal; the point group is a finite group consisting of ro-\\ntations and reflections of the crystal about a point. More specifically, a space group is a\\nsubgroup of G ⊂ E(2) whose translations are a set of the form {(I, t) : t ∈ L}, where L is a\\nlattice. Space groups are, of course, infinite. Using geometric arguments, we can prove the\\nfollowing theorem (see [5] or [6]).\\n\\nTheorem 12.19. Every translation group in R2 is isomorphic to Z× Z.\\n\\nThe point group of G is G0 = {A : (A, b) ∈ G for some b}. In particular, G0 must be a\\nsubgroup of O(2). Suppose that x is a vector in a lattice L with space group G, translation\\ngroup H, and point group G0. For any element (A,y) in G,\\n\\n(A,y)(I,x)(A,y)−1 = (A,Ax + y)(A−1,−A−1y)\\n= (AA−1,−AA−1y +Ax + y)\\n= (I,Ax);\\n\\n\\n\\n12.2. SYMMETRY 203\\n\\nhence, (I, Ax) is in the translation group of G. More specifically, Ax must be in the lattice\\nL. It is important to note that G0 is not usually a subgroup of the space group G; however,\\nif T is the translation subgroup of G, then G/T ∼= G0. The proof of the following theorem\\ncan be found in [2], [5], or [6].\\n\\nTheorem 12.20. The point group in the wallpaper groups is isomorphic to Zn or Dn,\\nwhere n = 1, 2, 3, 4, 6.\\n\\nTo answer the question of how the point groups and the translation groups can be\\ncombined, we must look at the different types of lattices. Lattices can be classified by the\\nstructure of a single lattice cell. The possible cell shapes are parallelogram, rectangular,\\nsquare, rhombic, and hexagonal (Figure 12.21). The wallpaper groups can now be classified\\naccording to the types of reflections that occur in each group: these are ordinarily reflections,\\nglide reflections, both, or none.\\n\\nRectangular\\nSquare Rhombic\\n\\nParallelogram\\nHexagonal\\n\\nFigure 12.21: Types of lattices in R2\\n\\n\\n\\n204 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\\n\\nNotation and Reflections or\\nSpace Groups Point Group Lattice Type Glide Reflections?\\n\\np1 Z1 parallelogram none\\np2 Z2 parallelogram none\\np3 Z3 hexagonal none\\np4 Z4 square none\\np6 Z6 hexagonal none\\npm D1 rectangular reflections\\npg D1 rectangular glide reflections\\ncm D1 rhombic both\\n\\npmm D2 rectangular reflections\\npmg D2 rectangular glide reflections\\npgg D2 rectangular both\\n\\nc2mm D2 rhombic both\\np3m1, p31m D3 hexagonal both\\n\\np4m, p4g D4 square both\\np6m D6 hexagonal both\\n\\nTable 12.22: The 17 wallpaper groups\\n\\nTheorem 12.23. There are exactly 17 wallpaper groups.\\n\\np4m p4g\\n\\nFigure 12.24: The wallpaper groups p4m and p4g\\n\\nThe 17 wallpaper groups are listed in Table 12.22. The groups p3m1 and p31m can\\nbe distinguished by whether or not all of their threefold centers lie on the reflection axes:\\nthose of p3m1 must, whereas those of p31m may not. Similarly, the fourfold centers of p4m\\nmust lie on the reflection axes whereas those of p4g need not (Figure 12.24). The complete\\nproof of this theorem can be found in several of the references at the end of this chapter,\\nincluding [5], [6], [10], and [11].\\n\\nHistorical Note\\n\\nSymmetry groups have intrigued mathematicians for a long time. Leonardo da Vinci\\nwas probably the first person to know all of the point groups. At the International Congress\\nof Mathematicians in 1900, David Hilbert gave a now-famous address outlining 23 problems\\nto guide mathematics in the twentieth century. Hilbert’s eighteenth problem asked whether\\nor not crystallographic groups in n dimensions were always finite. In 1910, L. Bieberbach\\n\\n\\n\\n12.3. EXERCISES 205\\n\\nproved that crystallographic groups are finite in every dimension. Finding out how many\\nof these groups there are in each dimension is another matter. In R3 there are 230 different\\nspace groups; in R4 there are 4783. No one has been able to compute the number of\\nspace groups for R5 and beyond. It is interesting to note that the crystallographic groups\\nwere found mathematically for R3 before the 230 different types of crystals were actually\\ndiscovered in nature.\\n\\n12.3 Exercises\\n1. Prove the identity\\n\\n⟨x,y⟩ = 1\\n\\n2\\n\\n[\\n∥x + y∥2 − ∥x∥2 − ∥y∥2\\n\\n]\\n.\\n\\n2. Show that O(n) is a group.\\n\\n3. Prove that the following matrices are orthogonal. Are any of these matrices in SO(n)?\\n\\n(a) (\\n1/\\n\\n√\\n2 −1/\\n\\n√\\n2\\n\\n1/\\n√\\n2 1/\\n\\n√\\n2\\n\\n)\\n\\n(b) (\\n1/\\n\\n√\\n5 2/\\n\\n√\\n5\\n\\n−2/\\n√\\n5 1/\\n\\n√\\n5\\n\\n)\\n\\n(c) \uf8eb\uf8ed 4/\\n√\\n5 0 3/\\n\\n√\\n5\\n\\n−3/\\n√\\n5 0 4/\\n\\n√\\n5\\n\\n0 −1 0\\n\\n\uf8f6\uf8f8\\n(d) \uf8eb\uf8ed 1/3 2/3 −2/3\\n\\n−2/3 2/3 1/3\\n\\n−2/3 1/3 2/3\\n\\n\uf8f6\uf8f8\\n4. Determine the symmetry group of each of the figures in Figure 12.25.\\n\\n(b)\\n\\n(a)\\n(c)\\n\\nFigure 12.25\\n\\n5. Let x, y, and w be vectors in Rn and α ∈ R. Prove each of the following properties of\\ninner products.\\n(a) ⟨x,y⟩ = ⟨y,x⟩.\\n(b) ⟨x,y + w⟩ = ⟨x,y⟩+ ⟨x,w⟩.\\n(c) ⟨αx,y⟩ = ⟨x, αy⟩ = α⟨x,y⟩.\\n(d) ⟨x,x⟩ ≥ 0 with equality exactly when x = 0.\\n(e) If ⟨x,y⟩ = 0 for all x in Rn, then y = 0.\\n\\n\\n\\n206 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\\n\\n6. Verify that\\nE(n) = {(A,x) : A ∈ O(n) and x ∈ Rn}\\n\\nis a group.\\n\\n7. Prove that {(2, 1), (1, 1)} and {(12, 5), (7, 3)} are bases for the same lattice.\\n\\n8. Let G be a subgroup of E(2) and suppose that T is the translation subgroup of G. Prove\\nthat the point group of G is isomorphic to G/T .\\n\\n9. Let A ∈ SL2(R) and suppose that the vectors x and y form two sides of a parallelogram\\nin R2. Prove that the area of this parallelogram is the same as the area of the parallelogram\\nwith sides Ax and Ay.\\n\\n10. Prove that SO(n) is a normal subgroup of O(n).\\n\\n11. Show that any isometry f in Rn is a one-to-one map.\\n\\n12. Prove or disprove: an element in E(2) of the form (A,x), where x ̸= 0, has infinite\\norder.\\n\\n13. Prove or disprove: There exists an infinite abelian subgroup of O(n).\\n\\n14. Let x = (x1, x2) be a point on the unit circle in R2; that is, x21 + x22 = 1. If A ∈ O(2),\\nshow that Ax is also a point on the unit circle.\\n\\n15. Let G be a group with a subgroup H (not necessarily normal) and a normal subgroup\\nN . Then G is a semidirect product of N by H if\\n\\n• H ∩N = {id};\\n• HN = G.\\n\\nShow that each of the following is true.\\n(a) S3 is the semidirect product of A3 by H = {(1), (12)}.\\n(b) The quaternion group, Q8, cannot be written as a semidirect product.\\n(c) E(2) is the semidirect product of O(2) by H, where H consists of all translations in\\n\\nR2.\\n\\n16. Determine which of the 17 wallpaper groups preserves the symmetry of the pattern in\\nFigure 12.16.\\n\\n17. Determine which of the 17 wallpaper groups preserves the symmetry of the pattern in\\nFigure 12.26.\\n\\nFigure 12.26\\n\\n\\n\\n12.4. REFERENCES AND SUGGESTED READINGS 207\\n\\n18. Find the rotation group of a dodecahedron.\\n\\n19. For each of the 17 wallpaper groups, draw a wallpaper pattern having that group as a\\nsymmetry group.\\n\\n12.4 References and Suggested Readings\\n[1] Coxeter, H. M. and Moser, W. O. J. Generators and Relations for Discrete Groups,\\n\\n3rd ed. Springer-Verlag, New York, 1972.\\n[2] Grove, L. C. and Benson, C. T. Finite Reflection Groups. 2nd ed. Springer-Verlag,\\n\\nNew York, 1985.\\n[3] Hiller, H. “Crystallography and Cohomology of Groups,” American Mathematical\\n\\nMonthly 93 (1986), 765–79.\\n[4] Lockwood, E. H. and Macmillan, R. H. Geometric Symmetry. Cambridge University\\n\\nPress, Cambridge, 1978.\\n[5] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\\n[6] Martin, G. Transformation Groups: An Introduction to Symmetry. Springer-Verlag,\\n\\nNew York, 1982.\\n[7] Milnor, J. “Hilbert’s Problem 18: On Crystallographic Groups, Fundamental Do-\\n\\nmains, and Sphere Packing,” t Proceedings of Symposia in Pure Mathematics 18,\\nAmerican Mathematical Society, 1976.\\n\\n[8] Phillips, F. C. An Introduction to Crystallography. 4th ed. Wiley, New York, 1971.\\n[9] Rose, B. I. and Stafford, R. D. “An Elementary Course in Mathematical Symmetry,”\\n\\nAmerican Mathematical Monthly 88 (1980), 54–64.\\n[10] Schattschneider, D. “The Plane Symmetry Groups: Their Recognition and Their\\n\\nNotation,” American Mathematical Monthly 85(1978), 439–50.\\n[11] Schwarzenberger, R. L. “The 17 Plane Symmetry Groups,” Mathematical Gazette\\n\\n58(1974), 123–31.\\n[12] Weyl, H. Symmetry. Princeton University Press, Princeton, NJ, 1952.\\n\\n12.5 Sage\\nThere is no Sage material for this chapter.\\n\\n12.6 Sage Exercises\\nThere are no Sage exercises for this chapter.\\n\\n\\n\\n13\\n\\nThe Structure of Groups\\n\\nThe ultimate goal of group theory is to classify all groups up to isomorphism; that is, given a\\nparticular group, we should be able to match it up with a known group via an isomorphism.\\nFor example, we have already proved that any finite cyclic group of order n is isomorphic\\nto Zn; hence, we “know” all finite cyclic groups. It is probably not reasonable to expect\\nthat we will ever know all groups; however, we can often classify certain types of groups or\\ndistinguish between groups in special cases.\\n\\nIn this chapter we will characterize all finite abelian groups. We shall also investigate\\ngroups with sequences of subgroups. If a group has a sequence of subgroups, say\\n\\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e},\\n\\nwhere each subgroup Hi is normal in Hi+1 and each of the factor groups Hi+1/Hi is abelian,\\nthen G is a solvable group. In addition to allowing us to distinguish between certain classes\\nof groups, solvable groups turn out to be central to the study of solutions to polynomial\\nequations.\\n\\n13.1 Finite Abelian Groups\\nIn our investigation of cyclic groups we found that every group of prime order was isomorphic\\nto Zp, where p was a prime number. We also determined that Zmn\\n\\n∼= Zm × Zn when\\ngcd(m,n) = 1. In fact, much more is true. Every finite abelian group is isomorphic to a\\ndirect product of cyclic groups of prime power order; that is, every finite abelian group is\\nisomorphic to a group of the type\\n\\nZp\\nα1\\n1\\n\\n× · · · × Zpαn\\nn\\n,\\n\\nwhere each pk is prime (not necessarily distinct).\\nFirst, let us examine a slight generalization of finite abelian groups. Suppose that G\\n\\nis a group and let {gi} be a set of elements in G, where i is in some index set I (not\\nnecessarily finite). The smallest subgroup of G containing all of the gi’s is the subgroup of\\nG generated by the gi’s. If this subgroup of G is in fact all of G, then G is generated by\\nthe set {gi : i ∈ I}. In this case the gi’s are said to be the generators of G. If there is a\\nfinite set {gi : i ∈ I} that generates G, then G is finitely generated.\\n\\nExample 13.1. Obviously, all finite groups are finitely generated. For example, the group\\nS3 is generated by the permutations (12) and (123). The group Z×Zn is an infinite group\\nbut is finitely generated by {(1, 0), (0, 1)}.\\n\\nExample 13.2. Not all groups are finitely generated. Consider the rational numbers Q\\nunder the operation of addition. Suppose that Q is finitely generated with generators\\n\\n208\\n\\n\\n\\n13.1. FINITE ABELIAN GROUPS 209\\n\\np1/q1, . . . , pn/qn, where each pi/qi is a fraction expressed in its lowest terms. Let p be some\\nprime that does not divide any of the denominators q1, . . . , qn. We claim that 1/p cannot\\nbe in the subgroup of Q that is generated by p1/q1, . . . , pn/qn, since p does not divide the\\ndenominator of any element in this subgroup. This fact is easy to see since the sum of any\\ntwo generators is\\n\\npi/qi + pj/qj = (piqj + pjqi)/(qiqj).\\n\\nProposition 13.3. Let H be the subgroup of a group G that is generated by {gi ∈ G : i ∈ I}.\\nThen h ∈ H exactly when it is a product of the form\\n\\nh = gα1\\ni1\\n\\n· · · gαn\\nin\\n,\\n\\nwhere the giks are not necessarily distinct.\\n\\nProof. Let K be the set of all products of the form gα1\\ni1\\n\\n· · · gαn\\nin\\n\\n, where the giks are not\\nnecessarily distinct. Certainly K is a subset of H. We need only show that K is a subgroup\\nof G. If this is the case, then K = H, since H is the smallest subgroup containing all the\\ngis.\\n\\nClearly, the set K is closed under the group operation. Since g0i = 1, the identity is in\\nK. It remains to show that the inverse of an element g = gk1i1 · · · gknin in K must also be in\\nK. However,\\n\\ng−1 = (gk1i1 · · · gknin )\\n−1 = (g−kn\\n\\nin\\n· · · g−k1\\n\\ni1\\n).\\n\\nThe reason that powers of a fixed gi may occur several times in the product is that we\\nmay have a nonabelian group. However, if the group is abelian, then the gis need occur\\nonly once. For example, a product such as a−3b5a7 in an abelian group could always be\\nsimplified (in this case, to a4b5).\\n\\nNow let us restrict our attention to finite abelian groups. We can express any finite\\nabelian group as a finite direct product of cyclic groups. More specifically, letting p be\\nprime, we define a group G to be a p-group if every element in G has as its order a power\\nof p. For example, both Z2 × Z2 and Z4 are 2-groups, whereas Z27 is a 3-group. We shall\\nprove the Fundamental Theorem of Finite Abelian Groups which tells us that every finite\\nabelian group is isomorphic to a direct product of cyclic p-groups.\\n\\nTheorem 13.4 (Fundamental Theorem of Finite Abelian Groups). Every finite abelian\\ngroup G is isomorphic to a direct product of cyclic groups of the form\\n\\nZp\\nα1\\n1\\n\\n× Zp\\nα2\\n2\\n\\n× · · · × Zpαn\\nn\\n\\nhere the pi’s are primes (not necessarily distinct).\\n\\nExample 13.5. Suppose that we wish to classify all abelian groups of order 540 = 22 ·33 ·5.\\nThe Fundamental Theorem of Finite Abelian Groups tells us that we have the following six\\npossibilities.\\n\\n• Z2 × Z2 × Z3 × Z3 × Z3 × Z5;\\n\\n• Z2 × Z2 × Z3 × Z9 × Z5;\\n\\n• Z2 × Z2 × Z27 × Z5;\\n\\n• Z4 × Z3 × Z3 × Z3 × Z5;\\n\\n• Z4 × Z3 × Z9 × Z5;\\n\\n\\n\\n210 CHAPTER 13. THE STRUCTURE OF GROUPS\\n\\n• Z4 × Z27 × Z5.\\n\\nThe proof of the Fundamental Theorem of Finite Abelian Groups depends on several\\nlemmas.\\n\\nLemma 13.6. Let G be a finite abelian group of order n. If p is a prime that divides n,\\nthen G contains an element of order p.\\n\\nProof. We will prove this lemma by induction. If n = 1, then there is nothing to show.\\nNow suppose that the order of G is n the lemma is true for all groups of order k, where\\nk < n. Furthermore, let p be a prime that divides n.\\n\\nIf G has no proper nontrivial subgroups, then G = ⟨a⟩, where a is any element other\\nthan the identity. By Exercise 4.4.39, the order of G must be prime. Since p divides n, we\\nknow that p = n, and G contains p− 1 elements of order p.\\n\\nNow suppose that G contains a nontrivial proper subgroup H. Then 1 < |H| < n. If\\np | |H|, then H contains an element of order p by induction and the lemma is true. Suppose\\nthat p does not divide the order of H. Since G is abelian, it must be the case that H is a\\nnormal subgroup of G, and |G| = |H| · |G/H|. Consequently, p must divide |G/H|. Since\\n|G/H| < |G| = n, we know that G/H contains an element aH of order p by the induction\\nhypothesis. Thus,\\n\\nH = (aH)p = apH,\\n\\nand ap ∈ H but a /∈ H. If |H| = r, then p and r are relatively prime, and there exist\\nintegers s and t such that sp + tr = 1. Furthermore, the order of ap must divide r, and\\n(ap)r = (ar)p = 1.\\n\\nWe claim that ar has order p. We must show that ar ̸= 1. Suppose ar = 1. Then\\n\\na = asp+tr\\n\\n= aspatr\\n\\n= (ap)s(ar)t\\n\\n= (ap)s1\\n\\n= (ap)s.\\n\\nSince ap ∈ H, it must be the case that a = (ap)s ∈ H, which is a contradiction. Therefore,\\nar ̸= 1 is an element of order p in G.\\n\\nLemma 13.6 is a special case of Cauchy’s Theorem (Theorem 15.1, which states that if\\nG be a finite group and p a prime such that p divides the order of G, then G contains a\\nsubgroup of order p. We will prove Cauchy’s Theorem in Chapter 15.\\n\\nLemma 13.7. A finite abelian group is a p-group if and only if its order is a power of p.\\n\\nProof. If |G| = pn then by Lagrange’s theorem, then the order of any g ∈ G must divide\\npn, and therefore must be a power of p. Conversely, if |G| is not a power of p, then it has\\nsome other prime divisor q, so by Lemma 13.6, G has an element of order q and thus is not\\na p-group.\\n\\nLemma 13.8. Let G be a finite abelian group of order n = pα1\\n1 · · · pαk\\n\\nk , where where p1, . . . , pk\\nare distinct primes and α1, α2, . . . , αk are positive integers. Then G is the internal direct\\nproduct of subgroups G1, G2, . . . , Gk, where Gi is the subgroup of G consisting of all elements\\nof order pki for some integer k.\\n\\n\\n\\n13.1. FINITE ABELIAN GROUPS 211\\n\\nProof. Since G is an abelian group, we are guaranteed that Gi is a subgroup of G for\\ni = 1, . . . , n. Since the identity has order p0i = 1, we know that 1 ∈ Gi. If g ∈ Gi has order\\npri , then g−1 must also have order pri . Finally, if h ∈ Gi has order psi , then\\n\\n(gh)p\\nt\\ni = gp\\n\\nt\\nihp\\n\\nt\\ni = 1 · 1 = 1,\\n\\nwhere t is the maximum of r and s.\\nWe must show that\\n\\nG = G1G2 · · ·Gn\\n\\nand Gi ∩ Gj = {1} for i ̸= j. Suppose that g1 ∈ G1 is in the subgroup generated by\\nG2, G3, . . . , Gk. Then g1 = g2g3 · · · gk for gi ∈ Gi. Since gi has order pαi , we know that\\ngp\\n\\nαi\\n\\ni = 1 for i = 2, 3, . . . , k, and g\\np\\nα2\\n2 ···pαk\\n\\nk\\n1 = 1. Since the order of g1 is a power of p1\\n\\nand gcd(p1, pα2\\n2 · · · pαk\\n\\nk ) = 1, it must be the case that g1 = 1 and the intersection of G1\\n\\nwith any of the subgroups G2, G3, . . . , Gk is the identity. A similar argument shows that\\nGi ∩ Gj = {1} for i ̸= j. Hence, G1G2 · · ·Gn is an internal direct product of subgroups.\\nSince\\n\\n|G1G2 · · ·Gk| = pα1\\n1 · · · pαk\\n\\nk = |G|,\\n\\nit follows that G = G1G2 · · ·Gk.\\n\\nIf remains for us to determine the possible structure of each pi-group Gi in Lemma 13.8.\\n\\nLemma 13.9. Let G be a finite abelian p-group and suppose that g ∈ G has maximal order.\\nThen G is isomorphic to ⟨g⟩ ×H for some subgroup H of G.\\n\\nProof. By Lemma 13.7, we may assume that the order of G is pn. We shall induct on n.\\nIf n = 1, then G is cyclic of order p and must be generated by g. Suppose now that the\\nstatement of the lemma holds for all integers k with 1 ≤ k < n and let g be of maximal\\norder in G, say |g| = pm. Then apm = e for all a ∈ G. Now choose h in G such that h /∈ ⟨g⟩,\\nwhere h has the smallest possible order. Certainly such an h exists; otherwise, G = ⟨g⟩ and\\nwe are done. Let H = ⟨h⟩.\\n\\nWe claim that ⟨g⟩ ∩H = {e}. It suffices to show that |H| = p. Since |hp| = |h|/p, the\\norder of hp is smaller than the order of h and must be in ⟨g⟩ by the minimality of h; that\\nis, hp = gr for some number r. Hence,\\n\\n(gr)p\\nm−1\\n\\n= (hp)p\\nm−1\\n\\n= hp\\nm\\n= e,\\n\\nand the order of gr must be less than or equal to pm−1. Therefore, gr cannot generate ⟨g⟩.\\nNotice that p must occur as a factor of r, say r = ps, and hp = gr = gps. Define a to be\\ng−sh. Then a cannot be in ⟨g⟩; otherwise, h would also have to be in ⟨g⟩. Also,\\n\\nap = g−sphp = g−rhp = h−php = e.\\n\\nWe have now formed an element a with order p such that a /∈ ⟨g⟩. Since h was chosen to\\nhave the smallest order of all of the elements that are not in ⟨g⟩, |H| = p.\\n\\nNow we will show that the order of gH in the factor group G/H must be the same as\\nthe order of g in G. If |gH| < |g| = pm, then\\n\\nH = (gH)p\\nm−1\\n\\n= gp\\nm−1\\n\\nH;\\n\\nhence, gpm−1 must be in ⟨g⟩ ∩ H = {e}, which contradicts the fact that the order of g is\\npm. Therefore, gH must have maximal order in G/H. By the Correspondence Theorem\\nand our induction hypothesis,\\n\\nG/H ∼= ⟨gH⟩ ×K/H\\n\\n\\n\\n212 CHAPTER 13. THE STRUCTURE OF GROUPS\\n\\nfor some subgroup K of G containing H. We claim that ⟨g⟩∩K = {e}. If b ∈ ⟨g⟩∩K, then\\nbH ∈ ⟨gH⟩ ∩K/H = {H} and b ∈ ⟨g⟩ ∩H = {e}. It follows that G = ⟨g⟩K implies that\\nG ∼= ⟨g⟩ ×K.\\n\\nThe proof of the Fundamental Theorem of Finite Abelian Groups follows very quickly\\nfrom Lemma 13.9. Suppose that G is a finite abelian group and let g be an element of\\nmaximal order in G. If ⟨g⟩ = G, then we are done; otherwise, G ∼= Z|g| × H for some\\nsubgroup H contained in G by the lemma. Since |H| < |G|, we can apply mathematical\\ninduction.\\n\\nWe now state the more general theorem for all finitely generated abelian groups. The\\nproof of this theorem can be found in any of the references at the end of this chapter.\\n\\nTheorem 13.10 (The Fundamental Theorem of Finitely Generated Abelian Groups). Ev-\\nery finitely generated abelian group G is isomorphic to a direct product of cyclic groups of\\nthe form\\n\\nZp\\nα1\\n1\\n\\n× Zp\\nα2\\n2\\n\\n× · · · × Zpαn\\nn\\n\\n× Z× · · · × Z,\\n\\nwhere the pi’s are primes (not necessarily distinct).\\n\\n13.2 Solvable Groups\\nA subnormal series of a group G is a finite sequence of subgroups\\n\\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e},\\n\\nwhere Hi is a normal subgroup of Hi+1. If each subgroup Hi is normal in G, then the series\\nis called a normal series. The length of a subnormal or normal series is the number of\\nproper inclusions.\\n\\nExample 13.11. Any series of subgroups of an abelian group is a normal series. Consider\\nthe following series of groups:\\n\\nZ ⊃ 9Z ⊃ 45Z ⊃ 180Z ⊃ {0},\\nZ24 ⊃ ⟨2⟩ ⊃ ⟨6⟩ ⊃ ⟨12⟩ ⊃ {0}.\\n\\nExample 13.12. A subnormal series need not be a normal series. Consider the following\\nsubnormal series of the group D4:\\n\\nD4 ⊃ {(1), (12)(34), (13)(24), (14)(23)} ⊃ {(1), (12)(34)} ⊃ {(1)}.\\n\\nThe subgroup {(1), (12)(34)} is not normal in D4; consequently, this series is not a normal\\nseries.\\n\\nA subnormal (normal) series {Kj} is a refinement of a subnormal (normal) series\\n{Hi} if {Hi} ⊂ {Kj}. That is, each Hi is one of the Kj .\\n\\nExample 13.13. The series\\n\\nZ ⊃ 3Z ⊃ 9Z ⊃ 45Z ⊃ 90Z ⊃ 180Z ⊃ {0}\\n\\nis a refinement of the series\\n\\nZ ⊃ 9Z ⊃ 45Z ⊃ 180Z ⊃ {0}.\\n\\n\\n\\n13.2. SOLVABLE GROUPS 213\\n\\nThe best way to study a subnormal or normal series of subgroups, {Hi} of G, is actually\\nto study the factor groups Hi+1/Hi. We say that two subnormal (normal) series {Hi} and\\n{Kj} of a group G are isomorphic if there is a one-to-one correspondence between the\\ncollections of factor groups {Hi+1/Hi} and {Kj+1/Kj}.\\n\\nExample 13.14. The two normal series\\n\\nZ60 ⊃ ⟨3⟩ ⊃ ⟨15⟩ ⊃ {0}\\nZ60 ⊃ ⟨4⟩ ⊃ ⟨20⟩ ⊃ {0}\\n\\nof the group Z60 are isomorphic since\\n\\nZ60/⟨3⟩ ∼= ⟨20⟩/{0} ∼= Z3\\n\\n⟨3⟩/⟨15⟩ ∼= ⟨4⟩/⟨20⟩ ∼= Z5\\n\\n⟨15⟩/{0} ∼= Z60/⟨4⟩ ∼= Z4.\\n\\nA subnormal series {Hi} of a group G is a composition series if all the factor groups\\nare simple; that is, if none of the factor groups of the series contains a normal subgroup. A\\nnormal series {Hi} of G is a principal series if all the factor groups are simple.\\n\\nExample 13.15. The group Z60 has a composition series\\n\\nZ60 ⊃ ⟨3⟩ ⊃ ⟨15⟩ ⊃ ⟨30⟩ ⊃ {0}\\n\\nwith factor groups\\n\\nZ60/⟨3⟩ ∼= Z3\\n\\n⟨3⟩/⟨15⟩ ∼= Z5\\n\\n⟨15⟩/⟨30⟩ ∼= Z2\\n\\n⟨30⟩/{0} ∼= Z2.\\n\\nSince Z60 is an abelian group, this series is automatically a principal series. Notice that a\\ncomposition series need not be unique. The series\\n\\nZ60 ⊃ ⟨2⟩ ⊃ ⟨4⟩ ⊃ ⟨20⟩ ⊃ {0}\\n\\nis also a composition series.\\n\\nExample 13.16. For n ≥ 5, the series\\n\\nSn ⊃ An ⊃ {(1)}\\n\\nis a composition series for Sn since Sn/An\\n∼= Z2 and An is simple.\\n\\nExample 13.17. Not every group has a composition series or a principal series. Suppose\\nthat\\n\\n{0} = H0 ⊂ H1 ⊂ · · · ⊂ Hn−1 ⊂ Hn = Z\\n\\nis a subnormal series for the integers under addition. Then H1 must be of the form kZ\\nfor some k ∈ N. In this case H1/H0\\n\\n∼= kZ is an infinite cyclic group with many nontrivial\\nproper normal subgroups.\\n\\nAlthough composition series need not be unique as in the case of Z60, it turns out that\\nany two composition series are related. The factor groups of the two composition series\\nfor Z60 are Z2, Z2, Z3, and Z5; that is, the two composition series are isomorphic. The\\nJordan-Hölder Theorem says that this is always the case.\\n\\n\\n\\n214 CHAPTER 13. THE STRUCTURE OF GROUPS\\n\\nTheorem 13.18 (Jordan-Hölder). Any two composition series of G are isomorphic.\\n\\nProof. We shall employ mathematical induction on the length of the composition series.\\nIf the length of a composition series is 1, then G must be a simple group. In this case any\\ntwo composition series are isomorphic.\\n\\nSuppose now that the theorem is true for all groups having a composition series of length\\nk, where 1 ≤ k < n. Let\\n\\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}\\nG = Km ⊃ Km−1 ⊃ · · · ⊃ K1 ⊃ K0 = {e}\\n\\nbe two composition series for G. We can form two new subnormal series for G since\\nHi ∩Km−1 is normal in Hi+1 ∩Km−1 and Kj ∩Hn−1 is normal in Kj+1 ∩Hn−1:\\n\\nG = Hn ⊃ Hn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e}\\nG = Km ⊃ Km−1 ⊃ Km−1 ∩Hn−1 ⊃ · · · ⊃ K0 ∩Hn−1 = {e}.\\n\\nSince Hi ∩ Km−1 is normal in Hi+1 ∩ Km−1, the Second Isomorphism Theorem (Theo-\\nrem 11.12) implies that\\n\\n(Hi+1 ∩Km−1)/(Hi ∩Km−1) = (Hi+1 ∩Km−1)/(Hi ∩ (Hi+1 ∩Km−1))\\n∼= Hi(Hi+1 ∩Km−1)/Hi,\\n\\nwhere Hi is normal in Hi(Hi+1 ∩ Km−1). Since {Hi} is a composition series, Hi+1/Hi\\n\\nmust be simple; consequently, Hi(Hi+1 ∩Km−1)/Hi is either Hi+1/Hi or Hi/Hi. That is,\\nHi(Hi+1 ∩Km−1) must be either Hi or Hi+1. Removing any nonproper inclusions from the\\nseries\\n\\nHn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e},\\n\\nwe have a composition series for Hn−1. Our induction hypothesis says that this series must\\nbe equivalent to the composition series\\n\\nHn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}.\\n\\nHence, the composition series\\n\\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}\\n\\nand\\nG = Hn ⊃ Hn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e}\\n\\nare equivalent. If Hn−1 = Km−1, then the composition series {Hi} and {Kj} are equivalent\\nand we are done; otherwise, Hn−1Km−1 is a normal subgroup of G properly containing\\nHn−1. In this case Hn−1Km−1 = G and we can apply the Second Isomorphism Theorem\\nonce again; that is,\\n\\nKm−1/(Km−1 ∩Hn−1) ∼= (Hn−1Km−1)/Hn−1 = G/Hn−1.\\n\\nTherefore,\\nG = Hn ⊃ Hn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e}\\n\\nand\\nG = Km ⊃ Km−1 ⊃ Km−1 ∩Hn−1 ⊃ · · · ⊃ K0 ∩Hn−1 = {e}\\n\\nare equivalent and the proof of the theorem is complete.\\n\\n\\n\\n13.3. EXERCISES 215\\n\\nA group G is solvable if it has a subnormal series {Hi} such that all of the factor groups\\nHi+1/Hi are abelian. Solvable groups will play a fundamental role when we study Galois\\ntheory and the solution of polynomial equations.\\n\\nExample 13.19. The group S4 is solvable since\\n\\nS4 ⊃ A4 ⊃ {(1), (12)(34), (13)(24), (14)(23)} ⊃ {(1)}\\n\\nhas abelian factor groups; however, for n ≥ 5 the series\\n\\nSn ⊃ An ⊃ {(1)}\\n\\nis a composition series for Sn with a nonabelian factor group. Therefore, Sn is not a solvable\\ngroup for n ≥ 5.\\n\\n13.3 Exercises\\n1. Find all of the abelian groups of order less than or equal to 40 up to isomorphism.\\n\\n2. Find all of the abelian groups of order 200 up to isomorphism.\\n\\n3. Find all of the abelian groups of order 720 up to isomorphism.\\n\\n4. Find all of the composition series for each of the following groups.\\n\\n(a) Z12\\n\\n(b) Z48\\n\\n(c) The quaternions, Q8\\n\\n(d) D4\\n\\n(e) S3 × Z4\\n\\n(f) S4\\n\\n(g) Sn, n ≥ 5\\n\\n(h) Q\\n\\n5. Show that the infinite direct product G = Z2 × Z2 × · · · is not finitely generated.\\n\\n6. Let G be an abelian group of order m. If n divides m, prove that G has a subgroup of\\norder n.\\n\\n7. A group G is a torsion group if every element of G has finite order. Prove that a\\nfinitely generated abelian torsion group must be finite.\\n\\n8. Let G, H, and K be finitely generated abelian groups. Show that if G ×H ∼= G ×K,\\nthen H ∼= K. Give a counterexample to show that this cannot be true in general.\\n\\n9. Let G and H be solvable groups. Show that G×H is also solvable.\\n\\n10. If G has a composition (principal) series and if N is a proper normal subgroup of G,\\nshow there exists a composition (principal) series containing N .\\n\\n11. Prove or disprove: Let N be a normal subgroup of G. If N and G/N have composition\\nseries, then G must also have a composition series.\\n\\n12. Let N be a normal subgroup of G. If N and G/N are solvable groups, show that G is\\nalso a solvable group.\\n\\n13. Prove that G is a solvable group if and only if G has a series of subgroups\\n\\nG = Pn ⊃ Pn−1 ⊃ · · · ⊃ P1 ⊃ P0 = {e}\\n\\nwhere Pi is normal in Pi+1 and the order of Pi+1/Pi is prime.\\n\\n\\n\\n216 CHAPTER 13. THE STRUCTURE OF GROUPS\\n\\n14. Let G be a solvable group. Prove that any subgroup of G is also solvable.\\n\\n15. Let G be a solvable group and N a normal subgroup of G. Prove that G/N is solvable.\\n\\n16. Prove that Dn is solvable for all integers n.\\n\\n17. Suppose that G has a composition series. If N is a normal subgroup of G, show that\\nN and G/N also have composition series.\\n\\n18. Let G be a cyclic p-group with subgroups H and K. Prove that either H is contained\\nin K or K is contained in H.\\n\\n19. Suppose that G is a solvable group with order n ≥ 2. Show that G contains a normal\\nnontrivial abelian subgroup.\\n\\n20. Recall that the commutator subgroup G′ of a group G is defined as the subgroup\\nof G generated by elements of the form a−1b−1ab for a, b ∈ G. We can define a series of\\nsubgroups of G by G(0) = G, G(1) = G′, and G(i+1) = (G(i))′.\\n(a) Prove that G(i+1) is normal in (G(i))′. The series of subgroups\\n\\nG(0) = G ⊃ G(1) ⊃ G(2) ⊃ · · ·\\n\\nis called the derived series of G.\\n(b) Show that G is solvable if and only if G(n) = {e} for some integer n.\\n\\n21. Suppose that G is a solvable group with order n ≥ 2. Show that G contains a normal\\nnontrivial abelian factor group.\\n\\n22. (Zassenhaus Lemma) Let H and K be subgroups of a group G. Suppose also that H∗\\n\\nand K∗ are normal subgroups of H and K respectively. Then\\n(a) H∗(H ∩K∗) is a normal subgroup of H∗(H ∩K).\\n(b) K∗(H∗ ∩K) is a normal subgroup of K∗(H ∩K).\\n(c) H∗(H ∩K)/H∗(H ∩K∗) ∼= K∗(H ∩K)/K∗(H∗ ∩K) ∼= (H ∩K)/(H∗ ∩K)(H ∩K∗).\\n\\n23. (Schreier’s Theorem) Use the Zassenhaus Lemma to prove that two subnormal (nor-\\nmal) series of a group G have isomorphic refinements.\\n\\n24. Use Schreier’s Theorem to prove the Jordan-Hölder Theorem.\\n\\n13.4 Programming Exercises\\n1. Write a program that will compute all possible abelian groups of order n. What is the\\nlargest n for which your program will work?\\n\\n13.5 References and Suggested Readings\\n[1] Hungerford, T. W. Algebra. Springer, New York, 1974.\\n[2] Lang, S. Algebra. 3rd ed. Springer, New York, 2002.\\n[3] Rotman, J. J. An Introduction to the Theory of Groups. 4th ed. Springer, New York,\\n\\n1995.\\n\\n\\n\\n13.6. SAGE 217\\n\\n13.6 Sage\\nCyclic groups, and direct products of cyclic groups, are implemented in Sage as permutation\\ngroups. However, these groups quickly become very unwieldly representations and it should\\nbe easier to work with finite abelian groups in Sage. So we will postpone any specifics for\\nthis chapter until that happens. However, now that we understand the notion of isomorphic\\ngroups and the structure of finite abelian groups, we can return to our quest to classify all\\nof the groups with order less than 16.\\n\\nClassification of Finite Groups\\nIt does not take any sophisticated tools to understand groups of order 2p, where p is an\\nodd prime. There are two possibilities — a cyclic group of order 2p and the dihedral group\\nof order 2p that is the set of symmetries of a regular p-gon. The proof requires some close,\\ntight reasoning, but the required theorems are generally just concern orders of elements,\\nLagrange’s Theorem and cosets. See Exercise 9.3.55. This takes care of orders n = 6, 10, 14.\\n\\nFor n = 9, the upcoming Corollary 14.16 will tell us that any group of order p2 (where\\np is a prime) is abelian. So we know from this section that the only two possibilities are Z9\\n\\nand Z3 × Z3. Similarly, the upcoming Theorem 15.10 will tell us that every group of order\\nn = 15 is abelian. Now this leaves just one possibility for this order: Z3 × Z5\\n\\n∼= Z15.\\nWe have just two orders left to analyze: n = 8 and n = 12. The possibilities are\\n\\ngroups we already know, with one exception. However, the analysis that these are the only\\npossibilities is more complicated, and will not be pursued now, nor in the next few chapters.\\nNotice that n = 16 is more complicated still, with 14 different possibilities (which explains\\nwhy we stopped here).\\n\\nFor n = 8 there are 3 abelian groups, and the two non-abelian groups are the dihedral\\ngroup (symmetries of a square) and the quaternions.\\n\\nFor n = 12 there are 2 abelian groups, and 3 non-abelian groups. We know two of the\\nnon-abelian groups as a dihedral group, and the alternating group on 4 symbols (which\\nis also the symmetries of a tetrahedron). The third non-abelian group is an example of a\\n“dicyclic” group, which is an infinite family of groups, each with order divisible by 4. The\\norder 12 dicyclic group can also be constructed as a “semi-direct product” of two cyclic\\ngroups — this is a construction worth knowing as you pursue further study of group theory.\\nThe order 8 dicyclic group is also the quaternions and more generally, the dicyclic groups\\nof order 2k, k > 2 are known as “generalized quaternion groups.”\\n\\nThe following examples will show you how to construct some of these groups, while also\\nexercising a few of the commands and allowing us to be more certain the following table is\\naccurate.\\n\\nS = SymmetricGroup (3)\\nD = DihedralGroup (3)\\nS.is_isomorphic(D)\\n\\nTrue\\n\\nC3 = CyclicPermutationGroup (3)\\nC5 = CyclicPermutationGroup (5)\\nDP = direct_product_permgroups ([C3, C5])\\nC = CyclicPermutationGroup (15)\\nDP.is_isomorphic(C)\\n\\nTrue\\n\\n\\n\\n218 CHAPTER 13. THE STRUCTURE OF GROUPS\\n\\nQ = QuaternionGroup ()\\nDI = DiCyclicGroup (2)\\nQ.is_isomorphic(DI)\\n\\nTrue\\n\\nGroups of Small Order as Permutation Groups\\n\\nWe list here constructions, as permutation groups in Sage, for all of the groups of order less\\nthan 16.\\n\\n\\n\\n13.7. SAGE EXERCISES 219\\n\\nOrder Construction Notes, Alternatives\\n1 CyclicPermutationGroup(1) Trivial\\n2 CyclicPermutationGroup(2) SymmetricGroup(2)\\n\\n3 CyclicPermutationGroup(3) Prime order\\n4 CyclicPermutationGroup(4) Cyclic\\n4 KleinFourGroup() Abelian, non-cyclic\\n5 CyclicPermutationGroup(5) Prime order\\n6 CyclicPermutationGroup(6) Cyclic\\n6 SymmetricGroup(3) Non-abelian\\n\\nDihedralGroup(3)\\n\\n7 CyclicPermutationGroup(7) Prime order\\n8 CyclicPermutationGroup(8) Cyclic\\n8 C2=CyclicPermutationGroup(2)\\n\\nC4=CyclicPermutationGroup(4)\\n\\nG=direct_product_permgroups([C2,C4]) Abelian, non-cyclic\\n8 C2=CyclicPermutationGroup(2)\\n\\nG=direct_product_permgroups([C2,C2,C2]) Abelian, non-cyclic\\n8 DihedralGroup(4) Non-abelian\\n8 QuaternionGroup() Quaternions\\n\\nDiCyclicGroup(2)\\n\\n9 CyclicPermutationGroup(9) Cyclic\\n9 C3=CyclicPermutationGroup(3)\\n\\nG=direct_product_permgroups([C3,C3]) Abelian, non-cyclic\\n10 CyclicPermutationGroup(10) Cyclic\\n10 DihedralGroup(5) Non-abelian\\n11 CyclicPermutationGroup(11) Prime order\\n12 CyclicPermutationGroup(12) Cyclic\\n12 C2=CyclicPermutationGroup(2)\\n\\nC6=CyclicPermutationGroup(6)\\n\\nG=direct_product_permgroups([C2,C6]) Abelian, non-cyclic\\n12 DihedralGroup(6) Non-abelian\\n12 AlternatingGroup(4) Non-abelian\\n\\nSymmetries of tetrahedron\\n12 DiCyclicGroup(3) Non-abelian\\n\\nSemi-direct product Z3 ⋊ Z4\\n\\n13 CyclicPermutationGroup(13) Prime order\\n14 CyclicPermutationGroup(14) Cyclic\\n14 DihedralGroup(7) Non-abelian\\n15 CyclicPermutationGroup(15) Cyclic\\n\\nTable 13.20: The Groups of Order 16 or Less in Sage\\n\\n13.7 Sage Exercises\\nThere are no Sage exercises for this chapter.\\n\\n\\n\\n14\\n\\nGroup Actions\\n\\nGroup actions generalize group multiplication. If G is a group and X is an arbitrary set, a\\ngroup action of an element g ∈ G and x ∈ X is a product, gx, living in X. Many problems\\nin algebra are best be attacked via group actions. For example, the proofs of the Sylow\\ntheorems and of Burnside’s Counting Theorem are most easily understood when they are\\nformulated in terms of group actions.\\n\\n14.1 Groups Acting on Sets\\nLet X be a set and G be a group. A (left) action of G on X is a map G×X → X given\\nby (g, x) 7→ gx, where\\n\\n1. ex = x for all x ∈ X;\\n\\n2. (g1g2)x = g1(g2x) for all x ∈ X and all g1, g2 ∈ G.\\n\\nUnder these considerations X is called a G-set. Notice that we are not requiring X to\\nbe related to G in any way. It is true that every group G acts on every set X by the trivial\\naction (g, x) 7→ x; however, group actions are more interesting if the set X is somehow\\nrelated to the group G.\\n\\nExample 14.1. Let G = GL2(R) and X = R2. Then G acts on X by left multiplication.\\nIf v ∈ R2 and I is the identity matrix, then Iv = v. If A and B are 2×2 invertible matrices,\\nthen (AB)v = A(Bv) since matrix multiplication is associative.\\n\\nExample 14.2. Let G = D4 be the symmetry group of a square. If X = {1, 2, 3, 4}\\nis the set of vertices of the square, then we can consider D4 to consist of the following\\npermutations:\\n\\n{(1), (13), (24), (1432), (1234), (12)(34), (14)(23), (13)(24)}.\\n\\nThe elements of D4 act on X as functions. The permutation (13)(24) acts on vertex 1 by\\nsending it to vertex 3, on vertex 2 by sending it to vertex 4, and so on. It is easy to see\\nthat the axioms of a group action are satisfied.\\n\\nIn general, if X is any set and G is a subgroup of SX , the group of all permutations\\nacting on X, then X is a G-set under the group action\\n\\n(σ, x) 7→ σ(x)\\n\\nfor σ ∈ G and x ∈ X.\\n\\n220\\n\\n\\n\\n14.1. GROUPS ACTING ON SETS 221\\n\\nExample 14.3. If we let X = G, then every group G acts on itself by the left regular\\nrepresentation; that is, (g, x) 7→ λg(x) = gx, where λg is left multiplication:\\n\\ne · x = λex = ex = x\\n\\n(gh) · x = λghx = λgλhx = λg(hx) = g · (h · x).\\n\\nIf H is a subgroup of G, then G is an H-set under left multiplication by elements of H.\\n\\nExample 14.4. Let G be a group and suppose that X = G. If H is a subgroup of G, then\\nG is an H-set under conjugation; that is, we can define an action of H on G,\\n\\nH ×G→ G,\\n\\nvia\\n(h, g) 7→ hgh−1\\n\\nfor h ∈ H and g ∈ G. Clearly, the first axiom for a group action holds. Observing that\\n\\n(h1h2, g) = h1h2g(h1h2)\\n−1\\n\\n= h1(h2gh\\n−1\\n2 )h−1\\n\\n1\\n\\n= (h1, (h2, g)),\\n\\nwe see that the second condition is also satisfied.\\n\\nExample 14.5. Let H be a subgroup of G and LH the set of left cosets of H. The set LH\\n\\nis a G-set under the action\\n(g, xH) 7→ gxH.\\n\\nAgain, it is easy to see that the first axiom is true. Since (gg′)xH = g(g′xH), the second\\naxiom is also true.\\n\\nIf G acts on a set X and x, y ∈ X, then x is said to be G-equivalent to y if there exists\\na g ∈ G such that gx = y. We write x ∼G y or x ∼ y if two elements are G-equivalent.\\n\\nProposition 14.6. Let X be a G-set. Then G-equivalence is an equivalence relation on X.\\n\\nProof. The relation ∼ is reflexive since ex = x. Suppose that x ∼ y for x, y ∈ X. Then\\nthere exists a g such that gx = y. In this case g−1y = x; hence, y ∼ x. To show that the\\nrelation is transitive, suppose that x ∼ y and y ∼ z. Then there must exist group elements\\ng and h such that gx = y and hy = z. So z = hy = (hg)x, and x is equivalent to z.\\n\\nIf X is a G-set, then each partition of X associated with G-equivalence is called an\\norbit of X under G. We will denote the orbit that contains an element x of X by Ox.\\n\\nExample 14.7. Let G be the permutation group defined by\\n\\nG = {(1), (123), (132), (45), (123)(45), (132)(45)}\\n\\nand X = {1, 2, 3, 4, 5}. Then X is a G-set. The orbits are O1 = O2 = O3 = {1, 2, 3} and\\nO4 = O5 = {4, 5}.\\n\\nNow suppose that G is a group acting on a set X and let g be an element of G. The\\nfixed point set of g in X, denoted by Xg, is the set of all x ∈ X such that gx = x. We can\\nalso study the group elements g that fix a given x ∈ X. This set is more than a subset of G,\\nit is a subgroup. This subgroup is called the stabilizer subgroup or isotropy subgroup\\nof x. We will denote the stabilizer subgroup of x by Gx.\\n\\n\\n\\n222 CHAPTER 14. GROUP ACTIONS\\n\\nRemark 14.8. It is important to remember that Xg ⊂ X and Gx ⊂ G.\\n\\nExample 14.9. Let X = {1, 2, 3, 4, 5, 6} and suppose that G is the permutation group\\ngiven by the permutations\\n\\n{(1), (12)(3456), (35)(46), (12)(3654)}.\\n\\nThen the fixed point sets of X under the action of G are\\n\\nX(1) = X,\\n\\nX(35)(46) = {1, 2},\\nX(12)(3456) = X(12)(3654) = ∅,\\n\\nand the stabilizer subgroups are\\n\\nG1 = G2 = {(1), (35)(46)},\\nG3 = G4 = G5 = G6 = {(1)}.\\n\\nIt is easily seen that Gx is a subgroup of G for each x ∈ X.\\n\\nProposition 14.10. Let G be a group acting on a set X and x ∈ X. The stabilizer group\\nof x, Gx, is a subgroup of G.\\n\\nProof. Clearly, e ∈ Gx since the identity fixes every element in the set X. Let g, h ∈ Gx.\\nThen gx = x and hx = x. So (gh)x = g(hx) = gx = x; hence, the product of two elements\\nin Gx is also in Gx. Finally, if g ∈ Gx, then x = ex = (g−1g)x = (g−1)gx = g−1x. So g−1\\n\\nis in Gx.\\n\\nWe will denote the number of elements in the fixed point set of an element g ∈ G by\\n|Xg| and denote the number of elements in the orbit of x ∈ X by |Ox|. The next theorem\\ndemonstrates the relationship between orbits of an element x ∈ X and the left cosets of Gx\\n\\nin G.\\n\\nTheorem 14.11. Let G be a finite group and X a finite G-set. If x ∈ X, then |Ox| = [G :\\nGx].\\n\\nProof. We know that |G|/|Gx| is the number of left cosets of Gx in G by Lagrange’s\\nTheorem (Theorem 6.10). We will define a bijective map ϕ between the orbit Ox of X and\\nthe set of left cosets LGx of Gx in G. Let y ∈ Ox. Then there exists a g in G such that\\ngx = y. Define ϕ by ϕ(y) = gGx. To show that ϕ is one-to-one, assume that ϕ(y1) = ϕ(y2).\\nThen\\n\\nϕ(y1) = g1Gx = g2Gx = ϕ(y2),\\n\\nwhere g1x = y1 and g2x = y2. Since g1Gx = g2Gx, there exists a g ∈ Gx such that g2 = g1g,\\n\\ny2 = g2x = g1gx = g1x = y1;\\n\\nconsequently, the map ϕ is one-to-one. Finally, we must show that the map ϕ is onto. Let\\ngGx be a left coset. If gx = y, then ϕ(y) = gGx.\\n\\n\\n\\n14.2. THE CLASS EQUATION 223\\n\\n14.2 The Class Equation\\nLet X be a finite G-set and XG be the set of fixed points in X; that is,\\n\\nXG = {x ∈ X : gx = x for all g ∈ G}.\\n\\nSince the orbits of the action partition X,\\n\\n|X| = |XG|+\\nn∑\\n\\ni=k\\n\\n|Oxi |,\\n\\nwhere xk, . . . , xn are representatives from the distinct nontrivial orbits of X.\\nNow consider the special case in which G acts on itself by conjugation, (g, x) 7→ gxg−1.\\n\\nThe center of G,\\nZ(G) = {x : xg = gx for all g ∈ G},\\n\\nis the set of points that are fixed by conjugation. The nontrivial orbits of the action are\\ncalled the conjugacy classes of G. If x1, . . . , xk are representatives from each of the\\nnontrivial conjugacy classes of G and |Ox1 | = n1, . . . , |Oxk\\n\\n| = nk, then\\n\\n|G| = |Z(G)|+ n1 + · · ·+ nk.\\n\\nThe stabilizer subgroups of each of the xi’s, C(xi) = {g ∈ G : gxi = xig}, are called the\\ncentralizer subgroups of the xi’s. From Theorem 14.11, we obtain the class equation:\\n\\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)].\\n\\nOne of the consequences of the class equation is that the order of each conjugacy class must\\ndivide the order of G.\\n\\nExample 14.12. It is easy to check that the conjugacy classes in S3 are the following:\\n\\n{(1)}, {(123), (132)}, {(12), (13), (23)}.\\n\\nThe class equation is 6 = 1 + 2 + 3.\\n\\nExample 14.13. The center of D4 is {(1), (13)(24)}, and the conjugacy classes are\\n\\n{(13), (24)}, {(1432), (1234)}, {(12)(34), (14)(23)}.\\n\\nThus, the class equation for D4 is 8 = 2 + 2 + 2 + 2.\\n\\nExample 14.14. For Sn it takes a bit of work to find the conjugacy classes. We begin\\nwith cycles. Suppose that σ = (a1, . . . , ak) is a cycle and let τ ∈ Sn. By Theorem 6.16,\\n\\nτστ−1 = (τ(a1), . . . , τ(ak)).\\n\\nConsequently, any two cycles of the same length are conjugate. Now let σ = σ1σ2 · · ·σr be\\na cycle decomposition, where the length of each cycle σi is ri. Then σ is conjugate to every\\nother τ ∈ Sn whose cycle decomposition has the same lengths.\\n\\nThe number of conjugate classes in Sn is the number of ways in which n can be parti-\\ntioned into sums of positive integers. In the case of S3 for example, we can partition the\\ninteger 3 into the following three sums:\\n\\n3 = 1 + 1 + 1\\n\\n3 = 1 + 2\\n\\n3 = 3;\\n\\n\\n\\n224 CHAPTER 14. GROUP ACTIONS\\n\\ntherefore, there are three conjugacy classes. The problem of finding the number of such\\npartitions for any positive integer n is what computer scientists call NP-complete. This\\neffectively means that the problem cannot be solved for a large n because the computations\\nwould be too time-consuming for even the largest computer.\\n\\nTheorem 14.15. Let G be a group of order pn where p is prime. Then G has a nontrivial\\ncenter.\\n\\nProof. We apply the class equation\\n\\n|G| = |Z(G)|+ n1 + · · ·+ nk.\\n\\nSince each ni > 1 and ni | |G|, it follows that p must divide each ni. Also, p | |G|; hence, p\\nmust divide |Z(G)|. Since the identity is always in the center of G, |Z(G)| ≥ 1. Therefore,\\n|Z(G)| ≥ p, and there exists some g ∈ Z(G) such that g ̸= 1.\\n\\nCorollary 14.16. Let G be a group of order p2 where p is prime. Then G is abelian.\\n\\nProof. By Theorem 14.15, |Z(G)| = p or p2. If |Z(G)| = p2, then we are done. Suppose\\nthat |Z(G)| = p. Then Z(G) and G/Z(G) both have order p and must both be cyclic\\ngroups. Choosing a generator aZ(G) for G/Z(G), we can write any element gZ(G) in the\\nquotient group as amZ(G) for some integer m; hence, g = amx for some x in the center of\\nG. Similarly, if hZ(G) ∈ G/Z(G), there exists a y in Z(G) such that h = any for some\\ninteger n. Since x and y are in the center of G, they commute with all other elements of G;\\ntherefore,\\n\\ngh = amxany = am+nxy = anyamx = hg,\\n\\nand G must be abelian.\\n\\n14.3 Burnside’s Counting Theorem\\nSuppose that we wish to color the vertices of a square with two different colors, say black and\\nwhite. We might suspect that there would be 24 = 16 different colorings. However, some of\\nthese colorings are equivalent. If we color the first vertex black and the remaining vertices\\nwhite, it is the same as coloring the second vertex black and the remaining ones white since\\nwe could obtain the second coloring simply by rotating the square 90◦ (Figure 14.17).\\n\\nB W\\n\\nW W\\n\\nW B\\n\\nW W\\n\\nW W\\n\\nB W\\n\\nW W\\n\\nW B\\n\\nFigure 14.17: Equivalent colorings of square\\n\\nBurnside’s Counting Theorem offers a method of computing the number of distinguish-\\nable ways in which something can be done. In addition to its geometric applications, the\\n\\n\\n\\n14.3. BURNSIDE’S COUNTING THEOREM 225\\n\\ntheorem has interesting applications to areas in switching theory and chemistry. The proof\\nof Burnside’s Counting Theorem depends on the following lemma.\\n\\nLemma 14.18. Let X be a G-set and suppose that x ∼ y. Then Gx is isomorphic to Gy.\\nIn particular, |Gx| = |Gy|.\\n\\nProof. Let G act on X by (g, x) 7→ g · x. Since x ∼ y, there exists a g ∈ G such that\\ng · x = y. Let a ∈ Gx. Since\\n\\ngag−1 · y = ga · g−1y = ga · x = g · x = y,\\n\\nwe can define a map ϕ : Gx → Gy by ϕ(a) = gag−1. The map ϕ is a homomorphism since\\n\\nϕ(ab) = gabg−1 = gag−1gbg−1 = ϕ(a)ϕ(b).\\n\\nSuppose that ϕ(a) = ϕ(b). Then gag−1 = gbg−1 or a = b; hence, the map is injective. To\\nshow that ϕ is onto, let b be in Gy; then g−1bg is in Gx since\\n\\ng−1bg · x = g−1b · gx = g−1b · y = g−1 · y = x;\\n\\nand ϕ(g−1bg) = b.\\n\\nTheorem 14.19 (Burnside). Let G be a finite group acting on a set X and let k denote\\nthe number of orbits of X. Then\\n\\nk =\\n1\\n\\n|G|\\n∑\\ng∈G\\n\\n|Xg|.\\n\\nProof. We look at all the fixed points x of all the elements in g ∈ G; that is, we look at\\nall g’s and all x’s such that gx = x. If viewed in terms of fixed point sets, the number of\\nall g’s fixing x’s is ∑\\n\\ng∈G\\n|Xg|.\\n\\nHowever, if viewed in terms of the stabilizer subgroups, this number is∑\\nx∈X\\n\\n|Gx|;\\n\\nhence,\\n∑\\n\\ng∈G |Xg| =\\n∑\\n\\nx∈X |Gx|. By Lemma 14.18,∑\\ny∈Ox\\n\\n|Gy| = |Ox| · |Gx|.\\n\\nBy Theorem 14.11 and Lagrange’s Theorem, this expression is equal to |G|. Summing over\\nall of the k distinct orbits, we conclude that∑\\n\\ng∈G\\n|Xg| =\\n\\n∑\\nx∈X\\n\\n|Gx| = k · |G|.\\n\\n\\n\\n226 CHAPTER 14. GROUP ACTIONS\\n\\nExample 14.20. Let X = {1, 2, 3, 4, 5} and suppose that G is the permutation group\\nG = {(1), (13), (13)(25), (25)}. The orbits of X are {1, 3}, {2, 5}, and {4}. The fixed point\\nsets are\\n\\nX(1) = X\\n\\nX(13) = {2, 4, 5}\\nX(13)(25) = {4}\\n\\nX(25) = {1, 3, 4}.\\n\\nBurnside’s Theorem says that\\n\\nk =\\n1\\n\\n|G|\\n∑\\ng∈G\\n\\n|Xg| =\\n1\\n\\n4\\n(5 + 3 + 1 + 3) = 3.\\n\\nA Geometric Example\\nBefore we apply Burnside’s Theorem to switching-theory problems, let us examine the\\nnumber of ways in which the vertices of a square can be colored black or white. Notice\\nthat we can sometimes obtain equivalent colorings by simply applying a rigid motion to the\\nsquare. For instance, as we have pointed out, if we color one of the vertices black and the\\nremaining three white, it does not matter which vertex was colored black since a rotation\\nwill give an equivalent coloring.\\n\\nThe symmetry group of a square, D4, is given by the following permutations:\\n\\n(1) (13) (24) (1432)\\n\\n(1234) (12)(34) (14)(23) (13)(24)\\n\\nThe group G acts on the set of vertices {1, 2, 3, 4} in the usual manner. We can describe\\nthe different colorings by mappings from X into Y = {B,W} where B and W represent\\nthe colors black and white, respectively. Each map f : X → Y describes a way to color\\nthe corners of the square. Every σ ∈ D4 induces a permutation σ̃ of the possible colorings\\ngiven by σ̃(f) = f ◦ σ for f : X → Y . For example, suppose that f is defined by\\n\\nf(1) = B\\n\\nf(2) =W\\n\\nf(3) =W\\n\\nf(4) =W\\n\\nand σ = (12)(34). Then σ̃(f) = f ◦ σ sends vertex 2 to B and the remaining vertices to\\nW . The set of all such σ̃ is a permutation group G̃ on the set of possible colorings. Let X̃\\ndenote the set of all possible colorings; that is, X̃ is the set of all possible maps from X to\\nY . Now we must compute the number of G̃-equivalence classes.\\n\\n1. X̃(1) = X̃ since the identity fixes every possible coloring. |X̃| = 24 = 16.\\n\\n2. X̃(1234) consists of all f ∈ X̃ such that f is unchanged by the permutation (1234). In\\nthis case f(1) = f(2) = f(3) = f(4), so that all values of f must be the same; that is,\\neither f(x) = B or f(x) =W for every vertex x of the square. So |X̃(1234)| = 2.\\n\\n3. |X̃(1432)| = 2.\\n\\n4. For X̃(13)(24), f(1) = f(3) and f(2) = f(4). Thus, |X̃(13)(24)| = 22 = 4.\\n\\n\\n\\n14.3. BURNSIDE’S COUNTING THEOREM 227\\n\\n5. |X̃(12)(34)| = 4.\\n\\n6. |X̃(14)(23)| = 4.\\n\\n7. For X̃(13), f(1) = f(3) and the other corners can be of any color; hence, |X̃(13)| =\\n23 = 8.\\n\\n8. |X̃(24)| = 8.\\n\\nBy Burnside’s Theorem, we can conclude that there are exactly\\n\\n1\\n\\n8\\n(24 + 21 + 22 + 21 + 22 + 22 + 23 + 23) = 6\\n\\nways to color the vertices of the square.\\n\\nProposition 14.21. Let G be a permutation group of X and X̃ the set of functions from\\nX to Y . Then there exists a permutation group G̃ acting on X̃, where σ̃ ∈ G̃ is defined by\\nσ̃(f) = f ◦ σ for σ ∈ G and f ∈ X̃. Furthermore, if n is the number of cycles in the cycle\\ndecomposition of σ, then |X̃σ| = |Y |n.\\n\\nProof. Let σ ∈ G and f ∈ X̃. Clearly, f ◦ σ is also in X̃. Suppose that g is another\\nfunction from X to Y such that σ̃(f) = σ̃(g). Then for each x ∈ X,\\n\\nf(σ(x)) = σ̃(f)(x) = σ̃(g)(x) = g(σ(x)).\\n\\nSince σ is a permutation of X, every element x′ in X is the image of some x in X under σ;\\nhence, f and g agree on all elements of X. Therefore, f = g and σ̃ is injective. The map\\nσ 7→ σ̃ is onto, since the two sets are the same size.\\n\\nSuppose that σ is a permutation of X with cycle decomposition σ = σ1σ2 · · ·σn. Any\\nf in X̃σ must have the same value on each cycle of σ. Since there are n cycles and |Y |\\npossible values for each cycle, |X̃σ| = |Y |n.\\n\\nExample 14.22. Let X = {1, 2, . . . , 7} and suppose that Y = {A,B,C}. If g is the\\npermutation of X given by (13)(245) = (13)(245)(6)(7), then n = 4. Any f ∈ X̃g must\\nhave the same value on each cycle in g. There are |Y | = 3 such choices for any value, so\\n|X̃g| = 34 = 81.\\n\\nExample 14.23. Suppose that we wish to color the vertices of a square using four different\\ncolors. By Proposition 14.21, we can immediately decide that there are\\n\\n1\\n\\n8\\n(44 + 41 + 42 + 41 + 42 + 42 + 43 + 43) = 55\\n\\npossible ways.\\n\\nSwitching Functions\\nIn switching theory we are concerned with the design of electronic circuits with binary\\ninputs and outputs. The simplest of these circuits is a switching function that has n inputs\\nand a single output (Figure 14.24). Large electronic circuits can often be constructed by\\ncombining smaller modules of this kind. The inherent problem here is that even for a simple\\ncircuit a large number of different switching functions can be constructed. With only four\\ninputs and a single output, we can construct 65,536 different switching functions. However,\\nwe can often replace one switching function with another merely by permuting the input\\nleads to the circuit (Figure 14.25).\\n\\n\\n\\n228 CHAPTER 14. GROUP ACTIONS\\n\\nf f(x1, x2, . . . , xn)\\n\\nxn\\n\\nx2\\n\\nx1\\n\\n...\\n\\nFigure 14.24: A switching function of n variables\\n\\nWe define a switching or Boolean function of n variables to be a function from Zn\\n2\\n\\nto Z2. Since any switching function can have two possible values for each binary n-tuple\\nand there are 2n binary n-tuples, 22n switching functions are possible for n variables. In\\ngeneral, allowing permutations of the inputs greatly reduces the number of different kinds\\nof modules that are needed to build a large circuit.\\n\\nf f(a, b)\\na\\n\\nb\\nf f(b, a) = g(a, b)\\n\\na\\n\\nb\\n\\nFigure 14.25: A switching function of two variables\\n\\nThe possible switching functions with two input variables a and b are listed in Ta-\\nble 14.26. Two switching functions f and g are equivalent if g can be obtained from f\\nby a permutation of the input variables. For example, g(a, b, c) = f(b, c, a). In this case\\ng ∼ f via the permutation (acb). In the case of switching functions of two variables, the\\npermutation (ab) reduces 16 possible switching functions to 12 equivalent functions since\\n\\nf2 ∼ f4\\n\\nf3 ∼ f5\\n\\nf10 ∼ f12\\n\\nf11 ∼ f13.\\n\\nInputs Outputs\\nf0 f1 f2 f3 f4 f5 f6 f7\\n\\n0 0 0 0 0 0 0 0 0 0\\n0 1 0 0 0 0 1 1 1 1\\n1 0 0 0 1 1 0 0 1 1\\n1 1 0 1 0 1 0 1 0 1\\nInputs Outputs\\n\\nf8 f9 f10 f11 f12 f13 f14 f15\\n0 0 1 1 1 1 1 1 1 1\\n0 1 0 0 0 0 1 1 1 1\\n1 0 0 0 1 1 0 0 1 1\\n1 1 0 1 0 1 0 1 0 1\\n\\nTable 14.26: Switching functions in two variables\\n\\nFor three input variables there are 22\\n3\\n= 256 possible switching functions; in the case\\n\\nof four variables there are 22\\n4\\n= 65,536. The number of equivalence classes is too large to\\n\\nreasonably calculate directly. It is necessary to employ Burnside’s Theorem.\\n\\n\\n\\n14.3. BURNSIDE’S COUNTING THEOREM 229\\n\\nConsider a switching function with three possible inputs, a, b, and c. As we have\\nmentioned, two switching functions f and g are equivalent if a permutation of the input\\nvariables of f gives g. It is important to notice that a permutation of the switching functions\\nis not simply a permutation of the input values {a, b, c}. A switching function is a set of\\noutput values for the inputs a, b, and c, so when we consider equivalent switching functions,\\nwe are permuting 23 possible outputs, not just three input values. For example, each binary\\ntriple (a, b, c) has a specific output associated with it. The permutation (acb) changes\\noutputs as follows:\\n\\n(0, 0, 0) 7→ (0, 0, 0)\\n\\n(0, 0, 1) 7→ (0, 1, 0)\\n\\n(0, 1, 0) 7→ (1, 0, 0)\\n\\n...\\n(1, 1, 0) 7→ (1, 0, 1)\\n\\n(1, 1, 1) 7→ (1, 1, 1).\\n\\nLet X be the set of output values for a switching function in n variables. Then |X| = 2n.\\nWe can enumerate these values as follows:\\n\\n(0, . . . , 0, 1) 7→ 0\\n\\n(0, . . . , 1, 0) 7→ 1\\n\\n(0, . . . , 1, 1) 7→ 2\\n\\n...\\n(1, . . . , 1, 1) 7→ 2n − 1.\\n\\nNow let us consider a circuit with four input variables and a single output. Suppose that\\nwe can permute the leads of any circuit according to the following permutation group:\\n\\n(a) (ac) (bd) (adcb)\\n\\n(abcd) (ab)(cd) (ad)(bc) (ac)(bd).\\n\\nThe permutations of the four possible input variables induce the permutations of the output\\nvalues in Table 14.27.\\n\\nHence, there are\\n\\n1\\n\\n8\\n(216 + 2 · 212 + 2 · 26 + 3 · 210) = 9616\\n\\npossible switching functions of four variables under this group of permutations. This number\\nwill be even smaller if we consider the full symmetric group on four letters.\\n\\n\\n\\n230 CHAPTER 14. GROUP ACTIONS\\n\\nGroup Number\\nPermutation Switching Function Permutation of Cycles\\n(a) (0) 16\\n(ac) (2, 8)(3, 9)(6, 12)(7, 13) 12\\n(bd) (1, 4)(3, 6)(9, 12)(11, 14) 12\\n(adcb) (1, 2, 4, 8)(3, 6.12, 9)(5, 10)(7, 14, 13, 11) 6\\n(abcd) (1, 8, 4, 2)(3, 9, 12, 6)(5, 10)(7, 11, 13, 14) 6\\n(ab)(cd) (1, 2)(4, 8)(5, 10)(6, 9)(7, 11)(13, 14) 10\\n(ad)(bc) (1, 8)(2, 4)(3, 12)(5, 10)(7, 14)(11, 13) 10\\n(ac)(bd) (1, 4)(2, 8)(3, 12)(6, 9)(7, 13)(11, 14) 10\\n\\nTable 14.27: Permutations of switching functions in four variables\\n\\nHistorical Note\\n\\nWilliam Burnside was born in London in 1852. He attended Cambridge University from\\n1871 to 1875 and won the Smith’s Prize in his last year. After his graduation he lectured\\nat Cambridge. He was made a member of the Royal Society in 1893. Burnside wrote\\napproximately 150 papers on topics in applied mathematics, differential geometry, and\\nprobability, but his most famous contributions were in group theory. Several of Burnside’s\\nconjectures have stimulated research to this day. One such conjecture was that every group\\nof odd order is solvable; that is, for a group G of odd order, there exists a sequence of\\nsubgroups\\n\\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}\\n\\nsuch that Hi is normal in Hi+1 and Hi+1/Hi is abelian. This conjecture was finally proven\\nby W. Feit and J. Thompson in 1963. Burnside’s The Theory of Groups of Finite Order,\\npublished in 1897, was one of the first books to treat groups in a modern context as opposed\\nto permutation groups. The second edition, published in 1911, is still a classic.\\n\\n14.4 Exercises\\n1. Examples 14.1–14.5 in the first section each describe an action of a group G on a set X,\\nwhich will give rise to the equivalence relation defined by G-equivalence. For each example,\\ncompute the equivalence classes of the equivalence relation, the G-equivalence classes.\\n\\n2. Compute all Xg and all Gx for each of the following permutation groups.\\n(a) X = {1, 2, 3},\\n\\nG = S3 = {(1), (12), (13), (23), (123), (132)}\\n(b) X = {1, 2, 3, 4, 5, 6}, G = {(1), (12), (345), (354), (12)(345), (12)(354)}\\n\\n3. Compute the G-equivalence classes of X for each of the G-sets in Exercise 14.4.2. For\\neach x ∈ X verify that |G| = |Ox| · |Gx|.\\n\\n4. Let G be the additive group of real numbers. Let the action of θ ∈ G on the real plane\\nR2 be given by rotating the plane counterclockwise about the origin through θ radians. Let\\nP be a point on the plane other than the origin.\\n(a) Show that R2 is a G-set.\\n(b) Describe geometrically the orbit containing P .\\n\\n\\n\\n14.4. EXERCISES 231\\n\\n(c) Find the group GP .\\n\\n5. Let G = A4 and suppose that G acts on itself by conjugation; that is, (g, h) 7→ ghg−1.\\n\\n(a) Determine the conjugacy classes (orbits) of each element of G.\\n\\n(b) Determine all of the isotropy subgroups for each element of G.\\n\\n6. Find the conjugacy classes and the class equation for each of the following groups.\\n\\n(a) S4 (b) D5 (c) Z9 (d) Q8\\n\\n7. Write the class equation for S5 and for A5.\\n\\n8. If a square remains fixed in the plane, how many different ways can the corners of the\\nsquare be colored if three colors are used?\\n\\n9. How many ways can the vertices of an equilateral triangle be colored using three different\\ncolors?\\n\\n10. Find the number of ways a six-sided die can be constructed if each side is marked\\ndifferently with 1, . . . , 6 dots.\\n\\n11. Up to a rotation, how many ways can the faces of a cube be colored with three different\\ncolors?\\n\\n12. Consider 12 straight wires of equal lengths with their ends soldered together to form\\nthe edges of a cube. Either silver or copper wire can be used for each edge. How many\\ndifferent ways can the cube be constructed?\\n\\n13. Suppose that we color each of the eight corners of a cube. Using three different colors,\\nhow many ways can the corners be colored up to a rotation of the cube?\\n\\n14. Each of the faces of a regular tetrahedron can be painted either red or white. Up to a\\nrotation, how many different ways can the tetrahedron be painted?\\n\\n15. Suppose that the vertices of a regular hexagon are to be colored either red or white.\\nHow many ways can this be done up to a symmetry of the hexagon?\\n\\n16. A molecule of benzene is made up of six carbon atoms and six hydrogen atoms, linked\\ntogether in a hexagonal shape as in Figure 14.28.\\n\\n(a) How many different compounds can be formed by replacing one or more of the hydrogen\\natoms with a chlorine atom?\\n\\n(b) Find the number of different chemical compounds that can be formed by replacing\\nthree of the six hydrogen atoms in a benzene ring with a CH3 radical.\\n\\n\\n\\n232 CHAPTER 14. GROUP ACTIONS\\n\\nH\\n\\nH\\n\\nHH\\n\\nH H\\n\\nFigure 14.28: A benzene ring\\n\\n17. How many equivalence classes of switching functions are there if the input variables x1,\\nx2, and x3 can be permuted by any permutation in S3? What if the input variables x1, x2,\\nx3, and x4 can be permuted by any permutation in S4?\\n\\n18. How many equivalence classes of switching functions are there if the input variables\\nx1, x2, x3, and x4 can be permuted by any permutation in the subgroup of S4 generated\\nby the permutation (x1x2x3x4)?\\n\\n19. A striped necktie has 12 bands of color. Each band can be colored by one of four\\npossible colors. How many possible different-colored neckties are there?\\n\\n20. A group acts faithfully on a G-set X if the identity is the only element of G that\\nleaves every element of X fixed. Show that G acts faithfully on X if and only if no two\\ndistinct elements of G have the same action on each element of X.\\n\\n21. Let p be prime. Show that the number of different abelian groups of order pn (up to\\nisomorphism) is the same as the number of conjugacy classes in Sn.\\n\\n22. Let a ∈ G. Show that for any g ∈ G, gC(a)g−1 = C(gag−1).\\n\\n23. Let |G| = pn and suppose that |Z(G)| = pn−1 for p prime. Prove that G is abelian.\\n\\n24. Let G be a group with order pn where p is prime and X a finite G-set. If XG = {x ∈\\nX : gx = x for all g ∈ G} is the set of elements in X fixed by the group action, then prove\\nthat |X| ≡ |XG| (mod p).\\n\\n25. If G is a group of order pn, where p is prime and n ≥ 2, show that G must have a\\nproper subgroup of order p. If n ≥ 3, is it true that G will have a proper subgroup of order\\np2?\\n\\n14.5 Programming Exercise\\n1. Write a program to compute the number of conjugacy classes in Sn. What is the largest\\nn for which your program will work?\\n\\n14.6 References and Suggested Reading\\n[1] De Bruijin, N. G. “Pólya’s Theory of Counting,” in Applied Combinatorial Mathemat-\\n\\nics, Beckenbach, E. F., ed. Wiley, New York, 1964.\\n\\n\\n\\n14.7. SAGE 233\\n\\n[2] Eidswick, J. A. “Cubelike Puzzles—What Are They and How Do You Solve Them?”\\nAmerican Mathematical Monthly 93(1986), 157–76.\\n\\n[3] Harary, F., Palmer, E. M., and Robinson, R. W. “Pólya’s Contributions to Chem-\\nical Enumeration,” in Chemical Applications of Graph Theory, Balaban, A. T., ed.\\nAcademic Press, London, 1976.\\n\\n[4] Gårding, L. and Tambour, T. Algebra for Computer Science. Springer-Verlag, New\\nYork, 1988.\\n\\n[5] Laufer, H. B. Discrete Mathematics and Applied Modern Algebra. PWS-Kent, Boston,\\n1984.\\n\\n[6] Pólya, G. and Read, R. C. Combinatorial Enumeration of Groups, Graphs, and Chem-\\nical Compounds. Springer-Verlag, New York, 1985.\\n\\n[7] Shapiro, L. W. “Finite Groups Acting on Sets with Applications,” Mathematics Mag-\\nazine, May–June 1973, 136–47.\\n\\n14.7 Sage\\nGroups can be realized in many ways, such as as sets of permutations, as sets of matrices,\\nor as sets of abstract symbols related by certain rules (“presentations”) and in myriad other\\nways. We have concentrated on permutation groups because of their concrete feel, with\\nelements written as functions, and because of their thorough implementation in Sage. Group\\nactions are of great interest when the set they act on is the group itself, and group actions\\nwill figure prominently in the proofs of the main results of the next chapter. However, any\\ntime we have a group action on a set, we can view that group as a permutation group on\\nthe elements of the set. So permutation groups are an area of group theory of independent\\ninterest, with its own definitions and theorems.\\n\\nWe will describe Sage’s commands applicable when a group action arises naturally via\\nconjugation, and then move into the more general situation in a more general application.\\n\\nConjugation as a Group Action\\nWe might think we need to be careful how Sage defines conjugation (gxg−1 versus g−1xg)\\nand the difference between Sage and the text on the order of products. However, if you look\\nat the definition of the center and centralizer subgroups you can see that any difference in\\nordering is irrelevant. Here are the group action commands for the particular action that\\nis conjugation of the elements of the group.\\n\\nSage has a permutation group method .center() which returns the subgroup of fixed\\npoints. The permutation group method, .centralizer(g), returns a subgroup that is the\\nstabilizer of the group element g. Finally, the orbits are given by conjugacy classes, but\\nSage will not flood you with the full conjugacy classes and instead gives back a list of\\none element per conjugacy class, the representatives, via the permutation group method\\n.conjugacy_classes_representatives(). You can manually reconstruct a conjugacy class\\nfrom a representative, as we do in the example below.\\n\\nHere is an example of the above commands in action. Notice that an abelian group\\nwould be a bad choice for this example.\\n\\nD = DihedralGroup (8)\\nC = D.center (); C\\n\\nSubgroup of (Dihedral group of order 16 as a permutation group)\\ngenerated by [(1,5)(2,6)(3,7)(4,8)]\\n\\n\\n\\n234 CHAPTER 14. GROUP ACTIONS\\n\\nC.list()\\n\\n[(), (1,5)(2,6)(3,7)(4,8)]\\n\\na = D(\\"(1,2)(3,8)(4,7)(5,6)\\")\\nC1 = D.centralizer(a); C1.list()\\n\\n[(), (1,2)(3,8)(4,7)(5,6), (1,5)(2,6)(3,7)(4,8),\\n(1,6)(2,5)(3,4)(7,8)]\\n\\nb = D(\\"(1,2,3,4,5,6,7,8)\\")\\nC2 = D.centralizer(b); C2.order()\\n\\n8\\n\\nCCR = D.conjugacy_classes_representatives (); CCR\\n\\n[(), (2,8)(3,7)(4,6), (1,2)(3,8)(4,7)(5,6), (1,2,3,4,5,6,7,8),\\n(1,3,5,7)(2,4,6,8), (1,4,7,2,5,8,3,6), (1,5)(2,6)(3,7)(4,8)]\\n\\nr = CCR [2]; r\\n\\n(1,2)(3,8)(4,7)(5,6)\\n\\nconj = []\\nx = [conj.append(g^-1*r*g) for g in D if not g^-1*r*g in conj]\\nconj\\n\\n[(1 ,2)(3,8)(4,7)(5,6), (1,4)(2,3)(5,8)(6,7),\\n(1,6)(2,5)(3,4)(7,8), (1,8)(2,7)(3,6)(4,5)]\\n\\nNotice that in the one conjugacy class constructed all the elements have the same cycle\\nstructure, which is no accident. Notice too that rep and a are the same element, and the\\nproduct of the order of the centralizer (4) and the size of the conjugacy class (4) equals the\\norder of the group (16), which is a variant of the conclusion of Theorem 14.11.\\n\\nVerify that the following is a demonstration of the class equation in the special case\\nwhen the action is conjugation, but would be valid for any group, rather than just D.\\n\\nsizes = [D.order()/D.centralizer(g).order()\\nfor g in D.conjugacy_classes_representatives ()]\\n\\nsizes\\n\\n[1, 4, 4, 2, 2, 2, 1]\\n\\nD.order() == sum(sizes)\\n\\nTrue\\n\\n\\n\\n14.7. SAGE 235\\n\\nGraph Automorphisms\\nAs mentioned, group actions can be even more interesting when the set they act on is\\ndifferent from the group itself. One class of examples is the group of symmetries of a\\ngeometric solid, where the objects in the set are the vertices of the object, or perhaps\\nsome other aspect such as edges, faces or diagonals. In this case, the group is all those\\npermutations that move the solid but leave it filling the same space before the motion\\n(“rigid motions”).\\n\\nIn this section we will examine something very similar. A graph is a mathematical\\nobject, consisting of vertices and edges, but the only structure is whether or not any given\\npair of vertices are joined by an edge or not. The group consists of permutations of vertices\\nthat preserve the structure, that is, permutations of vertices that take edges to edges and\\nnon-edges to non-edges. It is very similar to a symmetry group, but there is no notion of\\nany geometric relationships being preserved.\\n\\nHere is an example. You will need to run the first compute cell to define the graph and\\nget a nice graphic representation.\\n\\nQ = graphs.CubeGraph (3)\\nQ.plot(layout= \' spring \' )\\n\\nA = Q.automorphism_group ()\\nA.order()\\n\\n48\\n\\nYour plot should look like the vertices and edges of a cube, but may not quite look\\nregular, which is fine, since the geometry is not relevant. Vertices are labeled with strings\\nof three binary digits, 0 or 1, and any two vertices are connected by an edge if their strings\\ndiffer in exactly one location. We might expect the group of symmetries to have order 24,\\nrather than order 48, given its resemblance to a cube (in appearance and in name). However,\\nwhen not restricted to rigid motions, we have new permutations that preserve edges. One\\nin particular is to interchange two “opposite faces.” Locate two 4-cycles opposite of each\\nother, listed in the same order: 000, 010, 110, 100 and 001, 011, 111, 101. Notice that each\\ncycle looks very similar, but all the vertices of the first end in a zero and the second cycle\\nhas vertices ending in a one.\\n\\nWe can create explicitly the permutation that interchanges these two opposite faces,\\nusing a text version of the permutation in cycle notation.\\n\\na = A(\\" ( \'000 \' , \'001 \') ( \'010 \' , \'011 \') ( \'110 \' , \'111 \') ( \'100 \' , \'101 \') \\")\\na in A\\n\\nTrue\\n\\nWe can use this group to illustrate the relevant Sage commands for group actions.\\nA.orbits ()\\n\\n[[ \' 000 \' , \' 001 \' , \' 010 \' , \' 100 \' , \' 011 \' , \' 101 \' , \' 110 \' , \' 111 \' ]]\\n\\nSo this action has only one (big) orbit. This implies that every vertex is “like” any\\nother. When a permutation group behaves this way, we say the group is transitive.\\n\\nA.is_transitive ()\\n\\nTrue\\n\\n\\n\\n236 CHAPTER 14. GROUP ACTIONS\\n\\nIf every vertex is “the same” we can compute the stabilizer of any vertex, since they\\nwill all be isomorphic. Because vertex 000 is the simplest in some sense, we compute its\\nstabilizer.\\n\\nS = A.stabilizer( \' 000 \' )\\nS.list()\\n\\n[(),\\n( \' 001 \' , \' 100 \' , \' 010 \' )( \' 011 \' , \' 101 \' , \' 110 \' ),\\n( \' 010 \' , \' 100 \' )( \' 011 \' , \' 101 \' ),\\n( \' 001 \' , \' 010 \' , \' 100 \' )( \' 011 \' , \' 110 \' , \' 101 \' ),\\n( \' 001 \' , \' 100 \' )( \' 011 \' , \' 110 \' ),\\n( \' 001 \' , \' 010 \' )( \' 101 \' , \' 110 \' )]\\n\\nThat S has 6 elements is no surprise, since the group has order 48 and the size of the\\nlone orbit is 8. But we can go one step further. The three vertices of the graph attached\\ndirectly to 000 are 100, 010, 001. Any automorphism of the graph that fixes 000 must then\\npermute the three adjacent vertices. There are 3! = 6 possible ways to do this, and you can\\ncheck that each appears in one of the six elements of the stabilizer. So we can understand a\\ntransitive group by considering the smaller stabilizer, and in this case we can see that each\\nelement of the stabilizer is determined by how it permutes the neighbors of the stabilized\\nvertex.\\n\\nTransitive groups are both unusual and important. To contrast, here is a graph auto-\\nmorphism group that is far from transitive (without being trivial). A path is a graph that\\nhas all of its vertices in a line. Run the first compute cell to see a path on 11 vertices.\\n\\nP = graphs.PathGraph (11)\\nP.plot()\\n\\nA = P.automorphism_group ()\\nA.list()\\n\\n[(), (0,10)(1,9)(2,8)(3,7)(4,6)]\\n\\nThe automorphism group is the trivial identity automorphism (always) and an order 2\\npermutation that “flips” the path end-to-end. The group is far from transitive and there\\nare many orbits.\\n\\nA.is_transitive ()\\n\\nFalse\\n\\nA.orbits ()\\n\\n[[0, 10], [1, 9], [2, 8], [3, 7], [4, 6], [5]]\\n\\nMost of the stabilizers are trivial, with one exception. As subgroups of a group of order\\n2, there really are not too many options.\\n\\nA.stabilizer (2).list()\\n\\n[()]\\n\\nA.stabilizer (5).list()\\n\\n[(), (0,10)(1,9)(2,8)(3,7)(4,6)]\\n\\n\\n\\n14.8. SAGE EXERCISES 237\\n\\nHow would this final example have been different if we had used a path on 10 vertices?\\nNOTE: There was once a small bug with stabilizers being created as subgroups of\\n\\nsymmetric groups on fewer symbols than the correct number. This is fixed in Sage 4.8 and\\nnewer. Note the correct output below, and you can check your installation by running these\\ncommands. If you do not see the singleton [4] in your output, you should definitely update\\nyour copy of Sage.\\n\\nG = SymmetricGroup (4)\\nS = G.stabilizer (4)\\nS.orbits ()\\n\\n[[1, 2, 3], [4]]\\n\\n14.8 Sage Exercises\\n1. Construct the Higman-Sims graph with the command graphs.HigmanSimsGraph(). Then\\nconstruct the automorphism group and determine the order of the one interesting normal\\nsubgroup of this group. You can try plotting the graph, but the graphic is unlikely to be\\nvery informative.\\n\\n2. This exercise asks you to verify the class equation outside of the usual situation where\\nthe group action is conjugation. Consider the example of the automorphism group of the\\npath on 11 vertices. First construct the list of orbits. From each orbit, grab the first element\\nof the orbit as a representative. Compute the size of the orbit as the index of the stabilizer\\nof the representative in the group via Theorem 14.11. (Yes, you could just compute the size\\nof the full orbit, but the idea of the exercise is to use more group-theoretic results.) Then\\nsum these orbit-sizes, which should equal the size of the whole vertex set since the orbits\\nform a partition.\\n\\n3. Construct a simple graph (no lopps or multiple edges), with at least two vertices and\\nat least one edge, whose automorphism group is trivial. You might start experimenting by\\ndrawing pictures on paper before constructing the graph. A command like the following\\nwill let you construct a graph from edges. The graph below looks like a triangle or 3-cycle.\\n\\nG = Graph ([(1 ,2), (2,3), (3,1)])\\nG.plot()\\n\\n4. For the following two pairs of groups, compute the list of conjugacy class representatives\\nfor each group in the pair. For each part, compare and contrast the results for the two\\ngroups in the pair, with thoughtful and insightful comments.\\n(a) The full symmetric group on 5 symbols, S5, and the alternating group on 5 symbols,\\n\\nA5.\\n(b) The dihedral groups that are symmetries of a 7-gon and an 8-gon, D7 and D8.\\n\\n5. Use the command graphs.CubeGraph(4) to build the four-dimensional cube graph, Q4.\\nUsing a plain .plot() command (without a spring layout) should create a nice plot. Con-\\nstruct the automorphism group of the graph, which will provide a group action on the\\nvertex set.\\n(a) Construct the orbits of this action, and comment.\\n(b) Construct a stabilizer of a single vertex (which is a subgroup of the full automorphism\\n\\ngroup) and then consider the action of this group on the vertex set. Construct the orbits\\n\\n\\n\\n238 CHAPTER 14. GROUP ACTIONS\\n\\nof this new action, and comment carefully and fully on your observations, especially\\nin terms of the vertices of the graph.\\n\\n6. Build the graph given by the commands below. The result should be a symmetric-looking\\ngraph with an automorphism group of order 16.\\n\\nG = graphs.CycleGraph (8)\\nG.add_edges ([(0 ,2) ,(1,3) ,(4,6) ,(5,7)])\\nG.plot()\\n\\nRepeat the two parts of the previous exercise, but realize that in the second part there\\nare now two different stabilizers to create, so build both and compare the differences in\\nthe stabilizers and their orbits. Creating a second plot with G.plot(layout=\'planar\') might\\nprovide extra insight.\\n\\n\\n\\n15\\n\\nThe Sylow Theorems\\n\\nWe already know that the converse of Lagrange’s Theorem is false. If G is a group of\\norder m and n divides m, then G does not necessarily possess a subgroup of order n. For\\nexample, A4 has order 12 but does not possess a subgroup of order 6. However, the Sylow\\nTheorems do provide a partial converse for Lagrange’s Theorem—in certain cases they\\nguarantee us subgroups of specific orders. These theorems yield a powerful set of tools for\\nthe classification of all finite nonabelian groups.\\n\\n15.1 The Sylow Theorems\\nWe will use what we have learned about group actions to prove the Sylow Theorems. Recall\\nfor a moment what it means for G to act on itself by conjugation and how conjugacy classes\\nare distributed in the group according to the class equation, discussed in Chapter 14. A\\ngroup G acts on itself by conjugation via the map (g, x) 7→ gxg−1. Let x1, . . . , xk be\\nrepresentatives from each of the distinct conjugacy classes of G that consist of more than\\none element. Then the class equation can be written as\\n\\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)],\\n\\nwhere Z(G) = {g ∈ G : gx = xg for all x ∈ G} is the center of G and C(xi) = {g ∈ G :\\ngxi = xig} is the centralizer subgroup of xi.\\n\\nWe begin our investigation of the Sylow Theorems by examining subgroups of order p,\\nwhere p is prime. A group G is a p-group if every element in G has as its order a power of\\np, where p is a prime number. A subgroup of a group G is a p-subgroup if it is a p-group.\\n\\nTheorem 15.1 (Cauchy). Let G be a finite group and p a prime such that p divides the\\norder of G. Then G contains a subgroup of order p.\\n\\nProof. We will use induction on the order of G. If |G| = p, then clearly order k, where\\np ≤ k < n and p divides k, has an element of order p. Assume that |G| = n and p | n and\\nconsider the class equation of G:\\n\\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)].\\n\\nWe have two cases.\\nCase 1. The order of one of the centralizer subgroups, C(xi), is divisible by p for some i,\\n\\ni = 1, . . . , k. In this case, by our induction hypothesis, we are done. Since C(xi) is a proper\\nsubgroup of G and p divides |C(xi)|, C(xi) must contain an element of order p. Hence, G\\nmust contain an element of order p.\\n\\nCase 2. The order of no centralizer subgroup is divisible by p. Then p divides [G : C(xi)],\\nthe order of each conjugacy class in the class equation; hence, p must divide the center of\\n\\n239\\n\\n\\n\\n240 CHAPTER 15. THE SYLOW THEOREMS\\n\\nG, Z(G). Since Z(G) is abelian, it must have a subgroup of order p by the Fundamental\\nTheorem of Finite Abelian Groups. Therefore, the center of G contains an element of order\\np.\\n\\nCorollary 15.2. Let G be a finite group. Then G is a p-group if and only if |G| = pn.\\n\\nExample 15.3. Let us consider the group A5. We know that |A5| = 60 = 22 · 3 · 5. By\\nCauchy’s Theorem, we are guaranteed that A5 has subgroups of orders 2, 3 and 5. The\\nSylow Theorems will give us even more information about the possible subgroups of A5.\\n\\nWe are now ready to state and prove the first of the Sylow Theorems. The proof is very\\nsimilar to the proof of Cauchy’s Theorem.\\n\\nTheorem 15.4 (First Sylow Theorem). Let G be a finite group and p a prime such that pr\\ndivides |G|. Then G contains a subgroup of order pr.\\n\\nProof. We induct on the order of G once again. If |G| = p, then we are done. Now\\nsuppose that the order of G is n with n > p and that the theorem is true for all groups of\\norder less than n, where p divides n. We shall apply the class equation once again:\\n\\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)].\\n\\nFirst suppose that p does not divide [G : C(xi)] for some i. Then pr | |C(xi)|, since pr\\ndivides |G| = |C(xi)| · [G : C(xi)]. Now we can apply the induction hypothesis to C(xi).\\n\\nHence, we may assume that p divides [G : C(xi)] for all i. Since p divides |G|, the class\\nequation says that p must divide |Z(G)|; hence, by Cauchy’s Theorem, Z(G) has an element\\nof order p, say g. Let N be the group generated by g. Clearly, N is a normal subgroup\\nof Z(G) since Z(G) is abelian; therefore, N is normal in G since every element in Z(G)\\ncommutes with every element in G. Now consider the factor group G/N of order |G|/p. By\\nthe induction hypothesis, G/N contains a subgroup H of order pr−1. The inverse image of\\nH under the canonical homomorphism ϕ : G→ G/N is a subgroup of order pr in G.\\n\\nA Sylow p-subgroup P of a group G is a maximal p-subgroup of G. To prove the other\\ntwo Sylow Theorems, we need to consider conjugate subgroups as opposed to conjugate\\nelements in a group. For a group G, let S be the collection of all subgroups of G. For any\\nsubgroup H, S is a H-set, where H acts on S by conjugation. That is, we have an action\\n\\nH × S → S\\n\\ndefined by\\nh ·K 7→ hKh−1\\n\\nfor K in S.\\nThe set\\n\\nN(H) = {g ∈ G : gHg−1 = H}\\n\\nis a subgroup of G called the the normalizer of H in G. Notice that H is a normal\\nsubgroup of N(H). In fact, N(H) is the largest subgroup of G in which H is normal.\\n\\nLemma 15.5. Let P be a Sylow p-subgroup of a finite group G and let x have as its order\\na power of p. If x−1Px = P , then x ∈ P .\\n\\n\\n\\n15.1. THE SYLOW THEOREMS 241\\n\\nProof. Certainly x ∈ N(P ), and the cyclic subgroup, ⟨xP ⟩ ⊂ N(P )/P , has as its order a\\npower of p. By the Correspondence Theorem there exists a subgroup H of N(P ) containing\\nP such that H/P = ⟨xP ⟩. Since |H| = |P | · |⟨xP ⟩|, the order of H must be a power of\\np. However, P is a Sylow p-subgroup contained in H. Since the order of P is the largest\\npower of p dividing |G|, H = P . Therefore, H/P is the trivial subgroup and xP = P , or\\nx ∈ P .\\n\\nLemma 15.6. Let H and K be subgroups of G. The number of distinct H-conjugates of\\nK is [H : N(K) ∩H].\\nProof. We define a bijection between the conjugacy classes of K and the right cosets of\\nN(K)∩H by h−1Kh 7→ (N(K)∩H)h. To show that this map is a bijection, let h1, h2 ∈ H\\nand suppose that (N(K) ∩ H)h1 = (N(K) ∩ H)h2. Then h2h\\n\\n−1\\n1 ∈ N(K). Therefore,\\n\\nK = h2h\\n−1\\n1 Kh1h\\n\\n−1\\n2 or h−1\\n\\n1 Kh1 = h−1\\n2 Kh2, and the map is an injection. It is easy to\\n\\nsee that this map is surjective; hence, we have a one-to-one and onto map between the\\nH-conjugates of K and the right cosets of N(K) ∩H in H.\\n\\nTheorem 15.7 (Second Sylow Theorem). Let G be a finite group and p a prime dividing\\n|G|. Then all Sylow p-subgroups of G are conjugate. That is, if P1 and P2 are two Sylow\\np-subgroups, there exists a g ∈ G such that gP1g\\n\\n−1 = P2.\\nProof. Let P be a Sylow p-subgroup of G and suppose that |G| = prm with |P | = pr. Let\\n\\nS = {P = P1, P2, . . . , Pk}\\n\\nconsist of the distinct conjugates of P in G. By Lemma 15.6, k = [G : N(P )]. Notice that\\n\\n|G| = prm = |N(P )| · [G : N(P )] = |N(P )| · k.\\n\\nSince pr divides |N(P )|, p cannot divide k.\\nGiven any other Sylow p-subgroup Q, we must show that Q ∈ S. Consider the Q-\\n\\nconjugacy classes of each Pi. Clearly, these conjugacy classes partition S. The size of the\\npartition containing Pi is [Q : N(Pi)∩Q] by Lemma 15.6, and Lagrange’s Theorem tells us\\nthat |Q| = [Q : N(Pi)∩Q]|N(Pi)∩Q|. Thus, [Q : N(Pi)∩Q] must be a divisor of |Q| = pr.\\nHence, the number of conjugates in every equivalence class of the partition is a power of\\np. However, since p does not divide k, one of these equivalence classes must contain only a\\nsingle Sylow p-subgroup, say Pj . In this case, x−1Pjx = Pj for all x ∈ Q. By Lemma 15.5,\\nPj = Q.\\n\\nTheorem 15.8 (Third Sylow Theorem). Let G be a finite group and let p be a prime\\ndividing the order of G. Then the number of Sylow p-subgroups is congruent to 1 (mod p)\\nand divides |G|.\\nProof. Let P be a Sylow p-subgroup acting on the set of Sylow p-subgroups,\\n\\nS = {P = P1, P2, . . . , Pk},\\n\\nby conjugation. From the proof of the Second Sylow Theorem, the only P -conjugate of P\\nis itself and the order of the other P -conjugacy classes is a power of p. Each P -conjugacy\\nclass contributes a positive power of p toward |S| except the equivalence class {P}. Since\\n|S| is the sum of positive powers of p and 1, |S| ≡ 1 (mod p).\\n\\nNow suppose that G acts on S by conjugation. Since all Sylow p-subgroups are conju-\\ngate, there can be only one orbit under this action. For P ∈ S,\\n\\n|S| = |orbit of P | = [G : N(P )]\\n\\nby Lemma 15.6. But [G : N(P )] is a divisor of |G|; consequently, the number of Sylow\\np-subgroups of a finite group must divide the order of the group.\\n\\n\\n\\n242 CHAPTER 15. THE SYLOW THEOREMS\\n\\nHistorical Note\\n\\nPeter Ludvig Mejdell Sylow was born in 1832 in Christiania, Norway (now Oslo). After\\nattending Christiania University, Sylow taught high school. In 1862 he obtained a temporary\\nappointment at Christiania University. Even though his appointment was relatively brief,\\nhe influenced students such as Sophus Lie (1842–1899). Sylow had a chance at a permanent\\nchair in 1869, but failed to obtain the appointment. In 1872, he published a 10-page paper\\npresenting the theorems that now bear his name. Later Lie and Sylow collaborated on a\\nnew edition of Abel’s works. In 1898, a chair at Christiania University was finally created\\nfor Sylow through the efforts of his student and colleague Lie. Sylow died in 1918.\\n\\n15.2 Examples and Applications\\nExample 15.9. Using the Sylow Theorems, we can determine that A5 has subgroups of\\norders 2, 3, 4, and 5. The Sylow p-subgroups of A5 have orders 3, 4, and 5. The Third Sylow\\nTheorem tells us exactly how many Sylow p-subgroups A5 has. Since the number of Sylow\\n5-subgroups must divide 60 and also be congruent to 1 (mod 5), there are either one or six\\nSylow 5-subgroups in A5. All Sylow 5-subgroups are conjugate. If there were only a single\\nSylow 5-subgroup, it would be conjugate to itself; that is, it would be a normal subgroup of\\nA5. Since A5 has no normal subgroups, this is impossible; hence, we have determined that\\nthere are exactly six distinct Sylow 5-subgroups of A5.\\n\\nThe Sylow Theorems allow us to prove many useful results about finite groups. By\\nusing them, we can often conclude a great deal about groups of a particular order if certain\\nhypotheses are satisfied.\\n\\nTheorem 15.10. If p and q are distinct primes with p < q, then every group G of order\\npq has a single subgroup of order q and this subgroup is normal in G. Hence, G cannot be\\nsimple. Furthermore, if q ̸≡ 1 (mod p), then G is cyclic.\\n\\nProof. We know that G contains a subgroup H of order q. The number of conjugates of\\nH divides pq and is equal to 1 + kq for k = 0, 1, . . .. However, 1 + q is already too large to\\ndivide the order of the group; hence, H can only be conjugate to itself. That is, H must be\\nnormal in G.\\n\\nThe group G also has a Sylow p-subgroup, say K. The number of conjugates of K must\\ndivide q and be equal to 1 + kp for k = 0, 1, . . .. Since q is prime, either 1 + kp = q or\\n1 + kp = 1. If 1 + kp = 1, then K is normal in G. In this case, we can easily show that G\\nsatisfies the criteria, given in Chapter 9, for the internal direct product of H and K. Since\\nH is isomorphic to Zq and K is isomorphic to Zp, G ∼= Zp×Zq\\n\\n∼= Zpq by Theorem 9.21.\\n\\nExample 15.11. Every group of order 15 is cyclic. This is true because 15 = 5 · 3 and\\n5 ̸≡ 1 (mod 3).\\n\\nExample 15.12. Let us classify all of the groups of order 99 = 32 · 11 up to isomorphism.\\nFirst we will show that every group G of order 99 is abelian. By the Third Sylow Theorem,\\nthere are 1+3k Sylow 3-subgroups, each of order 9, for some k = 0, 1, 2, . . .. Also, 1+3k must\\ndivide 11; hence, there can only be a single normal Sylow 3-subgroup H in G. Similarly,\\nthere are 1+11k Sylow 11-subgroups and 1+11k must divide 9. Consequently, there is only\\none Sylow 11-subgroup K in G. By Corollary 14.16, any group of order p2 is abelian for p\\nprime; hence, H is isomorphic either to Z3 × Z3 or to Z9. Since K has order 11, it must\\nbe isomorphic to Z11. Therefore, the only possible groups of order 99 are Z3 × Z3 × Z11 or\\nZ9 × Z11 up to isomorphism.\\n\\n\\n\\n15.2. EXAMPLES AND APPLICATIONS 243\\n\\nTo determine all of the groups of order 5 · 7 · 47 = 1645, we need the following theorem.\\n\\nTheorem 15.13. Let G′ = ⟨aba−1b−1 : a, b ∈ G⟩ be the subgroup consisting of all finite\\nproducts of elements of the form aba−1b−1 in a group G. Then G′ is a normal subgroup of\\nG and G/G′ is abelian.\\n\\nThe subgroup G′ of G is called the commutator subgroup of G. We leave the proof\\nof this theorem as an exercise (Exercise 10.3.14 in Chapter 10).\\n\\nExample 15.14. We will now show that every group of order 5 · 7 · 47 = 1645 is abelian,\\nand cyclic by Corollary 9.21. By the Third Sylow Theorem, G has only one subgroup H1\\n\\nof order 47. So G/H1 has order 35 and must be abelian by Theorem 15.10. Hence, the\\ncommutator subgroup of G is contained in H which tells us that |G′| is either 1 or 47. If\\n|G′| = 1, we are done. Suppose that |G′| = 47. The Third Sylow Theorem tells us that\\nG has only one subgroup of order 5 and one subgroup of order 7. So there exist normal\\nsubgroups H2 and H3 in G, where |H2| = 5 and |H3| = 7. In either case the quotient group\\nis abelian; hence, G′ must be a subgroup of Hi, i = 1, 2. Therefore, the order of G′ is 1,\\n5, or 7. However, we already have determined that |G′| = 1 or 47. So the commutator\\nsubgroup of G is trivial, and consequently G is abelian.\\n\\nFinite Simple Groups\\n\\nGiven a finite group, one can ask whether or not that group has any normal subgroups.\\nRecall that a simple group is one with no proper nontrivial normal subgroups. As in the\\ncase of A5, proving a group to be simple can be a very difficult task; however, the Sylow\\nTheorems are useful tools for proving that a group is not simple. Usually, some sort of\\ncounting argument is involved.\\n\\nExample 15.15. Let us show that no group G of order 20 can be simple. By the Third\\nSylow Theorem, G contains one or more Sylow 5-subgroups. The number of such subgroups\\nis congruent to 1 (mod 5) and must also divide 20. The only possible such number is 1.\\nSince there is only a single Sylow 5-subgroup and all Sylow 5-subgroups are conjugate, this\\nsubgroup must be normal.\\n\\nExample 15.16. Let G be a finite group of order pn, n > 1 and p prime. By Theorem 14.15,\\nG has a nontrivial center. Since the center of any group G is a normal subgroup, G cannot\\nbe a simple group. Therefore, groups of orders 4, 8, 9, 16, 25, 27, 32, 49, 64, and 81 are not\\nsimple. In fact, the groups of order 4, 9, 25, and 49 are abelian by Corollary 14.16.\\n\\nExample 15.17. No group of order 56 = 23 ·7 is simple. We have seen that if we can show\\nthat there is only one Sylow p-subgroup for some prime p dividing 56, then this must be a\\nnormal subgroup and we are done. By the Third Sylow Theorem, there are either one or\\neight Sylow 7-subgroups. If there is only a single Sylow 7-subgroup, then it must be normal.\\n\\nOn the other hand, suppose that there are eight Sylow 7-subgroups. Then each of these\\nsubgroups must be cyclic; hence, the intersection of any two of these subgroups contains\\nonly the identity of the group. This leaves 8 · 6 = 48 distinct elements in the group, each\\nof order 7. Now let us count Sylow 2-subgroups. There are either one or seven Sylow\\n2-subgroups. Any element of a Sylow 2-subgroup other than the identity must have as its\\norder a power of 2; and therefore cannot be one of the 48 elements of order 7 in the Sylow\\n7-subgroups. Since a Sylow 2-subgroup has order 8, there is only enough room for a single\\nSylow 2-subgroup in a group of order 56. If there is only one Sylow 2-subgroup, it must be\\nnormal.\\n\\n\\n\\n244 CHAPTER 15. THE SYLOW THEOREMS\\n\\nFor other groups G, it is more difficult to prove that G is not simple. Suppose G has\\norder 48. In this case the technique that we employed in the last example will not work.\\nWe need the following lemma to prove that no group of order 48 is simple.\\n\\nLemma 15.18. Let H and K be finite subgroups of a group G. Then\\n\\n|HK| = |H| · |K|\\n|H ∩K|\\n\\n.\\n\\nProof. Recall that\\nHK = {hk : h ∈ H, k ∈ K}.\\n\\nCertainly, |HK| ≤ |H| · |K| since some element in HK could be written as the product of\\ndifferent elements in H and K. It is quite possible that h1k1 = h2k2 for h1, h2 ∈ H and\\nk1, k2 ∈ K. If this is the case, let\\n\\na = (h1)\\n−1h2 = k1(k2)\\n\\n−1.\\n\\nNotice that a ∈ H ∩K, since (h1)\\n−1h2 is in H and k2(k1)\\n\\n−1 is in K; consequently,\\n\\nh2 = h1a\\n−1\\n\\nk2 = ak1.\\n\\nConversely, let h = h1b\\n−1 and k = bk1 for b ∈ H ∩K. Then hk = h1k1, where h ∈ H\\n\\nand k ∈ K. Hence, any element hk ∈ HK can be written in the form hiki for hi ∈ H and\\nki ∈ K, as many times as there are elements in H ∩K; that is, |H ∩K| times. Therefore,\\n|HK| = (|H| · |K|)/|H ∩K|.\\n\\nExample 15.19. To demonstrate that a group G of order 48 is not simple, we will show\\nthat G contains either a normal subgroup of order 8 or a normal subgroup of order 16. By\\nthe Third Sylow Theorem, G has either one or three Sylow 2-subgroups of order 16. If there\\nis only one subgroup, then it must be a normal subgroup.\\n\\nSuppose that the other case is true, and two of the three Sylow 2-subgroups are H and\\nK. We claim that |H ∩K| = 8. If |H ∩K| ≤ 4, then by Lemma 15.18,\\n\\n|HK| = 16 · 16\\n4\\n\\n= 64,\\n\\nwhich is impossible. Notice that H ∩K has index two in both of H and K, so is normal in\\nboth, and thus H and K are each in the normalizer of H ∩K. Because H is a subgroup of\\nN(H ∩K) and because N(H ∩K) has strictly more than 16 elements, |N(H ∩K)| must\\nbe a multiple of 16 greater than 1, as well as dividing 48. The only possibility is that\\n|N(H ∩K)| = 48. Hence, N(H ∩K) = G.\\n\\nThe following famous conjecture of Burnside was proved in a long and difficult paper\\nby Feit and Thompson [2].\\n\\nTheorem 15.20 (Odd Order Theorem). Every finite simple group of nonprime order must\\nbe of even order.\\n\\nThe proof of this theorem laid the groundwork for a program in the 1960s and 1970s\\nthat classified all finite simple groups. The success of this program is one of the outstanding\\nachievements of modern mathematics.\\n\\n\\n\\n15.3. EXERCISES 245\\n\\n15.3 Exercises\\n1. What are the orders of all Sylow p-subgroups where G has order 18, 24, 54, 72, and 80?\\n\\n2. Find all the Sylow 3-subgroups of S4 and show that they are all conjugate.\\n\\n3. Show that every group of order 45 has a normal subgroup of order 9.\\n\\n4. Let H be a Sylow p-subgroup of G. Prove that H is the only Sylow p-subgroup of G\\ncontained in N(H).\\n\\n5. Prove that no group of order 96 is simple.\\n\\n6. Prove that no group of order 160 is simple.\\n\\n7. If H is a normal subgroup of a finite group G and |H| = pk for some prime p, show that\\nH is contained in every Sylow p-subgroup of G.\\n\\n8. Let G be a group of order p2q2, where p and q are distinct primes such that q ∤ p2 − 1\\nand p ∤ q2 − 1. Prove that G must be abelian. Find a pair of primes for which this is true.\\n\\n9. Show that a group of order 33 has only one Sylow 3-subgroup.\\n\\n10. Let H be a subgroup of a group G. Prove or disprove that the normalizer of H is\\nnormal in G.\\n\\n11. Let G be a finite group divisible by a prime p. Prove that if there is only one Sylow\\np-subgroup in G, it must be a normal subgroup of G.\\n\\n12. Let G be a group of order pr, p prime. Prove that G contains a normal subgroup of\\norder pr−1.\\n\\n13. Suppose that G is a finite group of order pnk, where k < p. Show that G must contain\\na normal subgroup.\\n\\n14. Let H be a subgroup of a finite group G. Prove that gN(H)g−1 = N(gHg−1) for any\\ng ∈ G.\\n\\n15. Prove that a group of order 108 must have a normal subgroup.\\n\\n16. Classify all the groups of order 175 up to isomorphism.\\n\\n17. Show that every group of order 255 is cyclic.\\n\\n18. Let G have order pe11 · · · penn and suppose that G has n Sylow p-subgroups P1, . . . , Pn\\n\\nwhere |Pi| = peii . Prove that G is isomorphic to P1 × · · · × Pn.\\n\\n19. Let P be a normal Sylow p-subgroup of G. Prove that every inner automorphism of G\\nfixes P .\\n\\n20. What is the smallest possible order of a group G such that G is nonabelian and |G| is\\nodd? Can you find such a group?\\n\\n21. (The Frattini Lemma) If H is a normal subgroup of a finite group G and P is a Sylow\\np-subgroup of H, for each g ∈ G show that there is an h in H such that gPg−1 = hPh−1.\\nAlso, show that if N is the normalizer of P , then G = HN .\\n\\n22. Show that if the order of G is pnq, where p and q are primes and p > q, then G contains\\na normal subgroup.\\n\\n\\n\\n246 CHAPTER 15. THE SYLOW THEOREMS\\n\\n23. Prove that the number of distinct conjugates of a subgroup H of a finite group G is\\n[G : N(H)].\\n\\n24. Prove that a Sylow 2-subgroup of S5 is isomorphic to D4.\\n\\n25. (Another Proof of the Sylow Theorems)\\n\\n(a) Suppose p is prime and p does not divide m. Show that\\n\\np ∤\\n(\\npkm\\n\\npk\\n\\n)\\n.\\n\\n(b) Let S denote the set of all pk element subsets of G. Show that p does not divide |S|.\\n\\n(c) Define an action of G on S by left multiplication, aT = {at : t ∈ T} for a ∈ G and\\nT ∈ S. Prove that this is a group action.\\n\\n(d) Prove p ∤ |OT | for some T ∈ S.\\n\\n(e) Let {T1, . . . , Tu} be an orbit such that p ∤ u and H = {g ∈ G : gT1 = T1}. Prove that\\nH is a subgroup of G and show that |G| = u|H|.\\n\\n(f) Show that pk divides |H| and pk ≤ |H|.\\n\\n(g) Show that |H| = |OT | ≤ pk; conclude that therefore pk = |H|.\\n\\n26. Let G be a group. Prove that G′ = ⟨aba−1b−1 : a, b ∈ G⟩ is a normal subgroup of G\\nand G/G′ is abelian. Find an example to show that {aba−1b−1 : a, b ∈ G} is not necessarily\\na group.\\n\\n15.4 A Project\\n\\nThe main objective of finite group theory is to classify all possible finite groups up to\\nisomorphism. This problem is very difficult even if we try to classify the groups of order\\nless than or equal to 60. However, we can break the problem down into several intermediate\\nproblems. This is a challenging project that requires a working knowledge of the group\\ntheory you have learned up to this point. Even if you do not complete it, it will teach you\\na great deal about finite groups. You can use Table 15.21 as a guide.\\n\\n\\n\\n15.5. REFERENCES AND SUGGESTED READINGS 247\\n\\nOrder Number Order Number Order Number Order Number\\n1 ? 16 14 31 1 46 2\\n2 ? 17 1 32 51 47 1\\n3 ? 18 ? 33 1 48 52\\n4 ? 19 ? 34 ? 49 ?\\n5 ? 20 5 35 1 50 5\\n6 ? 21 ? 36 14 51 ?\\n7 ? 22 2 37 1 52 ?\\n8 ? 23 1 38 ? 53 ?\\n9 ? 24 ? 39 2 54 15\\n10 ? 25 2 40 14 55 2\\n11 ? 26 2 41 1 56 ?\\n12 5 27 5 42 ? 57 2\\n13 ? 28 ? 43 1 58 ?\\n14 ? 29 1 44 4 59 1\\n15 1 30 4 45 ? 60 13\\n\\nTable 15.21: Numbers of distinct groups G, |G| ≤ 60\\n\\n1. Find all simple groups G ( |G| ≤ 60). Do not use the Odd Order Theorem unless you\\nare prepared to prove it.\\n\\n2. Find the number of distinct groups G, where the order of G is n for n = 1, . . . , 60.\\n\\n3. Find the actual groups (up to isomorphism) for each n.\\n\\n15.5 References and Suggested Readings\\n[1] Edwards, H. “A Short History of the Fields Medal,” Mathematical Intelligencer 1(1978),\\n\\n127–29.\\n[2] Feit, W. and Thompson, J. G. “Solvability of Groups of Odd Order,” Pacific Journal\\n\\nof Mathematics 13(1963), 775–1029.\\n[3] Gallian, J. A. “The Search for Finite Simple Groups,” Mathematics Magazine 49(1976),\\n\\n163–79.\\n[4] Gorenstein, D. “Classifying the Finite Simple Groups,” Bulletin of the American\\n\\nMathematical Society 14(1986), 1–98.\\n[5] Gorenstein, D. Finite Groups. AMS Chelsea Publishing, Providence RI, 1968.\\n[6] Gorenstein, D., Lyons, R., and Solomon, R. The Classification of Finite Simple\\n\\nGroups. American Mathematical Society, Providence RI, 1994.\\n\\n15.6 Sage\\nSylow Subgroups\\nThe Sage permutation group method .sylow_subgroup(p) will return a single Sylow p-\\nsubgroup. If the prime is not a proper divisor of the group order it returns a subgroup\\nof order p0, in other words, a trivial subgroup. So be careful about how you construct your\\nprimes. Sometimes, you may only want one such Sylow subgroup, since any two Sylow\\n\\n\\n\\n248 CHAPTER 15. THE SYLOW THEOREMS\\n\\np-subgroups are conjugate, and hence isomorphic (Theorem 15.7). This also means we can\\ncreate other Sylow p-subgroups by conjugating the one we have. The permutation group\\nmethod .conjugate(g) will conjugate the group by g.\\n\\nWith repeated conjugations of a single Sylow p-subgroup, we will always create duplicate\\nsubgroups. So we need to use a slightly complicated construction to form a list of just the\\nunique subgroups as the list of conjugates. This routine that computes all Sylow p-subgroups\\ncan be helpful throughout this section. It could be made much more efficient by conjugating\\nby just one element per coset of the normalizer, but it will be sufficient for our purposes\\nhere. Be sure to execute the next cell if you are online, so the function is defined for use\\nlater.\\n\\ndef all_sylow(G, p):\\n\' \' \' Form the set of all distinct Sylow p-subgroups of G \' \' \'\\nscriptP = []\\nP = G.sylow_subgroup(p)\\nfor x in G:\\n\\nH = P.conjugate(x)\\nif not(H in scriptP):\\n\\nscriptP.append(H)\\nreturn scriptP\\n\\nLets investigate the Sylow subgroups of the dihedral group D18. As a group of order\\n36 = 22 ·32, we know by the First Sylow Theorem that there is a Sylow 2-subgroup of order\\n4 and a Sylow 3-subgroup of order 9. First for p = 2, we obtain one Sylow 2-subgroup,\\nform all the conjugates, and form a list of non-duplicate subgroups. (These commands take\\na while to execute, so be patient.)\\n\\nG = DihedralGroup (18)\\nS2 = G.sylow_subgroup (2); S2\\n\\nSubgroup of (Dihedral group of order 36 as a permutation group)\\ngenerated by\\n[(2 ,18) (3,17)(4,16) (5,15)(6,14)(7,13)(8,12)(9,11),\\n(1,10)(2 ,11)(3 ,12) (4,13)(5,14)(6,15)(7,16)(8,17)(9,18)]\\n\\nuniqS2 = all_sylow(G, 2)\\nuniqS2\\n\\n[Permutation Group with generators\\n[(2 ,18) (3,17) (4,16)(5,15)(6,14)(7,13)(8,12)(9,11),\\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\\nPermutation Group with generators\\n[(1,3) (4,18) (5,17)(6,16)(7,15)(8,14)(9,13) (10 ,12),\\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\\nPermutation Group with generators\\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\\n(1,17) (2,16) (3,15)(4,14)(5,13)(6,12)(7,11)(8,10)],\\nPermutation Group with generators\\n[(1,5)(2,4)(6 ,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13),\\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\\nPermutation Group with generators\\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\\n(1,15) (2,14) (3,13)(4,12)(5,11)(6,10)(7,9)(16 ,18)],\\nPermutation Group with generators\\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\\n(1,13) (2,12) (3,11)(4,10)(5,9)(6,8)(14 ,18) (15 ,17)],\\n\\n\\n\\n15.6. SAGE 249\\n\\nPermutation Group with generators\\n[(1,7)(2,6)(3,5)(8,18)(9,17) (10 ,16) (11 ,15) (12 ,14),\\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\\nPermutation Group with generators\\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\\n(1,11) (2,10)(3,9)(4,8)(5,7)(12 ,18) (13 ,17) (14 ,16)],\\nPermutation Group with generators\\n[(1,9)(2,8)(3,7)(4,6)(10 ,18) (11 ,17) (12 ,16) (13 ,15),\\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)]]\\n\\nlen(uniqS2)\\n\\n9\\n\\nThe Third Sylow Theorem tells us that for p = 2 we would expect 1, 3 or 9 Sylow 2-\\nsubgroups, so our computational result of 9 subgroups is consistent with what the theory\\npredicts. Can you visualize each of these subgroups as symmetries of an 18-gon? Notice\\nthat we also have many subgroups of order 2 inside of these subgroups of order 4.\\n\\nNow for the case of p = 3.\\nG = DihedralGroup (18)\\nS3 = G.sylow_subgroup (3); S3\\n\\nSubgroup of (Dihedral group of order 36 as a permutation group)\\ngenerated by\\n[(1 ,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18),\\n(1,15,11,7,3,17,13,9,5)(2,16,12,8,4,18,14,10,6)]\\n\\nuniqS3 = all_sylow(G, 3)\\nuniqS3\\n\\n[Permutation Group with generators\\n[(1 ,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18),\\n(1,15,11,7,3,17,13,9,5)(2,16,12,8,4,18,14,10,6)]]\\n\\nlen(uniqS3)\\n\\n1\\n\\nWhat does the Third Sylow Theorem predict? Just 1 or 4 Sylow 3-subgroups. Having\\nfound just one subgroup computationally, we know that all of the conjugates of the lone\\nSylow 3-subgroup are equal. In other words, the Sylow 3-subgroup is normal in D18. Let\\nus check anyway.\\n\\nS3.is_normal(G)\\n\\nTrue\\n\\nAt least one of the subgroups of order 3 contained in this Sylow 3-subgroup should be\\nobvious by looking at the orders of the generators, and then you may even notice that the\\ngenerators given could be reduced, and one is a power of the other.\\n\\nS3.is_cyclic ()\\n\\nTrue\\n\\nRemember that there are many other subgroups, of other orders. For example, can you\\nconstruct a subgroup of order 6 = 2 · 3 in D18?\\n\\n\\n\\n250 CHAPTER 15. THE SYLOW THEOREMS\\n\\nNormalizers\\nA new command that is relevant to this section is the construction of a normalizer. The Sage\\ncommand G.normalizer(H) will return the subgroup of G containing elements that normalize\\nthe subgroup H. We illustrate its use with the Sylow subgroups from above.\\n\\nG = DihedralGroup (18)\\nS2 = G.sylow_subgroup (2)\\nS3 = G.sylow_subgroup (3)\\nN2 = G.normalizer(S2); N2\\n\\nSubgroup of (Dihedral group of order 36 as a permutation group)\\ngenerated by\\n[(2 ,18) (3,17)(4,16) (5,15)(6,14)(7,13)(8,12)(9,11),\\n(1,10)(2 ,11)(3 ,12) (4,13)(5,14)(6,15)(7,16)(8,17)(9,18)]\\n\\nN2 == S2\\n\\nTrue\\n\\nN3 = G.normalizer(S3); N3\\n\\nSubgroup of (Dihedral group of order 36 as a permutation group)\\ngenerated by\\n[(2 ,18) (3,17)(4,16) (5,15)(6,14)(7,13)(8,12)(9,11),\\n(1,2)(3,18) (4,17)(5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11),\\n(1,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18),\\n(1,15,11,7,3,17,13,9,5)(2,16,12,8,4,18,14,10,6)]\\n\\nN3 == G\\n\\nTrue\\n\\nThe normalizer of a subgroup always contains the whole subgroup, so the normalizer\\nof S2 is as small as possible. We already knew S3 is normal in G, so it is no surprise that\\nits normalizer is as big as possible — every element of G normalizes S3. Let us compute a\\nnormalizer in D18 that is more “interesting.”\\n\\nG = DihedralGroup (18)\\na = G(\\"(1,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18)\\")\\nb = G(\\"(1,5)(2,4)(6,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13)\\")\\nH = G.subgroup ([a, b])\\nH.order()\\n\\n6\\n\\nN = G.normalizer(H)\\nN\\n\\nSubgroup of (Dihedral group of order 36 as a permutation group)\\ngenerated by\\n[(1 ,2)(3 ,18)(4 ,17) (5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11),\\n(1,5)(2,4)(6 ,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13),\\n(1,13,7)(2,14,8)(3,15,9)(4,16,10)(5,17,11)(6,18,12)]\\n\\nN.order()\\n\\n\\n\\n15.6. SAGE 251\\n\\n12\\n\\nSo for this subgroup of order 6, the normalizer is strictly bigger than the subgroup, but\\nstill strictly smaller than the whole group (and hence not normal in the dihedral group).\\nTrivially, a subgroup is normal in its normalizer:\\n\\nH.is_normal(G)\\n\\nFalse\\n\\nH.is_normal(N)\\n\\nTrue\\n\\nFinite Simple Groups\\nWe saw earlier Sage’s permutation group method .is_simple(). Example 15.16 tells us that\\na group of order 64 is never simple. The dicyclic group DiCyclicGroup(16) is a non-abelian\\ngroup of 64, so we can test this method on this group. It turns out this group has many\\nnormal subgroups — the list will always contain the trivial subgroup and the group itself,\\nso any number exceeding 2 indicates a non-trivial normal subgroup.\\n\\nDC=DiCyclicGroup (16)\\nDC.order ()\\n\\n64\\n\\nDC.is_simple ()\\n\\nFalse\\n\\nns = DC.normal_subgroups ()\\nlen(ns)\\n\\n9\\n\\nHere is a rather interesting group, one of the 26 sporadic simple groups, known as the\\nHigman-Sims group, HS. The generators used below come from the representation on\\n100 points in gap format, available off of web.mat.bham.ac.uk/atlas/v2.0/spor/HS/. Two\\ngenerators of just order 2 and order 5 (as you can esily see), generating exactly 44 352 000\\nelements, but no normal subgroups. Amazing.\\n\\nG = SymmetricGroup (100)\\na = G([(1 ,60), (2,72), (3,81), (4,43), (5,11), (6,87),\\n\\n(7 ,34), (9,63), (12 ,46), (13 ,28), (14 ,71), (15 ,42),\\n(16 ,97), (18 ,57), (19 ,52), (21 ,32), (23 ,47), (24 ,54),\\n(25 ,83), (26 ,78), (29 ,89), (30 ,39), (33 ,61), (35 ,56),\\n(37 ,67), (44 ,76), (45 ,88), (48 ,59), (49 ,86), (50 ,74),\\n(51 ,66), (53 ,99), (55 ,75), (62 ,73), (65 ,79), (68 ,82),\\n(77 ,92), (84 ,90), (85 ,98), (94 ,100)])\\n\\nb = G([(1 ,86 ,13 ,10 ,47), (2,53,30,8,38),\\n(3 ,40,48,25,17), (4,29,92,88,43), (5,98,66,54, 65),\\n(6 ,27,51,73,24), (7,83,16,20,28), (9,23,89,95,61),\\n(11 ,42 ,46 ,91 ,32), (12,14, 81,55,68), (15 ,90 ,31 ,56 ,37),\\n(18 ,69 ,45 ,84 ,76), (19 ,59 ,79 ,35 ,93), (21 ,22 ,64 ,39 ,100),\\n(26 ,58 ,96 ,85 ,77), (33 ,52 ,94 ,75 ,44), (34 ,62 ,87 ,78 ,50),\\n(36 ,82 ,60 ,74 ,72), (41 ,80 ,70 ,49 ,67), (57 ,63 ,71 ,99 ,97)])\\n\\na.order(), b.order()\\n\\nhttp://web.mat.bham.ac.uk/atlas/v2.0/spor/HS/\\n\\n\\n252 CHAPTER 15. THE SYLOW THEOREMS\\n\\n(2, 5)\\n\\nHS = G.subgroup ([a, b])\\nHS.order ()\\n\\n44352000\\n\\nHS.is_simple ()\\n\\nTrue\\n\\nWe saw this group earlier in the exercises for Chapter 14 on group actions, where it\\nwas the single non-trivial normal subgroup of the automorphism group of the Higman-Sims\\ngraph, hence its name.\\n\\nGAP Console and Interface\\nThis concludes our exclusive study of group theory, though we will be using groups some\\nin the subsequent sections. As we have remarked, much of Sage’s computation with groups\\nis performed by the open source program, “Groups, Algorithms, and Programming,” which\\nis better know as simply gap. If after this course you outgrow Sage’s support for groups,\\nthen learning gap would be your next step as a group theorist. Every copy of Sage includes\\na copy of gap and is easy to see which version of gap is included:\\n\\ngap.version ()\\n\\n\' 4.8.3 \'\\n\\nYou can interact with gap in Sage in several ways. The most direct is by creating a\\npermutation group via Sage’s gap() command.\\n\\nG = gap( \' Group(␣(1,2,3,4,5,6),␣(1,3,5)␣) \' )\\nG\\n\\nGroup( [ (1,2,3,4,5,6), (1,3,5) ] )\\n\\nNow we can use most any gap command with G, via the convention that most gap\\ncommands expect a group as the first argument, and we instead provide the group by using\\nthe object-orientedG. syntax. If you consult the gap documentation you will see that Center\\n\\nis a gap command that expects a group as its lone argument, and Centralizer is a gap\\ncommand that expects two arguments — a group and then a group element.\\n\\nG.Center ()\\n\\nGroup( [ ( 1, 3, 5)( 2, 4, 6) ] )\\n\\nG.Centralizer( \' (1,␣3,␣5) \' )\\n\\nGroup( [ (1,3,5), (2,4,6), (1,3,5)(2,4,6) ] )\\n\\nIf you use the Sage Notebook interface you can set the first line of a compute cell to %gap\\n\\nand the entire cell will be interpreted as if you were interacting directly with gap. This\\nmeans you would now use gap’s syntax, which you can see above is slightly different than\\nSage’s universal syntax. You can also use the drop-down box at the top of a worksheet, and\\nselect gap as the system (rather than sage) and your whole worksheet will be interpreted as\\ngap commands. Here is one simple example, which you should be able to evaluate in your\\ncurrent worksheet. This particular example will not run properly in a Sage Cell in a web\\npage version of this section.\\n\\n\\n\\n15.7. SAGE EXERCISES 253\\n\\n%gap\\nG := Group( (1,2,3,4,5,6), (1,3,5) );\\nCentralizer(G, (1,3,5));\\n\\nNotice that\\n\\n• We do not need to wrap the individual permutations in as many quotation marks as\\nwe do in Sage.\\n\\n• Assignment is := not =. If you forget the colon, you will get an error message such as\\nVariable: \'G\' must have a value\\n\\n• A line must end with a semi-colon. If you forget, several lines will be merged together.\\n\\nYou can get help about gap commands with a command such as the following, though\\nyou will soon see that gap assumes you know a lot more algebra than Sage assumes you\\nknow.\\n\\nprint gap.help( \' SymmetricGroup \' , pager=False)\\n\\nIn the command-line version of Sage, you can also use the gap “console.” Again, you\\nneed to use gap syntax, and you do not have many of the conveniences of the Sage notebook.\\nIt is also good to know in advance that quit; is how you can leave the gap console and\\nget back to Sage. If you run Sage at the command-line, use the command gap_console() to\\nstart gap running.\\n\\nIt is a comfort to know that with Sage you get a complete copy of gap, installed and all\\nready to run. However, this is not a tutorial on gap, so consult the documentation available\\nat the main gap website: www.gap-system.org to learn how to get the most out of gap.\\n\\n15.7 Sage Exercises\\n\\n1. This exercise verifies Theorem 15.13. The commutator subgroup is computed with\\nthe permutation group method .commutator(). For the dihedral group of order 40, D20\\n\\n(DihedralGroup(20) in Sage), compute the commutator subgroup and form the quotient\\nwith the dihedral group. Then verify that this quotient is abelian. Can you identify the\\nquotient group exactly (in other words, up to isomorphism)?\\n\\n2. For each possible prime, find all of the distinct Sylow p-subgroups of the alternating\\ngroup A5. Confirm that your results are consistent with the Third Sylow Theorem for each\\nprime. We know that A5 is a simple group. Explain how this would explain or predict some\\naspects of your answers.\\nCount the number of distinct elements contained in the union of all the Sylow subgroups\\nyou just found. What is interesting about this count?\\n\\n3. For the dihedral group D36 (symmetries of a 36-gon) and each possible prime, determine\\nthe possibilities for the number of distinct Sylow p-subgroups as predicted by the Third\\nSylow Theorem (15.8). Now compute the actual number of distinct Sylow p-subgroups for\\neach prime and comment on the result.\\nIt can be proved that any group with order 72 is not a simple group, using techniques such\\nas those used in the later examples in this chapter. Discuss this result in teh context of\\nyour computations with Sage.\\n\\nhttp://www.gap-system.org\\n\\n\\n254 CHAPTER 15. THE SYLOW THEOREMS\\n\\n4. This exercise verifies Lemma 15.6. Let G be the dihedral group of order 36, D18. Let\\nH be the one Sylow 3-subgroup. Let K be the subgroup of order 6 generated by the two\\npermutations a and b given below. First, form a list of the distinct conjugates of K by the\\nelements of H, and determine the number of subgroups in this list. Compare this with the\\nindex given in the statement of the lemma, employing a single (long) statement making use\\nof the .order(), .normalizer() and .intersection() methods with G, H and K, only.\\n\\nG = DihedralGroup (18)\\na = G(\\"(1,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18)\\")\\nb = G(\\"(1,5)(2,4)(6,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13)\\")\\n\\n5. Example 15.19 shows that every group of order 48 has a normal subgroup. The dicyclic\\ngroups are an infinite family of non-abelian groups with order 4n, which includes the quater-\\nnions (the case of n = 2). So the permutation group DiCyclicGroup(12) has order 48. Use\\nSage to follow the logic of the proof in Example 15.19 and construct a normal subgroup in\\nthis group. (In other words, do not just ask for a list of the normal subgroups from Sage,\\nbut instead trace through the implications in the example to arrive at a normal subgroup,\\nand then check your answer.)\\n\\n6. The proofs of the Second and Third Sylow Theorems (15.7, 15.8) employ a group action\\non sets of Sylow p-subgroups, For the Second Theorem, the list is proposed as incomplete\\nand is proved to be all of the Sylow p-subgroups. In this exercise we will see how these\\nactions behave, and how they are different when we use different groups acting on the same\\nset.\\nConstruct the six Sylow 5-subgroups of the alternating group A5. This will be the set of\\nobjects for both of our actions. Conjugating one of these Sylow 5-subgroups by an element\\nof A5 will produce another Sylow 5-subgroup, and so can be used to create a group action.\\nFor such an action, from each group element form a Sage permutation of the subgroups by\\nnumbering the six subgroups and using these integers as markers for the subgroups. You\\nwill find the Python list method .index() very helpful. Now use all of these permutations to\\ngenerate a permutation group (a subgroup of S6). Finally, use permutation group methods\\nfor orbits and stabilisers, etc. to explore the actions.\\nFor the first action, use all of A5 as the group. Show that the resulting action is transitive.\\nIn other words, there is exactly one single orbit.\\nFor the second action, use just one of the Sylow 5-subgroups as the group. Write the class\\nequation for this action in a format that suggests the “congruent to 1 mod p” part of the\\nconclusion of the Third Theorem.\\n\\n\\n\\n16\\n\\nRings\\n\\nUp to this point we have studied sets with a single binary operation satisfying certain ax-\\nioms, but we are often more interested in working with sets that have two binary operations.\\nFor example, one of the most natural algebraic structures to study is the integers with the\\noperations of addition and multiplication. These operations are related to one another by\\nthe distributive property. If we consider a set with two such related binary operations sat-\\nisfying certain axioms, we have an algebraic structure called a ring. In a ring we add and\\nmultiply elements such as real numbers, complex numbers, matrices, and functions.\\n\\n16.1 Rings\\nA nonempty set R is a ring if it has two closed binary operations, addition and multipli-\\ncation, satisfying the following conditions.\\n\\n1. a+ b = b+ a for a, b ∈ R.\\n\\n2. (a+ b) + c = a+ (b+ c) for a, b, c ∈ R.\\n\\n3. There is an element 0 in R such that a+ 0 = a for all a ∈ R.\\n\\n4. For every element a ∈ R, there exists an element −a in R such that a+ (−a) = 0.\\n\\n5. (ab)c = a(bc) for a, b, c ∈ R.\\n\\n6. For a, b, c ∈ R,\\n\\na(b+ c) = ab+ ac\\n\\n(a+ b)c = ac+ bc.\\n\\nThis last condition, the distributive axiom, relates the binary operations of addition and\\nmultiplication. Notice that the first four axioms simply require that a ring be an abelian\\ngroup under addition, so we could also have defined a ring to be an abelian group (R,+)\\ntogether with a second binary operation satisfying the fifth and sixth conditions given above.\\n\\nIf there is an element 1 ∈ R such that 1 ̸= 0 and 1a = a1 = a for each element a ∈ R,\\nwe say that R is a ring with unity or identity. A ring R for which ab = ba for all a, b in R\\nis called a commutative ring. A commutative ring R with identity is called an integral\\ndomain if, for every a, b ∈ R such that ab = 0, either a = 0 or b = 0. A division ring\\nis a ring R, with an identity, in which every nonzero element in R is a unit; that is, for\\neach a ∈ R with a ̸= 0, there exists a unique element a−1 such that a−1a = aa−1 = 1. A\\ncommutative division ring is called a field. The relationship among rings, integral domains,\\ndivision rings, and fields is shown in Figure 16.1.\\n\\n255\\n\\n\\n\\n256 CHAPTER 16. RINGS\\n\\nRings with\\nIdentity\\n\\nDivision\\nRings\\n\\nCommutative\\nRings\\n\\nIntegral\\nDomains\\n\\nRings\\n\\nFields\\n\\nFigure 16.1: Types of rings\\n\\nExample 16.2. As we have mentioned previously, the integers form a ring. In fact, Z is\\nan integral domain. Certainly if ab = 0 for two integers a and b, either a = 0 or b = 0.\\nHowever, Z is not a field. There is no integer that is the multiplicative inverse of 2, since\\n1/2 is not an integer. The only integers with multiplicative inverses are 1 and −1.\\nExample 16.3. Under the ordinary operations of addition and multiplication, all of the\\nfamiliar number systems are rings: the rationals, Q; the real numbers, R; and the complex\\nnumbers, C. Each of these rings is a field.\\nExample 16.4. We can define the product of two elements a and b in Zn by ab (mod n).\\nFor instance, in Z12, 5 · 7 ≡ 11 (mod 12). This product makes the abelian group Zn into\\na ring. Certainly Zn is a commutative ring; however, it may fail to be an integral domain.\\nIf we consider 3 · 4 ≡ 0 (mod 12) in Z12, it is easy to see that a product of two nonzero\\nelements in the ring can be equal to zero.\\n\\nA nonzero element a in a ring R is called a zero divisor if there is a nonzero element\\nb in R such that ab = 0. In the previous example, 3 and 4 are zero divisors in Z12.\\nExample 16.5. In calculus the continuous real-valued functions on an interval [a, b] form\\na commutative ring. We add or multiply two functions by adding or multiplying the values\\nof the functions. If f(x) = x2 and g(x) = cosx, then (f + g)(x) = f(x) + g(x) = x2 + cosx\\nand (fg)(x) = f(x)g(x) = x2 cosx.\\nExample 16.6. The 2×2 matrices with entries in R form a ring under the usual operations\\nof matrix addition and multiplication. This ring is noncommutative, since it is usually the\\ncase that AB ̸= BA. Also, notice that we can have AB = 0 when neither A nor B is zero.\\nExample 16.7. For an example of a noncommutative division ring, let\\n\\n1 =\\n\\n(\\n1 0\\n\\n0 1\\n\\n)\\n, i =\\n\\n(\\n0 1\\n\\n−1 0\\n\\n)\\n, j =\\n\\n(\\n0 i\\n\\ni 0\\n\\n)\\n, k =\\n\\n(\\ni 0\\n\\n0 −i\\n\\n)\\n,\\n\\nwhere i2 = −1. These elements satisfy the following relations:\\ni2 = j2 = k2 = −1\\n\\nij = k\\njk = i\\nki = j\\nji = −k\\n\\nkj = −i\\nik = −j.\\n\\n\\n\\n16.1. RINGS 257\\n\\nLet H consist of elements of the form a + bi + cj + dk, where a, b, c, d are real numbers.\\nEquivalently, H can be considered to be the set of all 2× 2 matrices of the form(\\n\\nα β\\n\\n−β α\\n\\n)\\n,\\n\\nwhere α = a + di and β = b + ci are complex numbers. We can define addition and\\nmultiplication on H either by the usual matrix operations or in terms of the generators 1,\\ni, j, and k:\\n\\n(a1 + b1i + c1j + d1k) + (a2 + b2i + c2j + d2k)\\n= (a1 + a2) + (b1 + b2)i + (c1 + c2)j + (d1 + d2)k\\n\\nand\\n(a1 + b1i + c1j + d1k)(a2 + b2i + c2j + d2k) = α+ βi + γj + δk,\\n\\nwhere\\n\\nα = a1a2 − b1b2 − c1c2 − d1d2\\n\\nβ = a1b2 + a2b1 + c1d2 − d1c2\\n\\nγ = a1c2 − b1d2 + c1a2 − d1b2\\n\\nδ = a1d2 + b1c2 − c1b2 − d1a2.\\n\\nThough multiplication looks complicated, it is actually a straightforward computation if\\nwe remember that we just add and multiply elements in H like polynomials and keep in\\nmind the relationships between the generators i, j, and k. The ring H is called the ring of\\nquaternions.\\n\\nTo show that the quaternions are a division ring, we must be able to find an inverse for\\neach nonzero element. Notice that\\n\\n(a+ bi + cj + dk)(a− bi − cj − dk) = a2 + b2 + c2 + d2.\\n\\nThis element can be zero only if a, b, c, and d are all zero. So if a+ bi + cj + dk ̸= 0,\\n\\n(a+ bi + cj + dk)\\n(\\na− bi − cj − dk\\na2 + b2 + c2 + d2\\n\\n)\\n= 1.\\n\\nProposition 16.8. Let R be a ring with a, b ∈ R. Then\\n\\n1. a0 = 0a = 0;\\n\\n2. a(−b) = (−a)b = −ab;\\n\\n3. (−a)(−b) = ab.\\n\\nProof. To prove (1), observe that\\n\\na0 = a(0 + 0) = a0 + a0;\\n\\nhence, a0 = 0. Similarly, 0a = 0. For (2), we have ab + a(−b) = a(b − b) = a0 = 0;\\nconsequently, −ab = a(−b). Similarly, −ab = (−a)b. Part (3) follows directly from (2)\\nsince (−a)(−b) = −(a(−b)) = −(−ab) = ab.\\n\\nJust as we have subgroups of groups, we have an analogous class of substructures for\\nrings. A subring S of a ring R is a subset S of R such that S is also a ring under the\\ninherited operations from R.\\n\\n\\n\\n258 CHAPTER 16. RINGS\\n\\nExample 16.9. The ring nZ is a subring of Z. Notice that even though the original ring\\nmay have an identity, we do not require that its subring have an identity. We have the\\nfollowing chain of subrings:\\n\\nZ ⊂ Q ⊂ R ⊂ C.\\n\\nThe following proposition gives us some easy criteria for determining whether or not\\na subset of a ring is indeed a subring. (We will leave the proof of this proposition as an\\nexercise.)\\n\\nProposition 16.10. Let R be a ring and S a subset of R. Then S is a subring of R if and\\nonly if the following conditions are satisfied.\\n\\n1. S ̸= ∅.\\n\\n2. rs ∈ S for all r, s ∈ S.\\n\\n3. r − s ∈ S for all r, s ∈ S.\\n\\nExample 16.11. Let R = M2(R) be the ring of 2 × 2 matrices with entries in R. If T is\\nthe set of upper triangular matrices in R; i.e.,\\n\\nT =\\n\\n{(\\na b\\n\\n0 c\\n\\n)\\n: a, b, c ∈ R\\n\\n}\\n,\\n\\nthen T is a subring of R. If\\n\\nA =\\n\\n(\\na b\\n\\n0 c\\n\\n)\\nand B =\\n\\n(\\na′ b′\\n\\n0 c′\\n\\n)\\nare in T , then clearly A−B is also in T . Also,\\n\\nAB =\\n\\n(\\naa′ ab′ + bc′\\n\\n0 cc′\\n\\n)\\nis in T .\\n\\n16.2 Integral Domains and Fields\\nLet us briefly recall some definitions. If R is a ring and r is a nonzero element in R, then\\nr is said to be a zero divisor if there is some nonzero element s ∈ R such that rs = 0. A\\ncommutative ring with identity is said to be an integral domain if it has no zero divisors.\\nIf an element a in a ring R with identity has a multiplicative inverse, we say that a is a\\nunit. If every nonzero element in a ring R is a unit, then R is called a division ring. A\\ncommutative division ring is called a field.\\n\\nExample 16.12. If i2 = −1, then the set Z[i] = {m + ni : m,n ∈ Z} forms a ring known\\nas the Gaussian integers. It is easily seen that the Gaussian integers are a subring of the\\ncomplex numbers since they are closed under addition and multiplication. Let α = a + bi\\nbe a unit in Z[i]. Then α = a− bi is also a unit since if αβ = 1, then αβ = 1. If β = c+ di,\\nthen\\n\\n1 = αβαβ = (a2 + b2)(c2 + d2).\\n\\nTherefore, a2 + b2 must either be 1 or −1; or, equivalently, a + bi = ±1 or a + bi = ±i.\\nTherefore, units of this ring are ±1 and ±i; hence, the Gaussian integers are not a field.\\nWe will leave it as an exercise to prove that the Gaussian integers are an integral domain.\\n\\n\\n\\n16.2. INTEGRAL DOMAINS AND FIELDS 259\\n\\nExample 16.13. The set of matrices\\n\\nF =\\n\\n{(\\n1 0\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n1 1\\n\\n1 0\\n\\n)\\n,\\n\\n(\\n0 1\\n\\n1 1\\n\\n)\\n,\\n\\n(\\n0 0\\n\\n0 0\\n\\n)}\\nwith entries in Z2 forms a field.\\n\\nExample 16.14. The set Q(\\n√\\n2 ) = {a + b\\n\\n√\\n2 : a, b ∈ Q} is a field. The inverse of an\\n\\nelement a+ b\\n√\\n2 in Q(\\n\\n√\\n2 ) is\\n\\na\\n\\na2 − 2b2\\n+\\n\\n−b\\na2 − 2b2\\n\\n√\\n2.\\n\\nWe have the following alternative characterization of integral domains.\\n\\nProposition 16.15 (Cancellation Law). Let D be a commutative ring with identity. Then\\nD is an integral domain if and only if for all nonzero elements a ∈ D with ab = ac, we have\\nb = c.\\n\\nProof. Let D be an integral domain. Then D has no zero divisors. Let ab = ac with\\na ̸= 0. Then a(b− c) = 0. Hence, b− c = 0 and b = c.\\n\\nConversely, let us suppose that cancellation is possible in D. That is, suppose that\\nab = ac implies b = c. Let ab = 0. If a ̸= 0, then ab = a0 or b = 0. Therefore, a cannot be\\na zero divisor.\\n\\nThe following surprising theorem is due to Wedderburn.\\n\\nTheorem 16.16. Every finite integral domain is a field.\\n\\nProof. Let D be a finite integral domain and D∗ be the set of nonzero elements of D. We\\nmust show that every element in D∗ has an inverse. For each a ∈ D∗ we can define a map\\nλa : D∗ → D∗ by λa(d) = ad. This map makes sense, because if a ̸= 0 and d ̸= 0, then\\nad ̸= 0. The map λa is one-to-one, since for d1, d2 ∈ D∗,\\n\\nad1 = λa(d1) = λa(d2) = ad2\\n\\nimplies d1 = d2 by left cancellation. Since D∗ is a finite set, the map λa must also be\\nonto; hence, for some d ∈ D∗, λa(d) = ad = 1. Therefore, a has a left inverse. Since D is\\ncommutative, d must also be a right inverse for a. Consequently, D is a field.\\n\\nFor any nonnegative integer n and any element r in a ring R we write r + · · · + r (n\\ntimes) as nr. We define the characteristic of a ring R to be the least positive integer n\\nsuch that nr = 0 for all r ∈ R. If no such integer exists, then the characteristic of R is\\ndefined to be 0. We will denote the characteristic of R by charR.\\n\\nExample 16.17. For every prime p, Zp is a field of characteristic p. By Proposition 3.4,\\nevery nonzero element in Zp has an inverse; hence, Zp is a field. If a is any nonzero element\\nin the field, then pa = 0, since the order of any nonzero element in the abelian group Zp is\\np.\\n\\nLemma 16.18. Let R be a ring with identity. If 1 has order n, then the characteristic of\\nR is n.\\n\\nProof. If 1 has order n, then n is the least positive integer such that n1 = 0. Thus, for\\nall r ∈ R,\\n\\nnr = n(1r) = (n1)r = 0r = 0.\\n\\nOn the other hand, if no positive n exists such that n1 = 0, then the characteristic of R is\\nzero.\\n\\n\\n\\n260 CHAPTER 16. RINGS\\n\\nTheorem 16.19. The characteristic of an integral domain is either prime or zero.\\n\\nProof. Let D be an integral domain and suppose that the characteristic of D is n with\\nn ̸= 0. If n is not prime, then n = ab, where 1 < a < n and 1 < b < n. By Lemma 16.18,\\nwe need only consider the case n1 = 0. Since 0 = n1 = (ab)1 = (a1)(b1) and there are no\\nzero divisors in D, either a1 = 0 or b1 = 0. Hence, the characteristic of D must be less than\\nn, which is a contradiction. Therefore, n must be prime.\\n\\n16.3 Ring Homomorphisms and Ideals\\nIn the study of groups, a homomorphism is a map that preserves the operation of the group.\\nSimilarly, a homomorphism between rings preserves the operations of addition and multi-\\nplication in the ring. More specifically, if R and S are rings, then a ring homomorphism\\nis a map ϕ : R→ S satisfying\\n\\nϕ(a+ b) = ϕ(a) + ϕ(b)\\n\\nϕ(ab) = ϕ(a)ϕ(b)\\n\\nfor all a, b ∈ R. If ϕ : R → S is a one-to-one and onto homomorphism, then ϕ is called an\\nisomorphism of rings.\\n\\nThe set of elements that a ring homomorphism maps to 0 plays a fundamental role in\\nthe theory of rings. For any ring homomorphism ϕ : R→ S, we define the kernel of a ring\\nhomomorphism to be the set\\n\\nkerϕ = {r ∈ R : ϕ(r) = 0}.\\n\\nExample 16.20. For any integer n we can define a ring homomorphism ϕ : Z → Zn by\\na 7→ a (mod n). This is indeed a ring homomorphism, since\\n\\nϕ(a+ b) = (a+ b) (mod n)\\n\\n= a (mod n) + b (mod n)\\n\\n= ϕ(a) + ϕ(b)\\n\\nand\\n\\nϕ(ab) = ab (mod n)\\n\\n= a (mod n) · b (mod n)\\n\\n= ϕ(a)ϕ(b).\\n\\nThe kernel of the homomorphism ϕ is nZ.\\n\\nExample 16.21. Let C[a, b] be the ring of continuous real-valued functions on an interval\\n[a, b] as in Example 16.5. For a fixed α ∈ [a, b], we can define a ring homomorphism\\nϕα : C[a, b] → R by ϕα(f) = f(α). This is a ring homomorphism since\\n\\nϕα(f + g) = (f + g)(α) = f(α) + g(α) = ϕα(f) + ϕα(g)\\n\\nϕα(fg) = (fg)(α) = f(α)g(α) = ϕα(f)ϕα(g).\\n\\nRing homomorphisms of the type ϕα are called evaluation homomorphisms.\\n\\nIn the next proposition we will examine some fundamental properties of ring homomor-\\nphisms. The proof of the proposition is left as an exercise.\\n\\n\\n\\n16.3. RING HOMOMORPHISMS AND IDEALS 261\\n\\nProposition 16.22. Let ϕ : R→ S be a ring homomorphism.\\n\\n1. If R is a commutative ring, then ϕ(R) is a commutative ring.\\n\\n2. ϕ(0) = 0.\\n\\n3. Let 1R and 1S be the identities for R and S, respectively. If ϕ is onto, then ϕ(1R) = 1S.\\n\\n4. If R is a field and ϕ(R) ̸= {0}, then ϕ(R) is a field.\\n\\nIn group theory we found that normal subgroups play a special role. These subgroups\\nhave nice characteristics that make them more interesting to study than arbitrary subgroups.\\nIn ring theory the objects corresponding to normal subgroups are a special class of subrings\\ncalled ideals. An ideal in a ring R is a subring I of R such that if a is in I and r is in R,\\nthen both ar and ra are in I; that is, rI ⊂ I and Ir ⊂ I for all r ∈ R.\\n\\nExample 16.23. Every ring R has at least two ideals, {0} and R. These ideals are called\\nthe trivial ideals.\\n\\nLet R be a ring with identity and suppose that I is an ideal in R such that 1 is in I.\\nSince for any r ∈ R, r1 = r ∈ I by the definition of an ideal, I = R.\\n\\nExample 16.24. If a is any element in a commutative ring R with identity, then the set\\n\\n⟨a⟩ = {ar : r ∈ R}\\n\\nis an ideal in R. Certainly, ⟨a⟩ is nonempty since both 0 = a0 and a = a1 are in ⟨a⟩. The\\nsum of two elements in ⟨a⟩ is again in ⟨a⟩ since ar + ar′ = a(r + r′). The inverse of ar is\\n−ar = a(−r) ∈ ⟨a⟩. Finally, if we multiply an element ar ∈ ⟨a⟩ by an arbitrary element\\ns ∈ R, we have s(ar) = a(sr). Therefore, ⟨a⟩ satisfies the definition of an ideal.\\n\\nIf R is a commutative ring with identity, then an ideal of the form ⟨a⟩ = {ar : r ∈ R}\\nis called a principal ideal.\\n\\nTheorem 16.25. Every ideal in the ring of integers Z is a principal ideal.\\n\\nProof. The zero ideal {0} is a principal ideal since ⟨0⟩ = {0}. If I is any nonzero ideal\\nin Z, then I must contain some positive integer m. There exists a least positive integer n\\nin I by the Principle of Well-Ordering. Now let a be any element in I. Using the division\\nalgorithm, we know that there exist integers q and r such that\\n\\na = nq + r\\n\\nwhere 0 ≤ r < n. This equation tells us that r = a− nq ∈ I, but r must be 0 since n is the\\nleast positive element in I. Therefore, a = nq and I = ⟨n⟩.\\n\\nExample 16.26. The set nZ is ideal in the ring of integers. If na is in nZ and b is in Z,\\nthen nab is in nZ as required. In fact, by Theorem 16.25, these are the only ideals of Z.\\n\\nProposition 16.27. The kernel of any ring homomorphism ϕ : R→ S is an ideal in R.\\n\\nProof. We know from group theory that kerϕ is an additive subgroup of R. Suppose that\\nr ∈ R and a ∈ kerϕ. Then we must show that ar and ra are in kerϕ. However,\\n\\nϕ(ar) = ϕ(a)ϕ(r) = 0ϕ(r) = 0\\n\\nand\\nϕ(ra) = ϕ(r)ϕ(a) = ϕ(r)0 = 0.\\n\\n\\n\\n262 CHAPTER 16. RINGS\\n\\nRemark 16.28. In our definition of an ideal we have required that rI ⊂ I and Ir ⊂ I for\\nall r ∈ R. Such ideals are sometimes referred to as two-sided ideals. We can also consider\\none-sided ideals; that is, we may require only that either rI ⊂ I or Ir ⊂ I for r ∈ R\\nhold but not both. Such ideals are called left ideals and right ideals, respectively. Of\\ncourse, in a commutative ring any ideal must be two-sided. In this text we will concentrate\\non two-sided ideals.\\nTheorem 16.29. Let I be an ideal of R. The factor group R/I is a ring with multiplication\\ndefined by\\n\\n(r + I)(s+ I) = rs+ I.\\n\\nProof. We already know that R/I is an abelian group under addition. Let r+ I and s+ I\\nbe in R/I. We must show that the product (r + I)(s + I) = rs + I is independent of the\\nchoice of coset; that is, if r′ ∈ r + I and s′ ∈ s + I, then r′s′ must be in rs + I. Since\\nr′ ∈ r+ I, there exists an element a in I such that r′ = r+ a. Similarly, there exists a b ∈ I\\nsuch that s′ = s+ b. Notice that\\n\\nr′s′ = (r + a)(s+ b) = rs+ as+ rb+ ab\\n\\nand as + rb + ab ∈ I since I is an ideal; consequently, r′s′ ∈ rs + I. We will leave as\\nan exercise the verification of the associative law for multiplication and the distributive\\nlaws.\\n\\nThe ring R/I in Theorem 16.29 is called the factor or quotient ring. Just as with\\ngroup homomorphisms and normal subgroups, there is a relationship between ring homo-\\nmorphisms and ideals.\\nTheorem 16.30. Let I be an ideal of R. The map ϕ : R→ R/I defined by ϕ(r) = r+ I is\\na ring homomorphism of R onto R/I with kernel I.\\nProof. Certainly ϕ : R → R/I is a surjective abelian group homomorphism. It remains\\nto show that ϕ works correctly under ring multiplication. Let r and s be in R. Then\\n\\nϕ(r)ϕ(s) = (r + I)(s+ I) = rs+ I = ϕ(rs),\\n\\nwhich completes the proof of the theorem.\\n\\nThe map ϕ : R → R/I is often called the natural or canonical homomorphism. In\\nring theory we have isomorphism theorems relating ideals and ring homomorphisms similar\\nto the isomorphism theorems for groups that relate normal subgroups and homomorphisms\\nin Chapter 11. We will prove only the First Isomorphism Theorem for rings in this chapter\\nand leave the proofs of the other two theorems as exercises. All of the proofs are similar to\\nthe proofs of the isomorphism theorems for groups.\\nTheorem 16.31 (First Isomorphism Theorem). Let ψ : R → S be a ring homomorphism.\\nThen kerψ is an ideal of R. If ϕ : R → R/ kerψ is the canonical homomorphism, then\\nthere exists a unique isomorphism η : R/ kerψ → ψ(R) such that ψ = ηϕ.\\nProof. Let K = kerψ. By the First Isomorphism Theorem for groups, there exists a\\nwell-defined group homomorphism η : R/K → ψ(R) defined by η(r + K) = ψ(r) for the\\nadditive abelian groups R and R/K. To show that this is a ring homomorphism, we need\\nonly show that η((r +K)(s+K)) = η(r +K)η(s+K); but\\n\\nη((r +K)(s+K)) = η(rs+K)\\n\\n= ψ(rs)\\n\\n= ψ(r)ψ(s)\\n\\n= η(r +K)η(s+K).\\n\\n\\n\\n16.4. MAXIMAL AND PRIME IDEALS 263\\n\\nTheorem 16.32 (Second Isomorphism Theorem). Let I be a subring of a ring R and J an\\nideal of R. Then I ∩ J is an ideal of I and\\n\\nI/I ∩ J ∼= (I + J)/J.\\n\\nTheorem 16.33 (Third Isomorphism Theorem). Let R be a ring and I and J be ideals of\\nR where J ⊂ I. Then\\n\\nR/I ∼=\\nR/J\\n\\nI/J\\n.\\n\\nTheorem 16.34 (Correspondence Theorem). Let I be an ideal of a ring R. Then S 7→ S/I\\nis a one-to-one correspondence between the set of subrings S containing I and the set of\\nsubrings of R/I. Furthermore, the ideals of R containing I correspond to ideals of R/I.\\n\\n16.4 Maximal and Prime Ideals\\nIn this particular section we are especially interested in certain ideals of commutative rings.\\nThese ideals give us special types of factor rings. More specifically, we would like to char-\\nacterize those ideals I of a commutative ring R such that R/I is an integral domain or a\\nfield.\\n\\nA proper ideal M of a ring R is a maximal ideal of R if the ideal M is not a proper\\nsubset of any ideal of R except R itself. That is, M is a maximal ideal if for any ideal I\\nproperly containing M , I = R. The following theorem completely characterizes maximal\\nideals for commutative rings with identity in terms of their corresponding factor rings.\\n\\nTheorem 16.35. Let R be a commutative ring with identity and M an ideal in R. Then\\nM is a maximal ideal of R if and only if R/M is a field.\\n\\nProof. Let M be a maximal ideal in R. If R is a commutative ring, then R/M must also\\nbe a commutative ring. Clearly, 1 +M acts as an identity for R/M . We must also show\\nthat every nonzero element in R/M has an inverse. If a+M is a nonzero element in R/M ,\\nthen a /∈ M . Define I to be the set {ra +m : r ∈ R and m ∈ M}. We will show that I is\\nan ideal in R. The set I is nonempty since 0a+0 = 0 is in I. If r1a+m1 and r2a+m2 are\\ntwo elements in I, then\\n\\n(r1a+m1)− (r2a+m2) = (r1 − r2)a+ (m1 −m2)\\n\\nis in I. Also, for any r ∈ R it is true that rI ⊂ I; hence, I is closed under multiplication\\nand satisfies the necessary conditions to be an ideal. Therefore, by Proposition 16.10 and\\nthe definition of an ideal, I is an ideal properly containing M . Since M is a maximal ideal,\\nI = R; consequently, by the definition of I there must be an m in M and an element b in\\nR such that 1 = ab+m. Therefore,\\n\\n1 +M = ab+M = ba+M = (a+M)(b+M).\\n\\nConversely, suppose that M is an ideal and R/M is a field. Since R/M is a field, it\\nmust contain at least two elements: 0 +M = M and 1 +M . Hence, M is a proper ideal\\nof R. Let I be any ideal properly containing M . We need to show that I = R. Choose\\na in I but not in M . Since a +M is a nonzero element in a field, there exists an element\\nb+M in R/M such that (a+M)(b+M) = ab+M = 1 +M . Consequently, there exists\\nan element m ∈M such that ab+m = 1 and 1 is in I. Therefore, r1 = r ∈ I for all r ∈ R.\\nConsequently, I = R.\\n\\n\\n\\n264 CHAPTER 16. RINGS\\n\\nExample 16.36. Let pZ be an ideal in Z, where p is prime. Then pZ is a maximal ideal\\nsince Z/pZ ∼= Zp is a field.\\n\\nA proper ideal P in a commutative ring R is called a prime ideal if whenever ab ∈ P ,\\nthen either a ∈ P or b ∈ P .1\\n\\nExample 16.37. It is easy to check that the set P = {0, 2, 4, 6, 8, 10} is an ideal in Z12.\\nThis ideal is prime. In fact, it is a maximal ideal.\\n\\nProposition 16.38. Let R be a commutative ring with identity 1, where 1 ̸= 0. Then P is\\na prime ideal in R if and only if R/P is an integral domain.\\n\\nProof. First let us assume that P is an ideal in R and R/P is an integral domain. Suppose\\nthat ab ∈ P . If a+P and b+P are two elements of R/P such that (a+P )(b+P ) = 0+P = P ,\\nthen either a+ P = P or b+ P = P . This means that either a is in P or b is in P , which\\nshows that P must be prime.\\n\\nConversely, suppose that P is prime and\\n\\n(a+ P )(b+ P ) = ab+ P = 0 + P = P.\\n\\nThen ab ∈ P . If a /∈ P , then b must be in P by the definition of a prime ideal; hence,\\nb+ P = 0 + P and R/P is an integral domain.\\n\\nExample 16.39. Every ideal in Z is of the form nZ. The factor ring Z/nZ ∼= Zn is an\\nintegral domain only when n is prime. It is actually a field. Hence, the nonzero prime ideals\\nin Z are the ideals pZ, where p is prime. This example really justifies the use of the word\\n“prime” in our definition of prime ideals.\\n\\nSince every field is an integral domain, we have the following corollary.\\n\\nCorollary 16.40. Every maximal ideal in a commutative ring with identity is also a prime\\nideal.\\n\\nHistorical Note\\n\\nAmalie Emmy Noether, one of the outstanding mathematicians of the twentieth century,\\nwas born in Erlangen, Germany in 1882. She was the daughter of Max Noether (1844–\\n1921), a distinguished mathematician at the University of Erlangen. Together with Paul\\nGordon (1837–1912), Emmy Noether’s father strongly influenced her early education. She\\nentered the University of Erlangen at the age of 18. Although women had been admitted\\nto universities in England, France, and Italy for decades, there was great resistance to\\ntheir presence at universities in Germany. Noether was one of only two women among\\nthe university’s 986 students. After completing her doctorate under Gordon in 1907, she\\ncontinued to do research at Erlangen, occasionally lecturing when her father was ill.\\n\\nNoether went to Göttingen to study in 1916. David Hilbert and Felix Klein tried un-\\nsuccessfully to secure her an appointment at Göttingen. Some of the faculty objected to\\nwomen lecturers, saying, “What will our soldiers think when they return to the university\\nand are expected to learn at the feet of a woman?” Hilbert, annoyed at the question, re-\\nsponded, “Meine Herren, I do not see that the sex of a candidate is an argument against\\nher admission as a Privatdozent. After all, the Senate is not a bathhouse.” At the end of\\nWorld War I, attitudes changed and conditions greatly improved for women. After Noether\\n\\n1It is possible to define prime ideals in a noncommutative ring. See [1] or [3].\\n\\n\\n\\n16.5. AN APPLICATION TO SOFTWARE DESIGN 265\\n\\npassed her habilitation examination in 1919, she was given a title and was paid a small sum\\nfor her lectures.\\n\\nIn 1922, Noether became a Privatdozent at Göttingen. Over the next 11 years she used\\naxiomatic methods to develop an abstract theory of rings and ideals. Though she was not\\ngood at lecturing, Noether was an inspiring teacher. One of her many students was B. L.\\nvan der Waerden, author of the first text treating abstract algebra from a modern point of\\nview. Some of the other mathematicians Noether influenced or closely worked with were\\nAlexandroff, Artin, Brauer, Courant, Hasse, Hopf, Pontryagin, von Neumann, and Weyl.\\nOne of the high points of her career was an invitation to address the International Congress\\nof Mathematicians in Zurich in 1932. In spite of all the recognition she received from her\\ncolleagues, Noether’s abilities were never recognized as they should have been during her\\nlifetime. She was never promoted to full professor by the Prussian academic bureaucracy.\\n\\nIn 1933, Noether, a Jew, was banned from participation in all academic activities in\\nGermany. She emigrated to the United States, took a position at Bryn Mawr College, and\\nbecame a member of the Institute for Advanced Study at Princeton. Noether died suddenly\\non April 14, 1935. After her death she was eulogized by such notable scientists as Albert\\nEinstein.\\n\\n16.5 An Application to Software Design\\nThe Chinese Remainder Theorem is a result from elementary number theory about the\\nsolution of systems of simultaneous congruences. The Chinese mathematician Sun-tsï wrote\\nabout the theorem in the first century A.D. This theorem has some interesting consequences\\nin the design of software for parallel processors.\\n\\nLemma 16.41. Let m and n be positive integers such that gcd(m,n) = 1. Then for a, b ∈ Z\\nthe system\\n\\nx ≡ a (mod m)\\n\\nx ≡ b (mod n)\\n\\nhas a solution. If x1 and x2 are two solutions of the system, then x1 ≡ x2 (mod mn).\\n\\nProof. The equation x ≡ a (mod m) has a solution since a+km satisfies the equation for\\nall k ∈ Z. We must show that there exists an integer k1 such that\\n\\na+ k1m ≡ b (mod n).\\n\\nThis is equivalent to showing that\\n\\nk1m ≡ (b− a) (mod n)\\n\\nhas a solution for k1. Since m and n are relatively prime, there exist integers s and t such\\nthat ms+ nt = 1. Consequently,\\n\\n(b− a)ms = (b− a)− (b− a)nt,\\n\\nor\\n[(b− a)s]m ≡ (b− a) (mod n).\\n\\nNow let k1 = (b− a)s.\\n\\n\\n\\n266 CHAPTER 16. RINGS\\n\\nTo show that any two solutions are congruent modulo mn, let c1 and c2 be two solutions\\nof the system. That is,\\n\\nci ≡ a (mod m)\\n\\nci ≡ b (mod n)\\n\\nfor i = 1, 2. Then\\n\\nc2 ≡ c1 (mod m)\\n\\nc2 ≡ c1 (mod n).\\n\\nTherefore, both m and n divide c1 − c2. Consequently, c2 ≡ c1 (mod mn).\\n\\nExample 16.42. Let us solve the system\\n\\nx ≡ 3 (mod 4)\\n\\nx ≡ 4 (mod 5).\\n\\nUsing the Euclidean algorithm, we can find integers s and t such that 4s + 5t = 1. Two\\nsuch integers are s = 4 and t = −3. Consequently,\\n\\nx = a+ k1m = 3 + 4k1 = 3 + 4[(5− 4)4] = 19.\\n\\nTheorem 16.43 (Chinese Remainder Theorem). Let n1, n2, . . . , nk be positive integers such\\nthat gcd(ni, nj) = 1 for i ̸= j. Then for any integers a1, . . . , ak, the system\\n\\nx ≡ a1 (mod n1)\\n\\nx ≡ a2 (mod n2)\\n\\n...\\nx ≡ ak (mod nk)\\n\\nhas a solution. Furthermore, any two solutions of the system are congruent modulo\\nn1n2 · · ·nk.\\n\\nProof. We will use mathematical induction on the number of equations in the system. If\\nthere are k = 2 equations, then the theorem is true by Lemma 16.41. Now suppose that\\nthe result is true for a system of k equations or less and that we wish to find a solution of\\n\\nx ≡ a1 (mod n1)\\n\\nx ≡ a2 (mod n2)\\n\\n...\\nx ≡ ak+1 (mod nk+1).\\n\\nConsidering the first k equations, there exists a solution that is unique modulo n1 · · ·nk,\\nsay a. Since n1 · · ·nk and nk+1 are relatively prime, the system\\n\\nx ≡ a (mod n1 · · ·nk)\\nx ≡ ak+1 (mod nk+1)\\n\\nhas a solution that is unique modulo n1 . . . nk+1 by the lemma.\\n\\n\\n\\n16.5. AN APPLICATION TO SOFTWARE DESIGN 267\\n\\nExample 16.44. Let us solve the system\\n\\nx ≡ 3 (mod 4)\\n\\nx ≡ 4 (mod 5)\\n\\nx ≡ 1 (mod 9)\\n\\nx ≡ 5 (mod 7).\\n\\nFrom Example 16.42 we know that 19 is a solution of the first two congruences and any\\nother solution of the system is congruent to 19 (mod 20). Hence, we can reduce the system\\nto a system of three congruences:\\n\\nx ≡ 19 (mod 20)\\n\\nx ≡ 1 (mod 9)\\n\\nx ≡ 5 (mod 7).\\n\\nSolving the next two equations, we can reduce the system to\\n\\nx ≡ 19 (mod 180)\\n\\nx ≡ 5 (mod 7).\\n\\nSolving this last system, we find that 19 is a solution for the system that is unique up to\\nmodulo 1260.\\n\\nOne interesting application of the Chinese Remainder Theorem in the design of computer\\nsoftware is that the theorem allows us to break up a calculation involving large integers into\\nseveral less formidable calculations. A computer will handle integer calculations only up to\\na certain size due to the size of its processor chip, which is usually a 32 or 64-bit processor\\nchip. For example, the largest integer available on a computer with a 64-bit processor chip\\nis\\n\\n263 − 1 = 9,223,372,036,854,775,807.\\n\\nLarger processors such as 128 or 256-bit have been proposed or are under development.\\nThere is even talk of a 512-bit processor chip. The largest integer that such a chip could\\nstore with be 2511−1, which would be a 154 digit number. However, we would need to deal\\nwith much larger numbers to break sophisticated encryption schemes.\\n\\nSpecial software is required for calculations involving larger integers which cannot be\\nadded directly by the machine. By using the Chinese Remainder Theorem we can break\\ndown large integer additions and multiplications into calculations that the computer can\\nhandle directly. This is especially useful on parallel processing computers which have the\\nability to run several programs concurrently.\\n\\nMost computers have a single central processing unit (CPU) containing one processor\\nchip and can only add two numbers at a time. To add a list of ten numbers, the CPU must\\ndo nine additions in sequence. However, a parallel processing computer has more than one\\nCPU. A computer with 10 CPUs, for example, can perform 10 different additions at the\\nsame time. If we can take a large integer and break it down into parts, sending each part\\nto a different CPU, then by performing several additions or multiplications simultaneously\\non those parts, we can work with an integer that the computer would not be able to handle\\nas a whole.\\n\\nExample 16.45. Suppose that we wish to multiply 2134 by 1531. We will use the integers\\n95, 97, 98, and 99 because they are relatively prime. We can break down each integer into\\n\\n\\n\\n268 CHAPTER 16. RINGS\\n\\nfour parts:\\n\\n2134 ≡ 44 (mod 95)\\n\\n2134 ≡ 0 (mod 97)\\n\\n2134 ≡ 76 (mod 98)\\n\\n2134 ≡ 55 (mod 99)\\n\\nand\\n\\n1531 ≡ 11 (mod 95)\\n\\n1531 ≡ 76 (mod 97)\\n\\n1531 ≡ 61 (mod 98)\\n\\n1531 ≡ 46 (mod 99).\\n\\nMultiplying the corresponding equations, we obtain\\n\\n2134 · 1531 ≡ 44 · 11 ≡ 9 (mod 95)\\n\\n2134 · 1531 ≡ 0 · 76 ≡ 0 (mod 97)\\n\\n2134 · 1531 ≡ 76 · 61 ≡ 30 (mod 98)\\n\\n2134 · 1531 ≡ 55 · 46 ≡ 55 (mod 99).\\n\\nEach of these four computations can be sent to a different processor if our computer has\\nseveral CPUs. By the above calculation, we know that 2134 ·1531 is a solution of the system\\n\\nx ≡ 9 (mod 95)\\n\\nx ≡ 0 (mod 97)\\n\\nx ≡ 30 (mod 98)\\n\\nx ≡ 55 (mod 99).\\n\\nThe Chinese Remainder Theorem tells us that solutions are unique up to modulo 95 · 97 ·\\n98 · 99 = 89,403,930. Solving this system of congruences for x tells us that 2134 · 1531 =\\n3,267,154.\\n\\nThe conversion of the computation into the four subcomputations will take some com-\\nputing time. In addition, solving the system of congruences can also take considerable time.\\nHowever, if we have many computations to be performed on a particular set of numbers, it\\nmakes sense to transform the problem as we have done above and to perform the necessary\\ncalculations simultaneously.\\n\\n16.6 Exercises\\n1. Which of the following sets are rings with respect to the usual operations of addition\\nand multiplication? If the set is a ring, is it also a field?\\n(a) 7Z\\n(b) Z18\\n\\n(c) Q(\\n√\\n2 ) = {a+ b\\n\\n√\\n2 : a, b ∈ Q}\\n\\n(d) Q(\\n√\\n2,\\n√\\n3 ) = {a+ b\\n\\n√\\n2 + c\\n\\n√\\n3 + d\\n\\n√\\n6 : a, b, c, d ∈ Q}\\n\\n(e) Z[\\n√\\n3 ] = {a+ b\\n\\n√\\n3 : a, b ∈ Z}\\n\\n(f) R = {a+ b 3\\n√\\n3 : a, b ∈ Q}\\n\\n\\n\\n16.6. EXERCISES 269\\n\\n(g) Z[i] = {a+ bi : a, b ∈ Z and i2 = −1}\\n(h) Q( 3\\n\\n√\\n3 ) = {a+ b 3\\n\\n√\\n3 + c 3\\n\\n√\\n9 : a, b, c ∈ Q}\\n\\n2. Let R be the ring of 2× 2 matrices of the form(\\na b\\n\\n0 0\\n\\n)\\n,\\n\\nwhere a, b ∈ R. Show that although R is a ring that has no identity, we can find a subring\\nS of R with an identity.\\n\\n3. List or characterize all of the units in each of the following rings.\\n(a) Z10\\n\\n(b) Z12\\n\\n(c) Z7\\n\\n(d) M2(Z), the 2× 2 matrices with entries in Z\\n(e) M2(Z2), the 2× 2 matrices with entries in Z2\\n\\n4. Find all of the ideals in each of the following rings. Which of these ideals are maximal\\nand which are prime?\\n(a) Z18\\n\\n(b) Z25\\n\\n(c) M2(R), the 2× 2 matrices with entries in R\\n(d) M2(Z), the 2× 2 matrices with entries in Z\\n(e) Q\\n\\n5. For each of the following rings R with ideal I, give an addition table and a multiplication\\ntable for R/I.\\n(a) R = Z and I = 6Z\\n(b) R = Z12 and I = {0, 3, 6, 9}\\n\\n6. Find all homomorphisms ϕ : Z/6Z → Z/15Z.\\n\\n7. Prove that R is not isomorphic to C.\\n\\n8. Prove or disprove: The ring Q(\\n√\\n2 ) = {a + b\\n\\n√\\n2 : a, b ∈ Q} is isomorphic to the ring\\n\\nQ(\\n√\\n3 ) = {a+ b\\n\\n√\\n3 : a, b ∈ Q}.\\n\\n9. What is the characteristic of the field formed by the set of matrices\\n\\nF =\\n\\n{(\\n1 0\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n1 1\\n\\n1 0\\n\\n)\\n,\\n\\n(\\n0 1\\n\\n1 1\\n\\n)\\n,\\n\\n(\\n0 0\\n\\n0 0\\n\\n)}\\nwith entries in Z2?\\n\\n10. Define a map ϕ : C → M2(R) by\\n\\nϕ(a+ bi) =\\n\\n(\\na b\\n\\n−b a\\n\\n)\\n.\\n\\nShow that ϕ is an isomorphism of C with its image in M2(R).\\n\\n\\n\\n270 CHAPTER 16. RINGS\\n\\n11. Prove that the Gaussian integers, Z[i], are an integral domain.\\n\\n12. Prove that Z[\\n√\\n3 i] = {a+ b\\n\\n√\\n3 i : a, b ∈ Z} is an integral domain.\\n\\n13. Solve each of the following systems of congruences.\\n\\n\\n\\n16.6. EXERCISES 271\\n\\n(a)\\n\\nx ≡ 2 (mod 5)\\n\\nx ≡ 6 (mod 11)\\n\\n(b)\\n\\nx ≡ 3 (mod 7)\\n\\nx ≡ 0 (mod 8)\\n\\nx ≡ 5 (mod 15)\\n\\n(c)\\n\\nx ≡ 2 (mod 4)\\n\\nx ≡ 4 (mod 7)\\n\\nx ≡ 7 (mod 9)\\n\\nx ≡ 5 (mod 11)\\n\\n(d)\\n\\nx ≡ 3 (mod 5)\\n\\nx ≡ 0 (mod 8)\\n\\nx ≡ 1 (mod 11)\\n\\nx ≡ 5 (mod 13)\\n\\n14. Use the method of parallel computation outlined in the text to calculate 2234 + 4121\\nby dividing the calculation into four separate additions modulo 95, 97, 98, and 99.\\n\\n15. Explain why the method of parallel computation outlined in the text fails for 2134·1531\\nif we attempt to break the calculation down into two smaller calculations modulo 98 and\\n99.\\n\\n16. If R is a field, show that the only two ideals of R are {0} and R itself.\\n\\n17. Let a be any element in a ring R with identity. Show that (−1)a = −a.\\n\\n18. Let ϕ : R→ S be a ring homomorphism. Prove each of the following statements.\\n(a) If R is a commutative ring, then ϕ(R) is a commutative ring.\\n(b) ϕ(0) = 0.\\n(c) Let 1R and 1S be the identities for R and S, respectively. If ϕ is onto, then ϕ(1R) = 1S .\\n(d) If R is a field and ϕ(R) ̸= 0, then ϕ(R) is a field.\\n\\n19. Prove that the associative law for multiplication and the distributive laws hold in R/I.\\n\\n20. Prove the Second Isomorphism Theorem for rings: Let I be a subring of a ring R and\\nJ an ideal in R. Then I ∩ J is an ideal in I and\\n\\nI/I ∩ J ∼= I + J/J.\\n\\n21. Prove the Third Isomorphism Theorem for rings: Let R be a ring and I and J be ideals\\nof R, where J ⊂ I. Then\\n\\nR/I ∼=\\nR/J\\n\\nI/J\\n.\\n\\n22. Prove the Correspondence Theorem: Let I be an ideal of a ring R. Then S → S/I\\nis a one-to-one correspondence between the set of subrings S containing I and the set of\\nsubrings of R/I. Furthermore, the ideals of R correspond to ideals of R/I.\\n\\n23. Let R be a ring and S a subset of R. Show that S is a subring of R if and only if each\\nof the following conditions is satisfied.\\n(a) S ̸= ∅.\\n\\n\\n\\n272 CHAPTER 16. RINGS\\n\\n(b) rs ∈ S for all r, s ∈ S.\\n(c) r − s ∈ S for all r, s ∈ S.\\n\\n24. Let R be a ring with a collection of subrings {Rα}. Prove that\\n∩\\nRα is a subring of R.\\n\\nGive an example to show that the union of two subrings cannot be a subring.\\n\\n25. Let {Iα}α∈A be a collection of ideals in a ring R. Prove that\\n∩\\n\\nα∈A Iα is also an ideal\\nin R. Give an example to show that if I1 and I2 are ideals in R, then I1 ∪ I2 may not be\\nan ideal.\\n\\n26. Let R be an integral domain. Show that if the only ideals in R are {0} and R itself, R\\nmust be a field.\\n\\n27. Let R be a commutative ring. An element a in R is nilpotent if an = 0 for some\\npositive integer n. Show that the set of all nilpotent elements forms an ideal in R.\\n\\n28. A ring R is a Boolean ring if for every a ∈ R, a2 = a. Show that every Boolean ring\\nis a commutative ring.\\n\\n29. Let R be a ring, where a3 = a for all a ∈ R. Prove that R must be a commutative ring.\\n\\n30. Let R be a ring with identity 1R and S a subring of R with identity 1S . Prove or\\ndisprove that 1R = 1S .\\n\\n31. If we do not require the identity of a ring to be distinct from 0, we will not have a very\\ninteresting mathematical structure. Let R be a ring such that 1 = 0. Prove that R = {0}.\\n\\n32. Let S be a nonempty subset of a ring R. Prove that there is a subring R′ of R that\\ncontains S.\\n\\n33. Let R be a ring. Define the center of R to be\\n\\nZ(R) = {a ∈ R : ar = ra for all r ∈ R}.\\n\\nProve that Z(R) is a commutative subring of R.\\n\\n34. Let p be prime. Prove that\\n\\nZ(p) = {a/b : a, b ∈ Z and gcd(b, p) = 1}\\n\\nis a ring. The ring Z(p) is called the ring of integers localized at p.\\n\\n35. Prove or disprove: Every finite integral domain is isomorphic to Zp.\\n\\n36. Let R be a ring with identity.\\n(a) Let u be a unit in R. Define a map iu : R → R by r 7→ uru−1. Prove that iu is an\\n\\nautomorphism of R. Such an automorphism of R is called an inner automorphism of\\nR. Denote the set of all inner automorphisms of R by Inn(R).\\n\\n(b) Denote the set of all automorphisms of R by Aut(R). Prove that Inn(R) is a normal\\nsubgroup of Aut(R).\\n\\n(c) Let U(R) be the group of units in R. Prove that the map\\n\\nϕ : U(R) → Inn(R)\\n\\ndefined by u 7→ iu is a homomorphism. Determine the kernel of ϕ.\\n(d) Compute Aut(Z), Inn(Z), and U(Z).\\n\\n\\n\\n16.7. PROGRAMMING EXERCISE 273\\n\\n37. Let R and S be arbitrary rings. Show that their Cartesian product is a ring if we define\\naddition and multiplication in R× S by\\n(a) (r, s) + (r′, s′) = (r + r′, s+ s′)\\n\\n(b) (r, s)(r′, s′) = (rr′, ss′)\\n\\n38. An element x in a ring is called an idempotent if x2 = x. Prove that the only\\nidempotents in an integral domain are 0 and 1. Find a ring with a idempotent x not equal\\nto 0 or 1.\\n\\n39. Let gcd(a, n) = d and gcd(b, d) ̸= 1. Prove that ax ≡ b (mod n) does not have a\\nsolution.\\n\\n40. (The Chinese Remainder Theorem for Rings) Let R be a ring and I and J be ideals\\nin R such that I + J = R.\\n(a) Show that for any r and s in R, the system of equations\\n\\nx ≡ r (mod I)\\n\\nx ≡ s (mod J)\\n\\nhas a solution.\\n(b) In addition, prove that any two solutions of the system are congruent modulo I ∩ J .\\n(c) Let I and J be ideals in a ring R such that I + J = R. Show that there exists a ring\\n\\nisomorphism\\nR/(I ∩ J) ∼= R/I ×R/J.\\n\\n16.7 Programming Exercise\\n1. Write a computer program implementing fast addition and multiplication using the\\nChinese Remainder Theorem and the method outlined in the text.\\n\\n16.8 References and Suggested Readings\\n[1] Anderson, F. W. and Fuller, K. R. Rings and Categories of Modules. 2nd ed. Springer,\\n\\nNew York, 1992.\\n[2] Atiyah, M. F. and MacDonald, I. G. Introduction to Commutative Algebra. Westview\\n\\nPress, Boulder, CO, 1994.\\n[3] Herstein, I. N. Noncommutative Rings. Mathematical Association of America, Wash-\\n\\nington, DC, 1994.\\n[4] Kaplansky, I. Commutative Rings. Revised edition. University of Chicago Press,\\n\\nChicago, 1974.\\n[5] Knuth, D. E. The Art of Computer Programming: Semi-Numerical Algorithms, vol.\\n\\n2. 3rd ed. Addison-Wesley Professional, Boston, 1997.\\n[6] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998. A\\n\\ngood source for applications.\\n[7] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\\n[8] McCoy, N. H. Rings and Ideals. Carus Monograph Series, No. 8. Mathematical\\n\\nAssociation of America, Washington, DC, 1968.\\n\\n\\n\\n274 CHAPTER 16. RINGS\\n\\n[9] McCoy, N. H. The Theory of Rings. Chelsea, New York, 1972.\\n[10] Zariski, O. and Samuel, P. Commutative Algebra, vols. I and II. Springer, New York,\\n\\n1975, 1960.\\n\\n16.9 Sage\\nRings are very important in your study of abstract algebra, and similarly, they are very\\nimportant in the design and use of Sage. There is a lot of material in this chapter, and\\nthere are many corresponding commands in Sage.\\n\\nCreating Rings\\nHere is a list of various rings, domains and fields you can construct simply.\\n\\n1. Integers(), ZZ: the integral domain of positive and negative integers, Z.\\n\\n2. Integers(n): the integers mod n, Zn. A field when n is prime, but just a ring for\\ncomposite n.\\n\\n3. QQ: the field of rational numbers, Q.\\n\\n4. RR, CC: the field of real numbers and the field of complex numbers, R, C. It is\\nimpossible to create every real number inside a computer, so technically these sets do\\nnot behave as fields, but only give a good imitiation of the real thing. We say they\\nare inexact rings to make this point.\\n\\n5. QuadraticField(n): the field formed by combining the rationals with a solution to the\\npolynomial equation x2 − n = 0. The notation in the text is Q[\\n\\n√\\nn]. A functional\\n\\nequivalent can be made with the syntax QQ[sqrt(n)]. Note that n can be negative.\\n\\n6. CyclotomicField(n): the field formed by combining the rationals with the solutions to\\nthe polynomial equation xn − 1 = 0.\\n\\n7. QQbar: the field formed by combining the rationals with the solutions to every poly-\\nnomial equation with integer coefficients. This is known as a the field of algebraic\\nnumbers, denoted as Q.\\n\\n8. FiniteField(p): for a prime p, the field of integers Zp.\\n\\nIf you print a description of some of the above rings, you will sometimes see a new\\nsymbol introduced. Consider the following example:\\n\\nF = QuadraticField (7)\\nF\\n\\nNumber Field in a with defining polynomial x^2 - 7\\n\\nroot = F.gen(0)\\nroot^2\\n\\n7\\n\\nroot\\n\\na\\n\\n\\n\\n16.9. SAGE 275\\n\\n(2* root)^3\\n\\n56*a\\n\\nHere Number Field describes an object generally formed by combining the rationals with\\nanother number (here\\n\\n√\\n7). “a” is a new symbol which behaves as a root of the polynomial\\n\\nx2 − 7. We do not say which root,\\n√\\n7 or −\\n\\n√\\n7, and as we understand the theory better we\\n\\nwill see that this does not really matter.\\nWe can obtain this root as a generator of the number field, and then manipulate it.\\n\\nFirst squaring root yields 7. Notice that root prints as a. Notice, too, that computations\\nwith root behave as if it was either root of x2 − 7, and results print using a.\\n\\nThis can get a bit confusing, inputing computations with root and getting output in\\nterms of a. Fortunately, there is a better way. Consider the following example:\\n\\nF.<b> = QuadraticField (7)\\nF\\n\\nNumber Field in b with defining polynomial x^2 - 7\\n\\nb^2\\n\\n7\\n\\n(2*b)^3\\n\\n56*b\\n\\nWith the syntax F.<b> we can create the field F along with specifying a generator b using\\na name of our choosing. Then computations can use b in both input and output as a root\\nof x2 − 7.\\n\\nHere are three new rings that are best created using this new syntax.\\n\\n1. F.<a> = FiniteField(p^n): We will later have a theorem that tells us that finite fields\\nonly exist with orders equal to to a power of a prime. When the power is larger than\\n1, then we need a generator, here given as a.\\n\\n2. P.<x>=R[]: the ring of all polynomials in the variable x, with coefficients from the ring\\nR. Notice that R can be any ring, so this is a very general construction that uses one\\nring to form another. See an example below.\\n\\n3. Q.<r,s,t> = QuaternionAlgebra(n, m): the rationals combined with indeterminates r,\\ns and t such that r2 = n, s2 = m and t = rs = −sr. This is a generalization\\nof the quaternions described in this chapter, though over the rationals rather than\\nthe reals, so it is an exact ring. Notice that this is one of the few noncommuta-\\ntive rings in Sage. The “usual” quaternions would be constructed with Q.<I,J,K> =\\n\\nQuaternionAlgebra(-1, -1). (Notice that using I here is not a good choice, because it\\nwill then clobber the symbol I used for complex numbers.)\\n\\nSyntax specifying names for generators can be used for many of the above rings as well,\\nsuch as demonstrated above for quadratic fields and below for cyclotomic fields.\\n\\nC.<t> = CyclotomicField (8)\\nC.random_element ()\\n\\n-2/11*t^2 + t - 1\\n\\n\\n\\n276 CHAPTER 16. RINGS\\n\\nProperties of Rings\\nThe examples below demonstrate how to query certain properties of rings. If you are playing\\nalong, be sure to execute the first compute cell to define the various rings involved in the\\nexamples.\\n\\nZ7 = Integers (7)\\nZ9 = Integers (9)\\nQ = QuadraticField (-11)\\nF.<a> = FiniteField (3^2)\\nP.<x> = Z7[]\\nS.<f,g,h> = QuaternionAlgebra (-7, 3)\\n\\nExact versus inexact.\\nQQ.is_exact ()\\n\\nTrue\\n\\nRR.is_exact ()\\n\\nFalse\\n\\nFinite versus infinite.\\nZ7.is_finite ()\\n\\nTrue\\n\\nZ7.is_finite ()\\n\\nTrue\\n\\nIntegral domain?\\nZ7.is_integral_domain ()\\n\\nTrue\\n\\nZ9.is_integral_domain ()\\n\\nFalse\\n\\nField?\\nZ9.is_field ()\\n\\nFalse\\n\\nF.is_field ()\\n\\nTrue\\n\\nQ.is_field ()\\n\\nTrue\\n\\nCommutative?\\n\\n\\n\\n16.9. SAGE 277\\n\\nQ.is_commutative ()\\n\\nTrue\\n\\nS.is_commutative ()\\n\\nFalse\\n\\nCharacteristic.\\nZ7.characteristic ()\\n\\n7\\n\\nZ9.characteristic ()\\n\\n9\\n\\nQ.characteristic ()\\n\\n0\\n\\nF.characteristic ()\\n\\n3\\n\\nP.characteristic ()\\n\\n7\\n\\nS.characteristic ()\\n\\n0\\n\\nAdditive and multiplicative identities print like you would expect, but notice that while\\nthey may print identically, they could be different because of the ring they live in.\\n\\nb = Z9.zero(); b\\n\\n0\\n\\nb.parent ()\\n\\nRing of integers modulo 9\\n\\nc = Q.zero(); c\\n\\n0\\n\\nc.parent ()\\n\\nNumber Field in a with defining polynomial x^2 + 11\\n\\nb == c\\n\\n\\n\\n278 CHAPTER 16. RINGS\\n\\nFalse\\n\\nd = Z9.one(); d\\n\\n1\\n\\nd.parent ()\\n\\nRing of integers modulo 9\\n\\ne = Q.one(); e\\n\\n1\\n\\ne.parent ()\\n\\nNumber Field in a with defining polynomial x^2 + 11\\n\\nd == e\\n\\nFalse\\n\\nThere is some support for subrings. For example, Q and S are extensions of the rationals,\\nwhile F is totally distinct from the rationals.\\n\\nQQ.is_subring(Q)\\n\\nTrue\\n\\nQQ.is_subring(S)\\n\\nTrue\\n\\nQQ.is_subring(F)\\n\\nFalse\\n\\nNot every element of a ring may have a multiplicative inverse, in other words, not every\\nelement has to be a unit (unless the ring is a field). It would now be good practice to check\\nif an element is a unit before you try computing its inverse.\\n\\nthree = Z9(3)\\nthree.is_unit ()\\n\\nFalse\\n\\nthree*three\\n\\n0\\n\\nfour = Z9(4)\\nfour.is_unit ()\\n\\nTrue\\n\\n\\n\\n16.9. SAGE 279\\n\\ng = four^-1; g\\n\\n7\\n\\nfour*g\\n\\n1\\n\\nQuotient Structure\\nIdeals are the normal subgroups of rings and allow us to build “quotients” — basically new\\nrings defined on equivalence classes of elements of the original ring. Sage support for ideals\\nis variable. When they can be created, there is not always a lot you can do with them. But\\nthey work well in certain very important cases.\\n\\nThe integers, Z, have ideals that are just multiples of a single integer. We can create\\nthem with the .ideal() method or just by wrting a scalar multiple of ZZ. And then the\\nquotient is isomorphic to a well-understood ring. (Notice that I is a bad name for an ideal\\nif we want to work with complex numbers later.)\\n\\nI1 = ZZ.ideal (4)\\nI2 = 4*ZZ\\nI3 = (-4)*ZZ\\nI1 == I2\\n\\nTrue\\n\\nI2 == I3\\n\\nTrue\\n\\nQ = ZZ.quotient(I1); Q\\n\\nRing of integers modulo 4\\n\\nQ == Integers (4)\\n\\nTrue\\n\\nWe might normally be more careful about the last statement. The quotient is a set of\\nequivalence classes, each infinite, and certainly not a single integer. But the quotient is\\nisomorphic to Z4, so Sage just makes this identification.\\n\\nZ7 = Integers (7)\\nP.<y> = Z7[]\\nM = P.ideal(y^2+4)\\nQ = P.quotient(M)\\nQ\\n\\nUnivariate Quotient Polynomial Ring in ybar over\\nRing of integers modulo 7 with modulus y^2 + 4\\n\\nQ.random_element ()\\n\\n2*ybar + 6\\n\\n\\n\\n280 CHAPTER 16. RINGS\\n\\nQ.order()\\n\\n49\\n\\nQ.is_field ()\\n\\nTrue\\n\\nNotice that the construction of the quotient ring has created a new generator, converting\\ny (y) to ybar (y). We can override this as before with the syntax demonstrated below.\\n\\nQ.<t> = P.quotient(M); Q\\n\\nUnivariate Quotient Polynomial Ring in t over\\nRing of integers modulo 7 with modulus y^2 + 4\\n\\nQ.random_element ()\\n\\n4*t + 6\\n\\nSo from a quotient of an infinite ring and an ideal (which is also a ring), we create a\\nfield, which is finite. Understanding this construction will be an important theme in the\\nnext few chapters. To see how remarkable it is, consider what happens with just one little\\nchange.\\n\\nZ7 = Integers (7)\\nP.<y> = Z7[]\\nM = P.ideal(y^2+3)\\nQ.<t> = P.quotient(M)\\nQ\\n\\nUnivariate Quotient Polynomial Ring in t over\\nRing of integers modulo 7 with modulus y^2 + 3\\n\\nQ.random_element ()\\n\\n3*t + 1\\n\\nQ.order()\\n\\n49\\n\\nQ.is_field ()\\n\\nFalse\\n\\nThere are a few methods available which will give us properties of ideals. In particular,\\nwe can check for prime and maximal ideals in rings of polynomials. Examine the results\\nabove and below in the context of Theorem 16.35.\\n\\nZ7 = Integers (7)\\nP.<y> = Z7[]\\nM = P.ideal(y^2+4)\\nN = P.ideal(y^2+3)\\nM.is_maximal ()\\n\\n\\n\\n16.9. SAGE 281\\n\\nTrue\\n\\nN.is_maximal ()\\n\\nFalse\\n\\nThe fact that M is a prime ideal is verification of Corollary 16.40.\\nM.is_prime ()\\n\\nTrue\\n\\nN.is_prime ()\\n\\nFalse\\n\\nRing Homomorphisms\\nWhen Sage is presented with 3 + 4/3, how does it know that 3 is meant to be an integer?\\nAnd then to add it to a rational, how does it know that we really want to view the compu-\\ntation as 3/1 + 4/3? This is really easy for you and me, but devilishly hard for a program,\\nand you can imagine it getting ever more complicated with the many possible rings in\\nSage, subrings, matrices, etc. Part of the answer is that Sage uses ring homomorphisms to\\n“translate” objects (numbers) between rings.\\n\\nWe will give an example below, but not pursue the topic much further. For the curious,\\nreading the Sage documentation and experimenting would be a good exercise.\\n\\nH = Hom(ZZ , QQ)\\nphi = H([1])\\nphi\\n\\nRing morphism:\\nFrom: Integer Ring\\nTo: Rational Field\\nDefn: 1 |--> 1\\n\\nphi.parent ()\\n\\nSet of Homomorphisms from Integer Ring to Rational Field\\n\\na = 3; a\\n\\n3\\n\\na.parent ()\\n\\nInteger Ring\\n\\nb = phi (3); b\\n\\n3\\n\\nb.parent ()\\n\\n\\n\\n282 CHAPTER 16. RINGS\\n\\nRational Field\\n\\nSo phi is a homomorphism (“morphism”) that converts integers (the domain is ZZ) into\\nrationals (the codomain is QQ), whose parent is a set of homomorphisms that Sage calls\\na “homset.” Even though a and b both print as 3, which is indistinguishable to our eyes,\\nthe parents of a and b are different. Yet the numerical value of the two objects has not\\nchanged.\\n\\n16.10 Sage Exercises\\n1. Define the two rings Z11 and Z12 with the commands R = Integers(11) and S = Integers(12).\\nFor each ring, use the relevant command to determine: if the ring is finite, if it is commu-\\ntative, if it is an integral domain and if it is a field. Then use single Sage commands to find\\nthe order of the ring, list the elements, and output the multiplicative identity (i.e. 1, if it\\nexists).\\n\\n2. Define R to be the ring of integers, Z, by executing R = ZZ or R = Integers(). A command\\nlike R.ideal(4) will create the principal ideal ⟨4⟩. The same command can accept more than\\none generator, so for example, R.ideal(3, 5) will create the ideal {a · 3 + b · 5 | a, b ∈ Z}.\\nCreate several ideals of Z with two generators and ask Sage to print each as you create it.\\nExplain what you observe and then create code that will test your observation for thousands\\nof different examples.\\n\\n3. Create a finite field F of order 81 with F.<t>=FiniteField(3^4).\\n(a) List the elements of F .\\n(b) Obtain the generators of F with F.gens().\\n(c) Obtain the first generator of F and save it as u with u = F.0 (alternatively, u =\\n\\nF.gen(0)).\\n(d) Compute the first 80 powers of u and comment.\\n(e) The generator you have worked with above is a root of a polynomial over Z3. Obtain\\n\\nthis polynomial with F.modulus() and use this observation to explain the entry in your\\nlist of powers that is the fourth power of the generator.\\n\\n4. Build and analyze a quotient ring as follows:\\n(a) Use P.<z>=Integers(7)[] to construct a ring P of polynomials in z with coefficients\\n\\nfrom Z7.\\n(b) Use K = P.ideal(z^2+z+3) to build a principal ideal K generated by the polynomial\\n\\nz2 + z + 3.\\n(c) Use H = P.quotient(K) to build H, the quotient ring of P by K.\\n(d) Use Sage to verify that H is a field.\\n(e) As in the previous exercise, obtain a generator and examine the proper collection of\\n\\npowers of that generator.\\n\\n\\n\\n17\\n\\nPolynomials\\n\\nMost people are fairly familiar with polynomials by the time they begin to study abstract\\nalgebra. When we examine polynomial expressions such as\\n\\np(x) = x3 − 3x+ 2\\n\\nq(x) = 3x2 − 6x+ 5,\\n\\nwe have a pretty good idea of what p(x) + q(x) and p(x)q(x) mean. We just add and\\nmultiply polynomials as functions; that is,\\n\\n(p+ q)(x) = p(x) + q(x)\\n\\n= (x3 − 3x+ 2) + (3x2 − 6x+ 5)\\n\\n= x3 + 3x2 − 9x+ 7\\n\\nand\\n\\n(pq)(x) = p(x)q(x)\\n\\n= (x3 − 3x+ 2)(3x2 − 6x+ 5)\\n\\n= 3x5 − 6x4 − 4x3 + 24x2 − 27x+ 10.\\n\\nIt is probably no surprise that polynomials form a ring. In this chapter we shall emphasize\\nthe algebraic structure of polynomials by studying polynomial rings. We can prove many\\nresults for polynomial rings that are similar to the theorems we proved for the integers.\\nAnalogs of prime numbers, the division algorithm, and the Euclidean algorithm exist for\\npolynomials.\\n\\n17.1 Polynomial Rings\\nThroughout this chapter we shall assume that R is a commutative ring with identity. Any\\nexpression of the form\\n\\nf(x) =\\n\\nn∑\\ni=0\\n\\naix\\ni = a0 + a1x+ a2x\\n\\n2 + · · ·+ anx\\nn,\\n\\nwhere ai ∈ R and an ̸= 0, is called a polynomial over R with indeterminate x. The\\nelements a0, a1, . . . , an are called the coefficients of f . The coefficient an is called the\\nleading coefficient. A polynomial is called monic if the leading coefficient is 1. If n is\\nthe largest nonnegative number for which an ̸= 0, we say that the degree of f is n and write\\ndeg f(x) = n. If no such n exists—that is, if f = 0 is the zero polynomial—then the degree\\n\\n283\\n\\n\\n\\n284 CHAPTER 17. POLYNOMIALS\\n\\nof f is defined to be −∞. We will denote the set of all polynomials with coefficients in a\\nring R by R[x]. Two polynomials are equal exactly when their corresponding coefficients\\nare equal; that is, if we let\\n\\np(x) = a0 + a1x+ · · ·+ anx\\nn\\n\\nq(x) = b0 + b1x+ · · ·+ bmx\\nm,\\n\\nthen p(x) = q(x) if and only if ai = bi for all i ≥ 0.\\nTo show that the set of all polynomials forms a ring, we must first define addition and\\n\\nmultiplication. We define the sum of two polynomials as follows. Let\\n\\np(x) = a0 + a1x+ · · ·+ anx\\nn\\n\\nq(x) = b0 + b1x+ · · ·+ bmx\\nm.\\n\\nThen the sum of p(x) and q(x) is\\n\\np(x) + q(x) = c0 + c1x+ · · ·+ ckx\\nk,\\n\\nwhere ci = ai + bi for each i. We define the product of p(x) and q(x) to be\\n\\np(x)q(x) = c0 + c1x+ · · ·+ cm+nx\\nm+n,\\n\\nwhere\\n\\nci =\\n\\ni∑\\nk=0\\n\\nakbi−k = a0bi + a1bi−1 + · · ·+ ai−1b1 + aib0\\n\\nfor each i. Notice that in each case some of the coefficients may be zero.\\n\\nExample 17.1. Suppose that\\n\\np(x) = 3 + 0x+ 0x2 + 2x3 + 0x4\\n\\nand\\nq(x) = 2 + 0x− x2 + 0x3 + 4x4\\n\\nare polynomials in Z[x]. If the coefficient of some term in a polynomial is zero, then we\\nusually just omit that term. In this case we would write p(x) = 3 + 2x3 and q(x) =\\n2− x2 + 4x4. The sum of these two polynomials is\\n\\np(x) + q(x) = 5− x2 + 2x3 + 4x4.\\n\\nThe product,\\n\\np(x)q(x) = (3 + 2x3)(2− x2 + 4x4) = 6− 3x2 + 4x3 + 12x4 − 2x5 + 8x7,\\n\\ncan be calculated either by determining the ci’s in the definition or by simply multiplying\\npolynomials in the same way as we have always done.\\n\\nExample 17.2. Let\\n\\np(x) = 3 + 3x3 and q(x) = 4 + 4x2 + 4x4\\n\\nbe polynomials in Z12[x]. The sum of p(x) and q(x) is 7+ 4x2 +3x3 +4x4. The product of\\nthe two polynomials is the zero polynomial. This example tells us that we can not expect\\nR[x] to be an integral domain if R is not an integral domain.\\n\\n\\n\\n17.1. POLYNOMIAL RINGS 285\\n\\nTheorem 17.3. Let R be a commutative ring with identity. Then R[x] is a commutative\\nring with identity.\\n\\nProof. Our first task is to show that R[x] is an abelian group under polynomial addi-\\ntion. The zero polynomial, f(x) = 0, is the additive identity. Given a polynomial p(x) =∑n\\n\\ni=0 aix\\ni, the inverse of p(x) is easily verified to be −p(x) =\\n\\n∑n\\ni=0(−ai)xi = −\\n\\n∑n\\ni=0 aix\\n\\ni.\\nCommutativity and associativity follow immediately from the definition of polynomial ad-\\ndition and from the fact that addition in R is both commutative and associative.\\n\\nTo show that polynomial multiplication is associative, let\\n\\np(x) =\\n\\nm∑\\ni=0\\n\\naix\\ni,\\n\\nq(x) =\\n\\nn∑\\ni=0\\n\\nbix\\ni,\\n\\nr(x) =\\n\\np∑\\ni=0\\n\\ncix\\ni.\\n\\nThen\\n\\n[p(x)q(x)]r(x) =\\n\\n[(\\nm∑\\ni=0\\n\\naix\\ni\\n\\n)(\\nn∑\\n\\ni=0\\n\\nbix\\ni\\n\\n)](\\np∑\\n\\ni=0\\n\\ncix\\ni\\n\\n)\\n\\n=\\n\\n\uf8ee\uf8f0m+n∑\\ni=0\\n\\n\uf8eb\uf8ed i∑\\nj=0\\n\\najbi−j\\n\\n\uf8f6\uf8f8xi\\n\\n\uf8f9\uf8fb( p∑\\ni=0\\n\\ncix\\ni\\n\\n)\\n\\n=\\n\\nm+n+p∑\\ni=0\\n\\n\uf8ee\uf8f0 i∑\\nj=0\\n\\n(\\nj∑\\n\\nk=0\\n\\nakbj−k\\n\\n)\\nci−j\\n\\n\uf8f9\uf8fbxi\\n=\\n\\nm+n+p∑\\ni=0\\n\\n\uf8eb\uf8ed ∑\\nj+k+l=i\\n\\najbkcl\\n\\n\uf8f6\uf8f8xi\\n\\n=\\n\\nm+n+p∑\\ni=0\\n\\n\uf8ee\uf8f0 i∑\\nj=0\\n\\naj\\n\\n(\\ni−j∑\\nk=0\\n\\nbkci−j−k\\n\\n)\uf8f9\uf8fbxi\\n=\\n\\n(\\nm∑\\ni=0\\n\\naix\\ni\\n\\n)\uf8ee\uf8f0n+p∑\\ni=0\\n\\n\uf8eb\uf8ed i∑\\nj=0\\n\\nbjci−j\\n\\n\uf8f6\uf8f8xi\\n\\n\uf8f9\uf8fb\\n=\\n\\n(\\nm∑\\ni=0\\n\\naix\\ni\\n\\n)[(\\nn∑\\n\\ni=0\\n\\nbix\\ni\\n\\n)(\\np∑\\n\\ni=0\\n\\ncix\\ni\\n\\n)]\\n= p(x)[q(x)r(x)]\\n\\nThe commutativity and distribution properties of polynomial multiplication are proved in\\na similar manner. We shall leave the proofs of these properties as an exercise.\\n\\nProposition 17.4. Let p(x) and q(x) be polynomials in R[x], where R is an integral domain.\\nThen deg p(x) + deg q(x) = deg(p(x)q(x)). Furthermore, R[x] is an integral domain.\\n\\nProof. Suppose that we have two nonzero polynomials\\n\\np(x) = amx\\nm + · · ·+ a1x+ a0\\n\\n\\n\\n286 CHAPTER 17. POLYNOMIALS\\n\\nand\\nq(x) = bnx\\n\\nn + · · ·+ b1x+ b0\\n\\nwith am ̸= 0 and bn ̸= 0. The degrees of p(x) and q(x) are m and n, respectively. The\\nleading term of p(x)q(x) is ambnxm+n, which cannot be zero since R is an integral domain;\\nhence, the degree of p(x)q(x) is m + n, and p(x)q(x) ̸= 0. Since p(x) ̸= 0 and q(x) ̸= 0\\nimply that p(x)q(x) ̸= 0, we know that R[x] must also be an integral domain.\\n\\nWe also want to consider polynomials in two or more variables, such as x2 − 3xy+ 2y3.\\nLet R be a ring and suppose that we are given two indeterminates x and y. Certainly\\nwe can form the ring (R[x])[y]. It is straightforward but perhaps tedious to show that\\n(R[x])[y] ∼= R([y])[x]. We shall identify these two rings by this isomorphism and simply write\\nR[x, y]. The ring R[x, y] is called the ring of polynomials in two indeterminates x and\\ny with coefficients in R. We can define the ring of polynomials in n indeterminates\\nwith coefficients in R similarly. We shall denote this ring by R[x1, x2, . . . , xn].\\n\\nTheorem 17.5. Let R be a commutative ring with identity and α ∈ R. Then we have a\\nring homomorphism ϕα : R[x] → R defined by\\n\\nϕα(p(x)) = p(α) = anα\\nn + · · ·+ a1α+ a0,\\n\\nwhere p(x) = anx\\nn + · · ·+ a1x+ a0.\\n\\nProof. Let p(x) =\\n∑n\\n\\ni=0 aix\\ni and q(x) =\\n\\n∑m\\ni=0 bix\\n\\ni. It is easy to show that ϕα(p(x) +\\nq(x)) = ϕα(p(x)) + ϕα(q(x)). To show that multiplication is preserved under the map ϕα,\\nobserve that\\n\\nϕα(p(x))ϕα(q(x)) = p(α)q(α)\\n\\n=\\n\\n(\\nn∑\\n\\ni=0\\n\\naiα\\ni\\n\\n)(\\nm∑\\ni=0\\n\\nbiα\\ni\\n\\n)\\n\\n=\\nm+n∑\\ni=0\\n\\n(\\ni∑\\n\\nk=0\\n\\nakbi−k\\n\\n)\\nαi\\n\\n= ϕα(p(x)q(x)).\\n\\nThe map ϕα : R[x] → R is called the evaluation homomorphism at α.\\n\\n17.2 The Division Algorithm\\nRecall that the division algorithm for integers (Theorem 2.9) says that if a and b are integers\\nwith b > 0, then there exist unique integers q and r such that a = bq + r, where 0 ≤ r < b.\\nThe algorithm by which q and r are found is just long division. A similar theorem exists for\\npolynomials. The division algorithm for polynomials has several important consequences.\\nSince its proof is very similar to the corresponding proof for integers, it is worthwhile to\\nreview Theorem 2.9 at this point.\\n\\nTheorem 17.6 (Division Algorithm). Let f(x) and g(x) be polynomials in F [x], where F\\nis a field and g(x) is a nonzero polynomial. Then there exist unique polynomials q(x), r(x) ∈\\nF [x] such that\\n\\nf(x) = g(x)q(x) + r(x),\\n\\nwhere either deg r(x) < deg g(x) or r(x) is the zero polynomial.\\n\\n\\n\\n17.2. THE DIVISION ALGORITHM 287\\n\\nProof. We will first consider the existence of q(x) and r(x). If f(x) is the zero polynomial,\\nthen\\n\\n0 = 0 · g(x) + 0;\\n\\nhence, both q and r must also be the zero polynomial. Now suppose that f(x) is not the\\nzero polynomial and that deg f(x) = n and deg g(x) = m. If m > n, then we can let\\nq(x) = 0 and r(x) = f(x). Hence, we may assume that m ≤ n and proceed by induction on\\nn. If\\n\\nf(x) = anx\\nn + an−1x\\n\\nn−1 + · · ·+ a1x+ a0\\n\\ng(x) = bmx\\nm + bm−1x\\n\\nm−1 + · · ·+ b1x+ b0\\n\\nthe polynomial\\nf ′(x) = f(x)− an\\n\\nbm\\nxn−mg(x)\\n\\nhas degree less than n or is the zero polynomial. By induction, there exist polynomials q′(x)\\nand r(x) such that\\n\\nf ′(x) = q′(x)g(x) + r(x),\\n\\nwhere r(x) = 0 or the degree of r(x) is less than the degree of g(x). Now let\\n\\nq(x) = q′(x) +\\nan\\nbm\\nxn−m.\\n\\nThen\\nf(x) = g(x)q(x) + r(x),\\n\\nwith r(x) the zero polynomial or deg r(x) < deg g(x).\\nTo show that q(x) and r(x) are unique, suppose that there exist two other polynomials\\n\\nq1(x) and r1(x) such that f(x) = g(x)q1(x) + r1(x) with deg r1(x) < deg g(x) or r1(x) = 0,\\nso that\\n\\nf(x) = g(x)q(x) + r(x) = g(x)q1(x) + r1(x),\\n\\nand\\ng(x)[q(x)− q1(x)] = r1(x)− r(x).\\n\\nIf g(x) is not the zero polynomial, then\\n\\ndeg(g(x)[q(x)− q1(x)]) = deg(r1(x)− r(x)) ≥ deg g(x).\\n\\nHowever, the degrees of both r(x) and r1(x) are strictly less than the degree of g(x);\\ntherefore, r(x) = r1(x) and q(x) = q1(x).\\n\\nExample 17.7. The division algorithm merely formalizes long division of polynomials, a\\ntask we have been familiar with since high school. For example, suppose that we divide\\nx3 − x2 + 2x− 3 by x− 2.\\n\\nx2 + x + 4\\n\\nx − 2 x3 − x2 + 2x − 3\\n\\nx3 − 2x2\\n\\nx2 + 2x − 3\\n\\nx2 − 2x\\n\\n4x − 3\\n\\n4x − 8\\n\\n5\\n\\nHence, x3 − x2 + 2x− 3 = (x− 2)(x2 + x+ 4) + 5.\\n\\n\\n\\n288 CHAPTER 17. POLYNOMIALS\\n\\nLet p(x) be a polynomial in F [x] and α ∈ F . We say that α is a zero or root of p(x) if\\np(x) is in the kernel of the evaluation homomorphism ϕα. All we are really saying here is\\nthat α is a zero of p(x) if p(α) = 0.\\n\\nCorollary 17.8. Let F be a field. An element α ∈ F is a zero of p(x) ∈ F [x] if and only\\nif x− α is a factor of p(x) in F [x].\\n\\nProof. Suppose that α ∈ F and p(α) = 0. By the division algorithm, there exist polyno-\\nmials q(x) and r(x) such that\\n\\np(x) = (x− α)q(x) + r(x)\\n\\nand the degree of r(x) must be less than the degree of x − α. Since the degree of r(x) is\\nless than 1, r(x) = a for a ∈ F ; therefore,\\n\\np(x) = (x− α)q(x) + a.\\n\\nBut\\n0 = p(α) = 0 · q(α) + a = a;\\n\\nconsequently, p(x) = (x− α)q(x), and x− α is a factor of p(x).\\nConversely, suppose that x − α is a factor of p(x); say p(x) = (x − α)q(x). Then\\n\\np(α) = 0 · q(α) = 0.\\n\\nCorollary 17.9. Let F be a field. A nonzero polynomial p(x) of degree n in F [x] can have\\nat most n distinct zeros in F .\\n\\nProof. We will use induction on the degree of p(x). If deg p(x) = 0, then p(x) is a constant\\npolynomial and has no zeros. Let deg p(x) = 1. Then p(x) = ax+ b for some a and b in F .\\nIf α1 and α2 are zeros of p(x), then aα1 + b = aα2 + b or α1 = α2.\\n\\nNow assume that deg p(x) > 1. If p(x) does not have a zero in F , then we are done.\\nOn the other hand, if α is a zero of p(x), then p(x) = (x− α)q(x) for some q(x) ∈ F [x] by\\nCorollary 17.8. The degree of q(x) is n− 1 by Proposition 17.4. Let β be some other zero\\nof p(x) that is distinct from α. Then p(β) = (β −α)q(β) = 0. Since α ̸= β and F is a field,\\nq(β) = 0. By our induction hypothesis, p(x) can have at most n − 1 zeros in F that are\\ndistinct from α. Therefore, p(x) has at most n distinct zeros in F .\\n\\nLet F be a field. A monic polynomial d(x) is a greatest common divisor of poly-\\nnomials p(x), q(x) ∈ F [x] if d(x) evenly divides both p(x) and q(x); and, if for any other\\npolynomial d′(x) dividing both p(x) and q(x), d′(x) | d(x). We write d(x) = gcd(p(x), q(x)).\\nTwo polynomials p(x) and q(x) are relatively prime if gcd(p(x), q(x)) = 1.\\n\\nProposition 17.10. Let F be a field and suppose that d(x) is a greatest common divisor\\nof two polynomials p(x) and q(x) in F [x]. Then there exist polynomials r(x) and s(x) such\\nthat\\n\\nd(x) = r(x)p(x) + s(x)q(x).\\n\\nFurthermore, the greatest common divisor of two polynomials is unique.\\n\\nProof. Let d(x) be the monic polynomial of smallest degree in the set\\n\\nS = {f(x)p(x) + g(x)q(x) : f(x), g(x) ∈ F [x]}.\\n\\nWe can write d(x) = r(x)p(x) + s(x)q(x) for two polynomials r(x) and s(x) in F [x]. We\\nneed to show that d(x) divides both p(x) and q(x). We shall first show that d(x) divides\\n\\n\\n\\n17.3. IRREDUCIBLE POLYNOMIALS 289\\n\\np(x). By the division algorithm, there exist polynomials a(x) and b(x) such that p(x) =\\na(x)d(x)+ b(x), where b(x) is either the zero polynomial or deg b(x) < deg d(x). Therefore,\\n\\nb(x) = p(x)− a(x)d(x)\\n\\n= p(x)− a(x)(r(x)p(x) + s(x)q(x))\\n\\n= p(x)− a(x)r(x)p(x)− a(x)s(x)q(x)\\n\\n= p(x)(1− a(x)r(x)) + q(x)(−a(x)s(x))\\n\\nis a linear combination of p(x) and q(x) and therefore must be in S. However, b(x) must\\nbe the zero polynomial since d(x) was chosen to be of smallest degree; consequently, d(x)\\ndivides p(x). A symmetric argument shows that d(x) must also divide q(x); hence, d(x) is\\na common divisor of p(x) and q(x).\\n\\nTo show that d(x) is a greatest common divisor of p(x) and q(x), suppose that d′(x)\\nis another common divisor of p(x) and q(x). We will show that d′(x) | d(x). Since d′(x)\\nis a common divisor of p(x) and q(x), there exist polynomials u(x) and v(x) such that\\np(x) = u(x)d′(x) and q(x) = v(x)d′(x). Therefore,\\n\\nd(x) = r(x)p(x) + s(x)q(x)\\n\\n= r(x)u(x)d′(x) + s(x)v(x)d′(x)\\n\\n= d′(x)[r(x)u(x) + s(x)v(x)].\\n\\nSince d′(x) | d(x), d(x) is a greatest common divisor of p(x) and q(x).\\nFinally, we must show that the greatest common divisor of p(x) and q(x) is unique.\\n\\nSuppose that d′(x) is another greatest common divisor of p(x) and q(x). We have just\\nshown that there exist polynomials u(x) and v(x) in F [x] such that d(x) = d′(x)[r(x)u(x)+\\ns(x)v(x)]. Since\\n\\ndeg d(x) = deg d′(x) + deg[r(x)u(x) + s(x)v(x)]\\n\\nand d(x) and d′(x) are both greatest common divisors, deg d(x) = deg d′(x). Since d(x) and\\nd′(x) are both monic polynomials of the same degree, it must be the case that d(x) = d′(x).\\n\\nNotice the similarity between the proof of Proposition 17.10 and the proof of Theo-\\nrem 2.10.\\n\\n17.3 Irreducible Polynomials\\nA nonconstant polynomial f(x) ∈ F [x] is irreducible over a field F if f(x) cannot be\\nexpressed as a product of two polynomials g(x) and h(x) in F [x], where the degrees of g(x)\\nand h(x) are both smaller than the degree of f(x). Irreducible polynomials function as the\\n“prime numbers” of polynomial rings.\\n\\nExample 17.11. The polynomial x2 − 2 ∈ Q[x] is irreducible since it cannot be factored\\nany further over the rational numbers. Similarly, x2+1 is irreducible over the real numbers.\\n\\nExample 17.12. The polynomial p(x) = x3 + x2 + 2 is irreducible over Z3[x]. Suppose\\nthat this polynomial was reducible over Z3[x]. By the division algorithm there would have\\nto be a factor of the form x− a, where a is some element in Z3[x]. Hence, it would have to\\nbe true that p(a) = 0. However,\\n\\np(0) = 2\\n\\np(1) = 1\\n\\np(2) = 2.\\n\\n\\n\\n290 CHAPTER 17. POLYNOMIALS\\n\\nTherefore, p(x) has no zeros in Z3 and must be irreducible.\\nLemma 17.13. Let p(x) ∈ Q[x]. Then\\n\\np(x) =\\nr\\n\\ns\\n(a0 + a1x+ · · ·+ anx\\n\\nn),\\n\\nwhere r, s, a0, . . . , an are integers, the ai’s are relatively prime, and r and s are relatively\\nprime.\\nProof. Suppose that\\n\\np(x) =\\nb0\\nc0\\n\\n+\\nb1\\nc1\\nx+ · · ·+ bn\\n\\ncn\\nxn,\\n\\nwhere the bi’s and the ci’s are integers. We can rewrite p(x) as\\n\\np(x) =\\n1\\n\\nc0 · · · cn\\n(d0 + d1x+ · · ·+ dnx\\n\\nn),\\n\\nwhere d0, . . . , dn are integers. Let d be the greatest common divisor of d0, . . . , dn. Then\\n\\np(x) =\\nd\\n\\nc0 · · · cn\\n(a0 + a1x+ · · ·+ anx\\n\\nn),\\n\\nwhere di = dai and the ai’s are relatively prime. Reducing d/(c0 · · · cn) to its lowest terms,\\nwe can write\\n\\np(x) =\\nr\\n\\ns\\n(a0 + a1x+ · · ·+ anx\\n\\nn),\\n\\nwhere gcd(r, s) = 1.\\n\\nTheorem 17.14 (Gauss’s Lemma). Let p(x) ∈ Z[x] be a monic polynomial such that p(x)\\nfactors into a product of two polynomials α(x) and β(x) in Q[x], where the degrees of both\\nα(x) and β(x) are less than the degree of p(x). Then p(x) = a(x)b(x), where a(x) and b(x)\\nare monic polynomials in Z[x] with degα(x) = deg a(x) and degβ(x) = deg b(x).\\nProof. By Lemma 17.13, we can assume that\\n\\nα(x) =\\nc1\\nd1\\n\\n(a0 + a1x+ · · ·+ amx\\nm) =\\n\\nc1\\nd1\\nα1(x)\\n\\nβ(x) =\\nc2\\nd2\\n\\n(b0 + b1x+ · · ·+ bnx\\nn) =\\n\\nc2\\nd2\\nβ1(x),\\n\\nwhere the ai’s are relatively prime and the bi’s are relatively prime. Consequently,\\n\\np(x) = α(x)β(x) =\\nc1c2\\nd1d2\\n\\nα1(x)β1(x) =\\nc\\n\\nd\\nα1(x)β1(x),\\n\\nwhere c/d is the product of c1/d1 and c2/d2 expressed in lowest terms. Hence, dp(x) =\\ncα1(x)β1(x).\\n\\nIf d = 1, then cambn = 1 since p(x) is a monic polynomial. Hence, either c = 1\\nor c = −1. If c = 1, then either am = bn = 1 or am = bn = −1. In the first case\\np(x) = α1(x)β1(x), where α1(x) and β1(x) are monic polynomials with degα(x) = degα1(x)\\nand degβ(x) = degβ1(x). In the second case a(x) = −α1(x) and b(x) = −β1(x) are the\\ncorrect monic polynomials since p(x) = (−α1(x))(−β1(x)) = a(x)b(x). The case in which\\nc = −1 can be handled similarly.\\n\\nNow suppose that d ̸= 1. Since gcd(c, d) = 1, there exists a prime p such that p | d and\\np ̸ |c. Also, since the coefficients of α1(x) are relatively prime, there exists a coefficient ai\\nsuch that p ̸ |ai. Similarly, there exists a coefficient bj of β1(x) such that p ̸ |bj . Let α′\\n\\n1(x)\\nand β′1(x) be the polynomials in Zp[x] obtained by reducing the coefficients of α1(x) and\\nβ1(x) modulo p. Since p | d, α′\\n\\n1(x)β\\n′\\n1(x) = 0 in Zp[x]. However, this is impossible since\\n\\nneither α′\\n1(x) nor β′1(x) is the zero polynomial and Zp[x] is an integral domain. Therefore,\\n\\nd = 1 and the theorem is proven.\\n\\n\\n\\n17.3. IRREDUCIBLE POLYNOMIALS 291\\n\\nCorollary 17.15. Let p(x) = xn + an−1x\\nn−1 + · · ·+ a0 be a polynomial with coefficients in\\n\\nZ and a0 ̸= 0. If p(x) has a zero in Q, then p(x) also has a zero α in Z. Furthermore, α\\ndivides a0.\\n\\nProof. Let p(x) have a zero a ∈ Q. Then p(x) must have a linear factor x−a. By Gauss’s\\nLemma, p(x) has a factorization with a linear factor in Z[x]. Hence, for some α ∈ Z\\n\\np(x) = (x− α)(xn−1 + · · · − a0/α).\\n\\nThus a0/α ∈ Z and so α | a0.\\n\\nExample 17.16. Let p(x) = x4 − 2x3 + x + 1. We shall show that p(x) is irreducible\\nover Q[x]. Assume that p(x) is reducible. Then either p(x) has a linear factor, say p(x) =\\n(x− α)q(x), where q(x) is a polynomial of degree three, or p(x) has two quadratic factors.\\n\\nIf p(x) has a linear factor in Q[x], then it has a zero in Z. By Corollary 17.15, any zero\\nmust divide 1 and therefore must be ±1; however, p(1) = 1 and p(−1) = 3. Consequently,\\nwe have eliminated the possibility that p(x) has any linear factors.\\n\\nTherefore, if p(x) is reducible it must factor into two quadratic polynomials, say\\n\\np(x) = (x2 + ax+ b)(x2 + cx+ d)\\n\\n= x4 + (a+ c)x3 + (ac+ b+ d)x2 + (ad+ bc)x+ bd,\\n\\nwhere each factor is in Z[x] by Gauss’s Lemma. Hence,\\n\\na+ c = −2\\n\\nac+ b+ d = 0\\n\\nad+ bc = 1\\n\\nbd = 1.\\n\\nSince bd = 1, either b = d = 1 or b = d = −1. In either case b = d and so\\n\\nad+ bc = b(a+ c) = 1.\\n\\nSince a+c = −2, we know that −2b = 1. This is impossible since b is an integer. Therefore,\\np(x) must be irreducible over Q.\\n\\nTheorem 17.17 (Eisenstein’s Criterion). Let p be a prime and suppose that\\n\\nf(x) = anx\\nn + · · ·+ a0 ∈ Z[x].\\n\\nIf p | ai for i = 0, 1, . . . , n− 1, but p ̸ |an and p2 ̸ |a0, then f(x) is irreducible over Q.\\n\\nProof. By Gauss’s Lemma, we need only show that f(x) does not factor into polynomials\\nof lower degree in Z[x]. Let\\n\\nf(x) = (brx\\nr + · · ·+ b0)(csx\\n\\ns + · · ·+ c0)\\n\\nbe a factorization in Z[x], with br and cs not equal to zero and r, s < n. Since p2 does not\\ndivide a0 = b0c0, either b0 or c0 is not divisible by p. Suppose that p ̸ |b0 and p | c0. Since\\np ̸ |an and an = brcs, neither br nor cs is divisible by p. Let m be the smallest value of k\\nsuch that p ̸ |ck. Then\\n\\nam = b0cm + b1cm−1 + · · ·+ bmc0\\n\\nis not divisible by p, since each term on the right-hand side of the equation is divisible by p\\nexcept for b0cm. Therefore, m = n since ai is divisible by p for m < n. Hence, f(x) cannot\\nbe factored into polynomials of lower degree and therefore must be irreducible.\\n\\n\\n\\n292 CHAPTER 17. POLYNOMIALS\\n\\nExample 17.18. The polynomial\\n\\nf(x) = 16x5 − 9x4 + 3x2 + 6x− 21\\n\\nis easily seen to be irreducible over Q by Eisenstein’s Criterion if we let p = 3.\\n\\nEisenstein’s Criterion is more useful in constructing irreducible polynomials of a certain\\ndegree over Q than in determining the irreducibility of an arbitrary polynomial in Q[x]:\\ngiven an arbitrary polynomial, it is not very likely that we can apply Eisenstein’s Crite-\\nrion. The real value of Theorem 17.17 is that we now have an easy method of generating\\nirreducible polynomials of any degree.\\n\\nIdeals in F [x]\\n\\nLet F be a field. Recall that a principal ideal in F [x] is an ideal ⟨p(x)⟩ generated by some\\npolynomial p(x); that is,\\n\\n⟨p(x)⟩ = {p(x)q(x) : q(x) ∈ F [x]}.\\n\\nExample 17.19. The polynomial x2 in F [x] generates the ideal ⟨x2⟩ consisting of all\\npolynomials with no constant term or term of degree 1.\\n\\nTheorem 17.20. If F is a field, then every ideal in F [x] is a principal ideal.\\n\\nProof. Let I be an ideal of F [x]. If I is the zero ideal, the theorem is easily true. Suppose\\nthat I is a nontrivial ideal in F [x], and let p(x) ∈ I be a nonzero element of minimal degree.\\nIf deg p(x) = 0, then p(x) is a nonzero constant and 1 must be in I. Since 1 generates all\\nof F [x], ⟨1⟩ = I = F [x] and I is again a principal ideal.\\n\\nNow assume that deg p(x) ≥ 1 and let f(x) be any element in I. By the division\\nalgorithm there exist q(x) and r(x) in F [x] such that f(x) = p(x)q(x)+r(x) and deg r(x) <\\ndeg p(x). Since f(x), p(x) ∈ I and I is an ideal, r(x) = f(x)−p(x)q(x) is also in I. However,\\nsince we chose p(x) to be of minimal degree, r(x) must be the zero polynomial. Since we\\ncan write any element f(x) in I as p(x)q(x) for some q(x) ∈ F [x], it must be the case that\\nI = ⟨p(x)⟩.\\n\\nExample 17.21. It is not the case that every ideal in the ring F [x, y] is a principal ideal.\\nConsider the ideal of F [x, y] generated by the polynomials x and y. This is the ideal of\\nF [x, y] consisting of all polynomials with no constant term. Since both x and y are in the\\nideal, no single polynomial can generate the entire ideal.\\n\\nTheorem 17.22. Let F be a field and suppose that p(x) ∈ F [x]. Then the ideal generated\\nby p(x) is maximal if and only if p(x) is irreducible.\\n\\nProof. Suppose that p(x) generates a maximal ideal of F [x]. Then ⟨p(x)⟩ is also a prime\\nideal of F [x]. Since a maximal ideal must be properly contained inside F [x], p(x) cannot\\nbe a constant polynomial. Let us assume that p(x) factors into two polynomials of lesser\\ndegree, say p(x) = f(x)g(x). Since ⟨p(x)⟩ is a prime ideal one of these factors, say f(x), is\\nin ⟨p(x)⟩ and therefore be a multiple of p(x). But this would imply that ⟨p(x)⟩ ⊂ ⟨f(x)⟩,\\nwhich is impossible since ⟨p(x)⟩ is maximal.\\n\\nConversely, suppose that p(x) is irreducible over F [x]. Let I be an ideal in F [x] contain-\\ning ⟨p(x)⟩. By Theorem 17.20, I is a principal ideal; hence, I = ⟨f(x)⟩ for some f(x) ∈ F [x].\\nSince p(x) ∈ I, it must be the case that p(x) = f(x)g(x) for some g(x) ∈ F [x]. However,\\np(x) is irreducible; hence, either f(x) or g(x) is a constant polynomial. If f(x) is constant,\\nthen I = F [x] and we are done. If g(x) is constant, then f(x) is a constant multiple of I\\nand I = ⟨p(x)⟩. Thus, there are no proper ideals of F [x] that properly contain ⟨p(x)⟩.\\n\\n\\n\\n17.4. EXERCISES 293\\n\\nHistorical Note\\n\\nThroughout history, the solution of polynomial equations has been a challenging prob-\\nlem. The Babylonians knew how to solve the equation ax2 + bx + c = 0. Omar Khayyam\\n(1048–1131) devised methods of solving cubic equations through the use of geometric\\nconstructions and conic sections. The algebraic solution of the general cubic equation\\nax3 + bx2 + cx + d = 0 was not discovered until the sixteenth century. An Italian mathe-\\nmatician, Luca Pacioli (ca. 1445–1509), wrote in Summa de Arithmetica that the solution\\nof the cubic was impossible. This was taken as a challenge by the rest of the mathematical\\ncommunity.\\n\\nScipione del Ferro (1465–1526), of the University of Bologna, solved the “depressed\\ncubic,”\\n\\nax3 + cx+ d = 0.\\n\\nHe kept his solution an absolute secret. This may seem surprising today, when mathe-\\nmaticians are usually very eager to publish their results, but in the days of the Italian\\nRenaissance secrecy was customary. Academic appointments were not easy to secure and\\ndepended on the ability to prevail in public contests. Such challenges could be issued at\\nany time. Consequently, any major new discovery was a valuable weapon in such a contest.\\nIf an opponent presented a list of problems to be solved, del Ferro could in turn present a\\nlist of depressed cubics. He kept the secret of his discovery throughout his life, passing it\\non only on his deathbed to his student Antonio Fior (ca. 1506–?).\\n\\nAlthough Fior was not the equal of his teacher, he immediately issued a challenge to\\nNiccolo Fontana (1499–1557). Fontana was known as Tartaglia (the Stammerer). As a youth\\nhe had suffered a blow from the sword of a French soldier during an attack on his village.\\nHe survived the savage wound, but his speech was permanently impaired. Tartaglia sent\\nFior a list of 30 various mathematical problems; Fior countered by sending Tartaglia a list\\nof 30 depressed cubics. Tartaglia would either solve all 30 of the problems or absolutely fail.\\nAfter much effort Tartaglia finally succeeded in solving the depressed cubic and defeated\\nFior, who faded into obscurity.\\n\\nAt this point another mathematician, Gerolamo Cardano (1501–1576), entered the story.\\nCardano wrote to Tartaglia, begging him for the solution to the depressed cubic. Tartaglia\\nrefused several of his requests, then finally revealed the solution to Cardano after the latter\\nswore an oath not to publish the secret or to pass it on to anyone else. Using the knowledge\\nthat he had obtained from Tartaglia, Cardano eventually solved the general cubic\\n\\nax3 + bx2 + cx+ d = 0.\\n\\nCardano shared the secret with his student, Ludovico Ferrari (1522–1565), who solved the\\ngeneral quartic equation,\\n\\nax4 + bx3 + cx2 + dx+ e = 0.\\n\\nIn 1543, Cardano and Ferrari examined del Ferro’s papers and discovered that he had also\\nsolved the depressed cubic. Cardano felt that this relieved him of his obligation to Tartaglia,\\nso he proceeded to publish the solutions in Ars Magna (1545), in which he gave credit to\\ndel Ferro for solving the special case of the cubic. This resulted in a bitter dispute between\\nCardano and Tartaglia, who published the story of the oath a year later.\\n\\n17.4 Exercises\\n1. List all of the polynomials of degree 3 or less in Z2[x].\\n\\n\\n\\n294 CHAPTER 17. POLYNOMIALS\\n\\n2. Compute each of the following.\\n(a) (5x2 + 3x− 4) + (4x2 − x+ 9) in Z12\\n\\n(b) (5x2 + 3x− 4)(4x2 − x+ 9) in Z12\\n\\n(c) (7x3 + 3x2 − x) + (6x2 − 8x+ 4) in Z9\\n\\n(d) (3x2 + 2x− 4) + (4x2 + 2) in Z5\\n\\n(e) (3x2 + 2x− 4)(4x2 + 2) in Z5\\n\\n(f) (5x2 + 3x− 2)2 in Z12\\n\\n3. Use the division algorithm to find q(x) and r(x) such that a(x) = q(x)b(x) + r(x) with\\ndeg r(x) < deg b(x) for each of the following pairs of polynomials.\\n(a) a(x) = 5x3 + 6x2 − 3x+ 4 and b(x) = x− 2 in Z7[x]\\n\\n(b) a(x) = 6x4 − 2x3 + x2 − 3x+ 1 and b(x) = x2 + x− 2 in Z7[x]\\n\\n(c) a(x) = 4x5 − x3 + x2 + 4 and b(x) = x3 − 2 in Z5[x]\\n\\n(d) a(x) = x5 + x3 − x2 − x and b(x) = x3 + x in Z2[x]\\n\\n4. Find the greatest common divisor of each of the following pairs p(x) and q(x) of polyno-\\nmials. If d(x) = gcd(p(x), q(x)), find two polynomials a(x) and b(x) such that a(x)p(x) +\\nb(x)q(x) = d(x).\\n(a) p(x) = x3 − 6x2 + 14x− 15 and q(x) = x3 − 8x2 + 21x− 18, where p(x), q(x) ∈ Q[x]\\n\\n(b) p(x) = x3 + x2 − x+ 1 and q(x) = x3 + x− 1, where p(x), q(x) ∈ Z2[x]\\n\\n(c) p(x) = x3 + x2 − 4x+ 4 and q(x) = x3 + 3x− 2, where p(x), q(x) ∈ Z5[x]\\n\\n(d) p(x) = x3 − 2x+ 4 and q(x) = 4x3 + x+ 3, where p(x), q(x) ∈ Q[x]\\n\\n5. Find all of the zeros for each of the following polynomials.\\n\\n(a) 5x3 + 4x2 − x+ 9 in Z12\\n\\n(b) 3x3 − 4x2 − x+ 4 in Z5\\n\\n(c) 5x4 + 2x2 − 3 in Z7\\n\\n(d) x3 + x+ 1 in Z2\\n\\n6. Find all of the units in Z[x].\\n\\n7. Find a unit p(x) in Z4[x] such that deg p(x) > 1.\\n\\n8. Which of the following polynomials are irreducible over Q[x]?\\n\\n(a) x4 − 2x3 + 2x2 + x+ 4\\n\\n(b) x4 − 5x3 + 3x− 2\\n\\n(c) 3x5 − 4x3 − 6x2 + 6\\n\\n(d) 5x5 − 6x4 − 3x2 + 9x− 15\\n\\n9. Find all of the irreducible polynomials of degrees 2 and 3 in Z2[x].\\n\\n10. Give two different factorizations of x2 + x+ 8 in Z10[x].\\n\\n11. Prove or disprove: There exists a polynomial p(x) in Z6[x] of degree n with more than\\nn distinct zeros.\\n\\n12. If F is a field, show that F [x1, . . . , xn] is an integral domain.\\n\\n13. Show that the division algorithm does not hold for Z[x]. Why does it fail?\\n\\n\\n\\n17.4. EXERCISES 295\\n\\n14. Prove or disprove: xp + a is irreducible for any a ∈ Zp, where p is prime.\\n\\n15. Let f(x) be irreducible in F [x], where F is a field. If f(x) | p(x)q(x), prove that either\\nf(x) | p(x) or f(x) | q(x).\\n\\n16. Suppose that R and S are isomorphic rings. Prove that R[x] ∼= S[x].\\n\\n17. Let F be a field and a ∈ F . If p(x) ∈ F [x], show that p(a) is the remainder obtained\\nwhen p(x) is divided by x− a.\\n\\n18. (The Rational Root Theorem) Let\\n\\np(x) = anx\\nn + an−1x\\n\\nn−1 + · · ·+ a0 ∈ Z[x],\\n\\nwhere an ̸= 0. Prove that if p(r/s) = 0, where gcd(r, s) = 1, then r | a0 and s | an.\\n\\n19. Let Q∗ be the multiplicative group of positive rational numbers. Prove that Q∗ is\\nisomorphic to (Z[x],+).\\n\\n20. (Cyclotomic Polynomials) The polynomial\\n\\nΦn(x) =\\nxn − 1\\n\\nx− 1\\n= xn−1 + xn−2 + · · ·+ x+ 1\\n\\nis called the cyclotomic polynomial. Show that Φp(x) is irreducible over Q for any prime\\np.\\n\\n21. If F is a field, show that there are infinitely many irreducible polynomials in F [x].\\n\\n22. Let R be a commutative ring with identity. Prove that multiplication is commutative\\nin R[x].\\n\\n23. Let R be a commutative ring with identity. Prove that multiplication is distributive in\\nR[x].\\n\\n24. Show that xp − x has p distinct zeros in Zp, for any prime p. Conclude that\\n\\nxp − x = x(x− 1)(x− 2) · · · (x− (p− 1)).\\n\\n25. Let F be a field and f(x) = a0 + a1x + · · · + anx\\nn be in F [x]. Define f ′(x) = a1 +\\n\\n2a2x+ · · ·+ nanx\\nn−1 to be the derivative of f(x).\\n\\n(a) Prove that\\n(f + g)′(x) = f ′(x) + g′(x).\\n\\nConclude that we can define a homomorphism of abelian groups D : F [x] → F [x] by\\nD(f(x)) = f ′(x).\\n\\n(b) Calculate the kernel of D if charF = 0.\\n(c) Calculate the kernel of D if charF = p.\\n(d) Prove that\\n\\n(fg)′(x) = f ′(x)g(x) + f(x)g′(x).\\n\\n(e) Suppose that we can factor a polynomial f(x) ∈ F [x] into linear factors, say\\n\\nf(x) = a(x− a1)(x− a2) · · · (x− an).\\n\\nProve that f(x) has no repeated factors if and only if f(x) and f ′(x) are relatively\\nprime.\\n\\n\\n\\n296 CHAPTER 17. POLYNOMIALS\\n\\n26. Let F be a field. Show that F [x] is never a field.\\n\\n27. Let R be an integral domain. Prove that R[x1, . . . , xn] is an integral domain.\\n\\n28. Let R be a commutative ring with identity. Show that R[x] has a subring R′ isomorphic\\nto R.\\n\\n29. Let p(x) and q(x) be polynomials in R[x], where R is a commutative ring with identity.\\nProve that deg(p(x) + q(x)) ≤ max(deg p(x),deg q(x)).\\n\\n17.5 Additional Exercises: Solving the Cubic and Quartic\\nEquations\\n\\n1. Solve the general quadratic equation\\n\\nax2 + bx+ c = 0\\n\\nto obtain\\nx =\\n\\n−b±\\n√\\nb2 − 4ac\\n\\n2a\\n.\\n\\nThe discriminant of the quadratic equation ∆ = b2 − 4ac determines the nature of the\\nsolutions of the equation. If ∆ > 0, the equation has two distinct real solutions. If ∆ = 0,\\nthe equation has a single repeated real root. If ∆ < 0, there are two distinct imaginary\\nsolutions.\\n\\n2. Show that any cubic equation of the form\\n\\nx3 + bx2 + cx+ d = 0\\n\\ncan be reduced to the form y3 + py + q = 0 by making the substitution x = y − b/3.\\n\\n3. Prove that the cube roots of 1 are given by\\n\\nω =\\n−1 + i\\n\\n√\\n3\\n\\n2\\n\\nω2 =\\n−1− i\\n\\n√\\n3\\n\\n2\\nω3 = 1.\\n\\n4. Make the substitution\\ny = z − p\\n\\n3z\\n\\nfor y in the equation y3 + py + q = 0 and obtain two solutions A and B for z3.\\n\\n5. Show that the product of the solutions obtained in (4) is −p3/27, deducing that 3\\n√\\nAB =\\n\\n−p/3.\\n\\n6. Prove that the possible solutions for z in (4) are given by\\n3\\n√\\nA, ω\\n\\n3\\n√\\nA, ω2 3\\n\\n√\\nA,\\n\\n3\\n√\\nB, ω\\n\\n3\\n√\\nB, ω2 3\\n\\n√\\nB\\n\\nand use this result to show that the three possible solutions for y are\\n\\nωi 3\\n\\n√\\n−q\\n2\\n+\\n\\n√\\np3\\n\\n27\\n+\\nq2\\n\\n4\\n+ ω2i 3\\n\\n√\\n−q\\n2\\n−\\n√\\n\\np3\\n\\n27\\n+\\nq2\\n\\n4\\n,\\n\\nwhere i = 0, 1, 2.\\n\\n\\n\\n17.5. ADDITIONAL EXERCISES: SOLVING THE CUBIC AND QUARTIC EQUATIONS297\\n\\n7. The discriminant of the cubic equation is\\n\\n∆ =\\np3\\n\\n27\\n+\\nq2\\n\\n4\\n.\\n\\nShow that y3 + py + q = 0\\n\\n(a) has three real roots, at least two of which are equal, if ∆ = 0.\\n(b) has one real root and two conjugate imaginary roots if ∆ > 0.\\n(c) has three distinct real roots if ∆ < 0.\\n\\n8. Solve the following cubic equations.\\n(a) x3 − 4x2 + 11x+ 30 = 0\\n\\n(b) x3 − 3x+ 5 = 0\\n\\n(c) x3 − 3x+ 2 = 0\\n\\n(d) x3 + x+ 3 = 0\\n\\n9. Show that the general quartic equation\\n\\nx4 + ax3 + bx2 + cx+ d = 0\\n\\ncan be reduced to\\ny4 + py2 + qy + r = 0\\n\\nby using the substitution x = y − a/4.\\n\\n10. Show that (\\ny2 +\\n\\n1\\n\\n2\\nz\\n\\n)2\\n\\n= (z − p)y2 − qy +\\n\\n(\\n1\\n\\n4\\nz2 − r\\n\\n)\\n.\\n\\n11. Show that the right-hand side of Exercise 17.5.10 can be put in the form (my + k)2 if\\nand only if\\n\\nq2 − 4(z − p)\\n\\n(\\n1\\n\\n4\\nz2 − r\\n\\n)\\n= 0.\\n\\n12. From Exercise 17.5.11 obtain the resolvent cubic equation\\n\\nz3 − pz2 − 4rz + (4pr − q2) = 0.\\n\\nSolving the resolvent cubic equation, put the equation found in Exercise 17.5.10 in the form(\\ny2 +\\n\\n1\\n\\n2\\nz\\n\\n)2\\n\\n= (my + k)2\\n\\nto obtain the solution of the quartic equation.\\n\\n13. Use this method to solve the following quartic equations.\\n(a) x4 − x2 − 3x+ 2 = 0\\n\\n(b) x4 + x3 − 7x2 − x+ 6 = 0\\n\\n(c) x4 − 2x2 + 4x− 3 = 0\\n\\n(d) x4 − 4x3 + 3x2 − 5x+ 2 = 0\\n\\n\\n\\n298 CHAPTER 17. POLYNOMIALS\\n\\n17.6 Sage\\nSage is particularly adept at building, analyzing and manipulating polynomial rings. We\\nhave seen some of this in the previous chapter. Let’s begin by creating three polynomial\\nrings and checking some of their basic properties. There are several ways to construct\\npolynomial rings, but the syntax used here is the most straightforward.\\n\\nPolynomial Rings and their Elements\\n\\nR.<x> = Integers (8)[]; R\\n\\nUnivariate Polynomial Ring in x over Ring of integers modulo 8\\n\\nS.<y> = ZZ[]; S\\n\\nUnivariate Polynomial Ring in y over Integer Ring\\n\\nT.<z> = QQ[]; T\\n\\nUnivariate Polynomial Ring in z over Rational Field\\n\\nBasic properties of rings are availble for these examples.\\nR.is_finite ()\\n\\nFalse\\n\\nR.is_integral_domain ()\\n\\nFalse\\n\\nS.is_integral_domain ()\\n\\nTrue\\n\\nT.is_field ()\\n\\nFalse\\n\\nR.characteristic ()\\n\\n8\\n\\nT.characteristic ()\\n\\n0\\n\\nWith the construction syntax used above, the variables can be used to create elements of\\nthe polynomial ring without explicit coercion (though we need to be careful about constant\\npolynomials).\\n\\ny in S\\n\\nTrue\\n\\n\\n\\n17.6. SAGE 299\\n\\nx in S\\n\\nFalse\\n\\nq = (3/2) + (5/4)*z^2\\nq in T\\n\\nTrue\\n\\n3 in S\\n\\nTrue\\n\\nr = 3\\nr.parent ()\\n\\nInteger Ring\\n\\ns = 3*y^0\\ns.parent ()\\n\\nUnivariate Polynomial Ring in y over Integer Ring\\n\\nPolynomials can be evaluated like they are functions, so we can mimic the evaluation\\nhomomorphism.\\n\\np = 3 + 5*x + 2*x^2\\np.parent ()\\n\\nUnivariate Polynomial Ring in x over Ring of integers modulo 8\\n\\np(1)\\n\\n2\\n\\n[p(t) for t in Integers (8)]\\n\\n[3, 2, 5, 4, 7, 6, 1, 0]\\n\\nNotice that p is a degree two polynomial, yet through a brute-force examination we see\\nthat the polynomial only has one root, contrary to our usual expectations. It can be even\\nmore unusual.\\n\\nq = 4*x^2+4*x\\n[q(t) for t in Integers (8)]\\n\\n[0, 0, 0, 0, 0, 0, 0, 0]\\n\\nSage can create and manipulate rings of polynomials in more than one variable, though\\nwe will not have much occasion to use this functionality in this course.\\n\\nM.<s, t> = QQ[]; M\\n\\nMultivariate Polynomial Ring in s, t over Rational Field\\n\\n\\n\\n300 CHAPTER 17. POLYNOMIALS\\n\\nIrreducible Polynomials\\nWhether or not a polynomial factors, taking into consideration the ring used for its coeffi-\\ncients, is an important topic in this chapter and many of the following chapters. Sage can\\nfactor, and determine irreducibility, over the integers, the rationals, and finite fields.\\n\\nFirst, over the rationals.\\nR.<x> = QQ[]\\np = 1/4*x^4 - x^3 + x^2 - x - 1/2\\np.is_irreducible ()\\n\\nTrue\\n\\np.factor ()\\n\\n(1/4) * (x^4 - 4*x^3 + 4*x^2 - 4*x - 2)\\n\\nq = 2*x^5 + 5/2*x^4 + 3/4*x^3 - 25/24*x^2 - x - 1/2\\nq.is_irreducible ()\\n\\nFalse\\n\\nq.factor ()\\n\\n(2) * (x^2 + 3/2*x + 3/4) * (x^3 - 1/4*x^2 - 1/3)\\n\\nFactoring over the integers is really no different than factoring over the rationals. This is\\nthe content of Theorem 17.14 — finding a factorization over the integers can be converted\\nto finding a factorization over the rationals. So it is with Sage, there is little difference\\nbetween working over the rationals and the integers. It is a little different working over a\\nfinite field. Commentary follows.\\n\\nF.<a> = FiniteField (5^2)\\nS.<y> = F[]\\np = 2*y^5 + 2*y^4 + 4*y^3 + 2*y^2 + 3*y + 1\\np.is_irreducible ()\\n\\nTrue\\n\\np.factor ()\\n\\n(2) * (y^5 + y^4 + 2*y^3 + y^2 + 4*y + 3)\\n\\nq = 3*y^4+2*y^3-y+4; q.factor ()\\n\\n(3) * (y^2 + (a + 4)*y + 2*a + 3) * (y^2 + 4*a*y + 3*a)\\n\\nr = y^4+2*y^3+3*y^2+4; r.factor ()\\n\\n(y + 4) * (y^3 + 3*y^2 + y + 1)\\n\\ns = 3*y^4+2*y^3-y+3; s.factor ()\\n\\n(3) * (y + 1) * (y + 3) * (y + 2*a + 4) * (y + 3*a + 1)\\n\\n\\n\\n17.6. SAGE 301\\n\\nTo check these factorizations, we need to compute in the finite field, F, and so we need\\nto know how the symbol a behaves. This symbol is considered as a root of a degree two\\npolynomial over the integers mod 5, which we can get with the .modulus() method.\\n\\nF.modulus ()\\n\\nx^2 + 4*x + 2\\n\\nSo a2 + 4a + 2 = 0, or a2 = −4a − 3 = a + 2. So when checking the factorizations,\\nanytime you see an a2 you can replace it by a+ 2. Notice that by Corollary 17.8 we could\\nfind the one linear factor of r, and the four linear factors of s, through a brute-force search\\nfor roots. This is feasible because the field is finite.\\n\\n[t for t in F if r(t)==0]\\n\\n[1]\\n\\n[t for t in F if s(t)==0]\\n\\n[2, 3*a + 1, 4, 2*a + 4]\\n\\nHowever, q factors into a pair of degree 2 polynomials, so no amount of testing for roots\\nwill discover a factor.\\n\\nWith Eisenstein’s Criterion, we can create irreducible polynomials, such as in Exam-\\nple 17.18.\\n\\nW.<w> = QQ[]\\np = 16*w^5 - 9*w^4 +3*w^2 + 6*w -21\\np.is_irreducible ()\\n\\nTrue\\n\\nOver the field Zp, the field of integers mod a prime p, Conway polynomials are canonical\\nchoices of a polynomial of degree n that is irreducible over Zp. See the exercises for more\\nabout these polynomials.\\n\\nPolynomials over Fields\\nIf F is a field, then every ideal of F [x] is principal (Theorem 17.20). Nothing stops you\\nfrom giving Sage two (or more) generators to construct an ideal, but Sage will determine\\nthe element to use in a description of the ideal as a principal ideal.\\n\\nW.<w> = QQ[]\\nr = -w^5 + 5*w^4 - 4*w^3 + 14*w^2 - 67*w + 17\\ns = 3*w^5 - 14*w^4 + 12*w^3 - 6*w^2 + w\\nS = W.ideal(r, s)\\nS\\n\\nPrincipal ideal (w^2 - 4*w + 1) of\\nUnivariate Polynomial Ring in w over Rational Field\\n\\n(w^2)*r + (3*w-6)*s in S\\n\\nTrue\\n\\nTheorem 17.22 is the key fact that allows us to easily construct finite fields. Here is a\\nconstruction of a finite field of order 75 = 16 807. All we need is a polynomial of degree 5\\nthat is irreducible over Z7.\\n\\n\\n\\n302 CHAPTER 17. POLYNOMIALS\\n\\nF = Integers (7)\\nR.<x> = F[]\\np = x^5+ x + 4\\np.is_irreducible ()\\n\\nTrue\\n\\nid = R.ideal(p)\\nQ = R.quotient(id); Q\\n\\nUnivariate Quotient Polynomial Ring in xbar over\\nRing of integers modulo 7 with modulus x^5 + x + 4\\n\\nQ.is_field ()\\n\\nTrue\\n\\nQ.order() == 7^5\\n\\nTrue\\n\\nThe symbol xbar is a generator of the field, but right now it is not accessible. xbar is\\nthe coset x+ ⟨x5 + x+ 4⟩. A better construction would include specifying this generator.\\n\\nQ.gen (0)\\n\\nxbar\\n\\nQ.<t> = R.quotient(id); Q\\n\\nUnivariate Quotient Polynomial Ring in t over\\nRing of integers modulo 7 with modulus x^5 + x + 4\\n\\nt^5 + t + 4\\n\\n0\\n\\nt^5 == -(t+4)\\n\\nTrue\\n\\nt^5\\n\\n6*t + 3\\n\\n(3*t^3 + t + 5)*(t^2 + 4*t + 2)\\n\\n5*t^4 + 2*t^2 + 5*t + 5\\n\\na = 3*t^4 - 6*t^3 + 3*t^2 + 5*t + 2\\nainv = a^-1; ainv\\n\\n6*t^4 + 5*t^2 + 4\\n\\na*ainv\\n\\n1\\n\\n\\n\\n17.7. SAGE EXERCISES 303\\n\\n17.7 Sage Exercises\\n1. Consider the polynomial x3 − 3x+ 4. Compute the most thorough factorization of this\\npolynomial over each of the following fields: (a) the finite field Z5, (b) a finite field with\\n125 elements, (c) the rationals, (d) the real numbers and (e) the complex numbers. To do\\nthis, build the appropriate polynomial ring, and construct the polynomial as a member of\\nthis ring, and use the .factor() method.\\n\\n2. “Conway polynomials” are irreducible polynomials over Zp that Sage (and other soft-\\nware) uses to build maximal ideals in polynomial rings, and thus quotient rings that are\\nfields. Roughly speaking, they are “canonical” choices for each degree and each prime.\\nThe command conway_polynomial(p, n) will return a database entry that is an irreducible\\npolynomial of degree n over Zp.\\nExecute the command conway_polynomial(5, 4) to obtain an allegedly irreducible polynomial\\nof degree 4 over Z5: p = x4+4x2+4x+2. Construct the right polynomial ring (i.e., in the\\nindeterminate x) and verify that p is really an element of your polynomial ring.\\nFirst determine that p has no linear factors. The only possibility left is that p factors as\\ntwo quadratic polynomials over Z5. Use a list comprehension with three for statements\\nto create every possible quadratic polynomial over Z5. Now use this list to create every\\npossible product of two quadratic polynomials and check to see if p is in this list.\\nMore on Conway polynomials is available at Frank Lübeck’s site.\\n\\n3. Construct a finite field of order 729 as a quotient of a polynomial ring by a principal\\nideal generated with a Conway polynomial.\\n\\n4. Define the polynomials p = x3 + 2x2 + 2x + 4 and q = x4 + 2x2 as polynomials with\\ncoefficients from the integers. Compute gcd(p, q) and verify that the result divides both p\\n\\nand q (just form a fraction in Sage and see that it simplifies cleanly, or use the .quo_rem()\\n\\nmethod).\\nProposition 17.10 says there are polynomials r(x) and s(x) such that the greatest common\\ndivisor equals r(x)p(x) + s(x)q(x), if the coefficients come from a field. Since here we have\\ntwo polynomials over the integers, investigate the results returned by Sage for the extended\\ngcd, xgcd(p, q). In particular, show that the first result of the returned triple is a multiple\\nof the gcd. Then verify the “linear combination” property of the result.\\n\\n5. For a polynomial ring over a field, every ideal is principal. Begin with the ring of\\npolynomials over the rationals. Experiment with constructing ideals using two generators\\nand then see that Sage converts the ideal to a principal ideal with a single generator. (You\\ncan get this generator with the ideal method .gen().) Can you explain how this single\\ngenerator is computed?\\n\\nhttp://www.math.rwth-aachen.de/~Frank.Luebeck/data/ConwayPol/index.html\\n\\n\\n18\\n\\nIntegral Domains\\n\\nOne of the most important rings we study is the ring of integers. It was our first example of\\nan algebraic structure: the first polynomial ring that we examined was Z[x]. We also know\\nthat the integers sit naturally inside the field of rational numbers, Q. The ring of integers\\nis the model for all integral domains. In this chapter we will examine integral domains in\\ngeneral, answering questions about the ideal structure of integral domains, polynomial rings\\nover integral domains, and whether or not an integral domain can be embedded in a field.\\n\\n18.1 Fields of Fractions\\nEvery field is also an integral domain; however, there are many integral domains that are\\nnot fields. For example, the integers Z form an integral domain but not a field. A question\\nthat naturally arises is how we might associate an integral domain with a field. There is a\\nnatural way to construct the rationals Q from the integers: the rationals can be represented\\nas formal quotients of two integers. The rational numbers are certainly a field. In fact, it\\ncan be shown that the rationals are the smallest field that contains the integers. Given an\\nintegral domain D, our question now becomes how to construct a smallest field F containing\\nD. We will do this in the same way as we constructed the rationals from the integers.\\n\\nAn element p/q ∈ Q is the quotient of two integers p and q; however, different pairs of\\nintegers can represent the same rational number. For instance, 1/2 = 2/4 = 3/6. We know\\nthat\\n\\na\\n\\nb\\n=\\nc\\n\\nd\\n\\nif and only if ad = bc. A more formal way of considering this problem is to examine fractions\\nin terms of equivalence relations. We can think of elements in Q as ordered pairs in Z×Z.\\nA quotient p/q can be written as (p, q). For instance, (3, 7) would represent the fraction\\n3/7. However, there are problems if we consider all possible pairs in Z × Z. There is no\\nfraction 5/0 corresponding to the pair (5, 0). Also, the pairs (3, 6) and (2, 4) both represent\\nthe fraction 1/2. The first problem is easily solved if we require the second coordinate to\\nbe nonzero. The second problem is solved by considering two pairs (a, b) and (c, d) to be\\nequivalent if ad = bc.\\n\\nIf we use the approach of ordered pairs instead of fractions, then we can study integral\\ndomains in general. Let D be any integral domain and let\\n\\nS = {(a, b) : a, b ∈ D and b ̸= 0}.\\n\\nDefine a relation on S by (a, b) ∼ (c, d) if ad = bc.\\n\\nLemma 18.1. The relation ∼ between elements of S is an equivalence relation.\\n\\n304\\n\\n\\n\\n18.1. FIELDS OF FRACTIONS 305\\n\\nProof. Since D is commutative, ab = ba; hence, ∼ is reflexive on D. Now suppose that\\n(a, b) ∼ (c, d). Then ad = bc or cb = da. Therefore, (c, d) ∼ (a, b) and the relation is\\nsymmetric. Finally, to show that the relation is transitive, let (a, b) ∼ (c, d) and (c, d) ∼\\n(e, f). In this case ad = bc and cf = de. Multiplying both sides of ad = bc by f yields\\n\\nafd = adf = bcf = bde = bed.\\n\\nSince D is an integral domain, we can deduce that af = be or (a, b) ∼ (e, f).\\n\\nWe will denote the set of equivalence classes on S by FD. We now need to define\\nthe operations of addition and multiplication on FD. Recall how fractions are added and\\nmultiplied in Q:\\n\\na\\n\\nb\\n+\\nc\\n\\nd\\n=\\nad+ bc\\n\\nbd\\n;\\n\\na\\n\\nb\\n· c\\nd\\n=\\nac\\n\\nbd\\n.\\n\\nIt seems reasonable to define the operations of addition and multiplication on FD in a\\nsimilar manner. If we denote the equivalence class of (a, b) ∈ S by [a, b], then we are led to\\ndefine the operations of addition and multiplication on FD by\\n\\n[a, b] + [c, d] = [ad+ bc, bd]\\n\\nand\\n[a, b] · [c, d] = [ac, bd],\\n\\nrespectively. The next lemma demonstrates that these operations are independent of the\\nchoice of representatives from each equivalence class.\\n\\nLemma 18.2. The operations of addition and multiplication on FD are well-defined.\\n\\nProof. We will prove that the operation of addition is well-defined. The proof that mul-\\ntiplication is well-defined is left as an exercise. Let [a1, b1] = [a2, b2] and [c1, d1] = [c2, d2].\\nWe must show that\\n\\n[a1d1 + b1c1, b1d1] = [a2d2 + b2c2, b2d2]\\n\\nor, equivalently, that\\n\\n(a1d1 + b1c1)(b2d2) = (b1d1)(a2d2 + b2c2).\\n\\nSince [a1, b1] = [a2, b2] and [c1, d1] = [c2, d2], we know that a1b2 = b1a2 and c1d2 = d1c2.\\nTherefore,\\n\\n(a1d1 + b1c1)(b2d2) = a1d1b2d2 + b1c1b2d2\\n\\n= a1b2d1d2 + b1b2c1d2\\n\\n= b1a2d1d2 + b1b2d1c2\\n\\n= (b1d1)(a2d2 + b2c2).\\n\\nLemma 18.3. The set of equivalence classes of S, FD, under the equivalence relation ∼,\\ntogether with the operations of addition and multiplication defined by\\n\\n[a, b] + [c, d] = [ad+ bc, bd]\\n\\n[a, b] · [c, d] = [ac, bd],\\n\\nis a field.\\n\\n\\n\\n306 CHAPTER 18. INTEGRAL DOMAINS\\n\\nProof. The additive and multiplicative identities are [0, 1] and [1, 1], respectively. To show\\nthat [0, 1] is the additive identity, observe that\\n\\n[a, b] + [0, 1] = [a1 + b0, b1] = [a, b].\\n\\nIt is easy to show that [1, 1] is the multiplicative identity. Let [a, b] ∈ FD such that a ̸= 0.\\nThen [b, a] is also in FD and [a, b] · [b, a] = [1, 1]; hence, [b, a] is the multiplicative inverse for\\n[a, b]. Similarly, [−a, b] is the additive inverse of [a, b]. We leave as exercises the verification\\nof the associative and commutative properties of multiplication in FD. We also leave it to\\nthe reader to show that FD is an abelian group under addition.\\n\\nIt remains to show that the distributive property holds in FD; however,\\n\\n[a, b][e, f ] + [c, d][e, f ] = [ae, bf ] + [ce, df ]\\n\\n= [aedf + bfce, bdf2]\\n\\n= [aed+ bce, bdf ]\\n\\n= [ade+ bce, bdf ]\\n\\n= ([a, b] + [c, d])[e, f ]\\n\\nand the lemma is proved.\\n\\nThe field FD in Lemma 18.3 is called the field of fractions or field of quotients of\\nthe integral domain D.\\n\\nTheorem 18.4. Let D be an integral domain. Then D can be embedded in a field of\\nfractions FD, where any element in FD can be expressed as the quotient of two elements\\nin D. Furthermore, the field of fractions FD is unique in the sense that if E is any field\\ncontaining D, then there exists a map ψ : FD → E giving an isomorphism with a subfield\\nof E such that ψ(a) = a for all elements a ∈ D.\\n\\nProof. We will first demonstrate that D can be embedded in the field FD. Define a map\\nϕ : D → FD by ϕ(a) = [a, 1]. Then for a and b in D,\\n\\nϕ(a+ b) = [a+ b, 1] = [a, 1] + [b, 1] = ϕ(a) + ϕ(b)\\n\\nand\\nϕ(ab) = [ab, 1] = [a, 1][b, 1] = ϕ(a)ϕ(b);\\n\\nhence, ϕ is a homomorphism. To show that ϕ is one-to-one, suppose that ϕ(a) = ϕ(b).\\nThen [a, 1] = [b, 1], or a = a1 = 1b = b. Finally, any element of FD can be expressed as the\\nquotient of two elements in D, since\\n\\nϕ(a)[ϕ(b)]−1 = [a, 1][b, 1]−1 = [a, 1] · [1, b] = [a, b].\\n\\nNow let E be a field containing D and define a map ψ : FD → E by ψ([a, b]) = ab−1.\\nTo show that ψ is well-defined, let [a1, b1] = [a2, b2]. Then a1b2 = b1a2. Therefore, a1b−1\\n\\n1 =\\na2b\\n\\n−1\\n2 and ψ([a1, b1]) = ψ([a2, b2]).\\nIf [a, b] and [c, d] are in FD, then\\n\\nψ([a, b] + [c, d]) = ψ([ad+ bc, bd])\\n\\n= (ad+ bc)(bd)−1\\n\\n= ab−1 + cd−1\\n\\n= ψ([a, b]) + ψ([c, d])\\n\\n\\n\\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 307\\n\\nand\\n\\nψ([a, b] · [c, d]) = ψ([ac, bd])\\n\\n= (ac)(bd)−1\\n\\n= ab−1cd−1\\n\\n= ψ([a, b])ψ([c, d]).\\n\\nTherefore, ψ is a homomorphism.\\nTo complete the proof of the theorem, we need to show that ψ is one-to-one. Suppose\\n\\nthat ψ([a, b]) = ab−1 = 0. Then a = 0b = 0 and [a, b] = [0, b]. Therefore, the kernel of ψ is\\nthe zero element [0, b] in FD, and ψ is injective.\\n\\nExample 18.5. Since Q is a field, Q[x] is an integral domain. The field of fractions of Q[x]\\nis the set of all rational expressions p(x)/q(x), where p(x) and q(x) are polynomials over\\nthe rationals and q(x) is not the zero polynomial. We will denote this field by Q(x).\\n\\nWe will leave the proofs of the following corollaries of Theorem 18.4 as exercises.\\n\\nCorollary 18.6. Let F be a field of characteristic zero. Then F contains a subfield iso-\\nmorphic to Q.\\n\\nCorollary 18.7. Let F be a field of characteristic p. Then F contains a subfield isomorphic\\nto Zp.\\n\\n18.2 Factorization in Integral Domains\\nThe building blocks of the integers are the prime numbers. If F is a field, then irreducible\\npolynomials in F [x] play a role that is very similar to that of the prime numbers in the\\nring of integers. Given an arbitrary integral domain, we are led to the following series of\\ndefinitions.\\n\\nLet R be a commutative ring with identity, and let a and b be elements in R. We say\\nthat a divides b, and write a | b, if there exists an element c ∈ R such that b = ac. A unit\\nin R is an element that has a multiplicative inverse. Two elements a and b in R are said to\\nbe associates if there exists a unit u in R such that a = ub.\\n\\nLet D be an integral domain. A nonzero element p ∈ D that is not a unit is said to\\nbe irreducible provided that whenever p = ab, either a or b is a unit. Furthermore, p is\\nprime if whenever p | ab either p | a or p | b.\\n\\nExample 18.8. It is important to notice that prime and irreducible elements do not always\\ncoincide. Let R be the subring (with identity) of Q[x, y] generated by x2, y2, and xy. Each\\nof these elements is irreducible in R; however, xy is not prime, since xy divides x2y2 but\\ndoes not divide either x2 or y2.\\n\\nThe Fundamental Theorem of Arithmetic states that every positive integer n > 1 can\\nbe factored into a product of prime numbers p1 · · · pk, where the pi’s are not necessarily\\ndistinct. We also know that such factorizations are unique up to the order of the pi’s. We\\ncan easily extend this result to the integers. The question arises of whether or not such\\nfactorizations are possible in other rings. Generalizing this definition, we say an integral\\ndomain D is a unique factorization domain, or ufd, if D satisfies the following criteria.\\n\\n1. Let a ∈ D such that a ̸= 0 and a is not a unit. Then a can be written as the product\\nof irreducible elements in D.\\n\\n\\n\\n308 CHAPTER 18. INTEGRAL DOMAINS\\n\\n2. Let a = p1 · · · pr = q1 · · · qs, where the pi’s and the qi’s are irreducible. Then r = s\\nand there is a π ∈ Sr such that pi and qπ(j) are associates for j = 1, . . . , r.\\n\\nExample 18.9. The integers are a unique factorization domain by the Fundamental The-\\norem of Arithmetic.\\nExample 18.10. Not every integral domain is a unique factorization domain. The subring\\nZ[\\n√\\n3 i] = {a + b\\n\\n√\\n3 i} of the complex numbers is an integral domain (Exercise 16.6.12,\\n\\nChapter 16). Let z = a+ b\\n√\\n3 i and define ν : Z[\\n\\n√\\n3 i] → N ∪ {0} by ν(z) = |z|2 = a2 + 3b2.\\n\\nIt is clear that ν(z) ≥ 0 with equality when z = 0. Also, from our knowledge of complex\\nnumbers we know that ν(zw) = ν(z)ν(w). It is easy to show that if ν(z) = 1, then z is a\\nunit, and that the only units of Z[\\n\\n√\\n3 i] are 1 and −1.\\n\\nWe claim that 4 has two distinct factorizations into irreducible elements:\\n\\n4 = 2 · 2 = (1−\\n√\\n3 i)(1 +\\n\\n√\\n3 i).\\n\\nWe must show that each of these factors is an irreducible element in Z[\\n√\\n3 i]. If 2 is not\\n\\nirreducible, then 2 = zw for elements z, w in Z[\\n√\\n3 i] where ν(z) = ν(w) = 2. However,\\n\\nthere does not exist an element in z in Z[\\n√\\n3 i] such that ν(z) = 2 because the equation\\n\\na2+3b2 = 2 has no integer solutions. Therefore, 2 must be irreducible. A similar argument\\nshows that both 1−\\n\\n√\\n3 i and 1+\\n\\n√\\n3 i are irreducible. Since 2 is not a unit multiple of either\\n\\n1−\\n√\\n3 i or 1 +\\n\\n√\\n3 i, 4 has at least two distinct factorizations into irreducible elements.\\n\\nPrincipal Ideal Domains\\nLet R be a commutative ring with identity. Recall that a principal ideal generated by\\na ∈ R is an ideal of the form ⟨a⟩ = {ra : r ∈ R}. An integral domain in which every ideal\\nis principal is called a principal ideal domain, or pid.\\nLemma 18.11. Let D be an integral domain and let a, b ∈ D. Then\\n\\n1. a | b if and only if ⟨b⟩ ⊂ ⟨a⟩.\\n\\n2. a and b are associates if and only if ⟨b⟩ = ⟨a⟩.\\n\\n3. a is a unit in D if and only if ⟨a⟩ = D.\\nProof. (1) Suppose that a | b. Then b = ax for some x ∈ D. Hence, for every r in D,\\nbr = (ax)r = a(xr) and ⟨b⟩ ⊂ ⟨a⟩. Conversely, suppose that ⟨b⟩ ⊂ ⟨a⟩. Then b ∈ ⟨a⟩.\\nConsequently, b = ax for some x ∈ D. Thus, a | b.\\n\\n(2) Since a and b are associates, there exists a unit u such that a = ub. Therefore,\\nb | a and ⟨a⟩ ⊂ ⟨b⟩. Similarly, ⟨b⟩ ⊂ ⟨a⟩. It follows that ⟨a⟩ = ⟨b⟩. Conversely, suppose\\nthat ⟨a⟩ = ⟨b⟩. By part (1), a | b and b | a. Then a = bx and b = ay for some x, y ∈ D.\\nTherefore, a = bx = ayx. Since D is an integral domain, xy = 1; that is, x and y are units\\nand a and b are associates.\\n\\n(3) An element a ∈ D is a unit if and only if a is an associate of 1. However, a is an\\nassociate of 1 if and only if ⟨a⟩ = ⟨1⟩ = D.\\n\\nTheorem 18.12. Let D be a pid and ⟨p⟩ be a nonzero ideal in D. Then ⟨p⟩ is a maximal\\nideal if and only if p is irreducible.\\nProof. Suppose that ⟨p⟩ is a maximal ideal. If some element a in D divides p, then\\n⟨p⟩ ⊂ ⟨a⟩. Since ⟨p⟩ is maximal, either D = ⟨a⟩ or ⟨p⟩ = ⟨a⟩. Consequently, either a and p\\nare associates or a is a unit. Therefore, p is irreducible.\\n\\nConversely, let p be irreducible. If ⟨a⟩ is an ideal in D such that ⟨p⟩ ⊂ ⟨a⟩ ⊂ D, then\\na | p. Since p is irreducible, either a must be a unit or a and p are associates. Therefore,\\neither D = ⟨a⟩ or ⟨p⟩ = ⟨a⟩. Thus, ⟨p⟩ is a maximal ideal.\\n\\n\\n\\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 309\\n\\nCorollary 18.13. Let D be a pid. If p is irreducible, then p is prime.\\n\\nProof. Let p be irreducible and suppose that p | ab. Then ⟨ab⟩ ⊂ ⟨p⟩. By Corollary 16.40,\\nsince ⟨p⟩ is a maximal ideal, ⟨p⟩ must also be a prime ideal. Thus, either a ∈ ⟨p⟩ or b ∈ ⟨p⟩.\\nHence, either p | a or p | b.\\n\\nLemma 18.14. Let D be a pid. Let I1, I2, . . . be a set of ideals such that I1 ⊂ I2 ⊂ · · ·.\\nThen there exists an integer N such that In = IN for all n ≥ N .\\n\\nProof. We claim that I =\\n∪∞\\n\\ni=1 Ii is an ideal of D. Certainly I is not empty, since I1 ⊂ I\\nand 0 ∈ I. If a, b ∈ I, then a ∈ Ii and b ∈ Ij for some i and j in N. Without loss of\\ngenerality we can assume that i ≤ j. Hence, a and b are both in Ij and so a − b is also in\\nIj . Now let r ∈ D and a ∈ I. Again, we note that a ∈ Ii for some positive integer i. Since\\nIi is an ideal, ra ∈ Ii and hence must be in I. Therefore, we have shown that I is an ideal\\nin D.\\n\\nSince D is a principal ideal domain, there exists an element a ∈ D that generates I.\\nSince a is in IN for some N ∈ N, we know that IN = I = ⟨a⟩. Consequently, In = IN for\\nn ≥ N .\\n\\nAny commutative ring satisfying the condition in Lemma 18.14 is said to satisfy the\\nascending chain condition, or ACC. Such rings are called Noetherian rings, after\\nEmmy Noether.\\n\\nTheorem 18.15. Every pid is a ufd.\\n\\nProof. Existence of a factorization. Let D be a pid and a be a nonzero element in D that\\nis not a unit. If a is irreducible, then we are done. If not, then there exists a factorization\\na = a1b1, where neither a1 nor b1 is a unit. Hence, ⟨a⟩ ⊂ ⟨a1⟩. By Lemma 18.11, we know\\nthat ⟨a⟩ ̸= ⟨a1⟩; otherwise, a and a1 would be associates and b1 would be a unit, which\\nwould contradict our assumption. Now suppose that a1 = a2b2, where neither a2 nor b2 is a\\nunit. By the same argument as before, ⟨a1⟩ ⊂ ⟨a2⟩. We can continue with this construction\\nto obtain an ascending chain of ideals\\n\\n⟨a⟩ ⊂ ⟨a1⟩ ⊂ ⟨a2⟩ ⊂ · · · .\\n\\nBy Lemma 18.14, there exists a positive integer N such that ⟨an⟩ = ⟨aN ⟩ for all n ≥ N .\\nConsequently, aN must be irreducible. We have now shown that a is the product of two\\nelements, one of which must be irreducible.\\n\\nNow suppose that a = c1p1, where p1 is irreducible. If c1 is not a unit, we can repeat\\nthe preceding argument to conclude that ⟨a⟩ ⊂ ⟨c1⟩. Either c1 is irreducible or c1 = c2p2,\\nwhere p2 is irreducible and c2 is not a unit. Continuing in this manner, we obtain another\\nchain of ideals\\n\\n⟨a⟩ ⊂ ⟨c1⟩ ⊂ ⟨c2⟩ ⊂ · · · .\\n\\nThis chain must satisfy the ascending chain condition; therefore,\\n\\na = p1p2 · · · pr\\n\\nfor irreducible elements p1, . . . , pr.\\nUniqueness of the factorization. To show uniqueness, let\\n\\na = p1p2 · · · pr = q1q2 · · · qs,\\n\\n\\n\\n310 CHAPTER 18. INTEGRAL DOMAINS\\n\\nwhere each pi and each qi is irreducible. Without loss of generality, we can assume that\\nr < s. Since p1 divides q1q2 · · · qs, by Corollary 18.13 it must divide some qi. By rearranging\\nthe qi’s, we can assume that p1 | q1; hence, q1 = u1p1 for some unit u1 in D. Therefore,\\n\\na = p1p2 · · · pr = u1p1q2 · · · qs\\n\\nor\\np2 · · · pr = u1q2 · · · qs.\\n\\nContinuing in this manner, we can arrange the qi’s such that p2 = q2, p3 = q3, . . . , pr = qr,\\nto obtain\\n\\nu1u2 · · ·urqr+1 · · · qs = 1.\\n\\nIn this case qr+1 · · · qs is a unit, which contradicts the fact that qr+1, . . . , qs are irreducibles.\\nTherefore, r = s and the factorization of a is unique.\\n\\nCorollary 18.16. Let F be a field. Then F [x] is a ufd.\\n\\nExample 18.17. Every pid is a ufd, but it is not the case that every ufd is a pid.\\nIn Corollary 18.31, we will prove that Z[x] is a ufd. However, Z[x] is not a pid. Let\\nI = {5f(x) + xg(x) : f(x), g(x) ∈ Z[x]}. We can easily show that I is an ideal of Z[x].\\nSuppose that I = ⟨p(x)⟩. Since 5 ∈ I, 5 = f(x)p(x). In this case p(x) = p must be a\\nconstant. Since x ∈ I, x = pg(x); consequently, p = ±1. However, it follows from this\\nfact that ⟨p(x)⟩ = Z[x]. But this would mean that 3 is in I. Therefore, we can write\\n3 = 5f(x) + xg(x) for some f(x) and g(x) in Z[x]. Examining the constant term of this\\npolynomial, we see that 3 = 5f(x), which is impossible.\\n\\nEuclidean Domains\\nWe have repeatedly used the division algorithm when proving results about either Z or\\nF [x], where F is a field. We should now ask when a division algorithm is available for an\\nintegral domain.\\n\\nLet D be an integral domain such that for each a ∈ D there is a nonnegative integer\\nν(a) satisfying the following conditions.\\n\\n1. If a and b are nonzero elements in D, then ν(a) ≤ ν(ab).\\n\\n2. Let a, b ∈ D and suppose that b ̸= 0. Then there exist elements q, r ∈ D such that\\na = bq + r and either r = 0 or ν(r) < ν(b).\\n\\nThen D is called a Euclidean domain and ν is called a Euclidean valuation.\\n\\nExample 18.18. Absolute value on Z is a Euclidean valuation.\\n\\nExample 18.19. Let F be a field. Then the degree of a polynomial in F [x] is a Euclidean\\nvaluation.\\n\\nExample 18.20. Recall that the Gaussian integers in Example 16.12 of Chapter 16 are\\ndefined by\\n\\nZ[i] = {a+ bi : a, b ∈ Z}.\\n\\nWe usually measure the size of a complex number a + bi by its absolute value, |a + bi| =√\\na2 + b2; however,\\n\\n√\\na2 + b2 may not be an integer. For our valuation we will let ν(a+bi) =\\n\\na2 + b2 to ensure that we have an integer.\\n\\n\\n\\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 311\\n\\nWe claim that ν(a + bi) = a2 + b2 is a Euclidean valuation on Z[i]. Let z, w ∈ Z[i].\\nThen ν(zw) = |zw|2 = |z|2|w|2 = ν(z)ν(w). Since ν(z) ≥ 1 for every nonzero z ∈ Z[i],\\nν(z) ≤ ν(z)ν(w).\\n\\nNext, we must show that for any z = a + bi and w = c + di in Z[i] with w ̸= 0, there\\nexist elements q and r in Z[i] such that z = qw + r with either r = 0 or ν(r) < ν(w). We\\ncan view z and w as elements in Q(i) = {p + qi : p, q ∈ Q}, the field of fractions of Z[i].\\nObserve that\\n\\nzw−1 = (a+ bi)\\nc− di\\n\\nc2 + d2\\n\\n=\\nac+ bd\\n\\nc2 + d2\\n+\\nbc− ad\\n\\nc2 + d2\\ni\\n\\n=\\n\\n(\\nm1 +\\n\\nn1\\nc2 + d2\\n\\n)\\n+\\n\\n(\\nm2 +\\n\\nn2\\nc2 + d2\\n\\n)\\ni\\n\\n= (m1 +m2i) +\\n\\n(\\nn1\\n\\nc2 + d2\\n+\\n\\nn2\\nc2 + d2\\n\\ni\\n\\n)\\n= (m1 +m2i) + (s+ ti)\\n\\nin Q(i). In the last steps we are writing the real and imaginary parts as an integer plus a\\nproper fraction. That is, we take the closest integer mi such that the fractional part satisfies\\n|ni/(a2 + b2)| ≤ 1/2. For example, we write\\n\\n9\\n\\n8\\n= 1 +\\n\\n1\\n\\n8\\n15\\n\\n8\\n= 2− 1\\n\\n8\\n.\\n\\nThus, s and t are the “fractional parts” of zw−1 = (m1 +m2i) + (s + ti). We also know\\nthat s2 + t2 ≤ 1/4 + 1/4 = 1/2. Multiplying by w, we have\\n\\nz = zw−1w = w(m1 +m2i) + w(s+ ti) = qw + r,\\n\\nwhere q = m1 + m2i and r = w(s + ti). Since z and qw are in Z[i], r must be in Z[i].\\nFinally, we need to show that either r = 0 or ν(r) < ν(w). However,\\n\\nν(r) = ν(w)ν(s+ ti) ≤ 1\\n\\n2\\nν(w) < ν(w).\\n\\nTheorem 18.21. Every Euclidean domain is a principal ideal domain.\\n\\nProof. Let D be a Euclidean domain and let ν be a Euclidean valuation on D. Suppose\\nI is a nontrivial ideal in D and choose a nonzero element b ∈ I such that ν(b) is minimal\\nfor all a ∈ I. Since D is a Euclidean domain, there exist elements q and r in D such that\\na = bq + r and either r = 0 or ν(r) < ν(b). But r = a − bq is in I since I is an ideal;\\ntherefore, r = 0 by the minimality of b. It follows that a = bq and I = ⟨b⟩.\\n\\nCorollary 18.22. Every Euclidean domain is a unique factorization domain.\\n\\nFactorization in D[x]\\n\\nOne of the most important polynomial rings is Z[x]. One of the first questions that come\\nto mind about Z[x] is whether or not it is a ufd. We will prove a more general statement\\nhere. Our first task is to obtain a more general version of Gauss’s Lemma (Theorem 17.14).\\n\\n\\n\\n312 CHAPTER 18. INTEGRAL DOMAINS\\n\\nLet D be a unique factorization domain and suppose that\\n\\np(x) = anx\\nn + · · ·+ a1x+ a0\\n\\nin D[x]. Then the content of p(x) is the greatest common divisor of a0, . . . , an. We say\\nthat p(x) is primitive if gcd(a0, . . . , an) = 1.\\n\\nExample 18.23. In Z[x] the polynomial p(x) = 5x4−3x3+x−4 is a primitive polynomial\\nsince the greatest common divisor of the coefficients is 1; however, the polynomial q(x) =\\n4x2 − 6x+ 8 is not primitive since the content of q(x) is 2.\\n\\nTheorem 18.24 (Gauss’s Lemma). Let D be a ufd and let f(x) and g(x) be primitive\\npolynomials in D[x]. Then f(x)g(x) is primitive.\\n\\nProof. Let f(x) =\\n∑m\\n\\ni=0 aix\\ni and g(x) =\\n\\n∑n\\ni=0 bix\\n\\ni. Suppose that p is a prime dividing\\nthe coefficients of f(x)g(x). Let r be the smallest integer such that p ̸ |ar and s be the\\nsmallest integer such that p ̸ |bs. The coefficient of xr+s in f(x)g(x) is\\n\\ncr+s = a0br+s + a1br+s−1 + · · ·+ ar+s−1b1 + ar+sb0.\\n\\nSince p divides a0, . . . , ar−1 and b0, . . . , bs−1, p divides every term of cr+s except for the term\\narbs. However, since p | cr+s, either p divides ar or p divides bs. But this is impossible.\\n\\nLemma 18.25. Let D be a ufd, and let p(x) and q(x) be in D[x]. Then the content of\\np(x)q(x) is equal to the product of the contents of p(x) and q(x).\\n\\nProof. Let p(x) = cp1(x) and q(x) = dq1(x), where c and d are the contents of p(x)\\nand q(x), respectively. Then p1(x) and q1(x) are primitive. We can now write p(x)q(x) =\\ncdp1(x)q1(x). Since p1(x)q1(x) is primitive, the content of p(x)q(x) must be cd.\\n\\nLemma 18.26. Let D be a ufd and F its field of fractions. Suppose that p(x) ∈ D[x] and\\np(x) = f(x)g(x), where f(x) and g(x) are in F [x]. Then p(x) = f1(x)g1(x), where f1(x)\\nand g1(x) are in D[x]. Furthermore, deg f(x) = deg f1(x) and deg g(x) = deg g1(x).\\n\\nProof. Let a and b be nonzero elements of D such that af(x), bg(x) are in D[x]. We can\\nfind a1, b2 ∈ D such that af(x) = a1f1(x) and bg(x) = b1g1(x), where f1(x) and g1(x) are\\nprimitive polynomials in D[x]. Therefore, abp(x) = (a1f1(x))(b1g1(x)). Since f1(x) and\\ng1(x) are primitive polynomials, it must be the case that ab | a1b1 by Gauss’s Lemma.\\nThus there exists a c ∈ D such that p(x) = cf1(x)g1(x). Clearly, deg f(x) = deg f1(x) and\\ndeg g(x) = deg g1(x).\\n\\nThe following corollaries are direct consequences of Lemma 18.26.\\n\\nCorollary 18.27. Let D be a ufd and F its field of fractions. A primitive polynomial p(x)\\nin D[x] is irreducible in F [x] if and only if it is irreducible in D[x].\\n\\nCorollary 18.28. Let D be a ufd and F its field of fractions. If p(x) is a monic polynomial\\nin D[x] with p(x) = f(x)g(x) in F [x], then p(x) = f1(x)g1(x), where f1(x) and g1(x) are\\nin D[x]. Furthermore, deg f(x) = deg f1(x) and deg g(x) = deg g1(x).\\n\\nTheorem 18.29. If D is a ufd, then D[x] is a ufd.\\n\\nProof. Let p(x) be a nonzero polynomial in D[x]. If p(x) is a constant polynomial,\\nthen it must have a unique factorization since D is a ufd. Now suppose that p(x) is\\na polynomial of positive degree in D[x]. Let F be the field of fractions of D, and let\\np(x) = f1(x)f2(x) · · · fn(x) by a factorization of p(x), where each fi(x) is irreducible. Choose\\n\\n\\n\\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 313\\n\\nai ∈ D such that aifi(x) is in D[x]. There exist b1, . . . , bn ∈ D such that aifi(x) = bigi(x),\\nwhere gi(x) is a primitive polynomial in D[x]. By Corollary 18.27, each gi(x) is irreducible\\nin D[x]. Consequently, we can write\\n\\na1 · · · anp(x) = b1 · · · bng1(x) · · · gn(x).\\n\\nLet b = b1 · · · bn. Since g1(x) · · · gn(x) is primitive, a1 · · · an divides b. Therefore, p(x) =\\nag1(x) · · · gn(x), where a ∈ D. Since D is a ufd, we can factor a as uc1 · · · ck, where u is a\\nunit and each of the ci’s is irreducible in D.\\n\\nWe will now show the uniqueness of this factorization. Let\\n\\np(x) = a1 · · · amf1(x) · · · fn(x) = b1 · · · brg1(x) · · · gs(x)\\n\\nbe two factorizations of p(x), where all of the factors are irreducible in D[x]. By Corol-\\nlary 18.27, each of the fi’s and gi’s is irreducible in F [x]. The ai’s and the bi’s are units\\nin F . Since F [x] is a pid, it is a ufd; therefore, n = s. Now rearrange the gi(x)’s so that\\nfi(x) and gi(x) are associates for i = 1, . . . , n. Then there exist c1, . . . , cn and d1, . . . , dn in\\nD such that (ci/di)fi(x) = gi(x) or cifi(x) = digi(x). The polynomials fi(x) and gi(x) are\\nprimitive; hence, ci and di are associates in D. Thus, a1 · · · am = ub1 · · · br in D, where u is\\na unit in D. Since D is a unique factorization domain, m = s. Finally, we can reorder the\\nbi’s so that ai and bi are associates for each i. This completes the uniqueness part of the\\nproof.\\n\\nThe theorem that we have just proven has several obvious but important corollaries.\\n\\nCorollary 18.30. Let F be a field. Then F [x] is a ufd.\\n\\nCorollary 18.31. The ring of polynomials over the integers, Z[x], is a ufd.\\n\\nCorollary 18.32. Let D be a ufd. Then D[x1, . . . , xn] is a ufd.\\n\\nRemark 18.33. It is important to notice that every Euclidean domain is a pid and every\\npid is a ufd. However, as demonstrated by our examples, the converse of each of these\\nstatements fails. There are principal ideal domains that are not Euclidean domains, and\\nthere are unique factorization domains that are not principal ideal domains (Z[x]).\\n\\nHistorical Note\\n\\nKarl Friedrich Gauss, born in Brunswick, Germany on April 30, 1777, is considered to\\nbe one of the greatest mathematicians who ever lived. Gauss was truly a child prodigy. At\\nthe age of three he was able to detect errors in the books of his father’s business. Gauss\\nentered college at the age of 15. Before the age of 20, Gauss was able to construct a regular\\n17-sided polygon with a ruler and compass. This was the first new construction of a regular\\nn-sided polygon since the time of the ancient Greeks. Gauss succeeded in showing that if\\nN = 22\\n\\nn\\n+ 1 was prime, then it was possible to construct a regular N -sided polygon.\\n\\nGauss obtained his Ph.D. in 1799 under the direction of Pfaff at the University of\\nHelmstedt. In his dissertation he gave the first complete proof of the Fundamental Theorem\\nof Algebra, which states that every polynomial with real coefficients can be factored into\\nlinear factors over the complex numbers. The acceptance of complex numbers was brought\\nabout by Gauss, who was the first person to use the notation of i for\\n\\n√\\n−1.\\n\\nGauss then turned his attention toward number theory; in 1801, he published his fa-\\nmous book on number theory, Disquisitiones Arithmeticae. Throughout his life Gauss was\\n\\n\\n\\n314 CHAPTER 18. INTEGRAL DOMAINS\\n\\nintrigued with this branch of mathematics. He once wrote, “Mathematics is the queen of\\nthe sciences, and the theory of numbers is the queen of mathematics.”\\n\\nIn 1807, Gauss was appointed director of the Observatory at the University of Göttingen,\\na position he held until his death. This position required him to study applications of math-\\nematics to the sciences. He succeeded in making contributions to fields such as astronomy,\\nmechanics, optics, geodesy, and magnetism. Along with Wilhelm Weber, he coinvented the\\nfirst practical electric telegraph some years before a better version was invented by Samuel\\nF. B. Morse.\\n\\nGauss was clearly the most prominent mathematician in the world in the early nineteenth\\ncentury. His status naturally made his discoveries subject to intense scrutiny. Gauss’s cold\\nand distant personality many times led him to ignore the work of his contemporaries, mak-\\ning him many enemies. He did not enjoy teaching very much, and young mathematicians\\nwho sought him out for encouragement were often rebuffed. Nevertheless, he had many\\noutstanding students, including Eisenstein, Riemann, Kummer, Dirichlet, and Dedekind.\\nGauss also offered a great deal of encouragement to Sophie Germain (1776–1831), who\\novercame the many obstacles facing women in her day to become a very prominent mathe-\\nmatician. Gauss died at the age of 78 in Göttingen on February 23, 1855.\\n\\n18.3 Exercises\\n1. Let z = a+ b\\n\\n√\\n3 i be in Z[\\n\\n√\\n3 i]. If a2 + 3b2 = 1, show that z must be a unit. Show that\\n\\nthe only units of Z[\\n√\\n3 i] are 1 and −1.\\n\\n2. The Gaussian integers, Z[i], are a ufd. Factor each of the following elements in Z[i] into\\na product of irreducibles.\\n\\n(a) 5\\n(b) 1 + 3i\\n\\n(c) 6 + 8i\\n\\n(d) 2\\n\\n3. Let D be an integral domain.\\n(a) Prove that FD is an abelian group under the operation of addition.\\n(b) Show that the operation of multiplication is well-defined in the field of fractions, FD.\\n(c) Verify the associative and commutative properties for multiplication in FD.\\n\\n4. Prove or disprove: Any subring of a field F containing 1 is an integral domain.\\n\\n5. Prove or disprove: If D is an integral domain, then every prime element in D is also\\nirreducible in D.\\n\\n6. Let F be a field of characteristic zero. Prove that F contains a subfield isomorphic to\\nQ.\\n\\n7. Let F be a field.\\n(a) Prove that the field of fractions of F [x], denoted by F (x), is isomorphic to the set all\\n\\nrational expressions p(x)/q(x), where q(x) is not the zero polynomial.\\n(b) Let p(x1, . . . , xn) and q(x1, . . . , xn) be polynomials in F [x1, . . . , xn]. Show that the\\n\\nset of all rational expressions p(x1, . . . , xn)/q(x1, . . . , xn) is isomorphic to the field\\nof fractions of F [x1, . . . , xn]. We denote the field of fractions of F [x1, . . . , xn] by\\nF (x1, . . . , xn).\\n\\n\\n\\n18.3. EXERCISES 315\\n\\n8. Let p be prime and denote the field of fractions of Zp[x] by Zp(x). Prove that Zp(x) is\\nan infinite field of characteristic p.\\n\\n9. Prove that the field of fractions of the Gaussian integers, Z[i], is\\n\\nQ(i) = {p+ qi : p, q ∈ Q}.\\n\\n10. A field F is called a prime field if it has no proper subfields. If E is a subfield of F\\nand E is a prime field, then E is a prime subfield of F .\\n\\n(a) Prove that every field contains a unique prime subfield.\\n(b) If F is a field of characteristic 0, prove that the prime subfield of F is isomorphic to\\n\\nthe field of rational numbers, Q.\\n(c) If F is a field of characteristic p, prove that the prime subfield of F is isomorphic to\\n\\nZp.\\n\\n11. Let Z[\\n√\\n2 ] = {a+ b\\n\\n√\\n2 : a, b ∈ Z}.\\n\\n(a) Prove that Z[\\n√\\n2 ] is an integral domain.\\n\\n(b) Find all of the units in Z[\\n√\\n2 ].\\n\\n(c) Determine the field of fractions of Z[\\n√\\n2 ].\\n\\n(d) Prove that Z[\\n√\\n2i] is a Euclidean domain under the Euclidean valuation ν(a+b\\n\\n√\\n2 i) =\\n\\na2 + 2b2.\\n\\n12. Let D be a ufd. An element d ∈ D is a greatest common divisor of a and b in D\\nif d | a and d | b and d is divisible by any other element dividing both a and b.\\n\\n(a) If D is a pid and a and b are both nonzero elements of D, prove there exists a unique\\ngreatest common divisor of a and b up to associates. That is, if d and d′ are both\\ngreatest common divisors of a and b, then d and d′ are associates. We write gcd(a, b)\\nfor the greatest common divisor of a and b.\\n\\n(b) Let D be a pid and a and b be nonzero elements of D. Prove that there exist elements\\ns and t in D such that gcd(a, b) = as+ bt.\\n\\n13. Let D be an integral domain. Define a relation on D by a ∼ b if a and b are associates\\nin D. Prove that ∼ is an equivalence relation on D.\\n\\n14. Let D be a Euclidean domain with Euclidean valuation ν. If u is a unit in D, show\\nthat ν(u) = ν(1).\\n\\n15. Let D be a Euclidean domain with Euclidean valuation ν. If a and b are associates in\\nD, prove that ν(a) = ν(b).\\n\\n16. Show that Z[\\n√\\n5 i] is not a unique factorization domain.\\n\\n17. Prove or disprove: Every subdomain of a ufd is also a ufd.\\n\\n18. An ideal of a commutative ring R is said to be finitely generated if there exist\\nelements a1, . . . , an in R such that every element r ∈ R can be written as a1r1 + · · ·+ anrn\\nfor some r1, . . . , rn in R. Prove that R satisfies the ascending chain condition if and only if\\nevery ideal of R is finitely generated.\\n\\n\\n\\n316 CHAPTER 18. INTEGRAL DOMAINS\\n\\n19. Let D be an integral domain with a descending chain of ideals I1 ⊃ I2 ⊃ I3 ⊃ · · ·.\\nSuppose that there exists an N such that Ik = IN for all k ≥ N . A ring satisfying this\\ncondition is said to satisfy the descending chain condition, or DCC. Rings satisfying the\\nDCC are called Artinian rings, after Emil Artin. Show that if D satisfies the descending\\nchain condition, it must satisfy the ascending chain condition.\\n\\n20. Let R be a commutative ring with identity. We define a multiplicative subset of R\\nto be a subset S such that 1 ∈ S and ab ∈ S if a, b ∈ S.\\n(a) Define a relation ∼ on R × S by (a, s) ∼ (a′, s′) if there exists an s∗ ∈ S such that\\n\\ns∗(s′a− sa′) = 0. Show that ∼ is an equivalence relation on R× S.\\n(b) Let a/s denote the equivalence class of (a, s) ∈ R × S and let S−1R be the set of all\\n\\nequivalence classes with respect to ∼. Define the operations of addition and multipli-\\ncation on S−1R by\\n\\na\\n\\ns\\n+\\nb\\n\\nt\\n=\\nat+ bs\\n\\nst\\na\\n\\ns\\n\\nb\\n\\nt\\n=\\nab\\n\\nst\\n,\\n\\nrespectively. Prove that these operations are well-defined on S−1R and that S−1R\\nis a ring with identity under these operations. The ring S−1R is called the ring of\\nquotients of R with respect to S.\\n\\n(c) Show that the map ψ : R→ S−1R defined by ψ(a) = a/1 is a ring homomorphism.\\n(d) If R has no zero divisors and 0 /∈ S, show that ψ is one-to-one.\\n(e) Prove that P is a prime ideal of R if and only if S = R \\\\ P is a multiplicative subset\\n\\nof R.\\n(f) If P is a prime ideal of R and S = R \\\\ P , show that the ring of quotients S−1R has\\n\\na unique maximal ideal. Any ring that has a unique maximal ideal is called a local\\nring.\\n\\n18.4 References and Suggested Readings\\n[1] Atiyah, M. F. and MacDonald, I. G. Introduction to Commutative Algebra. Westview\\n\\nPress, Boulder, CO, 1994.\\n[2] Zariski, O. and Samuel, P. Commutative Algebra, vols. I and II. Springer, New York,\\n\\n1975, 1960.\\n\\n18.5 Sage\\nWe have already seen some integral domains and unique factorizations in the previous two\\nchapters. In addition to what we have already seen, Sage has support for some of the topics\\nfrom this section, but the coverage is limited. Some functions will work for some rings and\\nnot others, while some functions are not yet part of Sage. So we will give some examples,\\nbut this is far from comprehensive.\\n\\nField of Fractions\\nSage is frequently able to construct a field of fractions, or identify a certain field as the field\\nof fractions. For example, the ring of integers and the field of rational numbers are both\\nimplemented in Sage, and the integers “know” that the rationals is it’s field of fractions.\\n\\n\\n\\n18.5. SAGE 317\\n\\nQ = ZZ.fraction_field (); Q\\n\\nRational Field\\n\\nQ == QQ\\n\\nTrue\\n\\nIn other cases Sage will construct a fraction field, in the spirit of Lemma 18.3. So it is\\nthen possible to do basic calculations in the constructed field.\\n\\nR.<x> = ZZ[]\\nP = R.fraction_field ();P\\n\\nFraction Field of Univariate Polynomial Ring in x over Integer Ring\\n\\nf = P((x^2+3) /(7*x+4))\\ng = P((4*x^2) /(3*x^2-5*x+4))\\nh = P((-2*x^3+4*x^2+3)/(x^2+1))\\n((f+g)/h).numerator ()\\n\\n3*x^6 + 23*x^5 + 32*x^4 + 8*x^3 + 41*x^2 - 15*x + 12\\n\\n((f+g)/h).denominator ()\\n\\n-42*x^6 + 130*x^5 - 108*x^4 + 63*x^3 - 5*x^2 + 24*x + 48\\n\\nPrime Subfields\\nCorollary 18.7 says every field of characteristic p has a subfield isomorphic to Zp. For a\\nfinite field, the exact nature of this subfield is not a surprise, but Sage will allow us to\\nextract it easily.\\n\\nF.<c> = FiniteField (3^5)\\nF.characteristic ()\\n\\n3\\n\\nG = F.prime_subfield (); G\\n\\nFinite Field of size 3\\n\\nG.list()\\n\\n[0, 1, 2]\\n\\nMore generally, the fields mentioned in the conclusions of Corollary 18.6 and Corol-\\nlary 18.7 are known as the “prime subfield” of the ring containing them. Here is an example\\nof the characteristic zero case.\\n\\nK.<y>= QuadraticField (-7); K\\n\\nNumber Field in y with defining polynomial x^2 + 7\\n\\n\\n\\n318 CHAPTER 18. INTEGRAL DOMAINS\\n\\nK.prime_subfield ()\\n\\nRational Field\\n\\nIn a rough sense, every characteristic zero field contains a copy of the rational numbers\\n(the fraction field of the integers), which can explain Sage’s extensive support for rings and\\nfields that extend the integers and the rationals.\\n\\nIntegral Domains\\n\\nSage can determine if some rings are integral domains and we can test products in them.\\nHowever, notions of units, irreducibles or prime elements are not generally supported (out-\\nside of what we have seen for polynomials in the previous chapter). Worse, the construction\\nbelow creates a ring within a larger field and so some functions (such as .is_unit()) pass\\nthrough and give misleading results. This is because the construction below creates a ring\\nknown as an “order in a number field.”\\n\\nK.<x> = ZZ[sqrt(-3)]; K\\n\\nOrder in Number Field in a with defining polynomial x^2 + 3\\n\\nK.is_integral_domain ()\\n\\nTrue\\n\\nK.basis()\\n\\n[1, a]\\n\\nx\\n\\na\\n\\n(1+x)*(1-x) == 2*2\\n\\nTrue\\n\\nThe following is a bit misleading, since 4, as an element of Z[\\n√\\n3i] does not have a\\n\\nmultiplicative inverse, though seemingly we can compute one.\\n\\nfour = K(4)\\nfour.is_unit ()\\n\\nFalse\\n\\nfour^-1\\n\\n1/4\\n\\n\\n\\n18.6. SAGE EXERCISES 319\\n\\nPrincipal Ideals\\nWhen a ring is a principal ideal domain, such as the integers, or polynomials over a field,\\nSage works well. Beyond that, support begins to weaken.\\n\\nT.<x>=ZZ[]\\nT.is_integral_domain ()\\n\\nTrue\\n\\nJ = T.ideal(5, x); J\\n\\nIdeal (5, x) of Univariate Polynomial Ring in x over Integer Ring\\n\\nQ = T.quotient(J); Q\\n\\nQuotient of Univariate Polynomial Ring in x over\\nInteger Ring by the ideal (5, x)\\n\\nJ.is_principal ()\\n\\nTraceback (most recent call last):\\n...\\nNotImplementedError\\n\\nQ.is_field ()\\n\\nTraceback (most recent call last):\\n...\\nNotImplementedError\\n\\n18.6 Sage Exercises\\nThere are no Sage exercises for this section.\\n\\n\\n\\n19\\n\\nLattices and Boolean Algebras\\n\\nThe axioms of a ring give structure to the operations of addition and multiplication on a set.\\nHowever, we can construct algebraic structures, known as lattices and Boolean algebras,\\nthat generalize other types of operations. For example, the important operations on sets\\nare inclusion, union, and intersection. Lattices are generalizations of order relations on\\nalgebraic spaces, such as set inclusion in set theory and inequality in the familiar number\\nsystems N, Z, Q, and R. Boolean algebras generalize the operations of intersection and\\nunion. Lattices and Boolean algebras have found applications in logic, circuit theory, and\\nprobability.\\n\\n19.1 Lattices\\nPartially Ordered Sets\\nWe begin by the study of lattices and Boolean algebras by generalizing the idea of inequality.\\nRecall that a relation on a set X is a subset of X × X. A relation P on X is called a\\npartial order of X if it satisfies the following axioms.\\n\\n1. The relation is reflexive: (a, a) ∈ P for all a ∈ X.\\n\\n2. The relation is antisymmetric: if (a, b) ∈ P and (b, a) ∈ P , then a = b.\\n\\n3. The relation is transitive: if (a, b) ∈ P and (b, c) ∈ P , then (a, c) ∈ P .\\n\\nWe will usually write a ⪯ b to mean (a, b) ∈ P unless some symbol is naturally associated\\nwith a particular partial order, such as a ≤ b with integers a and b, or A ⊆ B with sets\\nA and B. A set X together with a partial order ⪯ is called a partially ordered set, or\\nposet.\\n\\nExample 19.1. The set of integers (or rationals or reals) is a poset where a ≤ b has the\\nusual meaning for two integers a and b in Z.\\n\\nExample 19.2. Let X be any set. We will define the power set of X to be the set of\\nall subsets of X. We denote the power set of X by P(X). For example, let X = {a, b, c}.\\nThen P(X) is the set of all subsets of the set {a, b, c}:\\n\\n∅ {a} {b} {c}\\n{a, b} {a, c} {b, c} {a, b, c}.\\n\\nOn any power set of a set X, set inclusion, ⊆, is a partial order. We can represent the order\\non {a, b, c} schematically by a diagram such as the one in Figure 19.3.\\n\\n320\\n\\n\\n\\n19.1. LATTICES 321\\n\\n{a, b, c}\\n\\n{a, b} {a, c} {b, c}\\n\\n{a} {b} {c}\\n\\n∅\\n\\nFigure 19.3: Partial order on P({a, b, c})\\n\\nExample 19.4. Let G be a group. The set of subgroups of G is a poset, where the partial\\norder is set inclusion.\\n\\nExample 19.5. There can be more than one partial order on a particular set. We can\\nform a partial order on N by a ⪯ b if a | b. The relation is certainly reflexive since a | a for\\nall a ∈ N. If m | n and n | m, then m = n; hence, the relation is also antisymmetric. The\\nrelation is transitive, because if m | n and n | p, then m | p.\\n\\nExample 19.6. Let X = {1, 2, 3, 4, 6, 8, 12, 24} be the set of divisors of 24 with the partial\\norder defined in Example 19.5. Figure 19.7 shows the partial order on X.\\n\\n24\\n\\n12\\n\\n6\\n\\n3\\n\\n8\\n\\n4\\n\\n2\\n\\n1\\n\\nFigure 19.7: A partial order on the divisors of 24\\n\\nLet Y be a subset of a poset X. An element u in X is an upper bound of Y if a ⪯ u\\nfor every element a ∈ Y . If u is an upper bound of Y such that u ⪯ v for every other upper\\nbound v of Y , then u is called a least upper bound or supremum of Y . An element l in\\nX is said to be a lower bound of Y if l ⪯ a for all a ∈ Y . If l is a lower bound of Y such\\nthat k ⪯ l for every other lower bound k of Y , then l is called a greatest lower bound or\\ninfimum of Y .\\n\\nExample 19.8. Let Y = {2, 3, 4, 6} be contained in the set X of Example 19.6. Then Y\\nhas upper bounds 12 and 24, with 12 as a least upper bound. The only lower bound is 1;\\nhence, it must be a greatest lower bound.\\n\\nAs it turns out, least upper bounds and greatest lower bounds are unique if they exist.\\n\\n\\n\\n322 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\nTheorem 19.9. Let Y be a nonempty subset of a poset X. If Y has a least upper bound,\\nthen Y has a unique least upper bound. If Y has a greatest lower bound, then Y has a\\nunique greatest lower bound.\\n\\nProof. Let u1 and u2 be least upper bounds for Y . By the definition of the least upper\\nbound, u1 ⪯ u for all upper bounds u of Y . In particular, u1 ⪯ u2. Similarly, u2 ⪯ u1.\\nTherefore, u1 = u2 by antisymmetry. A similar argument show that the greatest lower\\nbound is unique.\\n\\nOn many posets it is possible to define binary operations by using the greatest lower\\nbound and the least upper bound of two elements. A lattice is a poset L such that every\\npair of elements in L has a least upper bound and a greatest lower bound. The least upper\\nbound of a, b ∈ L is called the join of a and b and is denoted by a ∨ b. The greatest lower\\nbound of a, b ∈ L is called the meet of a and b and is denoted by a ∧ b.\\n\\nExample 19.10. Let X be a set. Then the power set of X, P(X), is a lattice. For two sets\\nA and B in P(X), the least upper bound of A and B is A∪B. Certainly A∪B is an upper\\nbound of A and B, since A ⊆ A∪B and B ⊆ A∪B. If C is some other set containing both\\nA and B, then C must contain A ∪B; hence, A ∪B is the least upper bound of A and B.\\nSimilarly, the greatest lower bound of A and B is A ∩B.\\n\\nExample 19.11. Let G be a group and suppose that X is the set of subgroups of G. Then\\nX is a poset ordered by set-theoretic inclusion, ⊆. The set of subgroups of G is also a\\nlattice. If H and K are subgroups of G, the greatest lower bound of H and K is H ∩K.\\nThe set H ∪K may not be a subgroup of G. We leave it as an exercise to show that the\\nleast upper bound of H and K is the subgroup generated by H ∪K.\\n\\nIn set theory we have certain duality conditions. For example, by De Morgan’s laws,\\nany statement about sets that is true about (A ∪B)′ must also be true about A′ ∩B′. We\\nalso have a duality principle for lattices.\\n\\nAxiom 19.12 (Principle of Duality). Any statement that is true for all lattices remains\\ntrue when ⪯ is replaced by ⪰ and ∨ and ∧ are interchanged throughout the statement.\\n\\nThe following theorem tells us that a lattice is an algebraic structure with two binary\\noperations that satisfy certain axioms.\\n\\nTheorem 19.13. If L is a lattice, then the binary operations ∨ and ∧ satisfy the following\\nproperties for a, b, c ∈ L.\\n\\n1. Commutative laws: a ∨ b = b ∨ a and a ∧ b = b ∧ a.\\n\\n2. Associative laws: a ∨ (b ∨ c) = (a ∨ b) ∨ c and a ∧ (b ∧ c) = (a ∧ b) ∧ c.\\n\\n3. Idempotent laws: a ∨ a = a and a ∧ a = a.\\n\\n4. Absorption laws: a ∨ (a ∧ b) = a and a ∧ (a ∨ b) = a.\\n\\nProof. By the Principle of Duality, we need only prove the first statement in each part.\\n(1) By definition a ∨ b is the least upper bound of {a, b}, and b ∨ a is the least upper\\n\\nbound of {b, a}; however, {a, b} = {b, a}.\\n(2) We will show that a∨ (b∨ c) and (a∨ b)∨ c are both least upper bounds of {a, b, c}.\\n\\nLet d = a ∨ b. Then c ⪯ d ∨ c = (a ∨ b) ∨ c. We also know that\\n\\na ⪯ a ∨ b = d ⪯ d ∨ c = (a ∨ b) ∨ c.\\n\\n\\n\\n19.2. BOOLEAN ALGEBRAS 323\\n\\nA similar argument demonstrates that b ⪯ (a ∨ b) ∨ c. Therefore, (a ∨ b) ∨ c is an upper\\nbound of {a, b, c}. We now need to show that (a∨ b)∨ c is the least upper bound of {a, b, c}.\\nLet u be some other upper bound of {a, b, c}. Then a ⪯ u and b ⪯ u; hence, d = a ∨ b ⪯ u.\\nSince c ⪯ u, it follows that (a ∨ b) ∨ c = d ∨ c ⪯ u. Therefore, (a ∨ b) ∨ c must be the least\\nupper bound of {a, b, c}. The argument that shows a ∨ (b ∨ c) is the least upper bound of\\n{a, b, c} is the same. Consequently, a ∨ (b ∨ c) = (a ∨ b) ∨ c.\\n\\n(3) The join of a and a is the least upper bound of {a}; hence, a ∨ a = a.\\n(4) Let d = a ∧ b. Then a ⪯ a ∨ d. On the other hand, d = a ∧ b ⪯ a, and so a ∨ d ⪯ a.\\n\\nTherefore, a ∨ (a ∧ b) = a.\\n\\nGiven any arbitrary set L with operations ∨ and ∧, satisfying the conditions of the\\nprevious theorem, it is natural to ask whether or not this set comes from some lattice. The\\nfollowing theorem says that this is always the case.\\n\\nTheorem 19.14. Let L be a nonempty set with two binary operations ∨ and ∧ satisfying\\nthe commutative, associative, idempotent, and absorption laws. We can define a partial\\norder on L by a ⪯ b if a ∨ b = b. Furthermore, L is a lattice with respect to ⪯ if for all\\na, b ∈ L, we define the least upper bound and greatest lower bound of a and b by a ∨ b and\\na ∧ b, respectively.\\n\\nProof. We first show that L is a poset under ⪯. Since a∨ a = a, a ⪯ a and ⪯ is reflexive.\\nTo show that ⪯ is antisymmetric, let a ⪯ b and b ⪯ a. Then a∨ b = b and b∨ a = a.By the\\ncommutative law, b = a ∨ b = b ∨ a = a. Finally, we must show that ⪯ is transitive. Let\\na ⪯ b and b ⪯ c. Then a ∨ b = b and b ∨ c = c. Thus,\\n\\na ∨ c = a ∨ (b ∨ c) = (a ∨ b) ∨ c = b ∨ c = c,\\n\\nor a ⪯ c.\\nTo show that L is a lattice, we must prove that a∨ b and a∧ b are, respectively, the least\\n\\nupper and greatest lower bounds of a and b. Since a = (a ∨ b) ∧ a = a ∧ (a ∨ b), it follows\\nthat a ⪯ a ∨ b. Similarly, b ⪯ a ∨ b. Therefore, a ∨ b is an upper bound for a and b. Let u\\nbe any other upper bound of both a and b. Then a ⪯ u and b ⪯ u. But a ∨ b ⪯ u since\\n\\n(a ∨ b) ∨ u = a ∨ (b ∨ u) = a ∨ u = u.\\n\\nThe proof that a ∧ b is the greatest lower bound of a and b is left as an exercise.\\n\\n19.2 Boolean Algebras\\nLet us investigate the example of the power set, P(X), of a set X more closely. The power\\nset is a lattice that is ordered by inclusion. By the definition of the power set, the largest\\nelement in P(X) is X itself and the smallest element is ∅, the empty set. For any set A\\nin P(X), we know that A ∩X = A and A ∪ ∅ = A. This suggests the following definition\\nfor lattices. An element I in a poset X is a largest element if a ⪯ I for all a ∈ X. An\\nelement O is a smallest element of X if O ⪯ a for all a ∈ X.\\n\\nLet A be in P(X). Recall that the complement of A is\\n\\nA′ = X \\\\A = {x : x ∈ X and x /∈ A}.\\n\\nWe know that A ∪ A′ = X and A ∩ A′ = ∅. We can generalize this example for lattices. A\\nlattice L with a largest element I and a smallest element O is complemented if for each\\na ∈ X, there exists an a′ such that a ∨ a′ = I and a ∧ a′ = O.\\n\\n\\n\\n324 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\nIn a lattice L, the binary operations ∨ and ∧ satisfy commutative and associative laws;\\nhowever, they need not satisfy the distributive law\\n\\na ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c);\\n\\nhowever, in P(X) the distributive law is satisfied since\\n\\nA ∩ (B ∪ C) = (A ∩B) ∪ (A ∩ C)\\n\\nfor A,B,C ∈ P(X). We will say that a lattice L is distributive if the following distributive\\nlaw holds:\\n\\na ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c)\\n\\nfor all a, b, c ∈ L.\\n\\nTheorem 19.15. A lattice L is distributive if and only if\\n\\na ∨ (b ∧ c) = (a ∨ b) ∧ (a ∨ c)\\n\\nfor all a, b, c ∈ L.\\n\\nProof. Let us assume that L is a distributive lattice.\\n\\na ∨ (b ∧ c) = [a ∨ (a ∧ c)] ∨ (b ∧ c)\\n= a ∨ [(a ∧ c) ∨ (b ∧ c)]\\n= a ∨ [(c ∧ a) ∨ (c ∧ b)]\\n= a ∨ [c ∧ (a ∨ b)]\\n= a ∨ [(a ∨ b) ∧ c]\\n= [(a ∨ b) ∧ a] ∨ [(a ∨ b) ∧ c]\\n= (a ∨ b) ∧ (a ∨ c).\\n\\nThe converse follows directly from the Duality Principle.\\n\\nA Boolean algebra is a lattice B with a greatest element I and a smallest element\\nO such that B is both distributive and complemented. The power set of X, P(X), is our\\nprototype for a Boolean algebra. As it turns out, it is also one of the most important\\nBoolean algebras. The following theorem allows us to characterize Boolean algebras in\\nterms of the binary relations ∨ and ∧ without mention of the fact that a Boolean algebra\\nis a poset.\\n\\nTheorem 19.16. A set B is a Boolean algebra if and only if there exist binary operations\\n∨ and ∧ on B satisfying the following axioms.\\n\\n1. a ∨ b = b ∨ a and a ∧ b = b ∧ a for a, b ∈ B.\\n\\n2. a ∨ (b ∨ c) = (a ∨ b) ∨ c and a ∧ (b ∧ c) = (a ∧ b) ∧ c for a, b, c ∈ B.\\n\\n3. a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c) and a ∨ (b ∧ c) = (a ∨ b) ∧ (a ∨ c) for a, b, c ∈ B.\\n\\n4. There exist elements I and O such that a ∨O = a and a ∧ I = a for all a ∈ B.\\n\\n5. For every a ∈ B there exists an a′ ∈ B such that a ∨ a′ = I and a ∧ a′ = O.\\n\\n\\n\\n19.2. BOOLEAN ALGEBRAS 325\\n\\nProof. Let B be a set satisfying (1)–(5) in the theorem. One of the idempotent laws is\\nsatisfied since\\n\\na = a ∨O\\n= a ∨ (a ∧ a′)\\n= (a ∨ a) ∧ (a ∨ a′)\\n= (a ∨ a) ∧ I\\n= a ∨ a.\\n\\nObserve that\\nI ∨ b = (I ∨ b) ∧ I = (I ∧ I) ∨ (b ∧ I) = I ∨ I = I.\\n\\nConsequently, the first of the two absorption laws holds, since\\n\\na ∨ (a ∧ b) = (a ∧ I) ∨ (a ∧ b)\\n= a ∧ (I ∨ b)\\n= a ∧ I\\n= a.\\n\\nThe other idempotent and absorption laws are proven similarly. Since B also satisfies (1)–\\n(3), the conditions of Theorem 19.14 are met; therefore, B must be a lattice. Condition (4)\\ntells us that B is a distributive lattice.\\n\\nFor a ∈ B, O ∨ a = a; hence, O ⪯ a and O is the smallest element in B. To show that\\nI is the largest element in B, we will first show that a ∨ b = b is equivalent to a ∧ b = a.\\nSince a ∨ I = a for all a ∈ B, using the absorption laws we can determine that\\n\\na ∨ I = (a ∧ I) ∨ I = I ∨ (I ∧ a) = I\\n\\nor a ⪯ I for all a in B. Finally, since we know that B is complemented by (5), B must be\\na Boolean algebra.\\n\\nConversely, suppose that B is a Boolean algebra. Let I and O be the greatest and least\\nelements in B, respectively. If we define a ∨ b and a ∧ b as least upper and greatest lower\\nbounds of {a, b}, then B is a Boolean algebra by Theorem 19.14, Theorem 19.15, and our\\nhypothesis.\\n\\nMany other identities hold in Boolean algebras. Some of these identities are listed in\\nthe following theorem.\\n\\nTheorem 19.17. Let B be a Boolean algebra. Then\\n\\n1. a ∨ I = I and a ∧O = O for all a ∈ B.\\n\\n2. If a ∨ b = a ∨ c and a ∧ b = a ∧ c for a, b, c ∈ B, then b = c.\\n\\n3. If a ∨ b = I and a ∧ b = O, then b = a′.\\n\\n4. (a′)′ = a for all a ∈ B.\\n\\n5. I ′ = O and O′ = I.\\n\\n6. (a ∨ b)′ = a′ ∧ b′ and (a ∧ b)′ = a′ ∨ b′ (De Morgan’s Laws).\\n\\n\\n\\n326 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\nProof. We will prove only (2). The rest of the identities are left as exercises. For a ∨ b =\\na ∨ c and a ∧ b = a ∧ c, we have\\n\\nb = b ∨ (b ∧ a)\\n= b ∨ (a ∧ b)\\n= b ∨ (a ∧ c)\\n= (b ∨ a) ∧ (b ∨ c)\\n= (a ∨ b) ∧ (b ∨ c)\\n= (a ∨ c) ∧ (b ∨ c)\\n= (c ∨ a) ∧ (c ∨ b)\\n= c ∨ (a ∧ b)\\n= c ∨ (a ∧ c)\\n= c ∨ (c ∧ a)\\n= c.\\n\\nFinite Boolean Algebras\\nA Boolean algebra is a finite Boolean algebra if it contains a finite number of elements\\nas a set. Finite Boolean algebras are particularly nice since we can classify them up to\\nisomorphism.\\n\\nLet B and C be Boolean algebras. A bijective map ϕ : B → C is an isomorphism of\\nBoolean algebras if\\n\\nϕ(a ∨ b) = ϕ(a) ∨ ϕ(b)\\nϕ(a ∧ b) = ϕ(a) ∧ ϕ(b)\\n\\nfor all a and b in B.\\nWe will show that any finite Boolean algebra is isomorphic to the Boolean algebra\\n\\nobtained by taking the power set of some finite set X. We will need a few lemmas and\\ndefinitions before we prove this result. Let B be a finite Boolean algebra. An element\\na ∈ B is an atom of B if a ̸= O and a ∧ b = a for all nonzero b ∈ B. Equivalently, a is an\\natom of B if there is no nonzero b ∈ B distinct from a such that O ⪯ b ⪯ a.\\n\\nLemma 19.18. Let B be a finite Boolean algebra. If b is a nonzero element of B, then\\nthere is an atom a in B such that a ⪯ b.\\n\\nProof. If b is an atom, let a = b. Otherwise, choose an element b1, not equal to O or b,\\nsuch that b1 ⪯ b. We are guaranteed that this is possible since b is not an atom. If b1 is an\\natom, then we are done. If not, choose b2, not equal to O or b1, such that b2 ⪯ b1. Again,\\nif b2 is an atom, let a = b2. Continuing this process, we can obtain a chain\\n\\nO ⪯ · · · ⪯ b3 ⪯ b2 ⪯ b1 ⪯ b.\\n\\nSince B is a finite Boolean algebra, this chain must be finite. That is, for some k, bk is an\\natom. Let a = bk.\\n\\nLemma 19.19. Let a and b be atoms in a finite Boolean algebra B such that a ̸= b. Then\\na ∧ b = O.\\n\\n\\n\\n19.2. BOOLEAN ALGEBRAS 327\\n\\nProof. Since a∧ b is the greatest lower bound of a and b, we know that a∧ b ⪯ a. Hence,\\neither a ∧ b = a or a ∧ b = O. However, if a ∧ b = a, then either a ⪯ b or a = O. In either\\ncase we have a contradiction because a and b are both atoms; therefore, a ∧ b = O.\\n\\nLemma 19.20. Let B be a Boolean algebra and a, b ∈ B. The following statements are\\nequivalent.\\n\\n1. a ⪯ b.\\n\\n2. a ∧ b′ = O.\\n\\n3. a′ ∨ b = I.\\n\\nProof. (1) ⇒ (2). If a ⪯ b, then a ∨ b = b. Therefore,\\n\\na ∧ b′ = a ∧ (a ∨ b)′\\n\\n= a ∧ (a′ ∧ b′)\\n= (a ∧ a′) ∧ b′\\n\\n= O ∧ b′\\n\\n= O.\\n\\n(2) ⇒ (3). If a ∧ b′ = O, then a′ ∨ b = (a ∧ b′)′ = O′ = I.\\n(3) ⇒ (1). If a′ ∨ b = I, then\\n\\na = a ∧ (a′ ∨ b)\\n= (a ∧ a′) ∨ (a ∧ b)\\n= O ∨ (a ∧ b)\\n= a ∧ b.\\n\\nThus, a ⪯ b.\\n\\nLemma 19.21. Let B be a Boolean algebra and b and c be elements in B such that b ̸⪯ c.\\nThen there exists an atom a ∈ B such that a ⪯ b and a ̸⪯ c.\\n\\nProof. By Lemma 19.20, b ∧ c′ ̸= O. Hence, there exists an atom a such that a ⪯ b ∧ c′.\\nConsequently, a ⪯ b and a ̸⪯ c.\\n\\nLemma 19.22. Let b ∈ B and a1, . . . , an be the atoms of B such that ai ⪯ b. Then\\nb = a1 ∨ · · · ∨ an. Furthermore, if a, a1, . . . , an are atoms of B such that a ⪯ b, ai ⪯ b, and\\nb = a ∨ a1 ∨ · · · ∨ an, then a = ai for some i = 1, . . . , n.\\n\\nProof. Let b1 = a1∨ · · ·∨an. Since ai ⪯ b for each i, we know that b1 ⪯ b. If we can show\\nthat b ⪯ b1, then the lemma is true by antisymmetry. Assume b ̸⪯ b1. Then there exists\\nan atom a such that a ⪯ b and a ̸⪯ b1. Since a is an atom and a ⪯ b, we can deduce that\\na = ai for some ai. However, this is impossible since a ⪯ b1. Therefore, b ⪯ b1.\\n\\nNow suppose that b = a1 ∨ · · · ∨ an. If a is an atom less than b,\\n\\na = a ∧ b = a ∧ (a1 ∨ · · · ∨ an) = (a ∧ a1) ∨ · · · ∨ (a ∧ an).\\n\\nBut each term is O or a with a ∧ ai occurring for only one ai. Hence, by Lemma 19.19,\\na = ai for some i.\\n\\nTheorem 19.23. Let B be a finite Boolean algebra. Then there exists a set X such that\\nB is isomorphic to P(X).\\n\\n\\n\\n328 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\nProof. We will show that B is isomorphic to P(X), where X is the set of atoms of B. Let\\na ∈ B. By Lemma 19.22, we can write a uniquely as a = a1 ∨ · · · ∨ an for a1, . . . , an ∈ X.\\nConsequently, we can define a map ϕ : B → P(X) by\\n\\nϕ(a) = ϕ(a1 ∨ · · · ∨ an) = {a1, . . . , an}.\\n\\nClearly, ϕ is onto.\\nNow let a = a1 ∨ · · · ∨an and b = b1 ∨ · · · ∨ bm be elements in B, where each ai and each\\n\\nbi is an atom. If ϕ(a) = ϕ(b), then {a1, . . . , an} = {b1, . . . , bm} and a = b. Consequently, ϕ\\nis injective.\\n\\nThe join of a and b is preserved by ϕ since\\n\\nϕ(a ∨ b) = ϕ(a1 ∨ · · · ∨ an ∨ b1 ∨ · · · ∨ bm)\\n\\n= {a1, . . . , an, b1, . . . , bm}\\n= {a1, . . . , an} ∪ {b1, . . . , bm}\\n= ϕ(a1 ∨ · · · ∨ an) ∪ ϕ(b1 ∧ · · · ∨ bm)\\n\\n= ϕ(a) ∪ ϕ(b).\\n\\nSimilarly, ϕ(a ∧ b) = ϕ(a) ∩ ϕ(b).\\n\\nWe leave the proof of the following corollary as an exercise.\\n\\nCorollary 19.24. The order of any finite Boolean algebra must be 2n for some positive\\ninteger n.\\n\\n19.3 The Algebra of Electrical Circuits\\nThe usefulness of Boolean algebras has become increasingly apparent over the past several\\ndecades with the development of the modern computer. The circuit design of computer\\nchips can be expressed in terms of Boolean algebras. In this section we will develop the\\nBoolean algebra of electrical circuits and switches; however, these results can easily be\\ngeneralized to the design of integrated computer circuitry.\\n\\nA switch is a device, located at some point in an electrical circuit, that controls the\\nflow of current through the circuit. Each switch has two possible states: it can be open,\\nand not allow the passage of current through the circuit, or a it can be closed, and allow\\nthe passage of current. These states are mutually exclusive. We require that every switch\\nbe in one state or the other—a switch cannot be open and closed at the same time. Also, if\\none switch is always in the same state as another, we will denote both by the same letter;\\nthat is, two switches that are both labeled with the same letter a will always be open at\\nthe same time and closed at the same time.\\n\\nGiven two switches, we can construct two fundamental types of circuits. Two switches a\\nand b are in series if they make up a circuit of the type that is illustrated in Figure 19.25.\\nCurrent can pass between the terminals A and B in a series circuit only if both of the\\nswitches a and b are closed. We will denote this combination of switches by a ∧ b. Two\\nswitches a and b are in parallel if they form a circuit of the type that appears in Fig-\\nure 19.26. In the case of a parallel circuit, current can pass between A and B if either one\\nof the switches is closed. We denote a parallel combination of circuits a and b by a ∨ b.\\n\\nA a b B\\n\\nFigure 19.25: a ∧ b\\n\\n\\n\\n19.3. THE ALGEBRA OF ELECTRICAL CIRCUITS 329\\n\\nA\\n\\na\\n\\nb\\n\\nB\\n\\nFigure 19.26: a ∨ b\\n\\nWe can build more complicated electrical circuits out of series and parallel circuits by\\nreplacing any switch in the circuit with one of these two fundamental types of circuits.\\nCircuits constructed in this manner are called series-parallel circuits.\\n\\nWe will consider two circuits equivalent if they act the same. That is, if we set the\\nswitches in equivalent circuits exactly the same we will obtain the same result. For example,\\nin a series circuit a∧b is exactly the same as b∧a. Notice that this is exactly the commutative\\nlaw for Boolean algebras. In fact, the set of all series-parallel circuits forms a Boolean algebra\\nunder the operations of ∨ and ∧. We can use diagrams to verify the different axioms of\\na Boolean algebra. The distributive law, a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c), is illustrated in\\nFigure 19.27. If a is a switch, then a′ is the switch that is always open when a is closed and\\nalways closed when a is open. A circuit that is always closed is I in our algebra; a circuit\\nthat is always open is O. The laws for a∧ a′ = O and a∨ a′ = I are shown in Figure 19.28.\\n\\na\\n\\nb\\n\\nc\\n\\na b\\n\\na c\\n\\nFigure 19.27: a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c)\\n\\na a′\\n\\na\\n\\na′\\n\\nFigure 19.28: a ∧ a′ = O and a ∨ a′ = I\\n\\nExample 19.29. Every Boolean expression represents a switching circuit. For example,\\ngiven the expression (a∨ b)∧ (a∨ b′)∧ (a∨ b), we can construct the circuit in Figure 19.32.\\n\\nTheorem 19.30. The set of all circuits is a Boolean algebra.\\n\\nWe leave as an exercise the proof of this theorem for the Boolean algebra axioms not\\nyet verified. We can now apply the techniques of Boolean algebras to switching theory.\\n\\nExample 19.31. Given a complex circuit, we can now apply the techniques of Boolean\\n\\n\\n\\n330 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\nalgebra to reduce it to a simpler one. Consider the circuit in Figure 19.32. Since\\n\\n(a ∨ b) ∧ (a ∨ b′) ∧ (a ∨ b) = (a ∨ b) ∧ (a ∨ b) ∧ (a ∨ b′)\\n= (a ∨ b) ∧ (a ∨ b′)\\n= a ∨ (b ∧ b′)\\n= a ∨O\\n= a,\\n\\nwe can replace the more complicated circuit with a circuit containing the single switch a\\nand achieve the same function.\\n\\na\\n\\nb\\n\\na\\n\\nb′\\n\\na\\n\\nb\\n\\nFigure 19.32: (a ∨ b) ∧ (a ∨ b′) ∧ (a ∨ b)\\n\\nHistorical Note\\n\\nGeorge Boole (1815–1864) was the first person to study lattices. In 1847, he published\\nThe Investigation of the Laws of Thought, a book in which he used lattices to formalize logic\\nand the calculus of propositions. Boole believed that mathematics was the study of form\\nrather than of content; that is, he was not so much concerned with what he was calculating\\nas with how he was calculating it. Boole’s work was carried on by his friend Augustus De\\nMorgan (1806–1871). De Morgan observed that the principle of duality often held in set\\ntheory, as is illustrated by De Morgan’s laws for set theory. He believed, as did Boole, that\\nmathematics was the study of symbols and abstract operations.\\n\\nSet theory and logic were further advanced by such mathematicians as Alfred North\\nWhitehead (1861–1947), Bertrand Russell (1872–1970), and David Hilbert (1862–1943). In\\nPrincipia Mathematica, Whitehead and Russell attempted to show the connection between\\nmathematics and logic by the deduction of the natural number system from the rules of\\nformal logic. If the natural numbers could be determined from logic itself, then so could\\nmuch of the rest of existing mathematics. Hilbert attempted to build up mathematics\\nby using symbolic logic in a way that would prove the consistency of mathematics. His\\napproach was dealt a mortal blow by Kurt Gödel (1906–1978), who proved that there will\\nalways be “undecidable” problems in any sufficiently rich axiomatic system; that is, that\\nin any mathematical system of any consequence, there will always be statements that can\\nnever be proven either true or false.\\n\\nAs often occurs, this basic research in pure mathematics later became indispensable in a\\nwide variety of applications. Boolean algebras and logic have become essential in the design\\nof the large-scale integrated circuitry found on today’s computer chips. Sociologists have\\nused lattices and Boolean algebras to model social hierarchies; biologists have used them to\\ndescribe biosystems.\\n\\n\\n\\n19.4. EXERCISES 331\\n\\n19.4 Exercises\\n1. Draw the lattice diagram for the power set of X = {a, b, c, d} with the set inclusion\\nrelation, ⊆.\\n\\n2. Draw the diagram for the set of positive integers that are divisors of 30. Is this poset a\\nBoolean algebra?\\n\\n3. Draw a diagram of the lattice of subgroups of Z12.\\n\\n4. Let B be the set of positive integers that are divisors of 36. Define an order on B by\\na ⪯ b if a | b. Prove that B is a Boolean algebra. Find a set X such that B is isomorphic\\nto P(X).\\n\\n5. Prove or disprove: Z is a poset under the relation a ⪯ b if a | b.\\n\\n6. Draw the switching circuit for each of the following Boolean expressions.\\n\\n(a) (a ∨ b ∨ a′) ∧ a\\n(b) (a ∨ b)′ ∧ (a ∨ b)\\n\\n(c) a ∨ (a ∧ b)\\n(d) (c ∨ a ∨ b) ∧ c′ ∧ (a ∨ b)′\\n\\n7. Draw a circuit that will be closed exactly when only one of three switches a, b, and c\\nare closed.\\n\\n8. Prove or disprove that the two circuits shown are equivalent.\\n\\na b c\\n\\na′ b\\n\\na c′\\n\\na\\n\\na\\n\\nb\\n\\nc′\\n\\n9. Let X be a finite set containing n elements. Prove that P(X) = 2n. Conclude that the\\norder of any finite Boolean algebra must be 2n for some n ∈ N.\\n\\n10. For each of the following circuits, write a Boolean expression. If the circuit can be\\nreplaced by one with fewer switches, give the Boolean expression and draw a diagram for\\nthe new circuit.\\n\\na b c\\n\\na′ b′ c\\n\\na b′ c′\\n\\na a b\\n\\na′\\n\\nb a′ b\\n\\na′\\n\\na b′\\n\\nb\\n\\n\\n\\n332 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\n11. Prove or disprove: The set of all nonzero integers is a lattice, where a ⪯ b is defined by\\na | b.\\n\\n12. Let L be a nonempty set with two binary operations ∨ and ∧ satisfying the commuta-\\ntive, associative, idempotent, and absorption laws. We can define a partial order on L, as\\nin Theorem 19.14, by a ⪯ b if a ∨ b = b. Prove that the greatest lower bound of a and b is\\na ∧ b.\\n\\n13. Let G be a group and X be the set of subgroups of G ordered by set-theoretic inclusion.\\nIf H and K are subgroups of G, show that the least upper bound of H and K is the subgroup\\ngenerated by H ∪K.\\n\\n14. Let R be a ring and suppose that X is the set of ideals of R. Show that X is a poset\\nordered by set-theoretic inclusion, ⊆. Define the meet of two ideals I and J in X by I ∩ J\\nand the join of I and J by I + J . Prove that the set of ideals of R is a lattice under these\\noperations.\\n\\n15. Let B be a Boolean algebra. Prove each of the following identities.\\n(a) a ∨ I = I and a ∧O = O for all a ∈ B.\\n(b) If a ∨ b = I and a ∧ b = O, then b = a′.\\n(c) (a′)′ = a for all a ∈ B.\\n(d) I ′ = O and O′ = I.\\n(e) (a ∨ b)′ = a′ ∧ b′ and (a ∧ b)′ = a′ ∨ b′ (De Morgan’s laws).\\n\\n16. By drawing the appropriate diagrams, complete the proof of Theorem 19.30 to show\\nthat the switching functions form a Boolean algebra.\\n\\n17. Let B be a Boolean algebra. Define binary operations + and · on B by\\n\\na+ b = (a ∧ b′) ∨ (a′ ∧ b)\\na · b = a ∧ b.\\n\\nProve that B is a commutative ring under these operations satisfying a2 = a for all a ∈ B.\\n\\n18. Let X be a poset such that for every a and b in X, either a ⪯ b or b ⪯ a. Then X is\\nsaid to be a totally ordered set.\\n(a) Is a | b a total order on N?\\n(b) Prove that N, Z, Q, and R are totally ordered sets under the usual ordering ≤.\\n\\n19. Let X and Y be posets. A map ϕ : X → Y is order-preserving if a ⪯ b implies that\\nϕ(a) ⪯ ϕ(b). Let L and M be lattices. A map ψ : L→M is a lattice homomorphism if\\nψ(a∨ b) = ψ(a)∨ψ(b) and ψ(a∧ b) = ψ(a)∧ψ(b). Show that every lattice homomorphism\\nis order-preserving, but that it is not the case that every order-preserving homomorphism\\nis a lattice homomorphism.\\n\\n20. Let B be a Boolean algebra. Prove that a = b if and only if (a ∧ b′) ∨ (a′ ∧ b) = O for\\na, b ∈ B.\\n\\n21. Let B be a Boolean algebra. Prove that a = O if and only if (a ∧ b′) ∨ (a′ ∧ b) = b for\\nall b ∈ B.\\n\\n22. Let L and M be lattices. Define an order relation on L×M by (a, b) ⪯ (c, d) if a ⪯ c\\nand b ⪯ d. Show that L×M is a lattice under this partial order.\\n\\n\\n\\n19.5. PROGRAMMING EXERCISES 333\\n\\n19.5 Programming Exercises\\n1. A Boolean or switching function on n variables is a map f : {O, I}n → {0, I}.\\nA Boolean polynomial is a special type of Boolean function: it is any type of Boolean\\nexpression formed from a finite combination of variables x1, . . . , xn together with O and I,\\nusing the operations ∨, ∧, and ′. The values of the functions are defined in Table 19.33.\\nWrite a program to evaluate Boolean polynomials.\\n\\nx y x′ x ∨ y x ∧ y\\n0 0 1 0 0\\n0 1 1 1 0\\n1 0 0 1 0\\n1 1 0 1 1\\n\\nTable 19.33: Boolean polynomials\\n\\n19.6 References and Suggested Readings\\n[1] Donnellan, T. Lattice Theory. Pergamon Press, Oxford, 1968.\\n[2] Halmos, P. R. “The Basic Concepts of Algebraic Logic,” American Mathematical\\n\\nMonthly 53(1956), 363–87.\\n[3] Hohn, F. “Some Mathematical Aspects of Switching,” American Mathematical Monthly\\n\\n62(1955), 75–90.\\n[4] Hohn, F. Applied Boolean Algebra. 2nd ed. Macmillan, New York, 1966.\\n[5] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\\n[6] Whitesitt, J. Boolean Algebra and Its Applications. Dover, Mineola, NY, 2010.\\n\\n19.7 Sage\\nSage has support for both partially ordered sets (“posets”) and lattices, and does an excellent\\njob of providing visual depictions of both.\\n\\nCreating Partially Ordered Sets\\n\\nExample 19.6 in the text is a good example to replicate as a demonstration of Sage com-\\nmands. We first define the elements of the set X.\\n\\nX = (24).divisors ()\\nX\\n\\n[1, 2, 3, 4, 6, 8, 12, 24]\\n\\nOne approach to creating the relation is to specify every instance where one element is\\ncomparable to the another. So we build a list of pairs, where each pair contains comparable\\nelements, with the lesser one first. This is the set of relations.\\n\\nR = [(a,b) for a in X for b in X if a.divides(b)]; R\\n\\n\\n\\n334 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\n[(1, 1), (1, 2), (1, 3), (1, 4), (1, 6), (1, 8), (1, 12), (1, 24),\\n(2, 2), (2, 4), (2, 6), (2, 8), (2, 12), (2, 24), (3, 3), (3, 6),\\n(3, 12), (3, 24), (4, 4), (4, 8), (4, 12), (4, 24), (6, 6),\\n(6, 12), (6, 24), (8, 8), (8, 24), (12, 12), (12, 24), (24, 24)]\\n\\nWe construct the poset by giving the the Poset constructor a list containing the elements\\nand the relations. We can then easily get a “plot” of the poset. Notice the plot just shows\\nthe “cover relations” — a minimal set of comparisons which the assumption of transitivity\\nwould expand into the set of all the relations.\\n\\nD = Poset([X, R])\\nD.plot()\\n\\nAnother approach to creating a Poset is to let the poset constructor run over all the\\npairs of elements, and all we do is give the constructor a way to test if two elements are\\ncomparable. Our comparison function should expect two elements and then return True or\\nFalse. A “lambda” function is one way to quickly build such a function. This may be a\\nnew idea for you, but mastering lambda functions can be a great convenience. Notice that\\n“lambda” is a word reserved for just this purpose (so, for example, lambda is a bad choice\\nfor the name of an eigenvalue of a matrix). There are other ways to make functions in Sage,\\nbut a lambda function is quickest when the function is simple.\\n\\ndivisible = lambda x, y: x.divides(y)\\nL = Poset([X, divisible ])\\nL == D\\n\\nTrue\\n\\nL.plot()\\n\\nSage also has a collection of stock posets. Some are one-shot constructions, while others\\nare members of parameterized families. Use tab-completion on Posets. to see the full list.\\nHere are some examples.\\n\\nA one-shot construction. Perhaps what you would expect, though there might be other,\\nequally plausible, alternatives.\\n\\nQ = Posets.PentagonPoset ()\\nQ.plot()\\n\\nA parameterized family. This is the classic example where the elements are subsets of a\\nset with n elements and the relation is “subset of.”\\n\\nS = Posets.BooleanLattice (4)\\nS.plot()\\n\\nAnd random posets. These can be useful for testing and experimenting, but are unlikely\\nto exhibit special cases that may be important. You might run the following command many\\ntimes and vary the second argument, which is a rough upper bound on the probability any\\ntwo elements are comparable. Remember that the plot only shows the cover relations. The\\nmore elements that are comparable, the more “vertically stretched” the plot will be.\\n\\nT = Posets.RandomPoset (20 ,0.05)\\nT.plot()\\n\\n\\n\\n19.7. SAGE 335\\n\\nProperties of a Poset\\nOnce you have a poset, what can you do with it? Let’s return to our first example, D.\\nWe can of course determine if one element is less than another, which is the fundamental\\nstructure of a poset.\\n\\nD.is_lequal(4, 8)\\n\\nTrue\\n\\nD.is_lequal(4, 4)\\n\\nTrue\\n\\nD.is_less_than (4, 8)\\n\\nTrue\\n\\nD.is_less_than (4, 4)\\n\\nFalse\\n\\nD.is_lequal(6, 8)\\n\\nFalse\\n\\nD.is_lequal(8, 6)\\n\\nFalse\\n\\nNotice that 6 and 8 are not comparable in this poset (it is a partial order). The methods\\n.is_gequal() and .is_greater_than() work similarly, but returns True if the first element is\\ngreater (or equal).\\n\\nD.is_gequal(8, 4)\\n\\nTrue\\n\\nD.is_greater_than (4, 8)\\n\\nFalse\\n\\nWe can find the largest and smallest elements of a poset. This is a random poset built\\nwith a 10%probability, but copied here to be repeatable.\\n\\nX = range (20)\\nC = [[18, 7], [9, 11], [9, 10], [11, 8], [6, 10],\\n\\n[10, 2], [0, 2], [2, 1], [1, 8], [8, 12],\\n[8, 3], [3, 15], [15, 7], [7, 16], [7, 4],\\n[16, 17], [16, 13], [4, 19], [4, 14], [14, 5]]\\n\\nP = Poset([X, C])\\nP.plot()\\n\\nP.minimal_elements ()\\n\\n[18, 9, 6, 0]\\n\\n\\n\\n336 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\nP.maximal_elements ()\\n\\n[5, 19, 13, 17, 12]\\n\\nElements of a poset can be partioned into level sets. In plots of posets, elements at the\\nsame level are plotted vertically at the same height. Each level set is obtained by removing\\nall of the previous level sets and then taking the minimal elements of the result.\\n\\nP.level_sets ()\\n\\n[[18, 9, 6, 0], [11, 10], [2], [1], [8], [3, 12],\\n[15], [7], [16, 4], [13, 17, 14, 19], [5]]\\n\\nIf we make two elements in R comparable when they had not previously been, this is an\\nextension of R. Consider all possible extensions of one poset — we can make a poset from\\nall of these, where set inclusion is the relation. A linear extension is a maximal element in\\nthis poset of posets. Informally, we are adding as many new relations as possible, consistent\\nwith the original poset and so that the result is a total order. In other words, there is an\\nordering of the elements that is consistent with the order in the poset. We can build such a\\nthing, but the output is just a list of the elements in the linear order. A computer scientist\\nwould be inclined to call this a “topological sort.”\\n\\nlinear = P.linear_extension (); linear\\n\\n[18, 9, 11, 6, 10, 0, 2, 1, 8, 3, 15,\\n7, 4, 14, 5, 19, 16, 13, 17, 12]\\n\\nWe can construct subposets by giving a set of elements to induce the new poset. Here\\nwe take roughly the “bottom half” of the random poset P by inducing the subposet on a\\nunion of some of the level sets.\\n\\nlevel = P.level_sets ()\\nbottomhalf = sum([level[i] for i in range (5)], [])\\nB = P.subposet(bottomhalf)\\nB.plot()\\n\\nThe dual of a poset retains the same set of elements, but reverses any comparisons.\\nPdual = P.dual()\\nPdual.plot()\\n\\nTaking the dual of the divisibility poset from Example 19.6 would be like changing the\\nrelation to “is a multiple of.”\\n\\nDdual = D.dual()\\nDdual.plot()\\n\\nLattices\\nEvery lattice is a poset, so all the commands above will perform equally well for a lattice.\\nBut how do you create a lattice? Simple — first create a poset and then feed it into the\\nLatticePoset() constructor. But realize that just because you give this constructor a poset,\\nit does not mean a lattice will always come back out. Only if the poset is already a lattice will\\nit get upgraded from a poset to a lattice for Sage’s purposes, and you will get a ValueError\\n\\nif the upgrade is not possible. Finally, notice that some of the posets Sage constructs are\\nalready recognized as lattices, such as the prototypical BooleanLattice.\\n\\n\\n\\n19.7. SAGE 337\\n\\nP = Posets.AntichainPoset (8)\\nP.is_lattice ()\\n\\nFalse\\n\\nLatticePoset(P)\\n\\nTraceback (most recent call last):\\n...\\nValueError: Not a lattice.\\n\\nAn integer composition of n is a list of positive integers that sum to n. A composition\\nC1 covers a composition C2 if C2 can be formed from C1 by adding consecutive parts. For\\nexample, C1 = [2, 1, 2] ⪰ [3, 2] = C2. With this relation, the set of all integer compositions\\nof a fixed integer n is a poset that is also a lattice.\\n\\nCP = Posets.IntegerCompositions (5)\\nC = LatticePoset(CP)\\nC.plot()\\n\\nA meet or a join is a fundamental operation in a lattice.\\npar = C.an_element ().parent ()\\na = par([1, 1, 1, 2])\\nb = par([2, 1, 1, 1])\\na, b\\n\\n([1, 1, 1, 2], [2, 1, 1, 1])\\n\\nC.meet(a, b)\\n\\n[2, 1, 2]\\n\\nc = par([1, 4])\\nd = par([2, 3])\\nc, d\\n\\n([1, 4], [2, 3])\\n\\nC.join(c, d)\\n\\n[1, 1, 3]\\n\\nOnce a poset is upgraded to lattice status, then additional commands become available,\\nor the character of their results changes.\\n\\nAn example of the former is the .is_distributive() method.\\nC.is_distributive ()\\n\\nTrue\\n\\nAn example of the latter is the .top() method. What your text calls a largest element\\nand a smallest element of a lattice, Sage calls a top and a bottom. For a poset, .top()\\n\\nand .bottom() may return an element or may not (returning None), but for a lattice it is\\nguaranteed to return exactly one element.\\n\\n\\n\\n338 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\\n\\nC.top()\\n\\n[1, 1, 1, 1, 1]\\n\\nC.bottom ()\\n\\n[5]\\n\\nNotice that the returned values are all elements of the lattice, in this case ordered lists\\nof integers summing to 5.\\n\\nComplements now make sense in a lattice. The result of the .complements() method is a\\ndictionary that uses elements of the lattice as the keys. We say the dictionary is “indexed”\\nby the elements of the lattice. The result is a list of the complements of the element. We call\\nthis the “value” of the key-value pair. (You may know dictionaries as “associative arrays”,\\nbut they are really just fancy functions.)\\n\\ncomp = C.complements ()\\ncomp[par([1, 1, 1, 2])]\\n\\n[[4, 1]]\\n\\nThe lattice of integer compositions is a complemented lattice, as we can see by the result\\nthat each element has a single (unique) complement, evidenced by the lists of length 1 in the\\nvalues of the dictionary. Or we can just ask Sage via .is_complemented(). Dictionaries have\\nno inherent order, so you may get different output each time you inspect the dictionary.\\n\\ncomp\\n\\n{[1, 1, 1, 1, 1]: [[5]],\\n[1, 1, 1, 2]: [[4, 1]],\\n[1, 1, 2, 1]: [[3, 2]],\\n[1, 1, 3]: [[3, 1, 1]],\\n[1, 2, 1, 1]: [[2, 3]],\\n[1, 2, 2]: [[2, 2, 1]],\\n[1, 3, 1]: [[2, 1, 2]],\\n[1, 4]: [[2, 1, 1, 1]],\\n[2, 1, 1, 1]: [[1, 4]],\\n[2, 1, 2]: [[1, 3, 1]],\\n[2, 2, 1]: [[1, 2, 2]],\\n[2, 3]: [[1, 2, 1, 1]],\\n[3, 1, 1]: [[1, 1, 3]],\\n[3, 2]: [[1, 1, 2, 1]],\\n[4, 1]: [[1, 1, 1, 2]],\\n[5]: [[1, 1, 1, 1, 1]]}\\n\\n[len(e[1]) for e in comp.items()]\\n\\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\\n\\nC.is_complemented ()\\n\\nTrue\\n\\nThere are many more commands which apply to posets and lattices, so build a few and\\nuse tab-completion liberally to explore. There is more to discover than we can cover in just\\na single chapter, but you now have the basic tools to profitably study posets and lattices in\\nSage.\\n\\n\\n\\n19.8. SAGE EXERCISES 339\\n\\n19.8 Sage Exercises\\n1. Use R = Posets.RandomPoset(30,0.05) to construct a random poset. Use R.plot() to get\\nan idea of what you have built.\\n(a) Illustrate the use of the poset methods: .is_lequal(), .is_less_than(), .is_gequal(),\\n\\nand .is_greater_than() to determine if two specific elements (of your choice) are related\\nor incomparable.\\n\\n(b) Use .minimal_elements() and .maximal_elements() to find the smallest and largest ele-\\nments of your poset.\\n\\n(c) Use LatticePoset(R) to see if the poset R is a lattice by attempting to convert it into a\\nlattice.\\n\\n(d) Find a linear extension of your poset. Confirm that any pair of elements that are\\ncomparable in the poset will be similarly comparable in the linear extension.\\n\\n2. Construct the poset on the positive divisors of 72 = 23 ·32 with divisiblity as the relation,\\nand then convert to a lattice.\\n(a) Determine the one and zero element using .top() and .bottom().\\n(b) Determine all the pairs of elements of the lattice that are complements of each other\\n\\nwithout using the .complement() method, but rather just use the .meet() and .join()\\n\\nmethods. Extra credit if you can output each pair just once.\\n(c) Determine if the lattice is distributive using just the .meet() and .join() methods, and\\n\\nnot the .is_distributive() method.\\n\\n3. Construct several specific diamond lattices with Posets.DiamondPoset(n) by varying the\\nvalue of n. Once you feel you have enough empirical evidence, give answers, with justifi-\\ncations, to the following questions for general values of n, based on observations obtained\\nfrom your experiments with Sage.\\n(a) Which elements have complements and which do not, and why?\\n(b) Read the documentation of the .antichains() method to learn what an antichain is.\\n\\nHow many antichains are there?\\n(c) Is the lattice distributive?\\n\\n4. Use Posets.BooleanLattice(4) to construct an instance of the prototypical Boolean alge-\\nbra on 16 elements (i.e., all subsets of a 4-set).\\nThen use Posets.IntegerCompositions(5) to construct the poset whose 16 elements are the\\ncompositions of the integer 5. We have seen above that the integer composition lattice is\\ndistributive and complemented, making it a Boolean algebra. And by Theorem 19.23 we\\ncan conclude that these two Boolean algebras are isomorphic.\\nUse the .plot() method to see the similarity visually. Then use the method .hasse_diagram()\\n\\non each poset to obtain a directed graph (which you can also plot, though the embedding\\ninto the plane may not be as informative). Employ the graph method .is_isomorphic() to\\nsee that the two Hasse diagrams really are the “same.”\\n\\n5. (Advanced) For the previous question, construct an explicit isomorphism between the two\\nBoolean algebras. This would be a bijective function (constructed with the def command)\\nthat converts compositions into sets (or if, you choose, sets into compositions) and which\\nrespects the meet and join operations. You can test and illustrate your function by its\\ninteraction with specific elements evaluated in the meet and join operations, as described\\nin the definition of an isomorphism of Boolean algebras.\\n\\n\\n\\n20\\n\\nVector Spaces\\n\\nIn a physical system a quantity can often be described with a single number. For example,\\nwe need to know only a single number to describe temperature, mass, or volume. However,\\nfor some quantities, such as location, we need several numbers. To give the location of\\na point in space, we need x, y, and z coordinates. Temperature distribution over a solid\\nobject requires four numbers: three to identify each point within the object and a fourth\\nto describe the temperature at that point. Often n-tuples of numbers, or vectors, also have\\ncertain algebraic properties, such as addition or scalar multiplication.\\n\\nIn this chapter we will examine mathematical structures called vector spaces. As with\\ngroups and rings, it is desirable to give a simple list of axioms that must be satisfied to\\nmake a set of vectors a structure worth studying.\\n\\n20.1 Definitions and Examples\\nA vector space V over a field F is an abelian group with a scalar product α · v or αv\\ndefined for all α ∈ F and all v ∈ V satisfying the following axioms.\\n\\n• α(βv) = (αβ)v;\\n\\n• (α+ β)v = αv + βv;\\n\\n• α(u+ v) = αu+ αv;\\n\\n• 1v = v;\\n\\nwhere α, β ∈ F and u, v ∈ V .\\nThe elements of V are called vectors; the elements of F are called scalars. It is\\n\\nimportant to notice that in most cases two vectors cannot be multiplied. In general, it is\\nonly possible to multiply a vector with a scalar. To differentiate between the scalar zero\\nand the vector zero, we will write them as 0 and 0, respectively.\\n\\nLet us examine several examples of vector spaces. Some of them will be quite familiar;\\nothers will seem less so.\\n\\nExample 20.1. The n-tuples of real numbers, denoted by Rn, form a vector space over R.\\nGiven vectors u = (u1, . . . , un) and v = (v1, . . . , vn) in Rn and α in R, we can define vector\\naddition by\\n\\nu+ v = (u1, . . . , un) + (v1, . . . , vn) = (u1 + v1, . . . , un + vn)\\n\\nand scalar multiplication by\\n\\nαu = α(u1, . . . , un) = (αu1, . . . , αun).\\n\\n340\\n\\n\\n\\n20.2. SUBSPACES 341\\n\\nExample 20.2. If F is a field, then F [x] is a vector space over F . The vectors in F [x]\\nare simply polynomials, and vector addition is just polynomial addition. If α ∈ F and\\np(x) ∈ F [x], then scalar multiplication is defined by αp(x).\\n\\nExample 20.3. The set of all continuous real-valued functions on a closed interval [a, b] is\\na vector space over R. If f(x) and g(x) are continuous on [a, b], then (f + g)(x) is defined\\nto be f(x) + g(x). Scalar multiplication is defined by (αf)(x) = αf(x) for α ∈ R. For\\nexample, if f(x) = sinx and g(x) = x2, then (2f + 5g)(x) = 2 sinx+ 5x2.\\n\\nExample 20.4. Let V = Q(\\n√\\n2 ) = {a + b\\n\\n√\\n2 : a, b ∈ Q}. Then V is a vector space over\\n\\nQ. If u = a+ b\\n√\\n2 and v = c+ d\\n\\n√\\n2, then u+ v = (a+ c) + (b+ d)\\n\\n√\\n2 is again in V . Also,\\n\\nfor α ∈ Q, αv is in V . We will leave it as an exercise to verify that all of the vector space\\naxioms hold for V .\\n\\nProposition 20.5. Let V be a vector space over F . Then each of the following statements\\nis true.\\n\\n1. 0v = 0 for all v ∈ V .\\n\\n2. α0 = 0 for all α ∈ F .\\n\\n3. If αv = 0, then either α = 0 or v = 0.\\n\\n4. (−1)v = −v for all v ∈ V .\\n\\n5. −(αv) = (−α)v = α(−v) for all α ∈ F and all v ∈ V .\\n\\nProof. To prove (1), observe that\\n\\n0v = (0 + 0)v = 0v + 0v;\\n\\nconsequently, 0 + 0v = 0v + 0v. Since V is an abelian group, 0 = 0v.\\nThe proof of (2) is almost identical to the proof of (1). For (3), we are done if α = 0.\\n\\nSuppose that α ̸= 0. Multiplying both sides of αv = 0 by 1/α, we have v = 0.\\nTo show (4), observe that\\n\\nv + (−1)v = 1v + (−1)v = (1− 1)v = 0v = 0,\\n\\nand so −v = (−1)v. We will leave the proof of (5) as an exercise.\\n\\n20.2 Subspaces\\nJust as groups have subgroups and rings have subrings, vector spaces also have substruc-\\ntures. Let V be a vector space over a field F , and W a subset of V . Then W is a subspace\\nof V if it is closed under vector addition and scalar multiplication; that is, if u, v ∈W and\\nα ∈ F , it will always be the case that u+ v and αv are also in W .\\n\\nExample 20.6. Let W be the subspace of R3 defined by W = {(x1, 2x1 + x2, x1 − x2) :\\nx1, x2 ∈ R}. We claim that W is a subspace of R3. Since\\n\\nα(x1, 2x1 + x2, x1 − x2) = (αx1, α(2x1 + x2), α(x1 − x2))\\n\\n= (αx1, 2(αx1) + αx2, αx1 − αx2),\\n\\nW is closed under scalar multiplication. To show that W is closed under vector addition,\\nlet u = (x1, 2x1 + x2, x1 − x2) and v = (y1, 2y1 + y2, y1 − y2) be vectors in W . Then\\n\\nu+ v = (x1 + y1, 2(x1 + y1) + (x2 + y2), (x1 + y1)− (x2 + y2)).\\n\\n\\n\\n342 CHAPTER 20. VECTOR SPACES\\n\\nExample 20.7. Let W be the subset of polynomials of F [x] with no odd-power terms. If\\np(x) and q(x) have no odd-power terms, then neither will p(x)+ q(x). Also, αp(x) ∈W for\\nα ∈ F and p(x) ∈W .\\n\\nLet V be any vector space over a field F and suppose that v1, v2, . . . , vn are vectors in\\nV and α1, α2, . . . , αn are scalars in F . Any vector w in V of the form\\n\\nw =\\n\\nn∑\\ni=1\\n\\nαivi = α1v1 + α2v2 + · · ·+ αnvn\\n\\nis called a linear combination of the vectors v1, v2, . . . , vn. The spanning set of vec-\\ntors v1, v2, . . . , vn is the set of vectors obtained from all possible linear combinations of\\nv1, v2, . . . , vn. If W is the spanning set of v1, v2, . . . , vn, then we say that W is spanned by\\nv1, v2, . . . , vn.\\n\\nProposition 20.8. Let S = {v1, v2, . . . , vn} be vectors in a vector space V . Then the span\\nof S is a subspace of V .\\n\\nProof. Let u and v be in S. We can write both of these vectors as linear combinations of\\nthe vi’s:\\n\\nu = α1v1 + α2v2 + · · ·+ αnvn\\n\\nv = β1v1 + β2v2 + · · ·+ βnvn.\\n\\nThen\\nu+ v = (α1 + β1)v1 + (α2 + β2)v2 + · · ·+ (αn + βn)vn\\n\\nis a linear combination of the vi’s. For α ∈ F ,\\n\\nαu = (αα1)v1 + (αα2)v2 + · · ·+ (ααn)vn\\n\\nis in the span of S.\\n\\n20.3 Linear Independence\\nLet S = {v1, v2, . . . , vn} be a set of vectors in a vector space V . If there exist scalars\\nα1, α2 . . . αn ∈ F such that not all of the αi’s are zero and\\n\\nα1v1 + α2v2 + · · ·+ αnvn = 0,\\n\\nthen S is said to be linearly dependent. If the set S is not linearly dependent, then it is\\nsaid to be linearly independent. More specifically, S is a linearly independent set if\\n\\nα1v1 + α2v2 + · · ·+ αnvn = 0\\n\\nimplies that\\nα1 = α2 = · · · = αn = 0\\n\\nfor any set of scalars {α1, α2 . . . αn}.\\n\\nProposition 20.9. Let {v1, v2, . . . , vn} be a set of linearly independent vectors in a vector\\nspace. Suppose that\\n\\nv = α1v1 + α2v2 + · · ·+ αnvn = β1v1 + β2v2 + · · ·+ βnvn.\\n\\nThen α1 = β1, α2 = β2, . . . , αn = βn.\\n\\n\\n\\n20.3. LINEAR INDEPENDENCE 343\\n\\nProof. If\\nv = α1v1 + α2v2 + · · ·+ αnvn = β1v1 + β2v2 + · · ·+ βnvn,\\n\\nthen\\n(α1 − β1)v1 + (α2 − β2)v2 + · · ·+ (αn − βn)vn = 0.\\n\\nSince v1, . . . , vn are linearly independent, αi − βi = 0 for i = 1, . . . , n.\\n\\nThe definition of linear dependence makes more sense if we consider the following propo-\\nsition.\\n\\nProposition 20.10. A set {v1, v2, . . . , vn} of vectors in a vector space V is linearly depen-\\ndent if and only if one of the vi’s is a linear combination of the rest.\\n\\nProof. Suppose that {v1, v2, . . . , vn} is a set of linearly dependent vectors. Then there\\nexist scalars α1, . . . , αn such that\\n\\nα1v1 + α2v2 + · · ·+ αnvn = 0,\\n\\nwith at least one of the αi’s not equal to zero. Suppose that αk ̸= 0. Then\\n\\nvk = −α1\\n\\nαk\\nv1 − · · · − αk−1\\n\\nαk\\nvk−1 −\\n\\nαk+1\\n\\nαk\\nvk+1 − · · · − αn\\n\\nαk\\nvn.\\n\\nConversely, suppose that\\n\\nvk = β1v1 + · · ·+ βk−1vk−1 + βk+1vk+1 + · · ·+ βnvn.\\n\\nThen\\nβ1v1 + · · ·+ βk−1vk−1 − vk + βk+1vk+1 + · · ·+ βnvn = 0.\\n\\nThe following proposition is a consequence of the fact that any system of homogeneous\\nlinear equations with more unknowns than equations will have a nontrivial solution. We\\nleave the details of the proof for the end-of-chapter exercises.\\n\\nProposition 20.11. Suppose that a vector space V is spanned by n vectors. If m > n,\\nthen any set of m vectors in V must be linearly dependent.\\n\\nA set {e1, e2, . . . , en} of vectors in a vector space V is called a basis for V if {e1, e2, . . . , en}\\nis a linearly independent set that spans V .\\n\\nExample 20.12. The vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1) form a basis\\nfor R3. The set certainly spans R3, since any arbitrary vector (x1, x2, x3) in R3 can be\\nwritten as x1e1 + x2e2 + x3e3. Also, none of the vectors e1, e2, e3 can be written as a linear\\ncombination of the other two; hence, they are linearly independent. The vectors e1, e2, e3\\nare not the only basis of R3: the set {(3, 2, 1), (3, 2, 0), (1, 1, 1)} is also a basis for R3.\\n\\nExample 20.13. Let Q(\\n√\\n2 ) = {a+b\\n\\n√\\n2 : a, b ∈ Q}. The sets {1,\\n\\n√\\n2 } and {1+\\n\\n√\\n2, 1−\\n\\n√\\n2 }\\n\\nare both bases of Q(\\n√\\n2 ).\\n\\nFrom the last two examples it should be clear that a given vector space has several bases.\\nIn fact, there are an infinite number of bases for both of these examples. In general, there\\nis no unique basis for a vector space. However, every basis of R3 consists of exactly three\\nvectors, and every basis of Q(\\n\\n√\\n2 ) consists of exactly two vectors. This is a consequence of\\n\\nthe next proposition.\\n\\n\\n\\n344 CHAPTER 20. VECTOR SPACES\\n\\nProposition 20.14. Let {e1, e2, . . . , em} and {f1, f2, . . . , fn} be two bases for a vector space\\nV . Then m = n.\\n\\nProof. Since {e1, e2, . . . , em} is a basis, it is a linearly independent set. By Proposi-\\ntion 20.11, n ≤ m. Similarly, {f1, f2, . . . , fn} is a linearly independent set, and the last\\nproposition implies that m ≤ n. Consequently, m = n.\\n\\nIf {e1, e2, . . . , en} is a basis for a vector space V , then we say that the dimension of\\nV is n and we write dimV = n. We will leave the proof of the following theorem as an\\nexercise.\\n\\nTheorem 20.15. Let V be a vector space of dimension n.\\n\\n1. If S = {v1, . . . , vn} is a set of linearly independent vectors for V , then S is a basis for\\nV .\\n\\n2. If S = {v1, . . . , vn} spans V , then S is a basis for V .\\n\\n3. If S = {v1, . . . , vk} is a set of linearly independent vectors for V with k < n, then\\nthere exist vectors vk+1, . . . , vn such that\\n\\n{v1, . . . , vk, vk+1, . . . , vn}\\n\\nis a basis for V .\\n\\n20.4 Exercises\\n1. If F is a field, show that F [x] is a vector space over F , where the vectors in F [x] are\\npolynomials. Vector addition is polynomial addition, and scalar multiplication is defined\\nby αp(x) for α ∈ F .\\n\\n2. Prove that Q(\\n√\\n2 ) is a vector space.\\n\\n3. Let Q(\\n√\\n2,\\n√\\n3 ) be the field generated by elements of the form a + b\\n\\n√\\n2 + c\\n\\n√\\n3, where\\n\\na, b, c are in Q. Prove that Q(\\n√\\n2,\\n√\\n3 ) is a vector space of dimension 4 over Q. Find a basis\\n\\nfor Q(\\n√\\n2,\\n√\\n3 ).\\n\\n4. Prove that the complex numbers are a vector space of dimension 2 over R.\\n\\n5. Prove that the set Pn of all polynomials of degree less than n form a subspace of the\\nvector space F [x]. Find a basis for Pn and compute the dimension of Pn.\\n\\n6. Let F be a field and denote the set of n-tuples of F by Fn. Given vectors u = (u1, . . . , un)\\nand v = (v1, . . . , vn) in Fn and α in F , define vector addition by\\n\\nu+ v = (u1, . . . , un) + (v1, . . . , vn) = (u1 + v1, . . . , un + vn)\\n\\nand scalar multiplication by\\n\\nαu = α(u1, . . . , un) = (αu1, . . . , αun).\\n\\nProve that Fn is a vector space of dimension n under these operations.\\n\\n7. Which of the following sets are subspaces of R3? If the set is indeed a subspace, find a\\nbasis for the subspace and compute its dimension.\\n(a) {(x1, x2, x3) : 3x1 − 2x2 + x3 = 0}\\n\\n\\n\\n20.4. EXERCISES 345\\n\\n(b) {(x1, x2, x3) : 3x1 + 4x3 = 0, 2x1 − x2 + x3 = 0}\\n(c) {(x1, x2, x3) : x1 − 2x2 + 2x3 = 2}\\n(d) {(x1, x2, x3) : 3x1 − 2x22 = 0}\\n\\n8. Show that the set of all possible solutions (x, y, z) ∈ R3 of the equations\\n\\nAx+By + Cz = 0\\n\\nDx+ Ey + Cz = 0\\n\\nform a subspace of R3.\\n\\n9. Let W be the subset of continuous functions on [0, 1] such that f(0) = 0. Prove that W\\nis a subspace of C[0, 1].\\n\\n10. Let V be a vector space over F . Prove that −(αv) = (−α)v = α(−v) for all α ∈ F and\\nall v ∈ V .\\n\\n11. Let V be a vector space of dimension n. Prove each of the following statements.\\n(a) If S = {v1, . . . , vn} is a set of linearly independent vectors for V , then S is a basis for\\n\\nV .\\n(b) If S = {v1, . . . , vn} spans V , then S is a basis for V .\\n(c) If S = {v1, . . . , vk} is a set of linearly independent vectors for V with k < n, then there\\n\\nexist vectors vk+1, . . . , vn such that\\n\\n{v1, . . . , vk, vk+1, . . . , vn}\\n\\nis a basis for V .\\n\\n12. Prove that any set of vectors containing 0 is linearly dependent.\\n\\n13. Let V be a vector space. Show that {0} is a subspace of V of dimension zero.\\n\\n14. If a vector space V is spanned by n vectors, show that any set of m vectors in V must\\nbe linearly dependent for m > n.\\n\\n15. (Linear Transformations) Let V and W be vector spaces over a field F , of dimensions\\nm and n, respectively. If T : V →W is a map satisfying\\n\\nT (u+ v) = T (u) + T (v)\\n\\nT (αv) = αT (v)\\n\\nfor all α ∈ F and all u, v ∈ V , then T is called a linear transformation from V into W .\\n(a) Prove that the kernel of T , ker(T ) = {v ∈ V : T (v) = 0}, is a subspace of V . The\\n\\nkernel of T is sometimes called the null space of T .\\n(b) Prove that the range or range space of T , R(V ) = {w ∈W : T (v) = w for some v ∈\\n\\nV }, is a subspace of W .\\n(c) Show that T : V →W is injective if and only if ker(T ) = {0}.\\n(d) Let {v1, . . . , vk} be a basis for the null space of T . We can extend this basis to be\\n\\na basis {v1, . . . , vk, vk+1, . . . , vm} of V . Why? Prove that {T (vk+1), . . . , T (vm)} is a\\nbasis for the range of T . Conclude that the range of T has dimension m− k.\\n\\n(e) Let dimV = dimW . Show that a linear transformation T : V →W is injective if and\\nonly if it is surjective.\\n\\n\\n\\n346 CHAPTER 20. VECTOR SPACES\\n\\n16. Let V and W be finite dimensional vector spaces of dimension n over a field F . Suppose\\nthat T : V → W is a vector space isomorphism. If {v1, . . . , vn} is a basis of V , show that\\n{T (v1), . . . , T (vn)} is a basis of W . Conclude that any vector space over a field F of\\ndimension n is isomorphic to Fn.\\n\\n17. (Direct Sums) Let U and V be subspaces of a vector space W . The sum of U and V ,\\ndenoted U + V , is defined to be the set of all vectors of the form u + v, where u ∈ U and\\nv ∈ V .\\n(a) Prove that U + V and U ∩ V are subspaces of W .\\n(b) If U + V = W and U ∩ V = 0, then W is said to be the direct sum. In this case,\\n\\nwe write W = U ⊕ V . Show that every element w ∈ W can be written uniquely as\\nw = u+ v, where u ∈ U and v ∈ V .\\n\\n(c) Let U be a subspace of dimension k of a vector space W of dimension n. Prove that\\nthere exists a subspace V of dimension n− k such that W = U ⊕ V . Is the subspace\\nV unique?\\n\\n(d) If U and V are arbitrary subspaces of a vector space W , show that\\n\\ndim(U + V ) = dimU + dimV − dim(U ∩ V ).\\n\\n18. (Dual Spaces) Let V and W be finite dimensional vector spaces over a field F .\\n(a) Show that the set of all linear transformations from V into W , denoted by Hom(V,W ),\\n\\nis a vector space over F , where we define vector addition as follows:\\n\\n(S + T )(v) = S(v) + T (v)\\n\\n(αS)(v) = αS(v),\\n\\nwhere S, T ∈ Hom(V,W ), α ∈ F , and v ∈ V .\\n(b) Let V be an F -vector space. Define the dual space of V to be V ∗ = Hom(V, F ).\\n\\nElements in the dual space of V are called linear functionals. Let v1, . . . , vn be\\nan ordered basis for V . If v = α1v1 + · · · + αnvn is any vector in V , define a linear\\nfunctional ϕi : V → F by ϕi(v) = αi. Show that the ϕi’s form a basis for V ∗. This\\nbasis is called the dual basis of v1, . . . , vn (or simply the dual basis if the context\\nmakes the meaning clear).\\n\\n(c) Consider the basis {(3, 1), (2,−2)} for R2. What is the dual basis for (R2)∗?\\n(d) Let V be a vector space of dimension n over a field F and let V ∗∗ be the dual space\\n\\nV ∗. Show that each element v ∈ V gives rise to an element λv in V ∗∗ and that the\\nmap v 7→ λv is an isomorphism of V with V ∗∗.\\n\\n20.5 References and Suggested Readings\\n[1] Beezer, R. A First Course in Linear Algebra. Available online at http://linear.ups.\\n\\nedu/. 2004–2014.\\n[2] Bretscher, O. Linear Algebra with Applications. 4th ed. Pearson, Upper Saddle River,\\n\\nNJ, 2009.\\n[3] Curtis, C. W. Linear Algebra: An Introductory Approach. 4th ed. Springer, New\\n\\nYork, 1984.\\n\\nhttp://linear.ups.edu/\\nhttp://linear.ups.edu/\\n\\n\\n20.6. SAGE 347\\n\\n[4] Hoffman, K. and Kunze, R. Linear Algebra. 2nd ed. Prentice-Hall, Englewood Cliffs,\\nNJ, 1971.\\n\\n[5] Johnson, L. W., Riess, R. D., and Arnold, J. T. Introduction to Linear Algebra. 6th\\ned. Pearson, Upper Saddle River, NJ, 2011.\\n\\n[6] Leon, S. J. Linear Algebra with Applications. 8th ed. Pearson, Upper Saddle River,\\nNJ, 2010.\\n\\n20.6 Sage\\nMany computations, in seemingly very different areas of mathematics, can be translated\\ninto questions about linear combinations, or other areas of linear algebra. So Sage has\\nextensive and thorough support for topics such as vector spaces.\\n\\nVector Spaces\\nThe simplest way to create a vector space is to begin with a field and use an exponent to\\nindicate the number of entries in the vectors of the space.\\n\\nV = QQ^4; V\\n\\nVector space of dimension 4 over Rational Field\\n\\nF.<a> = FiniteField (3^4)\\nW = F^5; W\\n\\nVector space of dimension 5 over Finite Field in a of size 3^4\\n\\nElements can be built with the vector constructor.\\nv = vector(QQ, [1, 1/2, 1/3, 1/4]); v\\n\\n(1, 1/2, 1/3, 1/4)\\n\\nv in V\\n\\nTrue\\n\\nw = vector(F, [1, a^2, a^4, a^6, a^8]); w\\n\\n(1, a^2, a^3 + 1, a^3 + a^2 + a + 1, a^2 + a + 2)\\n\\nw in W\\n\\nTrue\\n\\nNotice that vectors are printed with parentheses, which helps distinguish them from\\nlists (though they alos look like tuples). Vectors print horizontally, but in Sage there is no\\nsuch thing as a “row vector” or a “column vector,” though once matrices get involved we\\nneed to address this distinction. Finally, notice how the elements of the finite field have\\nbeen converted to an alternate representation.\\n\\nOnce we have vector spaces full of vectors, we can perform computations with them.\\nUltimately, all the action in a vector space comes back to vector addition and scalar mul-\\ntiplication, which together create linear combinations.\\n\\n\\n\\n348 CHAPTER 20. VECTOR SPACES\\n\\nu = vector(QQ, [ 1, 2, 3, 4, 5, 6])\\nv = vector(QQ, [-1, 2, -4, 8, -16, 32])\\n3*u - 2*v\\n\\n(5, 2, 17, -4, 47, -46)\\n\\nw = vector(F, [1, a^2, a^4, a^6, a^8])\\nx = vector(F, [1, a, 2*a, a, 1])\\ny = vector(F, [1, a^3, a^6, a^9, a^12])\\na^25*w + a^43*x + a^66*y\\n\\n(a^3 + a^2 + a + 2, a^2 + 2*a, 2*a^3 + a^2 + 2, 2*a^3 + a^2 + a,\\na^3 + 2*a^2 + a + 2)\\n\\nSubspaces\\nSage can create subspaces in a variety of ways, such as in the creation of row or column\\nspaces of matrices. However, the most direct way is to begin with a set of vectors to use as\\na spanning set.\\n\\nu = vector(QQ, [1, -1, 3])\\nv = vector(QQ, [2, 1, -1])\\nw = vector(QQ, [3, 0, 2])\\nS = (QQ^3).subspace ([u, v, w]); S\\n\\nVector space of degree 3 and dimension 2 over Rational Field\\nBasis matrix:\\n[ 1 0 2/3]\\n[ 0 1 -7/3]\\n\\n3*u - 6*v + (1/2)*w in S\\n\\nTrue\\n\\nvector(QQ, [4, -1, -2]) in S\\n\\nFalse\\n\\nNotice that the information printed about S includes a “basis matrix.” The rows of this\\nmatrix are a basis for the vector space. We can get the basis, as a list of vectors (not rows\\nof a matrix), with the .basis() method.\\n\\nS.basis()\\n\\n[\\n(1, 0, 2/3),\\n(0, 1, -7/3)\\n]\\n\\nNotice that Sage has converted the spanning set of three vectors into a basis with two\\nvectors. This is partially due to the fact that the original set of three vectors is linearly\\ndependent, but a more substantial change has occurred.\\n\\nThis is a good place to discuss some of the mathematics behind what makes Sage work.\\nA vector space over an infinite field, like the rationals or the reals, is an infinite set. No\\nmatter how expansive computer memory may seem, it is still finite. How does Sage fit\\n\\n\\n\\n20.6. SAGE 349\\n\\nan infinite set into our finite machines? The main idea is that a finite-dimensional vector\\nspace has a finite set of generators, which we know as a basis. So Sage really only needs\\nthe elements of a basis (two vectors in the previous example) to be able to work with the\\ninfinitely many possibilities for elements of the subspace.\\n\\nFurthermore, for every basis associated with a vector space, Sage performs linear com-\\nbinations to convert the given basis into another “standard” basis. This new basis has the\\nproperty that as the rows of a matrix, the matrix is in reduced row-echelon form. You can\\nsee this in the basis matrix above. The reduced row-echelon form of a matrix is unique, so\\nthis standard basis allows Sage to recognize when two vector spaces are equal. Here is an\\nexample.\\n\\nu = vector(QQ, [1, -1, 3])\\nv = vector(QQ, [2, 1, -1])\\nw = vector(QQ, [3, 0, 2])\\nu + v == w\\n\\nTrue\\n\\nS1 = (QQ^3).subspace ([u, v, w])\\nS2 = (QQ^3).subspace ([u-v, v-w, w-u])\\nS1 == S2\\n\\nTrue\\n\\nAs you might expect, it is easy to determine the dimension of a vector space.\\nu = vector(QQ, [1, -1, 3, 4])\\nv = vector(QQ, [2, 1, -1, -2])\\nS = (QQ^4).subspace ([u, v, 2*u + 3*v, -u + 2*v])\\nS.dimension ()\\n\\n2\\n\\nLinear Independence\\nThere are a variety of ways in Sage to determine if a set of vectors is linearly independent\\nor not, and to find relations of linear dependence if they exist. The technique we will show\\nhere is a simple test to see if a set of vectors is linearly independent or not. Simply use the\\nvectors as a spanning set for a subspace, and check the dimension of the subspace. The\\ndimension equals the number of vectors in the spanning set if and only if the spanning set\\nis linearly independent.\\n\\nF.<a> = FiniteField (3^4)\\nu = vector(F, [a^i for i in range(0, 7, 1)])\\nv = vector(F, [a^i for i in range(0, 14, 2)])\\nw = vector(F, [a^i for i in range(0, 21, 3)])\\nS = (F^7).subspace ([u, v, w])\\nS.dimension ()\\n\\n3\\n\\nS = (F^7).subspace ([u, v, a^3*u + a^11*v])\\nS.dimension ()\\n\\n2\\n\\nSo the first set of vectors, [u, v, w], is linearly independent, while the second set, [u,\\nv, a^3*u + a^11*v], is not.\\n\\n\\n\\n350 CHAPTER 20. VECTOR SPACES\\n\\nAbstract Vector Spaces\\nSage does not implement many abstract vector spaces directly, such as Pn, the vector space\\nof polynomials of degree n or less. This is due in part to the fact that a finite-dimensional\\nvector space over a field F is isomorphic to the vector space Fn. So Sage captures all\\nthe functionality of finite-dimensional vector spaces, and it is left to the user to perform\\nthe conversions according to the isomorphism (which is often trivial with the choice of an\\nobvious basis).\\n\\nHowever, there are instances where rings behave naturally as vector spaces and we can\\nexploit this extra structure. We will see much more of this in the chapters on fields and\\nGalois theory. As an example, finite fields have a single generator, and the first few powers\\nof the generator form a basis. Consider creating a vector space from the elements of a finite\\nfield of order 76 = 117 649. As elements of a field we know they can be added, so we will\\ndefine this to be the addition in our vector space. For any element of the integers mod 7,\\nwe can multiply an element of the field by the integer, so we define this to be our scalar\\nmultiplication. Later, we will be certain that these two definitions lead to a vector space,\\nbut take that for granted now. So here are some operations in our new vector space.\\n\\nF.<a> = FiniteField (7^6)\\nu = 2*a^5 + 6*a^4 + 2*a^3 + 3*a^2 + 2*a + 3\\nv = 4*a^5 + 4*a^4 + 4*a^3 + 6*a^2 + 5*a + 6\\nu + v\\n\\n6*a^5 + 3*a^4 + 6*a^3 + 2*a^2 + 2\\n\\n4*u\\n\\na^5 + 3*a^4 + a^3 + 5*a^2 + a + 5\\n\\n2*u + 5*v\\n\\n3*a^5 + 4*a^4 + 3*a^3 + a^2 + a + 1\\n\\nYou might recognize that this looks very familiar to how we add polynomials, and\\nmultiply polynomials by scalars. You would be correct. However, notice that in this vector\\nspace construction, we are totally ignoring the possibility of multiplying two field elements\\ntogether. As a vector space with scalars from Z7, a basis is the first six powers of the\\ngenerator, {1, a, a2, a3, a4, a5}. (Notice how counting from zero is natural here.) You may\\nhave noticed how Sage consistently rewrites elements of fields as linear combinations — now\\nyou have a good explanation.\\n\\nHere is what Sage knows about a finite field as a vector space. First, it knows that the\\nfinite field is a vector space, and what the field of scalars is.\\n\\nV = F.vector_space (); V\\n\\nVector space of dimension 6 over Finite Field of size 7\\n\\nR = V.base_ring (); R\\n\\nFinite Field of size 7\\n\\nR == FiniteField (7)\\n\\nTrue\\n\\n\\n\\n20.7. SAGE EXERCISES 351\\n\\nV.dimension ()\\n\\n6\\n\\nSo the finite field (as a vector space) is isomorphic to the vector space (Z7)\\n6. Notice this\\n\\nis not a ring or field isomorphism, as it does not fully address multiplication of elements,\\neven though that is possible in the field.\\n\\nSecond, elements of the field can be converted to elements of the vector space easily.\\nx = V(u); x\\n\\n(3, 2, 3, 2, 6, 2)\\n\\ny = V(v); y\\n\\n(6, 5, 6, 4, 4, 4)\\n\\nNotice that Sage writes field elements with high powers of the generator first, while\\nthe basis in use is ordered with low powers first. The computations below illustrate the\\nisomorphism preserving the structure between the finite field itself and its interpretation as\\na vector space, (Z7)\\n\\n6.\\nV(u + v) == V(u) + V(v)\\n\\nTrue\\n\\ntwo = R(2)\\nV(two*u) == two*V(u)\\n\\nTrue\\n\\nLinear Algebra\\nSage has extensive support for linear algebra, well beyond what we have described here,\\nor what we will need for the remaining chapters. Create vector spaces and vectors (with\\ndifferent fields of scalars), and then use tab-completion on these objects to explore the large\\nsets of available commands.\\n\\n20.7 Sage Exercises\\n1. Given two subspaces U and W of a vector space V , their sum U +W can be defined as\\nthe set U +W = {u + w | u ∈ U, w ∈ W}, in other words, the set of all possible sums of\\nan element from U and an element from W .\\nNotice this is not the direct sum of your text, nor the direct_sum() method in Sage. However,\\nyou can build this subspace in Sage as follows. Grab the bases of U and W individually,\\nas lists of vectors. Join the two lists together by just using a plus sign between them.\\nNow build the sum subspace by creating a subspace of V spanned by this set, by using the\\n.subspace() method.\\nIn the vector space (QQ^10) construct two subspaces that you expect to (a) have dimension 5\\nor 6 or so, and (b) have an intersection that is a vector space of dimension 2 or so. Compare\\ntheir individual dimensions with the dimensions of the intersection of U and W (U ∩W ,\\n.intersection() in Sage) and the sum U +W .\\n\\n\\n\\n352 CHAPTER 20. VECTOR SPACES\\n\\nRepeat the experiment with the two original vector spaces having dimension 8 or so, and\\nwith the intersection as small as possible. Form a general conjecture relating these four\\ndimensions based on the results of your two (or more)experiments.\\n\\n2. We can construct a field in Sage that extends the rationals by adding in a fourth root\\nof two, Q[ 4\\n\\n√\\n2], with the command F.<c> = QQ[2^(1/4)]. This is a vector space of dimension\\n\\n4 over the rationals, with a basis that is the first four powers of c = 4\\n√\\n2 (starting with the\\n\\nzero power).\\nThe command F.vector_space() will return three items in a triple (so be careful how you\\nhandle this output to extract what you need). The first part of the output is a vector\\nspace over the rationals that is isomorphic to F. The next is a vector space isomorphism\\n(invertible linear transformation) from the provided vector space to the field, while the\\nthird is an isomorphism in the opposite direction. These two isomorphisms can then be\\nused like functions. Notice that this is different behavior than for .vector_space() applied\\nto finite fields. Create non-trivial examples that show that these vector space isomorphisms\\nbehave as an isomorphism should. (You will have at least four such examples in a complete\\nsolution.)\\n\\n3. Build a finite field F of order pn in the usual way. Then construct the (multiplicative)\\ngroup of all invertible (nonsingular) m ×m matrices over this field with the command G\\n\\n= GL(m, F) (“the general linear group”). What is the order of this group? In other words,\\nfind a general expression for the order of this group.\\nYour answer should be a function of m, p and n. Provide a complete explanation of the\\nlogic behind your solution (i.e. something resembling a proof). Also provide tests in Sage\\nthat your answer is correct.\\nHints: G.order() will help you test and verify your hypotheses. Small examples in Sage\\n(listing all the elements of the group) might aid your intuition—which is why this is a Sage\\nexercise. Small means 2 × 2 and 3 × 3 matrices and finite fields with 2, 3, 4, 5 elements, at\\nmost. Results do not really depend on each of p and n, but rather just on pn.\\nRealize this group is interesting because it contains representations of all the invertible (i.e.\\n1-1 and onto) linear transformations from the (finite) vector space Fm to itself.\\n\\n4. What happens if we try to do linear algebra over a ring that is not also a field? The\\nobject that resembles a vector space, but with this one distinction, is known as a module.\\nYou can build one easily with a construction like ZZ^3. Evaluate the following to create a\\nmodule and a submodule.\\n\\nM = ZZ^3\\nu = M([1, 0, 0])\\nv = M([2, 2, 0])\\nw = M([0, 0, 4])\\nN = M.submodule ([u, v, w])\\n\\nExamine the bases and dimensions (aka “rank”) of the module and submodule, and check\\nthe equality of the module and submodule. How is this different than the situation for\\nvector spaces? Can you create a third module, P, that is a proper subset of M and properly\\ncontains N?\\n\\n5. A finite field, F , of order 53 is a vector space of dimension 3 over Z5. Suppose a is a\\ngenerator of F . Let M be any 3×3 matrix with entries from Z5 (carefule here, the elements\\nare from th field of scalars, not from the vector space). If we convert an element x ∈ F\\nto a vector (relative to the basis {1, a, a2}), then we can multiply it by M (with M on the\\nleft) to create another vector, which we can translate to a linear combination of the basis\\nelements, and hence another element of F . This function is a vector space homomorphism,\\n\\n\\n\\n20.7. SAGE EXERCISES 353\\n\\nbetter known as a linear transformation (implemented with a matrix representation relative\\nto the basis {1, a, a2}. Notice that each part below becomes less general and more specific.\\n(a) Create a non-invertible matrix R and give examples to show that the mapping de-\\n\\nscribed by R is a vector space homomorphism of F into F .\\n(b) Create an invertible matrix M . The mapping will now be an invertible homomorphism.\\n\\nDetermine the inverse function and give examples to verify its properties.\\n(c) Since a is a generator of the field, the mapping a 7→ a5 can be extended to a vector\\n\\nspace homomorphism (i.e. a linear transformation). Find a matrix M which effects this\\nlinear transformation, and from this, determine that the homomorphism is invertible.\\n\\n(d) None of the previous three parts applies to properties of multiplication in the field.\\nHowever, the mapping from the third part also preserves multiplication in the field,\\nthough a proof of this may not be obvious right now. So we are saying this mapping\\nis a field automorphism, preserving both addition and multiplication. Give a nontriv-\\nial example of the multiplication-preserving properties of this mapping. (This is the\\nFrobenius map which will be discussed further in Chapter 21.)\\n\\n\\n\\n21\\n\\nFields\\n\\nIt is natural to ask whether or not some field F is contained in a larger field. We think of\\nthe rational numbers, which reside inside the real numbers, while in turn, the real numbers\\nlive inside the complex numbers. We can also study the fields between Q and R and inquire\\nas to the nature of these fields.\\n\\nMore specifically if we are given a field F and a polynomial p(x) ∈ F [x], we can ask\\nwhether or not we can find a field E containing F such that p(x) factors into linear factors\\nover E[x]. For example, if we consider the polynomial\\n\\np(x) = x4 − 5x2 + 6\\n\\nin Q[x], then p(x) factors as (x2− 2)(x2− 3). However, both of these factors are irreducible\\nin Q[x]. If we wish to find a zero of p(x), we must go to a larger field. Certainly the field\\nof real numbers will work, since\\n\\np(x) = (x−\\n√\\n2)(x+\\n\\n√\\n2)(x−\\n\\n√\\n3)(x+\\n\\n√\\n3).\\n\\nIt is possible to find a smaller field in which p(x) has a zero, namely\\n\\nQ(\\n√\\n2) = {a+ b\\n\\n√\\n2 : a, b ∈ Q}.\\n\\nWe wish to be able to compute and study such fields for arbitrary polynomials over a field\\nF .\\n\\n21.1 Extension Fields\\nA field E is an extension field of a field F if F is a subfield of E. The field F is called\\nthe base field. We write F ⊂ E.\\n\\nExample 21.1. For example, let\\n\\nF = Q(\\n√\\n2 ) = {a+ b\\n\\n√\\n2 : a, b ∈ Q}\\n\\nand let E = Q(\\n√\\n2 +\\n\\n√\\n3 ) be the smallest field containing both Q and\\n\\n√\\n2 +\\n\\n√\\n3. Both\\n\\nE and F are extension fields of the rational numbers. We claim that E is an extension\\nfield of F . To see this, we need only show that\\n\\n√\\n2 is in E. Since\\n\\n√\\n2 +\\n\\n√\\n3 is in E,\\n\\n1/(\\n√\\n2 +\\n\\n√\\n3 ) =\\n\\n√\\n3 −\\n\\n√\\n2 must also be in E. Taking linear combinations of\\n\\n√\\n2 +\\n\\n√\\n3 and√\\n\\n3−\\n√\\n2, we find that\\n\\n√\\n2 and\\n\\n√\\n3 must both be in E.\\n\\nExample 21.2. Let p(x) = x2 + x + 1 ∈ Z2[x]. Since neither 0 nor 1 is a root of this\\npolynomial, we know that p(x) is irreducible over Z2. We will construct a field extension\\nof Z2 containing an element α such that p(α) = 0. By Theorem 17.22, the ideal ⟨p(x)⟩\\n\\n354\\n\\n\\n\\n21.1. EXTENSION FIELDS 355\\n\\ngenerated by p(x) is maximal; hence, Z2[x]/⟨p(x)⟩ is a field. Let f(x) + ⟨p(x)⟩ be an\\narbitrary element of Z2[x]/⟨p(x)⟩. By the division algorithm,\\n\\nf(x) = (x2 + x+ 1)q(x) + r(x),\\n\\nwhere the degree of r(x) is less than the degree of x2 + x+ 1. Therefore,\\n\\nf(x) + ⟨x2 + x+ 1⟩ = r(x) + ⟨x2 + x+ 1⟩.\\n\\nThe only possibilities for r(x) are then 0, 1, x, and 1+x. Consequently, E = Z2[x]/⟨x2+x+1⟩\\nis a field with four elements and must be a field extension of Z2, containing a zero α of p(x).\\nThe field Z2(α) consists of elements\\n\\n0 + 0α = 0\\n\\n1 + 0α = 1\\n\\n0 + 1α = α\\n\\n1 + 1α = 1 + α.\\n\\nNotice that α2 + α+ 1 = 0; hence, if we compute (1 + α)2,\\n\\n(1 + α)(1 + α) = 1 + α+ α+ (α)2 = α.\\n\\nOther calculations are accomplished in a similar manner. We summarize these computations\\nin the following tables, which tell us how to add and multiply elements in E.\\n\\n+ 0 1 α 1 + α\\n\\n0 0 1 α 1 + α\\n\\n1 1 0 1 + α α\\n\\nα α 1 + α 0 1\\n\\n1 + α 1 + α α 1 0\\n\\nTable 21.3: Addition Table for Z2(α)\\n\\n· 0 1 α 1 + α\\n\\n0 0 0 0 0\\n\\n1 0 1 α 1 + α\\n\\nα 0 α 1 + α 1\\n\\n1 + α 0 1 + α 1 α\\n\\nTable 21.4: Multiplication Table for Z2(α)\\n\\nThe following theorem, due to Kronecker, is so important and so basic to our under-\\nstanding of fields that it is often known as the Fundamental Theorem of Field Theory.\\n\\nTheorem 21.5. Let F be a field and let p(x) be a nonconstant polynomial in F [x]. Then\\nthere exists an extension field E of F and an element α ∈ E such that p(α) = 0.\\n\\n\\n\\n356 CHAPTER 21. FIELDS\\n\\nProof. To prove this theorem, we will employ the method that we used to construct\\nExample 21.2. Clearly, we can assume that p(x) is an irreducible polynomial. We wish to\\nfind an extension field E of F containing an element α such that p(α) = 0. The ideal ⟨p(x)⟩\\ngenerated by p(x) is a maximal ideal in F [x] by Theorem 17.22; hence, F [x]/⟨p(x)⟩ is a\\nfield. We claim that E = F [x]/⟨p(x)⟩ is the desired field.\\n\\nWe first show that E is a field extension of F . We can define a homomorphism of\\ncommutative rings by the map ψ : F → F [x]/⟨p(x)⟩, where ψ(a) = a+ ⟨p(x)⟩ for a ∈ F . It\\nis easy to check that ψ is indeed a ring homomorphism. Observe that\\n\\nψ(a) + ψ(b) = (a+ ⟨p(x)⟩) + (b+ ⟨p(x)⟩) = (a+ b) + ⟨p(x)⟩ = ψ(a+ b)\\n\\nand\\nψ(a)ψ(b) = (a+ ⟨p(x)⟩)(b+ ⟨p(x)⟩) = ab+ ⟨p(x)⟩ = ψ(ab).\\n\\nTo prove that ψ is one-to-one, assume that\\n\\na+ ⟨p(x)⟩ = ψ(a) = ψ(b) = b+ ⟨p(x)⟩.\\n\\nThen a−b is a multiple of p(x), since it lives in the ideal ⟨p(x)⟩. Since p(x) is a nonconstant\\npolynomial, the only possibility is that a − b = 0. Consequently, a = b and ψ is injective.\\nSince ψ is one-to-one, we can identify F with the subfield {a + ⟨p(x)⟩ : a ∈ F} of E and\\nview E as an extension field of F .\\n\\nIt remains for us to prove that p(x) has a zero α ∈ E. Set α = x+ ⟨p(x)⟩. Then α is in\\nE. If p(x) = a0 + a1x+ · · ·+ anx\\n\\nn, then\\n\\np(α) = a0 + a1(x+ ⟨p(x)⟩) + · · ·+ an(x+ ⟨p(x)⟩)n\\n\\n= a0 + (a1x+ ⟨p(x)⟩) + · · ·+ (anx\\nn + ⟨p(x)⟩)\\n\\n= a0 + a1x+ · · ·+ anx\\nn + ⟨p(x)⟩\\n\\n= 0 + ⟨p(x)⟩.\\n\\nTherefore, we have found an element α ∈ E = F [x]/⟨p(x)⟩ such that α is a zero of p(x).\\n\\nExample 21.6. Let p(x) = x5+x4+1 ∈ Z2[x]. Then p(x) has irreducible factors x2+x+1\\nand x3 + x+ 1. For a field extension E of Z2 such that p(x) has a root in E, we can let E\\nbe either Z2[x]/⟨x2 + x+ 1⟩ or Z2[x]/⟨x3 + x+ 1⟩. We will leave it as an exercise to show\\nthat Z2[x]/⟨x3 + x+ 1⟩ is a field with 23 = 8 elements.\\n\\nAlgebraic Elements\\nAn element α in an extension field E over F is algebraic over F if f(α) = 0 for some nonzero\\npolynomial f(x) ∈ F [x]. An element in E that is not algebraic over F is transcendental\\nover F . An extension field E of a field F is an algebraic extension of F if every element\\nin E is algebraic over F . If E is a field extension of F and α1, . . . , αn are contained in E,\\nwe denote the smallest field containing F and α1, . . . , αn by F (α1, . . . , αn). If E = F (α)\\nfor some α ∈ E, then E is a simple extension of F .\\n\\nExample 21.7. Both\\n√\\n2 and i are algebraic over Q since they are zeros of the polynomials\\n\\nx2−2 and x2+1, respectively. Clearly π and e are algebraic over the real numbers; however,\\nit is a nontrivial fact that they are transcendental over Q. Numbers in R that are algebraic\\nover Q are in fact quite rare. Almost all real numbers are transcendental over Q.1 (In many\\ncases we do not know whether or not a particular number is transcendental; for example,\\nit is still not known whether π + e is transcendental or algebraic.)\\n\\n1The probability that a real number chosen at random from the interval [0, 1] will be transcendental over\\nthe rational numbers is one.\\n\\n\\n\\n21.1. EXTENSION FIELDS 357\\n\\nA complex number that is algebraic over Q is an algebraic number. A transcendental\\nnumber is an element of C that is transcendental over Q.\\nExample 21.8. We will show that\\n\\n√\\n2 +\\n\\n√\\n3 is algebraic over Q. If α =\\n\\n√\\n2 +\\n\\n√\\n3, then\\n\\nα2 = 2 +\\n√\\n3. Hence, α2 − 2 =\\n\\n√\\n3 and (α2 − 2)2 = 3. Since α4 − 4α2 + 1 = 0, it must be\\n\\ntrue that α is a zero of the polynomial x4 − 4x2 + 1 ∈ Q[x].\\nIt is very easy to give an example of an extension field E over a field F , where E contains\\n\\nan element transcendental over F . The following theorem characterizes transcendental\\nextensions.\\nTheorem 21.9. Let E be an extension field of F and α ∈ E. Then α is transcendental\\nover F if and only if F (α) is isomorphic to F (x), the field of fractions of F [x].\\nProof. Let ϕα : F [x] → E be the evaluation homomorphism for α. Then α is transcenden-\\ntal over F if and only if ϕα(p(x)) = p(α) ̸= 0 for all nonconstant polynomials p(x) ∈ F [x].\\nThis is true if and only if kerϕα = {0}; that is, it is true exactly when ϕα is one-to-one.\\nHence, E must contain a copy of F [x]. The smallest field containing F [x] is the field of\\nfractions F (x). By Theorem 18.4, E must contain a copy of this field.\\n\\nWe have a more interesting situation in the case of algebraic extensions.\\nTheorem 21.10. Let E be an extension field of a field F and α ∈ E with α algebraic over\\nF . Then there is a unique irreducible monic polynomial p(x) ∈ F [x] of smallest degree such\\nthat p(α) = 0. If f(x) is another polynomial in F [x] such that f(α) = 0, then p(x) divides\\nf(x).\\nProof. Let ϕα : F [x] → E be the evaluation homomorphism. The kernel of ϕα is a\\nprincipal ideal generated by some p(x) ∈ F [x] with deg p(x) ≥ 1. We know that such a\\npolynomial exists, since F [x] is a principal ideal domain and α is algebraic. The ideal ⟨p(x)⟩\\nconsists exactly of those elements of F [x] having α as a zero. If f(α) = 0 and f(x) is not\\nthe zero polynomial, then f(x) ∈ ⟨p(x)⟩ and p(x) divides f(x). So p(x) is a polynomial of\\nminimal degree having α as a zero. Any other polynomial of the same degree having α as\\na zero must have the form βp(x) for some β ∈ F .\\n\\nSuppose now that p(x) = r(x)s(x) is a factorization of p(x) into polynomials of lower\\ndegree. Since p(α) = 0, r(α)s(α) = 0; consequently, either r(α) = 0 or s(α) = 0, which\\ncontradicts the fact that p is of minimal degree. Therefore, p(x) must be irreducible.\\n\\nLet E be an extension field of F and α ∈ E be algebraic over F . The unique monic\\npolynomial p(x) of the last theorem is called the minimal polynomial for α over F . The\\ndegree of p(x) is the degree of α over F .\\nExample 21.11. Let f(x) = x2 − 2 and g(x) = x4 − 4x2 + 1. These polynomials are the\\nminimal polynomials of\\n\\n√\\n2 and\\n\\n√\\n2 +\\n\\n√\\n3, respectively.\\n\\nProposition 21.12. Let E be a field extension of F and α ∈ E be algebraic over F . Then\\nF (α) ∼= F [x]/⟨p(x)⟩, where p(x) is the minimal polynomial of α over F .\\nProof. Let ϕα : F [x] → E be the evaluation homomorphism. The kernel of this map is\\n⟨p(x)⟩, where p(x) is the minimal polynomial of α. By the First Isomorphism Theorem for\\nrings, the image of ϕα in E is isomorphic to F (α) since it contains both F and α.\\n\\nTheorem 21.13. Let E = F (α) be a simple extension of F , where α ∈ E is algebraic over\\nF . Suppose that the degree of α over F is n. Then every element β ∈ E can be expressed\\nuniquely in the form\\n\\nβ = b0 + b1α+ · · ·+ bn−1α\\nn−1\\n\\nfor bi ∈ F .\\n\\n\\n\\n358 CHAPTER 21. FIELDS\\n\\nProof. Since ϕα(F [x]) ∼= F (α), every element in E = F (α) must be of the form ϕα(f(x)) =\\nf(α), where f(α) is a polynomial in α with coefficients in F . Let\\n\\np(x) = xn + an−1x\\nn−1 + · · ·+ a0\\n\\nbe the minimal polynomial of α. Then p(α) = 0; hence,\\n\\nαn = −an−1α\\nn−1 − · · · − a0.\\n\\nSimilarly,\\n\\nαn+1 = ααn\\n\\n= −an−1α\\nn − an−2α\\n\\nn−1 − · · · − a0α\\n\\n= −an−1(−an−1α\\nn−1 − · · · − a0)− an−2α\\n\\nn−1 − · · · − a0α.\\n\\nContinuing in this manner, we can express every monomial αm, m ≥ n, as a linear combi-\\nnation of powers of α that are less than n. Hence, any β ∈ F (α) can be written as\\n\\nβ = b0 + b1α+ · · ·+ bn−1α\\nn−1.\\n\\nTo show uniqueness, suppose that\\n\\nβ = b0 + b1α+ · · ·+ bn−1α\\nn−1 = c0 + c1α+ · · ·+ cn−1α\\n\\nn−1\\n\\nfor bi and ci in F . Then\\n\\ng(x) = (b0 − c0) + (b1 − c1)x+ · · ·+ (bn−1 − cn−1)x\\nn−1\\n\\nis in F [x] and g(α) = 0. Since the degree of g(x) is less than the degree of p(x), the\\nirreducible polynomial of α, g(x) must be the zero polynomial. Consequently,\\n\\nb0 − c0 = b1 − c1 = · · · = bn−1 − cn−1 = 0,\\n\\nor bi = ci for i = 0, 1, . . . , n− 1. Therefore, we have shown uniqueness.\\n\\nExample 21.14. Since x2+1 is irreducible over R, ⟨x2+1⟩ is a maximal ideal in R[x]. So\\nE = R[x]/⟨x2+1⟩ is a field extension of R that contains a root of x2+1. Let α = x+⟨x2+1⟩.\\nWe can identify E with the complex numbers. By Proposition 21.12, E is isomorphic to\\nR(α) = {a+ bα : a, b ∈ R}. We know that α2 = −1 in E, since\\n\\nα2 + 1 = (x+ ⟨x2 + 1⟩)2 + (1 + ⟨x2 + 1⟩)\\n= (x2 + 1) + ⟨x2 + 1⟩\\n= 0.\\n\\nHence, we have an isomorphism of R(α) with C defined by the map that takes a + bα to\\na+ bi.\\n\\nLet E be a field extension of a field F . If we regard E as a vector space over F , then we\\ncan bring the machinery of linear algebra to bear on the problems that we will encounter in\\nour study of fields. The elements in the field E are vectors; the elements in the field F are\\nscalars. We can think of addition in E as adding vectors. When we multiply an element in\\nE by an element of F , we are multiplying a vector by a scalar. This view of field extensions\\nis especially fruitful if a field extension E of F is a finite dimensional vector space over F ,\\n\\n\\n\\n21.1. EXTENSION FIELDS 359\\n\\nand Theorem 21.13 states that E = F (α) is finite dimensional vector space over F with\\nbasis {1, α, α2, . . . , αn−1}.\\n\\nIf an extension field E of a field F is a finite dimensional vector space over F of dimension\\nn, then we say that E is a finite extension of degree n over F . We write\\n\\n[E : F ] = n.\\n\\nto indicate the dimension of E over F .\\n\\nTheorem 21.15. Every finite extension field E of a field F is an algebraic extension.\\n\\nProof. Let α ∈ E. Since [E : F ] = n, the elements\\n\\n1, α, . . . , αn\\n\\ncannot be linearly independent. Hence, there exist ai ∈ F , not all zero, such that\\n\\nanα\\nn + an−1α\\n\\nn−1 + · · ·+ a1α+ a0 = 0.\\n\\nTherefore,\\np(x) = anx\\n\\nn + · · ·+ a0 ∈ F [x]\\n\\nis a nonzero polynomial with p(α) = 0.\\n\\nRemark 21.16. Theorem 21.15 says that every finite extension of a field F is an algebraic\\nextension. The converse is false, however. We will leave it as an exercise to show that the\\nset of all elements in R that are algebraic over Q forms an infinite field extension of Q.\\n\\nThe next theorem is a counting theorem, similar to Lagrange’s Theorem in group theory.\\nTheorem 21.17 will prove to be an extremely useful tool in our investigation of finite field\\nextensions.\\n\\nTheorem 21.17. If E is a finite extension of F and K is a finite extension of E, then K\\nis a finite extension of F and\\n\\n[K : F ] = [K : E][E : F ].\\n\\nProof. Let {α1, . . . , αn} be a basis for E as a vector space over F and {β1, . . . , βm} be a\\nbasis for K as a vector space over E. We claim that {αiβj} is a basis for K over F . We will\\nfirst show that these vectors span K. Let u ∈ K. Then u =\\n\\n∑m\\nj=1 bjβj and bj =\\n\\n∑n\\ni=1 aijαi,\\n\\nwhere bj ∈ E and aij ∈ F . Then\\n\\nu =\\n\\nm∑\\nj=1\\n\\n(\\nn∑\\n\\ni=1\\n\\naijαi\\n\\n)\\nβj =\\n\\n∑\\ni,j\\n\\naij(αiβj).\\n\\nSo the mn vectors αiβj must span K over F .\\nWe must show that {αiβj} are linearly independent. Recall that a set of vectors\\n\\nv1, v2, . . . , vn in a vector space V are linearly independent if\\n\\nc1v1 + c2v2 + · · ·+ cnvn = 0\\n\\nimplies that\\nc1 = c2 = · · · = cn = 0.\\n\\nLet\\nu =\\n\\n∑\\ni,j\\n\\ncij(αiβj) = 0\\n\\n\\n\\n360 CHAPTER 21. FIELDS\\n\\nfor cij ∈ F . We need to prove that all of the cij ’s are zero. We can rewrite u as\\nm∑\\nj=1\\n\\n(\\nn∑\\n\\ni=1\\n\\ncijαi\\n\\n)\\nβj = 0,\\n\\nwhere\\n∑\\n\\ni cijαi ∈ E. Since the βj ’s are linearly independent over E, it must be the case\\nthat\\n\\nn∑\\ni=1\\n\\ncijαi = 0\\n\\nfor all j. However, the αj are also linearly independent over F . Therefore, cij = 0 for all i\\nand j, which completes the proof.\\n\\nThe following corollary is easily proved using mathematical induction.\\n\\nCorollary 21.18. If Fi is a field for i = 1, . . . , k and Fi+1 is a finite extension of Fi, then\\nFk is a finite extension of F1 and\\n\\n[Fk : F1] = [Fk : Fk−1] · · · [F2 : F1].\\n\\nCorollary 21.19. Let E be an extension field of F . If α ∈ E is algebraic over F with\\nminimal polynomial p(x) and β ∈ F (α) with minimal polynomial q(x), then deg q(x) divides\\ndeg p(x).\\n\\nProof. We know that deg p(x) = [F (α) : F ] and deg q(x) = [F (β) : F ]. Since F ⊂ F (β) ⊂\\nF (α),\\n\\n[F (α) : F ] = [F (α) : F (β)][F (β) : F ].\\n\\nExample 21.20. Let us determine an extension field of Q containing\\n√\\n3 +\\n\\n√\\n5. It is easy\\n\\nto determine that the minimal polynomial of\\n√\\n3 +\\n\\n√\\n5 is x4 − 16x2 + 4. It follows that\\n\\n[Q(\\n√\\n3 +\\n\\n√\\n5 ) : Q] = 4.\\n\\nWe know that {1,\\n√\\n3 } is a basis for Q(\\n\\n√\\n3 ) over Q. Hence,\\n\\n√\\n3 +\\n\\n√\\n5 cannot be in\\n\\nQ(\\n√\\n3 ). It follows that\\n\\n√\\n5 cannot be in Q(\\n\\n√\\n3 ) either. Therefore, {1,\\n\\n√\\n5 } is a basis\\n\\nfor Q(\\n√\\n3,\\n√\\n5 ) = (Q(\\n\\n√\\n3 ))(\\n\\n√\\n5 ) over Q(\\n\\n√\\n3 ) and {1,\\n\\n√\\n3,\\n√\\n5,\\n√\\n3\\n√\\n5 =\\n\\n√\\n15 } is a basis for\\n\\nQ(\\n√\\n3,\\n√\\n5 ) = Q(\\n\\n√\\n3 +\\n\\n√\\n5 ) over Q. This example shows that it is possible that some\\n\\nextension F (α1, . . . , αn) is actually a simple extension of F even though n > 1.\\n\\nExample 21.21. Let us compute a basis for Q( 3\\n√\\n5,\\n√\\n5 i), where\\n\\n√\\n5 is the positive square\\n\\nroot of 5 and 3\\n√\\n5 is the real cube root of 5. We know that\\n\\n√\\n5 i /∈ Q( 3\\n\\n√\\n5 ), so\\n\\n[Q(\\n3\\n√\\n5,\\n√\\n5 i) : Q(\\n\\n3\\n√\\n5 )] = 2.\\n\\nIt is easy to determine that {1,\\n√\\n5i } is a basis for Q( 3\\n\\n√\\n5,\\n√\\n5 i) over Q( 3\\n\\n√\\n5 ). We also know\\n\\nthat {1, 3\\n√\\n5, ( 3\\n\\n√\\n5 )2} is a basis for Q( 3\\n\\n√\\n5 ) over Q. Hence, a basis for Q( 3\\n\\n√\\n5,\\n√\\n5 i) over Q is\\n\\n{1,\\n√\\n5 i,\\n\\n3\\n√\\n5, (\\n\\n3\\n√\\n5 )2, (\\n\\n6\\n√\\n5 )5i, (\\n\\n6\\n√\\n5 )7i = 5\\n\\n6\\n√\\n5 i or 6\\n\\n√\\n5 i}.\\n\\nNotice that 6\\n√\\n5 i is a zero of x6 + 5. We can show that this polynomial is irreducible over\\n\\nQ using Eisenstein’s Criterion, where we let p = 5. Consequently,\\n\\nQ ⊂ Q(\\n6\\n√\\n5 i) ⊂ Q(\\n\\n3\\n√\\n5,\\n√\\n5 i).\\n\\nBut it must be the case that Q( 6\\n√\\n5 i) = Q( 3\\n\\n√\\n5,\\n√\\n5 i), since the degree of both of these\\n\\nextensions is 6.\\n\\n\\n\\n21.1. EXTENSION FIELDS 361\\n\\nTheorem 21.22. Let E be a field extension of F . Then the following statements are\\nequivalent.\\n\\n1. E is a finite extension of F .\\n\\n2. There exists a finite number of algebraic elements α1, . . . , αn ∈ E such that E =\\nF (α1, . . . , αn).\\n\\n3. There exists a sequence of fields\\n\\nE = F (α1, . . . , αn) ⊃ F (α1, . . . , αn−1) ⊃ · · · ⊃ F (α1) ⊃ F,\\n\\nwhere each field F (α1, . . . , αi) is algebraic over F (α1, . . . , αi−1).\\n\\nProof. (1) ⇒ (2). Let E be a finite algebraic extension of F . Then E is a finite dimensional\\nvector space over F and there exists a basis consisting of elements α1, . . . , αn in E such that\\nE = F (α1, . . . , αn). Each αi is algebraic over F by Theorem 21.15.\\n\\n(2) ⇒ (3). Suppose that E = F (α1, . . . , αn), where every αi is algebraic over F . Then\\n\\nE = F (α1, . . . , αn) ⊃ F (α1, . . . , αn−1) ⊃ · · · ⊃ F (α1) ⊃ F,\\n\\nwhere each field F (α1, . . . , αi) is algebraic over F (α1, . . . , αi−1).\\n(3) ⇒ (1). Let\\n\\nE = F (α1, . . . , αn) ⊃ F (α1, . . . , αn−1) ⊃ · · · ⊃ F (α1) ⊃ F,\\n\\nwhere each field F (α1, . . . , αi) is algebraic over F (α1, . . . , αi−1). Since\\n\\nF (α1, . . . , αi) = F (α1, . . . , αi−1)(αi)\\n\\nis simple extension and αi is algebraic over F (α1, . . . , αi−1), it follows that\\n\\n[F (α1, . . . , αi) : F (α1, . . . , αi−1)]\\n\\nis finite for each i. Therefore, [E : F ] is finite.\\n\\nAlgebraic Closure\\nGiven a field F , the question arises as to whether or not we can find a field E such that\\nevery polynomial p(x) has a root in E. This leads us to the following theorem.\\n\\nTheorem 21.23. Let E be an extension field of F . The set of elements in E that are\\nalgebraic over F form a field.\\n\\nProof. Let α, β ∈ E be algebraic over F . Then F (α, β) is a finite extension of F . Since\\nevery element of F (α, β) is algebraic over F , α ± β, αβ, and α/β (β ̸= 0) are all algebraic\\nover F . Consequently, the set of elements in E that are algebraic over F form a field.\\n\\nCorollary 21.24. The set of all algebraic numbers forms a field; that is, the set of all\\ncomplex numbers that are algebraic over Q makes up a field.\\n\\nLet E be a field extension of a field F . We define the algebraic closure of a field F\\nin E to be the field consisting of all elements in E that are algebraic over F . A field F is\\nalgebraically closed if every nonconstant polynomial in F [x] has a root in F .\\n\\nTheorem 21.25. A field F is algebraically closed if and only if every nonconstant polyno-\\nmial in F [x] factors into linear factors over F [x].\\n\\n\\n\\n362 CHAPTER 21. FIELDS\\n\\nProof. Let F be an algebraically closed field. If p(x) ∈ F [x] is a nonconstant polynomial,\\nthen p(x) has a zero in F , say α. Therefore, x − α must be a factor of p(x) and so\\np(x) = (x − α)q1(x), where deg q1(x) = deg p(x) − 1. Continue this process with q1(x) to\\nfind a factorization\\n\\np(x) = (x− α)(x− β)q2(x),\\n\\nwhere deg q2(x) = deg p(x)− 2. The process must eventually stop since the degree of p(x)\\nis finite.\\n\\nConversely, suppose that every nonconstant polynomial p(x) in F [x] factors into linear\\nfactors. Let ax − b be such a factor. Then p(b/a) = 0. Consequently, F is algebraically\\nclosed.\\n\\nCorollary 21.26. An algebraically closed field F has no proper algebraic extension E.\\n\\nProof. Let E be an algebraic extension of F ; then F ⊂ E. For α ∈ E, the minimal\\npolynomial of α is x− α. Therefore, α ∈ F and F = E.\\n\\nTheorem 21.27. Every field F has a unique algebraic closure.\\n\\nIt is a nontrivial fact that every field has a unique algebraic closure. The proof is not\\nextremely difficult, but requires some rather sophisticated set theory. We refer the reader\\nto [3], [4], or [8] for a proof of this result.\\n\\nWe now state the Fundamental Theorem of Algebra, first proven by Gauss at the age\\nof 22 in his doctoral thesis. This theorem states that every polynomial with coefficients in\\nthe complex numbers has a root in the complex numbers. The proof of this theorem will\\nbe given in Chapter 23.\\n\\nTheorem 21.28 (Fundamental Theorem of Algebra). The field of complex numbers is\\nalgebraically closed.\\n\\n21.2 Splitting Fields\\nLet F be a field and p(x) be a nonconstant polynomial in F [x]. We already know that we\\ncan find a field extension of F that contains a root of p(x). However, we would like to know\\nwhether an extension E of F containing all of the roots of p(x) exists. In other words, can\\nwe find a field extension of F such that p(x) factors into a product of linear polynomials?\\nWhat is the “smallest” extension containing all the roots of p(x)?\\n\\nLet F be a field and p(x) = a0 + a1x+ · · ·+ anx\\nn be a nonconstant polynomial in F [x].\\n\\nAn extension field E of F is a splitting field of p(x) if there exist elements α1, . . . , αn in\\nE such that E = F (α1, . . . , αn) and\\n\\np(x) = (x− α1)(x− α2) · · · (x− αn).\\n\\nA polynomial p(x) ∈ F [x] splits in E if it is the product of linear factors in E[x].\\n\\nExample 21.29. Let p(x) = x4 + 2x2 − 8 be in Q[x]. Then p(x) has irreducible factors\\nx2 − 2 and x2 + 4. Therefore, the field Q(\\n\\n√\\n2, i) is a splitting field for p(x).\\n\\nExample 21.30. Let p(x) = x3 − 3 be in Q[x]. Then p(x) has a root in the field Q( 3\\n√\\n3 ).\\n\\nHowever, this field is not a splitting field for p(x) since the complex cube roots of 3,\\n\\n− 3\\n√\\n3± ( 6\\n\\n√\\n3 )5i\\n\\n2\\n,\\n\\nare not in Q( 3\\n√\\n3 ).\\n\\n\\n\\n21.2. SPLITTING FIELDS 363\\n\\nTheorem 21.31. Let p(x) ∈ F [x] be a nonconstant polynomial. Then there exists a splitting\\nfield E for p(x).\\n\\nProof. We will use mathematical induction on the degree of p(x). If deg p(x) = 1, then\\np(x) is a linear polynomial and E = F . Assume that the theorem is true for all polynomials\\nof degree k with 1 ≤ k < n and let deg p(x) = n. We can assume that p(x) is irreducible;\\notherwise, by our induction hypothesis, we are done. By Theorem 21.5, there exists a field\\nK such that p(x) has a zero α1 in K. Hence, p(x) = (x − α1)q(x), where q(x) ∈ K[x].\\nSince deg q(x) = n− 1, there exists a splitting field E ⊃ K of q(x) that contains the zeros\\nα2, . . . , αn of p(x) by our induction hypothesis. Consequently,\\n\\nE = K(α2, . . . , αn) = F (α1, . . . , αn)\\n\\nis a splitting field of p(x).\\n\\nThe question of uniqueness now arises for splitting fields. This question is answered in\\nthe affirmative. Given two splitting fields K and L of a polynomial p(x) ∈ F [x], there exists\\na field isomorphism ϕ : K → L that preserves F . In order to prove this result, we must first\\nprove a lemma.\\n\\nLemma 21.32. Let ϕ : E → F be an isomorphism of fields. Let K be an extension field\\nof E and α ∈ K be algebraic over E with minimal polynomial p(x). Suppose that L is\\nan extension field of F such that β is root of the polynomial in F [x] obtained from p(x)\\nunder the image of ϕ. Then ϕ extends to a unique isomorphism ϕ : E(α) → F (β) such that\\nϕ(α) = β and ϕ agrees with ϕ on E.\\n\\nProof. If p(x) has degree n, then by Theorem 21.13 we can write any element in E(α)\\nas a linear combination of 1, α, . . . , αn−1. Therefore, the isomorphism that we are seeking\\nmust be\\n\\nϕ(a0 + a1α+ · · ·+ an−1α\\nn−1) = ϕ(a0) + ϕ(a1)β + · · ·+ ϕ(an−1)β\\n\\nn−1,\\n\\nwhere\\na0 + a1α+ · · ·+ an−1α\\n\\nn−1\\n\\nis an element in E(α). The fact that ϕ is an isomorphism could be checked by direct\\ncomputation; however, it is easier to observe that ϕ is a composition of maps that we\\nalready know to be isomorphisms.\\n\\nWe can extend ϕ to be an isomorphism from E[x] to F [x], which we will also denote by\\nϕ, by letting\\n\\nϕ(a0 + a1x+ · · ·+ anx\\nn) = ϕ(a0) + ϕ(a1)x+ · · ·+ ϕ(an)x\\n\\nn.\\n\\nThis extension agrees with the original isomorphism ϕ : E → F , since constant polynomials\\nget mapped to constant polynomials. By assumption, ϕ(p(x)) = q(x); hence, ϕ maps ⟨p(x)⟩\\nonto ⟨q(x)⟩. Consequently, we have an isomorphism ψ : E[x]/⟨p(x)⟩ → F [x]/⟨q(x)⟩. By\\nProposition 21.12, we have isomorphisms σ : E[x]/⟨p(x)⟩ → E(α) and τ : F [x]/⟨q(x)⟩ →\\nF (β), defined by evaluation at α and β, respectively. Therefore, ϕ = τψσ−1 is the required\\nisomorphism.\\n\\n\\n\\n364 CHAPTER 21. FIELDS\\n\\nE F\\n\\nE(α) F (β)\\n\\nE[x]/⟨p(x)⟩ F [x]/⟨q(x)⟩\\n\\nϕ\\n\\nϕ\\n\\nψ\\n\\nσ τ\\n\\nWe leave the proof of uniqueness as a exercise.\\n\\nTheorem 21.33. Let ϕ : E → F be an isomorphism of fields and let p(x) be a nonconstant\\npolynomial in E[x] and q(x) the corresponding polynomial in F [x] under the isomorphism.\\nIf K is a splitting field of p(x) and L is a splitting field of q(x), then ϕ extends to an\\nisomorphism ψ : K → L.\\n\\nProof. We will use mathematical induction on the degree of p(x). We can assume that\\np(x) is irreducible over E. Therefore, q(x) is also irreducible over F . If deg p(x) = 1, then\\nby the definition of a splitting field, K = E and L = F and there is nothing to prove.\\n\\nAssume that the theorem holds for all polynomials of degree less than n. Since K is a\\nsplitting field of p(x), all of the roots of p(x) are in K. Choose one of these roots, say α, such\\nthat E ⊂ E(α) ⊂ K. Similarly, we can find a root β of q(x) in L such that F ⊂ F (β) ⊂ L.\\nBy Lemma 21.32, there exists an isomorphism ϕ : E(α) → F (β) such that ϕ(α) = β and ϕ\\nagrees with ϕ on E.\\n\\nE F\\n\\nE(α) F (β)\\n\\nK L\\n\\nϕ\\n\\nϕ\\n\\nψ\\n\\nNow write p(x) = (x − α)f(x) and q(x) = (x − β)g(x), where the degrees of f(x) and\\ng(x) are less than the degrees of p(x) and q(x), respectively. The field extension K is a\\nsplitting field for f(x) over E(α), and L is a splitting field for g(x) over F (β). By our\\ninduction hypothesis there exists an isomorphism ψ : K → L such that ψ agrees with ϕ on\\nE(α). Hence, there exists an isomorphism ψ : K → L such that ψ agrees with ϕ on E.\\n\\nCorollary 21.34. Let p(x) be a polynomial in F [x]. Then there exists a splitting field K\\nof p(x) that is unique up to isomorphism.\\n\\n21.3 Geometric Constructions\\nIn ancient Greece, three classic problems were posed. These problems are geometric in\\nnature and involve straightedge-and-compass constructions from what is now high school\\ngeometry; that is, we are allowed to use only a straightedge and compass to solve them.\\nThe problems can be stated as follows.\\n\\n\\n\\n21.3. GEOMETRIC CONSTRUCTIONS 365\\n\\n1. Given an arbitrary angle, can one trisect the angle into three equal subangles using\\nonly a straightedge and compass?\\n\\n2. Given an arbitrary circle, can one construct a square with the same area using only a\\nstraightedge and compass?\\n\\n3. Given a cube, can one construct the edge of another cube having twice the volume of\\nthe original? Again, we are only allowed to use a straightedge and compass to do the\\nconstruction.\\n\\nAfter puzzling mathematicians for over two thousand years, each of these constructions\\nwas finally shown to be impossible. We will use the theory of fields to provide a proof that\\nthe solutions do not exist. It is quite remarkable that the long-sought solution to each of\\nthese three geometric problems came from abstract algebra.\\n\\nFirst, let us determine more specifically what we mean by a straightedge and compass,\\nand also examine the nature of these problems in a bit more depth. To begin with, a\\nstraightedge is not a ruler. We cannot measure arbitrary lengths with a straightedge. It is\\nmerely a tool for drawing a line through two points. The statement that the trisection of\\nan arbitrary angle is impossible means that there is at least one angle that is impossible to\\ntrisect with a straightedge-and-compass construction. Certainly it is possible to trisect an\\nangle in special cases. We can construct a 30◦ angle; hence, it is possible to trisect a 90◦\\n\\nangle. However, we will show that it is impossible to construct a 20◦ angle. Therefore, we\\ncannot trisect a 60◦ angle.\\n\\nConstructible Numbers\\nA real number α is constructible if we can construct a line segment of length |α| in a finite\\nnumber of steps from a segment of unit length by using a straightedge and compass.\\n\\nTheorem 21.35. The set of all constructible real numbers forms a subfield F of the field\\nof real numbers.\\n\\nProof. Let α and β be constructible numbers. We must show that α+ β, α− β, αβ, and\\nα/β (β ̸= 0) are also constructible numbers. We can assume that both α and β are positive\\nwith α > β. It is quite obvious how to construct α + β and α − β. To find a line segment\\nwith length αβ, we assume that β > 1 and construct the triangle in Figure 21.36 such that\\ntriangles △ABC and △ADE are similar. Since α/1 = x/β, the line segment x has length\\nαβ. A similar construction can be made if β < 1. We will leave it as an exercise to show\\nthat the same triangle can be used to construct α/β for β ̸= 0.\\n\\nA E\\n\\nB\\n\\nC\\n\\nD\\n\\n1\\n\\nα\\n\\nβ\\n\\nx\\n\\nFigure 21.36: Construction of products\\n\\nLemma 21.37. If α is a constructible number, then\\n√\\nα is a constructible number.\\n\\n\\n\\n366 CHAPTER 21. FIELDS\\n\\nProof. In Figure 21.38 the triangles △ABD, △BCD, and △ABC are similar; hence,\\n1/x = x/α, or x2 = α.\\n\\nDA C\\n\\nB\\n\\nα1\\n\\nx\\n\\nFigure 21.38: Construction of roots\\n\\nBy Theorem 21.35, we can locate in the plane any point P = (p, q) that has rational\\ncoordinates p and q. We need to know what other points can be constructed with a compass\\nand straightedge from points with rational coordinates.\\n\\nLemma 21.39. Let F be a subfield of R.\\n\\n1. If a line contains two points in F , then it has the equation ax+ by + c = 0, where a,\\nb, and c are in F .\\n\\n2. If a circle has a center at a point with coordinates in F and a radius that is also in\\nF , then it has the equation x2 + y2 + dx+ ey + f = 0, where d, e, and f are in F .\\n\\nProof. Let (x1, y1) and (x2, y2) be points on a line whose coordinates are in F . If x1 = x2,\\nthen the equation of the line through the two points is x − x1 = 0, which has the form\\nax+ by+ c = 0. If x1 ̸= x2, then the equation of the line through the two points is given by\\n\\ny − y1 =\\n\\n(\\ny2 − y1\\nx2 − x1\\n\\n)\\n(x− x1),\\n\\nwhich can also be put into the proper form.\\nTo prove the second part of the lemma, suppose that (x1, y1) is the center of a circle of\\n\\nradius r. Then the circle has the equation\\n\\n(x− x1)\\n2 + (y − y1)\\n\\n2 − r2 = 0.\\n\\nThis equation can easily be put into the appropriate form.\\n\\nStarting with a field of constructible numbers F , we have three possible ways of con-\\nstructing additional points in R with a compass and straightedge.\\n\\n1. To find possible new points in R, we can take the intersection of two lines, each of\\nwhich passes through two known points with coordinates in F .\\n\\n2. The intersection of a line that passes through two points that have coordinates in F\\nand a circle whose center has coordinates in F with radius of a length in F will give\\nnew points in R.\\n\\n3. We can obtain new points in R by intersecting two circles whose centers have coordi-\\nnates in F and whose radii are of lengths in F .\\n\\n\\n\\n21.3. GEOMETRIC CONSTRUCTIONS 367\\n\\nThe first case gives no new points in R, since the solution of two equations of the form\\nax+ by+ c = 0 having coefficients in F will always be in F . The third case can be reduced\\nto the second case. Let\\n\\nx2 + y2 + d1x+ e1y + f1 = 0\\n\\nx2 + y2 + d2x+ e2y + f2 = 0\\n\\nbe the equations of two circles, where di, ei, and fi are in F for i = 1, 2. These circles have\\nthe same intersection as the circle\\n\\nx2 + y2 + d1x+ e1x+ f1 = 0\\n\\nand the line\\n(d1 − d2)x+ b(e2 − e1)y + (f2 − f1) = 0.\\n\\nThe last equation is that of the chord passing through the intersection points of the two\\ncircles. Hence, the intersection of two circles can be reduced to the case of an intersection\\nof a line with a circle.\\n\\nConsidering the case of the intersection of a line and a circle, we must determine the\\nnature of the solutions of the equations\\n\\nax+ by + c = 0\\n\\nx2 + y2 + dx+ ey + f = 0.\\n\\nIf we eliminate y from these equations, we obtain an equation of the form Ax2+Bx+C = 0,\\nwhere A, B, and C are in F . The x coordinate of the intersection points is given by\\n\\nx =\\n−B ±\\n\\n√\\nB2 − 4AC\\n\\n2A\\n\\nand is in F (\\n√\\nα ), where α = B2 − 4AC > 0. We have proven the following lemma.\\n\\nLemma 21.40. Let F be a field of constructible numbers. Then the points determined by\\nthe intersections of lines and circles in F lie in the field F (\\n\\n√\\nα ) for some α in F .\\n\\nTheorem 21.41. A real number α is a constructible number if and only if there exists a\\nsequence of fields\\n\\nQ = F0 ⊂ F1 ⊂ · · · ⊂ Fk\\n\\nsuch that Fi = Fi−1(\\n√\\nαi ) with αi ∈ Fi and α ∈ Fk. In particular, there exists an integer\\n\\nk > 0 such that [Q(α) : Q] = 2k.\\n\\nProof. The existence of the Fi’s and the αi’s is a direct consequence of Lemma 21.40 and\\nof the fact that\\n\\n[Fk : Q] = [Fk : Fk−1][Fk−1 : Fk−2] · · · [F1 : Q] = 2k.\\n\\nCorollary 21.42. The field of all constructible numbers is an algebraic extension of Q.\\n\\nAs we can see by the field of constructible numbers, not every algebraic extension of a\\nfield is a finite extension.\\n\\n\\n\\n368 CHAPTER 21. FIELDS\\n\\nDoubling the Cube and Squaring the Circle\\nWe are now ready to investigate the classical problems of doubling the cube and squaring\\nthe circle. We can use the field of constructible numbers to show exactly when a particular\\ngeometric construction can be accomplished.\\n\\nDoubling the cube is impossible. Given the edge of the cube, it is impossible to construct\\nwith a straightedge and compass the edge of the cube that has twice the volume of the\\noriginal cube. Let the original cube have an edge of length 1 and, therefore, a volume of 1.\\nIf we could construct a cube having a volume of 2, then this new cube would have an edge\\nof length 3\\n\\n√\\n2. However, 3\\n\\n√\\n2 is a zero of the irreducible polynomial x3 − 2 over Q; hence,\\n\\n[Q(\\n3\\n√\\n2 ) : Q] = 3\\n\\nThis is impossible, since 3 is not a power of 2.\\nSquaring the circle. Suppose that we have a circle of radius 1. The area of the circle\\n\\nis π; therefore, we must be able to construct a square with side\\n√\\nπ. This is impossible\\n\\nsince π and consequently\\n√\\nπ are both transcendental. Therefore, using a straightedge and\\n\\ncompass, it is not possible to construct a square with the same area as the circle.\\n\\nTrisecting an Angle\\nTrisecting an arbitrary angle is impossible. We will show that it is impossible to construct\\na 20◦ angle. Consequently, a 60◦ angle cannot be trisected. We first need to calculate the\\ntriple-angle formula for the cosine:\\n\\ncos 3θ = cos(2θ + θ)\\n\\n= cos 2θ cos θ − sin 2θ sin θ\\n= (2 cos2 θ − 1) cos θ − 2 sin2 θ cos θ\\n= (2 cos2 θ − 1) cos θ − 2(1− cos2 θ) cos θ\\n= 4 cos3 θ − 3 cos θ.\\n\\nThe angle θ can be constructed if and only if α = cos θ is constructible. Let θ = 20◦. Then\\ncos 3θ = cos 60◦ = 1/2. By the triple-angle formula for the cosine,\\n\\n4α3 − 3α =\\n1\\n\\n2\\n.\\n\\nTherefore, α is a zero of 8x3 − 6x − 1. This polynomial has no factors in Z[x], and hence\\nis irreducible over Q[x]. Thus, [Q(α) : Q] = 3. Consequently, α cannot be a constructible\\nnumber.\\n\\nHistorical Note\\n\\nAlgebraic number theory uses the tools of algebra to solve problems in number theory.\\nModern algebraic number theory began with Pierre de Fermat (1601–1665). Certainly we\\ncan find many positive integers that satisfy the equation x2 + y2 = z2; Fermat conjectured\\nthat the equation xn+ yn = zn has no positive integer solutions for n ≥ 3. He stated in the\\nmargin of his copy of the Latin translation of Diophantus’ Arithmetica that he had found a\\nmarvelous proof of this theorem, but that the margin of the book was too narrow to contain\\nit. Building on work of other mathematicians, it was Andrew Wiles who finally succeeded\\nin proving Fermat’s Last Theorem in the 1990s. Wiles’s achievement was reported on the\\nfront page of the New York Times.\\n\\n\\n\\n21.4. EXERCISES 369\\n\\nAttempts to prove Fermat’s Last Theorem have led to important contributions to al-\\ngebraic number theory by such notable mathematicians as Leonhard Euler (1707–1783).\\nSignificant advances in the understanding of Fermat’s Last Theorem were made by Ernst\\nKummer (1810–1893). Kummer’s student, Leopold Kronecker (1823–1891), became one of\\nthe leading algebraists of the nineteenth century. Kronecker’s theory of ideals and his study\\nof algebraic number theory added much to the understanding of fields.\\n\\nDavid Hilbert (1862–1943) and Hermann Minkowski (1864–1909) were among the math-\\nematicians who led the way in this subject at the beginning of the twentieth century. Hilbert\\nand Minkowski were both mathematicians at Göttingen University in Germany. Göttingen\\nwas truly one the most important centers of mathematical research during the last two cen-\\nturies. The large number of exceptional mathematicians who studied there included Gauss,\\nDirichlet, Riemann, Dedekind, Noether, and Weyl.\\n\\nAndré Weil answered questions in number theory using algebraic geometry, a field of\\nmathematics that studies geometry by studying commutative rings. From about 1955 to\\n1970, Alexander Grothendieck dominated the field of algebraic geometry. Pierre Deligne,\\na student of Grothendieck, solved several of Weil’s number-theoretic conjectures. One of\\nthe most recent contributions to algebra and number theory is Gerd Falting’s proof of the\\nMordell-Weil conjecture. This conjecture of Mordell and Weil essentially says that certain\\npolynomials p(x, y) in Z[x, y] have only a finite number of integral solutions.\\n\\n21.4 Exercises\\n1. Show that each of the following numbers is algebraic over Q by finding the minimal\\npolynomial of the number over Q.\\n\\n(a)\\n√\\n\\n1/3 +\\n√\\n7\\n\\n(b)\\n√\\n3 + 3\\n\\n√\\n5\\n\\n(c)\\n√\\n3 +\\n\\n√\\n2 i\\n\\n(d) cos θ + i sin θ for θ = 2π/n with n ∈ N\\n\\n(e)\\n√\\n\\n3\\n√\\n2− i\\n\\n2. Find a basis for each of the following field extensions. What is the degree of each\\nextension?\\n(a) Q(\\n\\n√\\n3,\\n√\\n6 ) over Q\\n\\n(b) Q( 3\\n√\\n2, 3\\n\\n√\\n3 ) over Q\\n\\n(c) Q(\\n√\\n2, i) over Q\\n\\n(d) Q(\\n√\\n3,\\n√\\n5,\\n√\\n7 ) over Q\\n\\n(e) Q(\\n√\\n2, 3\\n\\n√\\n2 ) over Q\\n\\n(f) Q(\\n√\\n8 ) over Q(\\n\\n√\\n2 )\\n\\n(g) Q(i,\\n√\\n2 + i,\\n\\n√\\n3 + i) over Q\\n\\n(h) Q(\\n√\\n2 +\\n\\n√\\n5 ) over Q(\\n\\n√\\n5 )\\n\\n(i) Q(\\n√\\n2,\\n√\\n6 +\\n\\n√\\n10 ) over Q(\\n\\n√\\n3 +\\n\\n√\\n5 )\\n\\n3. Find the splitting field for each of the following polynomials.\\n\\n\\n\\n370 CHAPTER 21. FIELDS\\n\\n(a) x4 − 10x2 + 21 over Q\\n(b) x4 + 1 over Q\\n\\n(c) x3 + 2x+ 2 over Z3\\n\\n(d) x3 − 3 over Q\\n\\n4. Consider the field extension Q( 4\\n√\\n3, i) over Q.\\n\\n(a) Find a basis for the field extension Q( 4\\n√\\n3, i) over Q. Conclude that [Q( 4\\n\\n√\\n3, i) : Q] = 8.\\n\\n(b) Find all subfields F of Q( 4\\n√\\n3, i) such that [F : Q] = 2.\\n\\n(c) Find all subfields F of Q( 4\\n√\\n3, i) such that [F : Q] = 4.\\n\\n5. Show that Z2[x]/⟨x3 + x+ 1⟩ is a field with eight elements. Construct a multiplication\\ntable for the multiplicative group of the field.\\n\\n6. Show that the regular 9-gon is not constructible with a straightedge and compass, but\\nthat the regular 20-gon is constructible.\\n\\n7. Prove that the cosine of one degree (cos 1◦) is algebraic over Q but not constructible.\\n\\n8. Can a cube be constructed with three times the volume of a given cube?\\n\\n9. Prove that Q(\\n√\\n3, 4\\n\\n√\\n3, 8\\n\\n√\\n3, . . .) is an algebraic extension of Q but not a finite extension.\\n\\n10. Prove or disprove: π is algebraic over Q(π3).\\n\\n11. Let p(x) be a nonconstant polynomial of degree n in F [x]. Prove that there exists a\\nsplitting field E for p(x) such that [E : F ] ≤ n!.\\n\\n12. Prove or disprove: Q(\\n√\\n2 ) ∼= Q(\\n\\n√\\n3 ).\\n\\n13. Prove that the fields Q( 4\\n√\\n3 ) and Q( 4\\n\\n√\\n3 i) are isomorphic but not equal.\\n\\n14. Let K be an algebraic extension of E, and E an algebraic extension of F . Prove that\\nK is algebraic over F . [ Caution: Do not assume that the extensions are finite.]\\n\\n15. Prove or disprove: Z[x]/⟨x3 − 2⟩ is a field.\\n\\n16. Let F be a field of characteristic p. Prove that p(x) = xp − a either is irreducible over\\nF or splits in F .\\n\\n17. Let E be the algebraic closure of a field F . Prove that every polynomial p(x) in F [x]\\nsplits in E.\\n\\n18. If every irreducible polynomial p(x) in F [x] is linear, show that F is an algebraically\\nclosed field.\\n\\n19. Prove that if α and β are constructible numbers such that β ̸= 0, then so is α/β.\\n\\n20. Show that the set of all elements in R that are algebraic over Q form a field extension\\nof Q that is not finite.\\n\\n21. Let E be an algebraic extension of a field F , and let σ be an automorphism of E leaving\\nF fixed. Let α ∈ E. Show that σ induces a permutation of the set of all zeros of the minimal\\npolynomial of α that are in E.\\n\\n22. Show that Q(\\n√\\n3,\\n√\\n7 ) = Q(\\n\\n√\\n3 +\\n\\n√\\n7 ). Extend your proof to show that Q(\\n\\n√\\na,\\n√\\nb ) =\\n\\nQ(\\n√\\na+\\n\\n√\\nb ), where gcd(a, b) = 1.\\n\\n23. Let E be a finite extension of a field F . If [E : F ] = 2, show that E is a splitting field\\nof F .\\n\\n\\n\\n21.5. REFERENCES AND SUGGESTED READINGS 371\\n\\n24. Prove or disprove: Given a polynomial p(x) in Z6[x], it is possible to construct a ring\\nR such that p(x) has a root in R.\\n\\n25. Let E be a field extension of F and α ∈ E. Determine [F (α) : F (α3)].\\n\\n26. Let α, β be transcendental over Q. Prove that either αβ or α+β is also transcendental.\\n\\n27. Let E be an extension field of F and α ∈ E be transcendental over F . Prove that every\\nelement in F (α) that is not in F is also transcendental over F .\\n\\n21.5 References and Suggested Readings\\n[1] Dean, R. A. Elements of Abstract Algebra. Wiley, New York, 1966.\\n[2] Dudley, U. A Budget of Trisections. Springer-Verlag, New York, 1987. An interesting\\n\\nand entertaining account of how not to trisect an angle.\\n[3] Fraleigh, J. B. A First Course in Abstract Algebra. 7th ed. Pearson, Upper Saddle\\n\\nRiver, NJ, 2003.\\n[4] Kaplansky, I. Fields and Rings, 2nd ed. University of Chicago Press, Chicago, 1972.\\n[5] Klein, F. Famous Problems of Elementary Geometry. Chelsea, New York, 1955.\\n[6] Martin, G. Geometric Constructions. Springer, New York, 1998.\\n[7] H. Pollard and H. G. Diamond. Theory of Algebraic Numbers, Dover, Mineola, NY,\\n\\n2010.\\n[8] Walker, E. A. Introduction to Abstract Algebra. Random House, New York, 1987.\\n\\nThis work contains a proof showing that every field has an algebraic closure.\\n\\n21.6 Sage\\nIn Sage, and other places, an extension of the rationals is called a “number field.” They are\\none of Sage’s most mature features.\\n\\nNumber Fields\\nThere are several ways to create a number field. We are familiar with the syntax where we\\nadjoin an irrational number that we can write with traditional combinations of arithmetic\\nand roots.\\n\\nM.<a> = QQ[sqrt (2)+sqrt (3)]; M\\n\\nNumber Field in a with defining polynomial x^4 - 10*x^2 + 1\\n\\nWe can also specify the element we want to adjoin as the root of a monic irreducible\\npolynomial. One approach is to construct the polynomial ring first so that the polynomial\\nhas the location of its coefficients specified properly.\\n\\nF.<y> = QQ[]\\np = y^3 - 1/4*y^2 - 1/16*y + 1/4\\np.is_irreducible ()\\n\\nTrue\\n\\nN.<b> = NumberField(p, \' b \' ); N\\n\\n\\n\\n372 CHAPTER 21. FIELDS\\n\\nNumber Field in b with\\ndefining polynomial y^3 - 1/4*y^2 - 1/16*y + 1/4\\n\\nRather than building the whole polynomial ring, we can simply introduce a variable as\\nthe generator of a polynomial ring and then create polynomials from this variable. This\\nspares us naming the polynomial ring. Notice in the example that both instances of z are\\nnecessary.\\n\\nz = polygen(QQ, \' z \' )\\nq = z^3 - 1/4*z^2 - 1/16*z + 1/4\\nq.parent ()\\n\\nUnivariate Polynomial Ring in z over Rational Field\\n\\nP.<c> = NumberField(q, \' c \' ); P\\n\\nNumber Field in c with\\ndefining polynomial z^3 - 1/4*z^2 - 1/16*z + 1/4\\n\\nWe can recover the polynomial used to create a number field, even if we constructed\\nit by giving an expression for an irrational element. In this case, the polynomial is the\\nminimal polynomial of the element.\\n\\nM.polynomial ()\\n\\nx^4 - 10*x^2 + 1\\n\\nN.polynomial ()\\n\\ny^3 - 1/4*y^2 - 1/16*y + 1/4\\n\\nFor any element of a number field, Sage will obligingly compute its minimal polynomial.\\n\\nelement = -b^2 + 1/3*b + 4\\nelement.parent ()\\n\\nNumber Field in b with\\ndefining polynomial y^3 - 1/4*y^2 - 1/16*y + 1/4\\n\\nr = element.minpoly( \' t \' ); r\\n\\nt^3 - 571/48*t^2 + 108389/2304*t - 13345/216\\n\\nr.parent ()\\n\\nUnivariate Polynomial Ring in t over Rational Field\\n\\nr.subs(t=element)\\n\\n0\\n\\nSubstituting element back into the alleged minimal polynomial and getting back zero is\\nnot convincing evidence that it is the minimal polynomial, but it is heartening.\\n\\n\\n\\n21.6. SAGE 373\\n\\nRelative and Absolute Number Fields\\nWith Sage we can adjoin several elements at once and we can build nested towers of number\\nfields. Sage uses the term “absolute” to refer to a number field viewed as an extension of\\nthe rationals themselves, and the term “relative” to refer to a number field constructed, or\\nviewed, as an extension of another (nontrivial) number field.\\n\\nA.<a,b> = QQ[sqrt (2), sqrt (3)]\\nA\\n\\nNumber Field in sqrt2 with defining polynomial x^2 - 2 over\\nits base field\\n\\nB = A.base_field (); B\\n\\nNumber Field in sqrt3 with defining polynomial x^2 - 3\\n\\nA.is_relative ()\\n\\nTrue\\n\\nB.is_relative ()\\n\\nFalse\\n\\nThe number field A has been constructed mathematically as what we would write as\\nQ ⊂ Q[\\n\\n√\\n3] ⊂ Q[\\n\\n√\\n3,\\n√\\n2]. Notice the slight difference in ordering of the elements we are\\n\\nadjoining, and notice how the number fields use slightly fancier internal names (sqrt2, sqrt3)\\nfor the new elements.\\n\\nWe can “flatten” a relative field to view it as an absolute field, which may have been our\\nintention from the start. Here we create a new number field from A that makes it a pure\\nabsolute number field.\\n\\nC.<c> = A.absolute_field ()\\nC\\n\\nNumber Field in c with defining polynomial x^4 - 10*x^2 + 1\\n\\nOnce we construct an absolute number field this way, we can recover isomorphisms to\\nand from the absolute field. Recall that our tower was built with generators a and b, while\\nthe flattened tower is generated by c. The .structure() method returns a pair of functions,\\nwith the absolute number field as the domain and codomain (in that order).\\n\\nfromC , toC = C.structure ()\\nfromC(c)\\n\\nsqrt2 - sqrt3\\n\\ntoC(a)\\n\\n1/2*c^3 - 9/2*c\\n\\ntoC(b)\\n\\n1/2*c^3 - 11/2*c\\n\\n\\n\\n374 CHAPTER 21. FIELDS\\n\\nThis tells us that the single generator of the flattened tower, c, is equal to\\n√\\n2 −\\n\\n√\\n3,\\n\\nand further, each of\\n√\\n2 and\\n\\n√\\n3 can be expressed as polynomial functions of c. With\\n\\nthese connections, you might want to compute the final two expressions in c by hand, and\\nappreciate the work Sage does to determine these for us. This computation is an example\\nof the conclusion of the upcoming Theorem 23.12.\\n\\nMany number field methods have both relative and absolute versions, and we will also\\nfind it more convenient to work in a tower or a flattened version, thus the isomorphisms\\nbetween the two can be invaluable for translating both questions and answers.\\n\\nAs a vector space over Q, or over another number field, number fields that are finite\\nextensions have a dimension, called the degree. These are easy to get from Sage, though\\nfor a relative field, we need to be more precise about which degree we desire.\\n\\nB.degree ()\\n\\n2\\n\\nA.absolute_degree ()\\n\\n4\\n\\nA.relative_degree ()\\n\\n2\\n\\nSplitting Fields\\nHere is a concrete example of how to use Sage to construct a splitting field of a polynomial.\\nConsider p(x) = x4 + x2 − 1. We first build a number field with a single root, and then\\nfactor the polynomial over this new, larger, field.\\n\\nx = polygen(QQ, \' x \' )\\np = x^4 + x^2 - 1\\np.parent ()\\n\\nUnivariate Polynomial Ring in x over Rational Field\\n\\np.is_irreducible ()\\n\\nTrue\\n\\nM.<a> = NumberField(p, \' a \' )\\ny = polygen(M, \' y \' )\\np = p.subs(x = y)\\np\\n\\ny^4 + y^2 - 1\\n\\np.parent ()\\n\\nUnivariate Polynomial Ring in y over Number Field in a with\\ndefining polynomial x^4 + x^2 - 1\\n\\np.factor ()\\n\\n\\n\\n21.6. SAGE 375\\n\\n(y - a) * (y + a) * (y^2 + a^2 + 1)\\n\\na^2 + 1 in QQ\\n\\nFalse\\n\\nSo our polynomial factors partially into two linear factors and a quadratic factor. But\\nnotice that the quadratic factor has a coefficient that is irrational, a2 + 1, so the quadratic\\nfactor properly belongs in the polynomial ring over M and not over QQ.\\n\\nWe build an extension containing a root of the quadratic factor, called q here. Then,\\nrather than using the polygen() function, we build an entire polynomial ring R over N with\\nthe indeterminate z. The reason for doing this is we can illustrate how we “upgrade” the\\npolynomial p with the syntax R(p) to go from having coefficients in M to having coefficients\\nin N.\\n\\nq = y^2 + a^2 + 1\\nN.<b> = NumberField(q, \' b \' )\\nR.<z> = N[]\\ns = R(p)\\ns\\n\\nz^4 + z^2 - 1\\n\\ns.parent ()\\n\\nUnivariate Polynomial Ring in z over Number Field in b with\\ndefining polynomial y^2 + a^2 + 1 over its base field\\n\\ns.factor ()\\n\\n(z + b) * (z + a) * (z - a) * (z - b)\\n\\na in N, b in N\\n\\n(True , True)\\n\\nSo we have a field, N, where our polynomial factors into linear factors with coefficients\\nfrom the field. We can get another factorization by converting N to an absolute number\\nfield and factoring there. We need to recreate the polynomial over N, since a substitution\\nwill carry coefficients from the wrong ring.\\n\\nP.<c> = N.absolute_field ()\\nw = polygen(P, \' w \' )\\np = w^4 + w^2- 1\\np.factor ()\\n\\n(w - 7/18966*c^7 + 110/9483*c^5 + 923/9483*c^3 + 3001/6322*c) *\\n(w - 7/37932*c^7 + 55/9483*c^5 + 923/18966*c^3 - 3321/12644*c) *\\n(w + 7/37932*c^7 - 55/9483*c^5 - 923/18966*c^3 + 3321/12644*c) *\\n(w + 7/18966*c^7 - 110/9483*c^5 - 923/9483*c^3 - 3001/6322*c)\\n\\nThis is an interesting alternative, in that the roots of the polynomial are expressions\\nin terms of the single generator c. Since the roots involve a seventh power of c, we might\\nsuspect (but not be certain) that the minimal polynomial of c has degree 8 and that P is a\\ndegree 8 extension of the rationals. Indeed P (or N) is a splitting field for p(x) = x4+x2−1.\\n\\n\\n\\n376 CHAPTER 21. FIELDS\\n\\nThe roots are not really as bad as they appear — lets convert them back to the relative\\nnumber field.\\n\\nFirst we want to rewrite a single factor (the first) in the form (w − r) to identify the\\nroot with the correct signs.\\n\\n(w - 7/18966*c^7 + 110/9483*c^5 + 923/9483*c^3 + 3001/6322*c) =\\n(w - (7/18966*c^7 - 110/9483*c^5 - 923/9483*c^3 - 3001/6322*c))\\n\\nWith the conversion isomorphisms, we can recognize the roots for what they are.\\nfromP , toP = P.structure ()\\nfromP (7/18966*c^7 - 110/9483*c^5 - 923/9483*c^3 - 3001/6322*c)\\n\\n-b\\n\\nSo the rather complicated expression in c is just the negative of the root we adjoined\\nin the second step of constructing the tower of number fields. It would be a good exercise\\nto see what happens to the other three roots (being careful to get the signs right on each\\nroot).\\n\\nThis is a good opportunity to illustrate Theorem 21.17.\\nM.degree ()\\n\\n4\\n\\nN.relative_degree ()\\n\\n2\\n\\nP.degree ()\\n\\n8\\n\\nM.degree ()*N.relative_degree () == P.degree ()\\n\\nTrue\\n\\nAlgebraic Numbers\\nCorollary 21.24 says that the set of all algebraic numbers forms a field. This field is imple-\\nmented in Sage as QQbar. This allows for finding roots of polynomials as exact quantities\\nwhich display as inexact numbers.\\n\\nx = polygen(QQ, \' x \' )\\np = x^4 + x^2 - 1\\nr = p.roots(ring=QQbar); r\\n\\n[( -0.7861513777574233? , 1), (0.7861513777574233? , 1),\\n( -1.272019649514069?*I, 1), (1.272019649514069?*I, 1)]\\n\\nSo we asked for the roots of a polynomial over the rationals, but requested any root\\nthat may lie outside the rationals and within the field of algebraic numbers. Since the field\\nof algebraic numbers contains all such roots, we get a full four roots of the fourth-degree\\npolynomial. These roots are computed to lie within an interval and the question mark\\nindicates that the preceding digits are accurate. (The integers paired with each root are the\\nmultiplicities of that root. Use the keyword multiplicities=False to turn them off.) Let us\\ntake a look under the hood and see how Sage manages the field of algebraic numbers.\\n\\n\\n\\n21.7. SAGE EXERCISES 377\\n\\nr1 = r[0][0]; r1\\n\\n-0.7861513777574233?\\n\\nr1.as_number_field_element ()\\n\\n(Number Field in a with defining polynomial y^4 + y^2 - 1, a, Ring\\nmorphism:\\nFrom: Number Field in a with defining polynomial y^4 + y^2 - 1\\nTo: Algebraic Real Field\\nDefn: a |--> -0.7861513777574233?)\\n\\nThree items are associated with this initial root. First is a number field, with generator\\na and a defining polynomial similar to the polynomial we are finding the roots of, but not\\nidentical. Second is an expression in the generator a, which is the actual root. In this\\nexample, the expression is simple, but it could be more complex in other examples. Finally,\\nthere is a ring homomorphism from the number field to the “Algebraic Real Field”, AA, the\\nsubfield of QQbar with just real elements, which associates the generator a with the number\\n-0.7861513777574233?. Let us verify, in two ways, that the root given is really a root.\\n\\nr1^4 + r1^2 - 1\\n\\n0\\n\\nN, rexact , homomorphism = r1.as_number_field_element ()\\n(rexact)^4 + rexact ^2 - 1\\n\\n0\\n\\nNow that we have enough theory to understand the field of algebraic numbers, and a\\nnatural way to represent them exactly, you might consider the operations in the field. If\\nwe take two algebraic numbers and add them together, we get another algebraic number\\n(Corollary 21.24). So what is the resulting minimal polynomial? How is it computed in\\nSage? You could read the source code if you wanted the answer.\\n\\nGeometric Constructions\\nSage can do a lot of things, but it is not yet able to lay out lines with a straightedge\\nand compass. However, we can very quickly determine that trisecting a 60 degree angle\\nis impossible. We adjoin the cosine of a 20 degree angle (in radians) to the rationals,\\ndetermine the degree of the extension, and check that it is not an integer power of 2. In\\none line. Sweet.\\n\\nlog(QQ[cos(pi/9)]. degree (), 2) in ZZ\\n\\nFalse\\n\\n21.7 Sage Exercises\\n1. Create the polynomial p(x) = x5+2x4+1 over Z3. Verify that it does not have any linear\\nfactors by evaluating p(x) with each element of Z3, and then check that p(x) is irreducible.\\nCreate a finite field of order 35 with the FiniteField() command, but include the modulus\\n\\nkeyword set to the polynomial p(x) to override the default choice.\\n\\n\\n\\n378 CHAPTER 21. FIELDS\\n\\nRecreate p(x) as a polynomial over this field. Check each of the 35 = 243 elements of the\\nfield to see if they are roots of the polynomial and list all of the elements which are roots.\\nFinally, request that Sage give a factorization of p(x) over the field, and comment on the\\nrelationship between your list of roots and your factorization.\\n\\n2. This problem continues the previous one. Build the ring of polynomials over Z3 and\\nwithin this ring use p(x) to generate a principal ideal. Finally construct the quotient of\\nthe polynomial ring by the ideal. Since the polynomial is irreducible, this quotient ring is\\na field, and by Proposition 21.12 this quotient ring is isomorphic to the number field in the\\nprevious problem.\\nBorrowing from your results in the previous question, construct five roots of the polynomial\\np(x) within this quotient ring, but now as expressions in the generator of the quotient\\nring (which is technically a coset). Use Sage to verify that they are indeed roots. This\\ndemonstrates using a quotient ring to create a splitting field for an irreducible polynomial\\nover a finite field.\\n\\n3. The subsection “⟨⟨Unresolved xref, reference ”subsection-algebraic-elements”; check spelling\\nor use ”provisional” attribute⟩⟩” relies on techniques from linear algebra and contains The-\\norem 21.15: every finite extension is an algebraic extension. This exercise will help you\\nunderstand this proof.\\nThe polynomial r(x) = x4 + 2x + 2 is irreducible over the rationals (Eisenstein’s criterion\\nwith prime p = 2). Create a number field that contains a root of r(x). By Theorem 21.15,\\nand the remark following, every element of this finite field extension is an algebraic number,\\nand hence satisfies some polynomial over the base field (it is this polynomial that Sage will\\nproduce with the .minpoly() method). This exercise will show how we can use just linear\\nalgebra to determine this minimal polynomial.\\nSuppose that a is the generator of the number field you just created with r(x). Then we\\nwill determine the minimal polynomial of t = 3a + 1 using just linear algebra. According\\nto the proof, the first five powers of t (start counting from zero) will be linearly dependent.\\n(Why?) So a nontrivial relation of linear dependence on these powers will provide the\\ncoefficients of a polynomial with t as a root. Compute these five powers, then construct\\nthe correct linear system to determine the coefficients of the minimal polynomial, solve the\\nsystem, and suitably interpret its solutions.\\nHints: The vector() and matrix() commands will create vectors and matrices, and the\\n.solve_right() method for matrices can be used to find solutions. Given an element of\\nthe number field, which will necessarily be a polynomial in the generator a, the .vector()\\n\\nmethod of the element will provide the coefficients of this polynomial in a list.\\n\\n4. Construct the splitting field of s(x) = x4 + x2 + 1 and find a factorization of s(x) over\\nthis field into linear factors.\\n\\n5. Form the number field, K, which contains a root of the irreducible polynomial q(x) =\\nx3 + 3x2 + 3x− 2. Name your root a. Verify that q(x) factors, but does not split, over K.\\nWith K now as the base field, form an extension of K where the quadratic factor of q(x)\\nhas a root. Name this root b, and call this second extension of the tower L.\\nUse M.<c> = L.absolute_field() to form the flattened tower that is the absolute number field\\nM. Find the defining polynomial of M with the .polynomial() method. From this polynomial,\\nwhich must have the generator c as a root, you should be able to use elementary algebra to\\nwrite the generator as a fairly simple expression.\\nM should be the splitting field of q(x). To see this, start over, and build from scratch a new\\nnumber field, P , using the simple expression for c that you just found. Use d as the name\\nof the root used to construct P. Since d is a root of the simple minimal polynomial for c,\\n\\n\\n\\n21.7. SAGE EXERCISES 379\\n\\nyou should be able to write an expression for d that a pre-calculus student would recognize.\\nNow factor the original polynomial q(x) (with rational coefficients) over P , to see the\\npolynomial split (as expected). Using this factorization, and your simple expression for d\\n\\nwrite simplified expressions for the three roots of q(x). See if you can convert between the\\ntwo versions of the roots “by hand”, and without using the isomorphisms provided by the\\n.structure() method on M.\\n\\n\\n\\n22\\n\\nFinite Fields\\n\\nFinite fields appear in many applications of algebra, including coding theory and cryptog-\\nraphy. We already know one finite field, Zp, where p is prime. In this chapter we will show\\nthat a unique finite field of order pn exists for every prime p, where n is a positive integer.\\nFinite fields are also called Galois fields in honor of Évariste Galois, who was one of the\\nfirst mathematicians to investigate them.\\n\\n22.1 Structure of a Finite Field\\nRecall that a field F has characteristic p if p is the smallest positive integer such that\\nfor every nonzero element α in F , we have pα = 0. If no such integer exists, then F has\\ncharacteristic 0. From Theorem 16.19 we know that p must be prime. Suppose that F is a\\nfinite field with n elements. Then nα = 0 for all α in F . Consequently, the characteristic of\\nF must be p, where p is a prime dividing n. This discussion is summarized in the following\\nproposition.\\n\\nProposition 22.1. If F is a finite field, then the characteristic of F is p, where p is prime.\\n\\nThroughout this chapter we will assume that p is a prime number unless otherwise\\nstated.\\n\\nProposition 22.2. If F is a finite field of characteristic p, then the order of F is pn for\\nsome n ∈ N.\\n\\nProof. Let ϕ : Z → F be the ring homomorphism defined by ϕ(n) = n · 1. Since the\\ncharacteristic of F is p, the kernel of ϕ must be pZ and the image of ϕ must be a subfield\\nof F isomorphic to Zp. We will denote this subfield by K. Since F is a finite field, it\\nmust be a finite extension of K and, therefore, an algebraic extension of K. Suppose that\\n[F : K] = n is the dimension of F , where F is a K vector space. There must exist elements\\nα1, . . . , αn ∈ F such that any element α in F can be written uniquely in the form\\n\\nα = a1α1 + · · ·+ anαn,\\n\\nwhere the ai’s are in K. Since there are p elements in K, there are pn possible linear\\ncombinations of the αi’s. Therefore, the order of F must be pn.\\n\\nLemma 22.3 (Freshman’s Dream). Let p be prime and D be an integral domain of char-\\nacteristic p. Then\\n\\nap\\nn\\n+ bp\\n\\nn\\n= (a+ b)p\\n\\nn\\n\\nfor all positive integers n.\\n\\n380\\n\\n\\n\\n22.1. STRUCTURE OF A FINITE FIELD 381\\n\\nProof. We will prove this lemma using mathematical induction on n. We can use the\\nbinomial formula (see Chapter 2, Example 2.4) to verify the case for n = 1; that is,\\n\\n(a+ b)p =\\n\\np∑\\nk=0\\n\\n(\\np\\n\\nk\\n\\n)\\nakbp−k.\\n\\nIf 0 < k < p, then (\\np\\n\\nk\\n\\n)\\n=\\n\\np!\\n\\nk!(p− k)!\\n\\nmust be divisible by p, since p cannot divide k!(p− k)!. Note that D is an integral domain\\nof characteristic p, so all but the first and last terms in the sum must be zero. Therefore,\\n(a+ b)p = ap + bp.\\n\\nNow suppose that the result holds for all k, where 1 ≤ k ≤ n. By the induction\\nhypothesis,\\n\\n(a+ b)p\\nn+1\\n\\n= ((a+ b)p)p\\nn\\n= (ap + bp)p\\n\\nn\\n= (ap)p\\n\\nn\\n+ (bp)p\\n\\nn\\n= ap\\n\\nn+1\\n+ bp\\n\\nn+1\\n.\\n\\nTherefore, the lemma is true for n+ 1 and the proof is complete.\\n\\nLet F be a field. A polynomial f(x) ∈ F [x] of degree n is separable if it has n distinct\\nroots in the splitting field of f(x); that is, f(x) is separable when it factors into distinct\\nlinear factors over the splitting field of f . An extension E of F is a separable extension\\nof F if every element in E is the root of a separable polynomial in F [x].\\n\\nExample 22.4. The polynomial x2−2 is separable over Q since it factors as (x−\\n√\\n2 )(x+√\\n\\n2 ). In fact, Q(\\n√\\n2 ) is a separable extension of Q. Let α = a + b\\n\\n√\\n2 be any element in\\n\\nQ(\\n√\\n2 ). If b = 0, then α is a root of x − a. If b ̸= 0, then α is the root of the separable\\n\\npolynomial\\nx2 − 2ax+ a2 − 2b2 = (x− (a+ b\\n\\n√\\n2 ))(x− (a− b\\n\\n√\\n2 )).\\n\\nFortunately, we have an easy test to determine the separability of any polynomial. Let\\n\\nf(x) = a0 + a1x+ · · ·+ anx\\nn\\n\\nbe any polynomial in F [x]. Define the derivative of f(x) to be\\n\\nf ′(x) = a1 + 2a2x+ · · ·+ nanx\\nn−1.\\n\\nLemma 22.5. Let F be a field and f(x) ∈ F [x]. Then f(x) is separable if and only if f(x)\\nand f ′(x) are relatively prime.\\n\\nProof. Let f(x) be separable. Then f(x) factors over some extension field of F as f(x) =\\n(x−α1)(x−α2) · · · (x−αn), where αi ̸= αj for i ̸= j. Taking the derivative of f(x), we see\\nthat\\n\\nf ′(x) = (x− α2) · · · (x− αn)\\n\\n+ (x− α1)(x− α3) · · · (x− αn)\\n\\n+ · · ·+ (x− α1) · · · (x− αn−1).\\n\\nHence, f(x) and f ′(x) can have no common factors.\\nTo prove the converse, we will show that the contrapositive of the statement is true.\\n\\nSuppose that f(x) = (x− α)kg(x), where k > 1. Differentiating, we have\\n\\nf ′(x) = k(x− α)k−1g(x) + (x− α)kg′(x).\\n\\nTherefore, f(x) and f ′(x) have a common factor.\\n\\n\\n\\n382 CHAPTER 22. FINITE FIELDS\\n\\nTheorem 22.6. For every prime p and every positive integer n, there exists a finite field\\nF with pn elements. Furthermore, any field of order pn is isomorphic to the splitting field\\nof xpn − x over Zp.\\n\\nProof. Let f(x) = xp\\nn − x and let F be the splitting field of f(x). Then by Lemma 22.5,\\n\\nf(x) has pn distinct zeros in F , since f ′(x) = pnxp\\nn−1 − 1 = −1 is relatively prime to\\n\\nf(x). We claim that the roots of f(x) form a subfield of F . Certainly 0 and 1 are zeros\\nof f(x). If α and β are zeros of f(x), then α + β and αβ are also zeros of f(x), since\\nαpn +βp\\n\\nn\\n= (α+β)p\\n\\nn and αpnβp\\nn\\n= (αβ)p\\n\\nn . We also need to show that the additive inverse\\nand the multiplicative inverse of each root of f(x) are roots of f(x). For any zero α of f(x),\\n−α = (p − 1)α is also a zero of f(x). If α ̸= 0, then (α−1)p\\n\\nn\\n= (αpn)−1 = α−1. Since the\\n\\nzeros of f(x) form a subfield of F and f(x) splits in this subfield, the subfield must be all\\nof F .\\n\\nLet E be any other field of order pn. To show that E is isomorphic to F , we must show\\nthat every element in E is a root of f(x). Certainly 0 is a root of f(x). Let α be a nonzero\\nelement of E. The order of the multiplicative group of nonzero elements of E is pn − 1;\\nhence, αpn−1 = 1 or αpn −α = 0. Since E contains pn elements, E must be a splitting field\\nof f(x); however, by Corollary 21.34, the splitting field of any polynomial is unique up to\\nisomorphism.\\n\\nThe unique finite field with pn elements is called the Galois field of order pn. We will\\ndenote this field by GF(pn).\\n\\nTheorem 22.7. Every subfield of the Galois field GF(pn) has pm elements, where m divides\\nn. Conversely, if m | n for m > 0, then there exists a unique subfield of GF(pn) isomorphic\\nto GF(pm).\\n\\nProof. Let F be a subfield of E = GF(pn). Then F must be a field extension of K that\\ncontains pm elements, where K is isomorphic to Zp. Then m | n, since [E : K] = [E : F ][F :\\nK].\\n\\nTo prove the converse, suppose that m | n for some m > 0. Then pm − 1 divides pn − 1.\\nConsequently, xpm−1 − 1 divides xpn−1 − 1. Therefore, xpm − x must divide xpn − x, and\\nevery zero of xpm − x is also a zero of xpn − x. Thus, GF(pn) contains, as a subfield, a\\nsplitting field of xpm − x, which must be isomorphic to GF(pm).\\n\\nExample 22.8. The lattice of subfields of GF(p24) is given in Figure 22.9.\\n\\nGF(p24)\\n\\nGF(p12)\\n\\nGF(p6)\\n\\nGF(p3)\\n\\nGF(p8)\\n\\nGF(p4)\\n\\nGF(p2)\\n\\nGF(p)\\n\\nFigure 22.9: Subfields of GF(p24)\\n\\n\\n\\n22.2. POLYNOMIAL CODES 383\\n\\nWith each field F we have a multiplicative group of nonzero elements of F which we\\nwill denote by F ∗. The multiplicative group of any finite field is cyclic. This result follows\\nfrom the more general result that we will prove in the next theorem.\\n\\nTheorem 22.10. If G is a finite subgroup of F ∗, the multiplicative group of nonzero\\nelements of a field F , then G is cyclic.\\n\\nProof. Let G be a finite subgroup of F ∗ of order n. By the Fundamental Theorem of\\nFinite Abelian Groups (Theorem 13.4),\\n\\nG ∼= Zp\\ne1\\n1\\n\\n× · · · × Zp\\nek\\nk\\n,\\n\\nwhere n = pe11 · · · pekk and the p1, . . . , pk are (not necessarily distinct) primes. Let m be the\\nleast common multiple of pe11 , . . . , p\\n\\nek\\nk . Then G contains an element of order m. Since every\\n\\nα in G satisfies xr − 1 for some r dividing m, α must also be a root of xm− 1. Since xm− 1\\nhas at most m roots in F , n ≤ m. On the other hand, we know that m ≤ |G|; therefore,\\nm = n. Thus, G contains an element of order n and must be cyclic.\\n\\nCorollary 22.11. The multiplicative group of all nonzero elements of a finite field is cyclic.\\n\\nCorollary 22.12. Every finite extension E of a finite field F is a simple extension of F .\\n\\nProof. Let α be a generator for the cyclic group E∗ of nonzero elements of E. Then\\nE = F (α).\\n\\nExample 22.13. The finite field GF(24) is isomorphic to the field Z2/⟨1+x+x4⟩. Therefore,\\nthe elements of GF(24) can be taken to be\\n\\n{a0 + a1α+ a2α\\n2 + a3α\\n\\n3 : ai ∈ Z2 and 1 + α+ α4 = 0}.\\n\\nRemembering that 1 + α+ α4 = 0, we add and multiply elements of GF(24) exactly as we\\nadd and multiply polynomials. The multiplicative group of GF(24) is isomorphic to Z15\\n\\nwith generator α:\\n\\nα1 = α α6 = α2 + α3 α11 = α+ α2 + α3\\n\\nα2 = α2 α7 = 1 + α+ α3 α12 = 1 + α+ α2 + α3\\n\\nα3 = α3 α8 = 1 + α2 α13 = 1 + α2 + α3\\n\\nα4 = 1 + α α9 = α+ α3 α14 = 1 + α3\\n\\nα5 = α+ α2 α10 = 1 + α+ α2 α15 = 1.\\n\\n22.2 Polynomial Codes\\nWith knowledge of polynomial rings and finite fields, it is now possible to derive more\\nsophisticated codes than those of Chapter 8. First let us recall that an (n, k)-block code\\nconsists of a one-to-one encoding function E : Zk\\n\\n2 → Zn\\n2 and a decoding function D : Zn\\n\\n2 →\\nZk\\n2. The code is error-correcting if D is onto. A code is a linear code if it is the null space\\n\\nof a matrix H ∈ Mk×n(Z2).\\nWe are interested in a class of codes known as cyclic codes. Let ϕ : Zk\\n\\n2 → Zn\\n2 be a\\n\\nbinary (n, k)-block code. Then ϕ is a cyclic code if for every codeword (a1, a2, . . . , an),\\nthe cyclically shifted n-tuple (an, a1, a2, . . . , an−1) is also a codeword. Cyclic codes are\\nparticularly easy to implement on a computer using shift registers [2, 3].\\n\\n\\n\\n384 CHAPTER 22. FINITE FIELDS\\n\\nExample 22.14. Consider the (6, 3)-linear codes generated by the two matrices\\n\\nG1 =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 0 0\\n\\n0 1 0\\n\\n0 0 1\\n\\n1 0 0\\n\\n0 1 0\\n\\n0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\nand G2 =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 0 0\\n\\n1 1 0\\n\\n1 1 1\\n\\n1 1 1\\n\\n0 1 1\\n\\n0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\n.\\n\\nMessages in the first code are encoded as follows:\\n\\n(000) 7→ (000000) (100) 7→ (100100)\\n\\n(001) 7→ (001001) (101) 7→ (101101)\\n\\n(010) 7→ (010010) (110) 7→ (110110)\\n\\n(011) 7→ (011011) (111) 7→ (111111).\\n\\nIt is easy to see that the codewords form a cyclic code. In the second code, 3-tuples are\\nencoded in the following manner:\\n\\n(000) 7→ (000000) (100) 7→ (111100)\\n\\n(001) 7→ (001111) (101) 7→ (110011)\\n\\n(010) 7→ (011110) (110) 7→ (100010)\\n\\n(011) 7→ (010001) (111) 7→ (101101).\\n\\nThis code cannot be cyclic, since (101101) is a codeword but (011011) is not a codeword.\\n\\nPolynomial Codes\\nWe would like to find an easy method of obtaining cyclic linear codes. To accomplish this, we\\ncan use our knowledge of finite fields and polynomial rings over Z2. Any binary n-tuple can\\nbe interpreted as a polynomial in Z2[x]. Stated another way, the n-tuple (a0, a1, . . . , an−1)\\ncorresponds to the polynomial\\n\\nf(x) = a0 + a1x+ · · ·+ an−1x\\nn−1,\\n\\nwhere the degree of f(x) is at most n − 1. For example, the polynomial corresponding to\\nthe 5-tuple (10011) is\\n\\n1 + 0x+ 0x2 + 1x3 + 1x4 = 1 + x3 + x4.\\n\\nConversely, with any polynomial f(x) ∈ Z2[x] with deg f(x) < n we can associate a binary\\nn-tuple. The polynomial x+ x2 + x4 corresponds to the 5-tuple (01101).\\n\\nLet us fix a nonconstant polynomial g(x) in Z2[x] of degree n− k. We can define an\\n(n, k)-code C in the following manner. If (a0, . . . , ak−1) is a k-tuple to be encoded, then\\nf(x) = a0+a1x+ · · ·+ak−1x\\n\\nk−1 is the corresponding polynomial in Z2[x]. To encode f(x),\\nwe multiply by g(x). The codewords in C are all those polynomials in Z2[x] of degree less\\nthan n that are divisible by g(x). Codes obtained in this manner are called polynomial\\ncodes.\\n\\nExample 22.15. If we let g(x) = 1+x3, we can define a (6, 3)-code C as follows. To encode\\na 3-tuple (a0, a1, a2), we multiply the corresponding polynomial f(x) = a0 + a1x+ a2x\\n\\n2 by\\n1+x3. We are defining a map ϕ : Z3\\n\\n2 → Z6\\n2 by ϕ : f(x) 7→ g(x)f(x). It is easy to check that\\n\\nthis map is a group homomorphism. In fact, if we regard Zn\\n2 as a vector space over Z2, ϕ is\\n\\n\\n\\n22.2. POLYNOMIAL CODES 385\\n\\na linear transformation of vector spaces (see Exercise 20.4.15, Chapter 20). Let us compute\\nthe kernel of ϕ. Observe that ϕ(a0, a1, a2) = (000000) exactly when\\n\\n0 + 0x+ 0x2 + 0x3 + 0x4 + 0x5 = (1 + x3)(a0 + a1x+ a2x\\n2)\\n\\n= a0 + a1x+ a2x\\n2 + a0x\\n\\n3 + a1x\\n4 + a2x\\n\\n5.\\n\\nSince the polynomials over a field form an integral domain, a0 + a1x + a2x\\n2 must be the\\n\\nzero polynomial. Therefore, kerϕ = {(000)} and ϕ is one-to-one.\\nTo calculate a generator matrix for C, we merely need to examine the way the polyno-\\n\\nmials 1, x, and x2 are encoded:\\n\\n(1 + x3) · 1 = 1 + x3\\n\\n(1 + x3)x = x+ x4\\n\\n(1 + x3)x2 = x2 + x5.\\n\\nWe obtain the code corresponding to the generator matrix G1 in Example 22.14. The\\nparity-check matrix for this code is\\n\\nH =\\n\\n\uf8eb\uf8ed1 0 0 1 0 0\\n\\n0 1 0 0 1 0\\n\\n0 0 1 0 0 1\\n\\n\uf8f6\uf8f8 .\\n\\nSince the smallest weight of any nonzero codeword is 2, this code has the ability to detect\\nall single errors.\\n\\nRings of polynomials have a great deal of structure; therefore, our immediate goal\\nis to establish a link between polynomial codes and ring theory. Recall that xn − 1 =\\n(x− 1)(xn−1 + · · ·+ x+ 1). The factor ring\\n\\nRn = Z2[x]/⟨xn − 1⟩\\n\\ncan be considered to be the ring of polynomials of the form\\n\\nf(t) = a0 + a1t+ · · ·+ an−1t\\nn−1\\n\\nthat satisfy the condition tn = 1. It is an easy exercise to show that Zn\\n2 and Rn are isomor-\\n\\nphic as vector spaces. We will often identify elements in Zn\\n2 with elements in Z[x]/⟨xn− 1⟩.\\n\\nIn this manner we can interpret a linear code as a subset of Z[x]/⟨xn − 1⟩.\\nThe additional ring structure on polynomial codes is very powerful in describing cyclic\\n\\ncodes. A cyclic shift of an n-tuple can be described by polynomial multiplication. If\\nf(t) = a0 + a1t+ · · ·+ an−1t\\n\\nn−1 is a code polynomial in Rn, then\\n\\ntf(t) = an−1 + a0t+ · · ·+ an−2t\\nn−1\\n\\nis the cyclically shifted word obtained from multiplying f(t) by t. The following theorem\\ngives a beautiful classification of cyclic codes in terms of the ideals of Rn.\\n\\nTheorem 22.16. A linear code C in Zn\\n2 is cyclic if and only if it is an ideal in Rn =\\n\\nZ[x]/⟨xn − 1⟩.\\n\\nProof. Let C be a linear cyclic code and suppose that f(t) is in C. Then tf(t) must also\\nbe in C. Consequently, tkf(t) is in C for all k ∈ N. Since C is a linear code, any linear\\ncombination of the codewords f(t), tf(t), t2f(t), . . . , tn−1f(t) is also a codeword; therefore,\\nfor every polynomial p(t), p(t)f(t) is in C. Hence, C is an ideal.\\n\\nConversely, let C be an ideal in Z2[x]/⟨xn + 1⟩. Suppose that f(t) = a0 + a1t + · · · +\\nan−1t\\n\\nn−1 is a codeword in C. Then tf(t) is a codeword in C; that is, (a1, . . . , an−1, a0) is\\nin C.\\n\\n\\n\\n386 CHAPTER 22. FINITE FIELDS\\n\\nTheorem 22.16 tells us that knowing the ideals of Rn is equivalent to knowing the linear\\ncyclic codes in Zn\\n\\n2 . Fortunately, the ideals in Rn are easy to describe. The natural ring\\nhomomorphism ϕ : Z2[x] → Rn defined by ϕ[f(x)] = f(t) is a surjective homomorphism.\\nThe kernel of ϕ is the ideal generated by xn − 1. By Theorem 16.34, every ideal C in Rn is\\nof the form ϕ(I), where I is an ideal in Z2[x] that contains ⟨xn− 1⟩. By Theorem 17.20, we\\nknow that every ideal I in Z2[x] is a principal ideal, since Z2 is a field. Therefore, I = ⟨g(x)⟩\\nfor some unique monic polynomial in Z2[x]. Since ⟨xn− 1⟩ is contained in I, it must be the\\ncase that g(x) divides xn − 1. Consequently, every ideal C in Rn is of the form\\n\\nC = ⟨g(t)⟩ = {f(t)g(t) : f(t) ∈ Rn and g(x) | (xn − 1) in Z2[x]}.\\n\\nThe unique monic polynomial of the smallest degree that generates C is called the minimal\\ngenerator polynomial of C.\\n\\nExample 22.17. If we factor x7 − 1 into irreducible components, we have\\n\\nx7 − 1 = (1 + x)(1 + x+ x3)(1 + x2 + x3).\\n\\nWe see that g(t) = (1 + t+ t3) generates an ideal C in R7. This code is a (7, 4)-block code.\\nAs in Example 22.15, it is easy to calculate a generator matrix by examining what g(t) does\\nto the polynomials 1, t, t2, and t3. A generator matrix for C is\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 0 0 0\\n\\n1 1 0 0\\n\\n0 1 1 0\\n\\n1 0 1 1\\n\\n0 1 0 1\\n\\n0 0 1 0\\n\\n0 0 0 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\n.\\n\\nIn general, we can determine a generator matrix for an (n, k)-code C by the manner in\\nwhich the elements tk are encoded. Let xn − 1 = g(x)h(x) in Z2[x]. If g(x) = g0 + g1x +\\n· · ·+ gn−kx\\n\\nn−k and h(x) = h0 + h1x+ · · ·+ hkx\\nk, then the n× k matrix\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\ng0 0 · · · 0\\n\\ng1 g0 · · · 0\\n...\\n\\n... . . . ...\\ngn−k gn−k−1 · · · g0\\n0 gn−k · · · g1\\n...\\n\\n... . . . ...\\n0 0 · · · gn−k\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\nis a generator matrix for the code C with generator polynomial g(t). The parity-check\\nmatrix for C is the (n− k)× n matrix\\n\\nH =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n0 · · · 0 0 hk · · · h0\\n0 · · · 0 hk · · · h0 0\\n\\n· · · · · · · · · · · · · · · · · · · · ·\\nhk · · · h0 0 0 · · · 0\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\nWe will leave the details of the proof of the following proposition as an exercise.\\n\\n\\n\\n22.2. POLYNOMIAL CODES 387\\n\\nProposition 22.18. Let C = ⟨g(t)⟩ be a cyclic code in Rn and suppose that xn − 1 =\\ng(x)h(x). Then G and H are generator and parity-check matrices for C, respectively.\\nFurthermore, HG = 0.\\n\\nExample 22.19. In Example 22.17,\\n\\nx7 − 1 = g(x)h(x) = (1 + x+ x3)(1 + x+ x2 + x4).\\n\\nTherefore, a parity-check matrix for this code is\\n\\nH =\\n\\n\uf8eb\uf8ed0 0 1 0 1 1 1\\n\\n0 1 0 1 1 1 0\\n\\n1 0 1 1 1 0 0\\n\\n\uf8f6\uf8f8 .\\n\\nTo determine the error-detecting and error-correcting capabilities of a cyclic code, we\\nneed to know something about determinants. If α1, . . . , αn are elements in a field F , then\\nthe n× n matrix \uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 1 · · · 1\\n\\nα1 α2 · · · αn\\n\\nα2\\n1 α2\\n\\n2 · · · α2\\nn\\n\\n...\\n... . . . ...\\n\\nαn−1\\n1 αn−1\\n\\n2 · · · αn−1\\nn\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\nis called the Vandermonde matrix. The determinant of this matrix is called the Van-\\ndermonde determinant. We will need the following lemma in our investigation of cyclic\\ncodes.\\n\\nLemma 22.20. Let α1, . . . , αn be elements in a field F with n ≥ 2. Then\\n\\ndet\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n1 1 · · · 1\\n\\nα1 α2 · · · αn\\n\\nα2\\n1 α2\\n\\n2 · · · α2\\nn\\n\\n...\\n... . . . ...\\n\\nαn−1\\n1 αn−1\\n\\n2 · · · αn−1\\nn\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 =\\n∏\\n\\n1≤j<i≤n\\n\\n(αi − αj).\\n\\nIn particular, if the αi’s are distinct, then the determinant is nonzero.\\n\\nProof. We will induct on n. If n = 2, then the determinant is α2−α1. Let us assume the\\nresult for n− 1 and consider the polynomial p(x) defined by\\n\\np(x) = det\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n1 1 · · · 1 1\\n\\nα1 α2 · · · αn−1 x\\n\\nα2\\n1 α2\\n\\n2 · · · α2\\nn−1 x2\\n\\n...\\n... . . . ...\\n\\n...\\nαn−1\\n1 αn−1\\n\\n2 · · · αn−1\\nn−1 xn−1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\nExpanding this determinant by cofactors on the last column, we see that p(x) is a polynomial\\nof at most degree n−1. Moreover, the roots of p(x) are α1, . . . , αn−1, since the substitution\\nof any one of these elements in the last column will produce a column identical to the last\\ncolumn in the matrix. Remember that the determinant of a matrix is zero if it has two\\nidentical columns. Therefore,\\n\\np(x) = (x− α1)(x− α2) · · · (x− αn−1)β,\\n\\n\\n\\n388 CHAPTER 22. FINITE FIELDS\\n\\nwhere\\n\\nβ = (−1)n+n det\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n1 1 · · · 1\\n\\nα1 α2 · · · αn−1\\n\\nα2\\n1 α2\\n\\n2 · · · α2\\nn−1\\n\\n...\\n... . . . ...\\n\\nαn−2\\n1 αn−2\\n\\n2 · · · αn−2\\nn−1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\nBy our induction hypothesis,\\n\\nβ = (−1)n+n\\n∏\\n\\n1≤j<i≤n−1\\n\\n(αi − αj).\\n\\nIf we let x = αn, the result now follows immediately.\\n\\nThe following theorem gives us an estimate on the error detection and correction capa-\\nbilities for a particular generator polynomial.\\n\\nTheorem 22.21. Let C = ⟨g(t)⟩ be a cyclic code in Rn and suppose that ω is a primitive\\nnth root of unity over Z2. If s consecutive powers of ω are roots of g(x), then the minimum\\ndistance of C is at least s+ 1.\\n\\nProof. Suppose that\\n\\ng(ωr) = g(ωr+1) = · · · = g(ωr+s−1) = 0.\\n\\nLet f(x) be some polynomial in C with s or fewer nonzero coefficients. We can assume that\\n\\nf(x) = ai0x\\ni0 + ai1x\\n\\ni1 + · · ·+ ais−1x\\nis−1\\n\\nbe some polynomial in C. It will suffice to show that all of the ai’s must be 0. Since\\n\\ng(ωr) = g(ωr+1) = · · · = g(ωr+s−1) = 0\\n\\nand g(x) divides f(x),\\n\\nf(ωr) = f(ωr+1) = · · · = f(ωr+s−1) = 0.\\n\\nEquivalently, we have the following system of equations:\\n\\nai0(ω\\nr)i0 + ai1(ω\\n\\nr)i1 + · · ·+ ais−1(ω\\nr)is−1 = 0\\n\\nai0(ω\\nr+1)i0 + ai1(ω\\n\\nr+1)i2 + · · ·+ ais−1(ω\\nr+1)is−1 = 0\\n\\n...\\nai0(ω\\n\\nr+s−1)i0 + ai1(ω\\nr+s−1)i1 + · · ·+ ais−1(ω\\n\\nr+s−1)is−1 = 0.\\n\\nTherefore, (ai0 , ai1 , . . . , ais−1) is a solution to the homogeneous system of linear equations\\n\\n(ωi0)rx0 + (ωi1)rx1 + · · ·+ (ωis−1)rxn−1 = 0\\n\\n(ωi0)r+1x0 + (ωi1)r+1x1 + · · ·+ (ωis−1)r+1xn−1 = 0\\n\\n...\\n(ωi0)r+s−1x0 + (ωi1)r+s−1x1 + · · ·+ (ωis−1)r+s−1xn−1 = 0.\\n\\n\\n\\n22.2. POLYNOMIAL CODES 389\\n\\nHowever, this system has a unique solution, since the determinant of the matrix\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n(ωi0)r (ωi1)r · · · (ωis−1)r\\n\\n(ωi0)r+1 (ωi1)r+1 · · · (ωis−1)r+1\\n\\n...\\n... . . . ...\\n\\n(ωi0)r+s−1 (ωi1)r+s−1 · · · (ωis−1)r+s−1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\\ncan be shown to be nonzero using Lemma 22.20 and the basic properties of determinants\\n(Exercise). Therefore, this solution must be ai0 = ai1 = · · · = ais−1 = 0.\\n\\nBCH Codes\\nSome of the most important codes, discovered independently by A. Hocquenghem in 1959\\nand by R. C. Bose and D. V. Ray-Chaudhuri in 1960, are bch codes. The European\\nand transatlantic communication systems both use bch codes. Information words to be\\nencoded are of length 231, and a polynomial of degree 24 is used to generate the code.\\nSince 231 + 24 = 255 = 28 − 1, we are dealing with a (255, 231)-block code. This bch code\\nwill detect six errors and has a failure rate of 1 in 16 million. One advantage of bch codes\\nis that efficient error correction algorithms exist for them.\\n\\nThe idea behind bch codes is to choose a generator polynomial of smallest degree that\\nhas the largest error detection and error correction capabilities. Let d = 2r + 1 for some\\nr ≥ 0. Suppose that ω is a primitive nth root of unity over Z2, and let mi(x) be the minimal\\npolynomial over Z2 of ωi. If\\n\\ng(x) = lcm[m1(x),m2(x), . . . ,m2r(x)],\\n\\nthen the cyclic code ⟨g(t)⟩ in Rn is called the bch code of length n and distance d. By\\nTheorem 22.21, the minimum distance of C is at least d.\\nTheorem 22.22. Let C = ⟨g(t)⟩ be a cyclic code in Rn. The following statements are\\nequivalent.\\n\\n1. The code C is a bch code whose minimum distance is at least d.\\n\\n2. A code polynomial f(t) is in C if and only if f(ωi) = 0 for 1 ≤ i < d.\\n\\n3. The matrix\\n\\nH =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n1 ω ω2 · · · ωn−1\\n\\n1 ω2 ω4 · · · ω(n−1)(2)\\n\\n1 ω3 ω6 · · · ω(n−1)(3)\\n\\n...\\n...\\n\\n... . . . ...\\n1 ω2r ω4r · · · ω(n−1)(2r)\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\nis a parity-check matrix for C.\\n\\nProof. (1) ⇒ (2). If f(t) is in C, then g(x) | f(x) in Z2[x]. Hence, for i = 1, . . . , 2r,\\nf(ωi) = 0 since g(ωi) = 0. Conversely, suppose that f(ωi) = 0 for 1 ≤ i ≤ d. Then f(x) is\\ndivisible by each mi(x), since mi(x) is the minimal polynomial of ωi. Therefore, g(x) | f(x)\\nby the definition of g(x). Consequently, f(x) is a codeword.\\n\\n(2) ⇒ (3). Let f(t) = a0 + a1t+ · · ·+ an−1vt\\nn−1 be in Rn. The corresponding n-tuple\\n\\nin Zn\\n2 is x = (a0a1 · · · an−1)\\n\\nt. By (2),\\n\\nHx =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\na0 + a1ω + · · ·+ an−1ω\\n\\nn−1\\n\\na0 + a1ω\\n2 + · · ·+ an−1(ω\\n\\n2)n−1\\n\\n...\\na0 + a1ω\\n\\n2r + · · ·+ an−1(ω\\n2r)n−1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\nf(ω)\\n\\nf(ω2)\\n...\\n\\nf(ω2r)\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 = 0\\n\\n\\n\\n390 CHAPTER 22. FINITE FIELDS\\n\\nexactly when f(t) is in C. Thus, H is a parity-check matrix for C.\\n(3) ⇒ (1). By (3), a code polynomial f(t) = a0+a1t+· · ·+an−1t\\n\\nn−1 is in C exactly when\\nf(ωi) = 0 for i = 1, . . . , 2r. The smallest such polynomial is g(t) = lcm[m1(t), . . . ,m2r(t)].\\nTherefore, C = ⟨g(t)⟩.\\n\\nExample 22.23. It is easy to verify that x15 − 1 ∈ Z2[x] has a factorization\\n\\nx15 − 1 = (x+ 1)(x2 + x+ 1)(x4 + x+ 1)(x4 + x3 + 1)(x4 + x3 + x2 + x+ 1),\\n\\nwhere each of the factors is an irreducible polynomial. Let ω be a root of 1 + x+ x4. The\\nGalois field GF(24) is\\n\\n{a0 + a1ω + a2ω\\n2 + a3ω\\n\\n3 : ai ∈ Z2 and 1 + ω + ω4 = 0}.\\n\\nBy Example 22.8, ω is a primitive 15th root of unity. The minimal polynomial of ω is\\nm1(x) = 1 + x+ x4. It is easy to see that ω2 and ω4 are also roots of m1(x). The minimal\\npolynomial of ω3 is m2(x) = 1 + x+ x2 + x3 + x4. Therefore,\\n\\ng(x) = m1(x)m2(x) = 1 + x4 + x6 + x7 + x8\\n\\nhas roots ω, ω2, ω3, ω4. Since both m1(x) and m2(x) divide x15 − 1, the bch code is a\\n(15, 7)-code. If x15 − 1 = g(x)h(x), then h(x) = 1 + x4 + x6 + x7; therefore, a parity-check\\nmatrix for this code is\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n0 0 0 0 0 0 0 1 1 0 1 0 0 0 1\\n\\n0 0 0 0 0 0 1 1 0 1 0 0 0 1 0\\n\\n0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\\n\\n0 0 0 0 1 1 0 1 0 0 0 1 0 0 0\\n\\n0 0 0 1 1 0 1 0 0 0 1 0 0 0 0\\n\\n0 0 1 1 0 1 0 0 0 1 0 0 0 0 0\\n\\n0 1 1 0 1 0 0 0 1 0 0 0 0 0 0\\n\\n1 1 0 1 0 0 0 1 0 0 0 0 0 0 0\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\n.\\n\\n22.3 Exercises\\n1. Calculate each of the following.\\n\\n(a) [GF(36) : GF(33)]\\n(b) [GF(128) : GF(16)]\\n\\n(c) [GF(625) : GF(25)]\\n(d) [GF(p12) : GF(p2)]\\n\\n2. Calculate [GF(pm) : GF(pn)], where n | m.\\n\\n3. What is the lattice of subfields for GF(p30)?\\n\\n4. Let α be a zero of x3 + x2 + 1 over Z2. Construct a finite field of order 8. Show that\\nx3 + x2 + 1 splits in Z2(α).\\n\\n5. Construct a finite field of order 27.\\n\\n6. Prove or disprove: Q∗ is cyclic.\\n\\n7. Factor each of the following polynomials in Z2[x].\\n\\n\\n\\n22.3. EXERCISES 391\\n\\n(a) x5 − 1\\n\\n(b) x6 + x5 + x4 + x3 + x2 + x+ 1\\n\\n(c) x9 − 1\\n\\n(d) x4 + x3 + x2 + x+ 1\\n\\n8. Prove or disprove: Z2[x]/⟨x3 + x+ 1⟩ ∼= Z2[x]/⟨x3 + x2 + 1⟩.\\n\\n9. Determine the number of cyclic codes of length n for n = 6, 7, 8, 10.\\n\\n10. Prove that the ideal ⟨t+1⟩ in Rn is the code in Zn\\n2 consisting of all words of even parity.\\n\\n11. Construct all bch codes of\\n\\n(a) length 7. (b) length 15.\\n\\n12. Prove or disprove: There exists a finite field that is algebraically closed.\\n\\n13. Let p be prime. Prove that the field of rational functions Zp(x) is an infinite field of\\ncharacteristic p.\\n\\n14. Let D be an integral domain of characteristic p. Prove that (a − b)p\\nn\\n= ap\\n\\nn − bp\\nn for\\n\\nall a, b ∈ D.\\n\\n15. Show that every element in a finite field can be written as the sum of two squares.\\n\\n16. Let E and F be subfields of a finite field K. If E is isomorphic to F , show that E = F .\\n\\n17. Let F ⊂ E ⊂ K be fields. If K is separable over F , show that K is also separable over\\nE.\\n\\n18. Let E be an extension of a finite field F , where F has q elements. Let α ∈ E be\\nalgebraic over F of degree n. Prove that F (α) has qn elements.\\n\\n19. Show that every finite extension of a finite field F is simple; that is, if E is a finite\\nextension of a finite field F , prove that there exists an α ∈ E such that E = F (α).\\n\\n20. Show that for every n there exists an irreducible polynomial of degree n in Zp[x].\\n\\n21. Prove that the Frobenius map Φ : GF(pn) → GF(pn) given by Φ : α 7→ αp is an\\nautomorphism of order n.\\n\\n22. Show that every element in GF(pn) can be written in the form ap for some unique\\na ∈ GF(pn).\\n\\n23. Let E and F be subfields of GF(pn). If |E| = pr and |F | = ps, what is the order of\\nE ∩ F?\\n\\n24. (Wilson’s Theorem) Let p be prime. Prove that (p− 1)! ≡ −1 (mod p).\\n\\n25. If g(t) is the minimal generator polynomial for a cyclic code C in Rn, prove that the\\nconstant term of g(x) is 1.\\n\\n26. Often it is conceivable that a burst of errors might occur during transmission, as in\\nthe case of a power surge. Such a momentary burst of interference might alter several\\nconsecutive bits in a codeword. Cyclic codes permit the detection of such error bursts. Let\\nC be an (n, k)-cyclic code. Prove that any error burst up to n− k digits can be detected.\\n\\n27. Prove that the rings Rn and Zn\\n2 are isomorphic as vector spaces.\\n\\n\\n\\n392 CHAPTER 22. FINITE FIELDS\\n\\n28. Let C be a code in Rn that is generated by g(t). If ⟨f(t)⟩ is another code in Rn, show\\nthat ⟨g(t)⟩ ⊂ ⟨f(t)⟩ if and only if f(x) divides g(x) in Z2[x].\\n\\n29. Let C = ⟨g(t)⟩ be a cyclic code in Rn and suppose that xn − 1 = g(x)h(x), where\\ng(x) = g0 + g1x + · · · + gn−kx\\n\\nn−k and h(x) = h0 + h1x + · · · + hkx\\nk. Define G to be the\\n\\nn× k matrix\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\ng0 0 · · · 0\\n\\ng1 g0 · · · 0\\n...\\n\\n... . . . ...\\ngn−k gn−k−1 · · · g0\\n0 gn−k · · · g1\\n...\\n\\n... . . . ...\\n0 0 · · · gn−k\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\nand H to be the (n− k)× n matrix\\n\\nH =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n0 · · · 0 0 hk · · · h0\\n0 · · · 0 hk · · · h0 0\\n\\n· · · · · · · · · · · · · · · · · · · · ·\\nhk · · · h0 0 0 · · · 0\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\n(a) Prove that G is a generator matrix for C.\\n(b) Prove that H is a parity-check matrix for C.\\n(c) Show that HG = 0.\\n\\n22.4 Additional Exercises: Error Correction for BCH Codes\\nbch codes have very attractive error correction algorithms. Let C be a bch code in Rn,\\nand suppose that a code polynomial c(t) = c0 + c1t + · · · + cn−1t\\n\\nn−1 is transmitted. Let\\nw(t) = w0 + w1t + · · ·wn−1t\\n\\nn−1 be the polynomial in Rn that is received. If errors have\\noccurred in bits a1, . . . , ak, then w(t) = c(t) + e(t), where e(t) = ta1 + ta2 + · · ·+ tak is the\\nerror polynomial. The decoder must determine the integers ai and then recover c(t) from\\nw(t) by flipping the aith bit. From w(t) we can compute w(ωi) = si for i = 1, . . . , 2r, where\\nω is a primitive nth root of unity over Z2. We say the syndrome of w(t) is s1, . . . , s2r.\\n1. Show that w(t) is a code polynomial if and only if si = 0 for all i.\\n\\n2. Show that\\nsi = w(ωi) = e(ωi) = ωia1 + ωia2 + · · ·+ ωiak\\n\\nfor i = 1, . . . , 2r. The error-locator polynomial is defined to be\\n\\ns(x) = (x+ ωa1)(x+ ωa2) · · · (x+ ωak).\\n\\n3. Recall the (15, 7)-block bch code in Example 22.19. By Theorem 8.13, this code is\\ncapable of correcting two errors. Suppose that these errors occur in bits a1 and a2. The\\nerror-locator polynomial is s(x) = (x+ ωa1)(x+ ωa2). Show that\\n\\ns(x) = x2 + s1x+\\n\\n(\\ns21 +\\n\\ns3\\ns1\\n\\n)\\n.\\n\\n4. Let w(t) = 1 + t2 + t4 + t5 + t7 + t12 + t13. Determine what the originally transmitted\\ncode polynomial was.\\n\\n\\n\\n22.5. REFERENCES AND SUGGESTED READINGS 393\\n\\n22.5 References and Suggested Readings\\n[1] Childs, L. A Concrete Introduction to Higher Algebra. 2nd ed. Springer-Verlag, New\\n\\nYork, 1995.\\n[2] Gåding, L. and Tambour, T. Algebra for Computer Science. Springer-Verlag, New\\n\\nYork, 1988.\\n[3] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\\n\\nAn excellent presentation of finite fields and their applications.\\n[4] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\\n[5] Roman, S. Coding and Information Theory. Springer-Verlag, New York, 1992.\\n[6] van Lint, J. H. Introduction to Coding Theory. Springer, New York, 1999.\\n\\n22.6 Sage\\nYou have noticed in this chapter that finite fields have a great deal of structure. We have\\nalso seen finite fields in Sage regularly as examples of rings and fields. Now we can combine\\nthe two, mostly using commands we already know, plus a few new ones.\\n\\nCreating Finite Fields\\n\\nBy Theorem 22.6 we know that all finite fields of a given order are isomorphic and that\\npossible orders are limited to powers of primes. We can use the FiniteField() command, as\\nbefore, or a shorter equivalent is GF(). Optionally, we can specify an irreducible polynomial\\nfor the contruction of the field. We can view this polynomial as the generator of the principal\\nideal of a polynomial ring, or we can view it as a “re-writing” rule for powers of the field’s\\ngenerator that allow us to multiply elements and reformulate them as linear combinations\\nof lesser powers.\\n\\nAbsent providing an irreducible polynomial, Sage will use a Conway polynomial. You\\ncan determine these with the conway_polynomial() command, or just build a finite field and\\nrequest the defining polynomial with the .polynomial() method.\\n\\nF.<a> = GF (7^15); F\\n\\nFinite Field in a of size 7^15\\n\\nF.polynomial ()\\n\\na^15 + 5*a^6 + 6*a^5 + 6*a^4 + 4*a^3 + a^2 + 2*a + 4\\n\\na^15 + 5*a^6 + 6*a^5 + 6*a^4 + 4*a^3 + a^2 + 2*a + 4\\n\\n0\\n\\nconway_polynomial (7, 15)\\n\\nx^15 + 5*x^6 + 6*x^5 + 6*x^4 + 4*x^3 + x^2 + 2*x + 4\\n\\nJust to be more readable, we coerce a list of coefficients into the set of polynomials\\n(obtained with the .parent() method on a simple polynomial) to define a polynomial.\\n\\n\\n\\n394 CHAPTER 22. FINITE FIELDS\\n\\ny = polygen(Integers (7), \' y \' )\\nP = y.parent ()\\np = P([4, 5, 2, 6, 3, 3, 6, 2, 1, 1, 2, 5, 6, 3, 5, 1]); p\\n\\ny^15 + 5*y^14 + 3*y^13 + 6*y^12 + 5*y^11 + 2*y^10 + y^9 +\\ny^8 + 2*y^7 + 6*y^6 + 3*y^5 + 3*y^4 + 6*y^3 + 2*y^2 + 5*y + 4\\n\\np.is_irreducible ()\\n\\nTrue\\n\\nT.<b> = GF(7^15, modulus=p); T\\n\\nFinite Field in b of size 7^15\\n\\nLogarithms in Finite Fields\\nOne useful command we have not described is the .log() method for elements of a finite\\nfield. Since we now know that the multiplicative group of nonzero elements is cyclic, we can\\nexpress every element as a power of the generator. The log method will return that power.\\n\\nUsually we will want to use the generator as the base of a lograithm computation in a\\nfinite field. However, other bases may be used, wih the understanding that if the base is\\nnot a generator, then the logarithm may not exist (i.e. there may not be a solution to the\\nrelevant equation).\\n\\nF.<a> = GF (5^4)\\na^458\\n\\n3*a^3 + 2*a^2 + a + 3\\n\\n(3*a^3 + 2*a^2 + a + 3).log(a)\\n\\n458\\n\\nexponent = (3*a^3 + 2*a^2 + a + 3).log(2*a^3 + 4*a^2 + 4*a)\\nexponent\\n\\n211\\n\\n(2*a^3 + 4*a^2 + 4*a)^exponent == 3*a^3 + 2*a^2 + a + 3\\n\\nTrue\\n\\n(3*a^3 + 2*a^2 + a + 3).log(a^2 + 4*a + 4)\\n\\nTraceback (most recent call last):\\n...\\nValueError: No discrete log of 3*a^3 + 2*a^2 + a + 3 found\\nto base a^2 + 4*a + 4\\n\\nSince we already know many Sage commands, there is not much else worth introducing\\nbefore we can work profitably with finite fields. The exercises explore the ways we can\\nexamine and exploit the structure of finite fields in Sage.\\n\\n\\n\\n22.7. SAGE EXERCISES 395\\n\\n22.7 Sage Exercises\\n1. Create a finite field of order 52 and then factor p(x) = x25 − x over this field. Comment\\non what is interesting about this result and why it is not a surprise.\\n\\n2. Corollary 22.11 says that the nonzero elements of a finite field are a cyclic group under\\nmultiplication. The generator used in Sage is also a generator of this multiplicative group.\\nTo see this, create a finite field of order 27. Create two lists of the elements of the field:\\nfirst, use the .list() method, then use a list comprehension to generate the proper powers\\nof the generator you specified when you created the field.\\nThe second list should be the whole field, but will be missing zero. Create the zero element\\nof the field (perhaps by coercing 0 into the field) and .append() it to the list of powers.\\nApply the sorted() command to each list and then test the lists for equality.\\n\\n3. Subfields of a finite field are completely classified by Theorem 22.7. It is possible to\\ncreate two finite fields of the correct orders for the supefield/subfield realtionship to hold,\\nand to translate between one and the other. However, in this exercise we will create a\\nsubfield of a finite field from scratch. Since the group of nonzero elements in a finite field\\nis cyclic, the nonzero elements of a subfield will form a subgroup of the cyclic group, and\\nnecessarily will be cyclic.\\nCreate a finite field of order 36. Theory says there is a subfield of order 32, since 2|6.\\nDetermine a generator of multiplicative order 8 for the nonzero elements of this subfield,\\nand construct these 8 elements. Add in the field’s zero element to this list. It should be\\nclear that this set of 9 elements is closed under multiplication. Absent our theorems about\\nfinite fields and cyclic groups, the closure under addition is not a given. Write a single\\nstatement that checks if this set is also closed under addition, by considering all possible\\nsums of elements from the set.\\n\\n4. This problem investigates the “separableness” of Q(\\n√\\n3,\\n√\\n7). You can create this number\\n\\nfield quickly with the NumberFieldTower constructor, along with the polynomials x2 − 3 and\\nx2 − 7. Flatten the tower with the .absolute_field() method and use the .structure()\\n\\nmethod to retrieve mappings between the tower and the flattened version. Name the tower\\nN and use a and b as generators. Name the flattened version L with c as a generator.\\nCreate a nontrivial (“random”) element of L using as many powers of c as possible (check\\nthe degree of L to see how many linearly independent powers there are). Request from\\nSage the minimum polynomial of your random element, thus ensuring the element is a root.\\nConstruct the minimum polynomial as a polynomial over N, the field tower, and find its\\nfactorization. Your factorization should have only linear factors. Each root should be an\\nexpression in a and b, so convert each root into an expression with mathematical notation\\ninvolving\\n\\n√\\n3 and\\n\\n√\\n7. Use one of the mappings to verify that one of the roots is indeed the\\n\\noriginal random element.\\nCreate a few more random elements, and find a factorization (in N or in L). For a field to be\\nseparable, every element of the field should be a root of some separable polynomial. The\\nminimal polynomial is a good polynomial to test. (Why?) Based on the evidence, does it\\nappear that Q(\\n\\n√\\n3,\\n√\\n7) is a separable extension?\\n\\n5. Exercise 22.3.21 describes the Frobenius Map, an automorphism of a finite field. If F is\\na finite field in Sage, then End(F) will create the automorphism group of F, the set of all\\nbijective mappings between the field and itself.\\n(a) Work Exercise 22.3.21 to gain an understanding of how and why the Frobenius mapping\\n\\nis a field automorphism. (Do not include any of this in your answer to this question,\\nbut understand that the following will be much easier if you do this problem first.)\\n\\n\\n\\n396 CHAPTER 22. FINITE FIELDS\\n\\n(b) For some small, but not trivial, finite fields locate the Frobenius map in the automor-\\nphism group. Small might mean p = 2, 3, 5, 7 and 3 ≤ n ≤ 10, with n prime versus\\ncomposite.\\n\\n(c) Once you have located the Frobenius map, describe the other automorphisms. In other\\nwords, with a bit of investigation, you should find a description of the automorphisms\\nwhich will allow you to accurately predict the entire automorphism group for a finite\\nfield you have not already explored. (Hint: the automorphism group is a group.\\nWhat if you “do the operation” between the Frobenius map and itself? Just what\\nis the operation? Try using Sage’s multiplicative notation with the elements of the\\nautomorphism group.)\\n\\n(d) What is the “structure” of the automorphism group? What special status does the\\nFrobenius map have in this group?\\n\\n(e) For any field, the subfield known as the fixed field is an important construction, and\\nwill be especially important in the next chapter. Given an automorphism τ of a field\\nE, the subset, K = {b ∈ E | τ(b) = b}, can be shown to be a subfield of E. It is known\\nas the fixed field of τ in E. For each automorphism of E = GF (36) identify the fixed\\nfield of the automorphism. Since we understand the structure of subfields of a finite\\nfield, it is enough to just determine the order of the fixed field to be able to identify\\nthe subfield precisely.\\n\\n6. Exercise 22.3.15 suggests that every element of a finite field may be written (expressed)\\nas a sum of squares. This exercise suggests computational experiments which might help\\nyou formulate a proof for the exercise.\\n(a) Construct two small, but not too small, finite fields, one with p = 2 and the other with\\n\\nan odd prime. Repeat the following for each field, F .\\n(b) Choose a “random” element of the field, say a ∈ F . Construct the sets\\n\\n{x2|x ∈ F} {a− x2|x ∈ F}\\n\\nusing Sage sets with the Set() constructor. (Be careful: set() is a Python command\\nwhich behaves differently in fundamental ways.)\\n\\n(c) Examine the size of the two sets and the size of their intersection (.intersection()).\\nTry different elements for a, perhaps writing a loop to try all possible values. Note\\nthat p = 2 will behave quite differently.\\n\\n(d) Suppose you have an element of the intersection. (You can get one with .an_element().)\\nHow does this lead to the sum of squares proposed in the exercise?\\n\\n(e) Can you write a Python function that accepts a finite field whose order is a power of\\nan odd prime and then lists each element as a sum of squares?\\n\\n\\n\\n23\\n\\nGalois Theory\\n\\nA classic problem of algebra is to find the solutions of a polynomial equation. The solution\\nto the quadratic equation was known in antiquity. Italian mathematicians found general\\nsolutions to the general cubic and quartic equations in the sixteenth century; however,\\nattempts to solve the general fifth-degree, or quintic, polynomial were repulsed for the next\\nthree hundred years. Certainly, equations such as x5 − 1 = 0 or x6 − x3 − 6 = 0 could be\\nsolved, but no solution like the quadratic formula was found for the general quintic,\\n\\nax5 + bx4 + cx3 + dx2 + ex+ f = 0.\\n\\nFinally, at the beginning of the nineteenth century, Ruffini and Abel both found quintics\\nthat could not be solved with any formula. It was Galois, however, who provided the full\\nexplanation by showing which polynomials could and could not be solved by formulas. He\\ndiscovered the connection between groups and field extensions. Galois theory demonstrates\\nthe strong interdependence of group and field theory, and has had far-reaching implications\\nbeyond its original purpose.\\n\\nIn this chapter we will prove the Fundamental Theorem of Galois Theory. This result\\nwill be used to establish the insolvability of the quintic and to prove the Fundamental\\nTheorem of Algebra.\\n\\n23.1 Field Automorphisms\\nOur first task is to establish a link between group theory and field theory by examining\\nautomorphisms of fields.\\n\\nProposition 23.1. The set of all automorphisms of a field F is a group under composition\\nof functions.\\n\\nProof. If σ and τ are automorphisms of E, then so are στ and σ−1. The identity is\\ncertainly an automorphism; hence, the set of all automorphisms of a field F is indeed a\\ngroup.\\n\\nProposition 23.2. Let E be a field extension of F . Then the set of all automorphisms of\\nE that fix F elementwise is a group; that is, the set of all automorphisms σ : E → E such\\nthat σ(α) = α for all α ∈ F is a group.\\n\\nProof. We need only show that the set of automorphisms of E that fix F elementwise is\\na subgroup of the group of all automorphisms of E. Let σ and τ be two automorphisms\\nof E such that σ(α) = α and τ(α) = α for all α ∈ F . Then στ(α) = σ(α) = α and\\nσ−1(α) = α. Since the identity fixes every element of E, the set of automorphisms of E\\nthat leave elements of F fixed is a subgroup of the entire group of automorphisms of E.\\n\\n397\\n\\n\\n\\n398 CHAPTER 23. GALOIS THEORY\\n\\nLet E be a field extension of F . We will denote the full group of automorphisms of E\\nby Aut(E). We define the Galois group of E over F to be the group of automorphisms\\nof E that fix F elementwise; that is,\\n\\nG(E/F ) = {σ ∈ Aut(E) : σ(α) = α for all α ∈ F}.\\n\\nIf f(x) is a polynomial in F [x] and E is the splitting field of f(x) over F , then we define\\nthe Galois group of f(x) to be G(E/F ).\\n\\nExample 23.3. Complex conjugation, defined by σ : a+ bi 7→ a− bi, is an automorphism\\nof the complex numbers. Since\\n\\nσ(a) = σ(a+ 0i) = a− 0i = a,\\n\\nthe automorphism defined by complex conjugation must be in G(C/R).\\n\\nExample 23.4. Consider the fields Q ⊂ Q(\\n√\\n5 ) ⊂ Q(\\n\\n√\\n3,\\n√\\n5 ). Then for a, b ∈ Q(\\n\\n√\\n5 ),\\n\\nσ(a+ b\\n√\\n3 ) = a− b\\n\\n√\\n3\\n\\nis an automorphism of Q(\\n√\\n3,\\n√\\n5 ) leaving Q(\\n\\n√\\n5 ) fixed. Similarly,\\n\\nτ(a+ b\\n√\\n5 ) = a− b\\n\\n√\\n5\\n\\nis an automorphism of Q(\\n√\\n3,\\n√\\n5 ) leaving Q(\\n\\n√\\n3 ) fixed. The automorphism µ = στ moves\\n\\nboth\\n√\\n3 and\\n\\n√\\n5. It will soon be clear that {id, σ, τ, µ} is the Galois group of Q(\\n\\n√\\n3,\\n√\\n5 )\\n\\nover Q. The following table shows that this group is isomorphic to Z2 × Z2.\\n\\nid σ τ µ\\n\\nid id σ τ µ\\n\\nσ σ id µ τ\\n\\nτ τ µ id σ\\n\\nµ µ τ σ id\\n\\nWe may also regard the field Q(\\n√\\n3,\\n√\\n5 ) as a vector space over Q that has basis {1,\\n\\n√\\n3,\\n√\\n5,\\n√\\n15 }.\\n\\nIt is no coincidence that |G(Q(\\n√\\n3,\\n√\\n5 )/Q)| = [Q(\\n\\n√\\n3,\\n√\\n5 ) : Q)] = 4.\\n\\nProposition 23.5. Let E be a field extension of F and f(x) be a polynomial in F [x]. Then\\nany automorphism in G(E/F ) defines a permutation of the roots of f(x) that lie in E.\\n\\nProof. Let\\nf(x) = a0 + a1x+ a2x\\n\\n2 + · · ·+ anx\\nn\\n\\nand suppose that α ∈ E is a zero of f(x). Then for σ ∈ G(E/F ),\\n\\n0 = σ(0)\\n\\n= σ(f(α))\\n\\n= σ(a0 + a1α+ a2α\\n2 + · · ·+ anα\\n\\nn)\\n\\n= a0 + a1σ(α) + a2[σ(α)]\\n2 + · · ·+ an[σ(α)]\\n\\nn;\\n\\ntherefore, σ(α) is also a zero of f(x).\\n\\nLet E be an algebraic extension of a field F . Two elements α, β ∈ E are conjugate over\\nF if they have the same minimal polynomial. For example, in the field Q(\\n\\n√\\n2 ) the elements√\\n\\n2 and −\\n√\\n2 are conjugate over Q since they are both roots of the irreducible polynomial\\n\\nx2 − 2.\\nA converse of the last proposition exists. The proof follows directly from Lemma 21.32.\\n\\n\\n\\n23.1. FIELD AUTOMORPHISMS 399\\n\\nProposition 23.6. If α and β are conjugate over F , there exists an isomorphism σ :\\nF (α) → F (β) such that σ is the identity when restricted to F .\\n\\nTheorem 23.7. Let f(x) be a polynomial in F [x] and suppose that E is the splitting field\\nfor f(x) over F . If f(x) has no repeated roots, then\\n\\n|G(E/F )| = [E : F ].\\n\\nProof. We will use mathematical induction on the degree of f(x). If the degree of f(x)\\nis 0 or 1, then E = F and there is nothing to show. Assume that the result holds for all\\npolynomials of degree k with 0 ≤ k < n. Suppose that the degree of f(x) is n. Let p(x)\\nbe an irreducible factor of f(x) of degree r. Since all of the roots of p(x) are in E, we can\\nchoose one of these roots, say α, so that F ⊂ F (α) ⊂ E. Then\\n\\n[E : F (α)] = n/r and [F (α) : F ] = r.\\n\\nIf β is any other root of p(x), then F ⊂ F (β) ⊂ E. By Lemma 21.32, there exists a\\nunique isomorphism σ : F (α) → F (β) for each such β that fixes F elementwise. Since\\nE is a splitting field of F (β), there are exactly r such isomorphisms. For each of these\\nautomorphisms, we can use our induction hypothesis on [E : F (α)] = n/r < n to conclude\\nthat\\n\\n|G(E/F (α))| = [E : F (α)].\\n\\nConsequently, there are\\n[E : F ] = [E : F (α)][F (α) : F ] = n\\n\\npossible automorphisms of E that fix F , or |G(E/F )| = [E : F ].\\n\\nCorollary 23.8. Let F be a finite field with a finite extension E such that [E : F ] = k.\\nThen G(E/F ) is cyclic of order k.\\n\\nProof. Let p be the characteristic of E and F and assume that the orders of E and F are\\npm and pn, respectively. Then nk = m. We can also assume that E is the splitting field of\\nxp\\n\\nm − x over a subfield of order p. Therefore, E must also be the splitting field of xpm − x\\nover F . Applying Theorem 23.7, we find that |G(E/F )| = k.\\n\\nTo prove that G(E/F ) is cyclic, we must find a generator for G(E/F ). Let σ : E → E\\nbe defined by σ(α) = αpn . We claim that σ is the element in G(E/F ) that we are seeking.\\nWe first need to show that σ is in Aut(E). If α and β are in E,\\n\\nσ(α+ β) = (α+ β)p\\nn\\n= αpn + βp\\n\\nn\\n= σ(α) + σ(β)\\n\\nby Lemma 22.3 Also, it is easy to show that σ(αβ) = σ(α)σ(β). Since σ is a nonzero\\nhomomorphism of fields, it must be injective. It must also be onto, since E is a finite field.\\nWe know that σ must be in G(E/F ), since F is the splitting field of xpn − x over the base\\nfield of order p. This means that σ leaves every element in F fixed. Finally, we must show\\nthat the order of σ is k. By Theorem 23.7, we know that\\n\\nσk(α) = αpnk\\n= αpm = α\\n\\nis the identity of G(E/F ). However, σr cannot be the identity for 1 ≤ r < k; otherwise,\\nxp\\n\\nnr − x would have pm roots, which is impossible.\\n\\nExample 23.9. We can now confirm that the Galois group of Q(\\n√\\n3,\\n√\\n5 ) over Q in Exam-\\n\\nple 23.4 is indeed isomorphic to Z2×Z2. Certainly the group H = {id, σ, τ, µ} is a subgroup\\nof G(Q(\\n\\n√\\n3,\\n√\\n5 )/Q); however, H must be all of G(Q(\\n\\n√\\n3,\\n√\\n5 )/Q), since\\n\\n|H| = [Q(\\n√\\n3,\\n√\\n5 ) : Q] = |G(Q(\\n\\n√\\n3,\\n√\\n5 )/Q)| = 4.\\n\\n\\n\\n400 CHAPTER 23. GALOIS THEORY\\n\\nExample 23.10. Let us compute the Galois group of\\n\\nf(x) = x4 + x3 + x2 + x+ 1\\n\\nover Q. We know that f(x) is irreducible by Exercise 17.4.20 in Chapter 17. Furthermore,\\nsince (x− 1)f(x) = x5 − 1, we can use DeMoivre’s Theorem to determine that the roots of\\nf(x) are ωi, where i = 1, . . . , 4 and\\n\\nω = cos(2π/5) + i sin(2π/5).\\n\\nHence, the splitting field of f(x) must be Q(ω). We can define automorphisms σi of Q(ω) by\\nσi(ω) = ωi for i = 1, . . . , 4. It is easy to check that these are indeed distinct automorphisms\\nin G(Q(ω)/Q). Since\\n\\n[Q(ω) : Q] = |G(Q(ω)/Q)| = 4,\\n\\nthe σi’s must be all of G(Q(ω)/Q). Therefore, G(Q(ω)/Q) ∼= Z4 since ω is a generator for\\nthe Galois group.\\n\\nSeparable Extensions\\nMany of the results that we have just proven depend on the fact that a polynomial f(x) in\\nF [x] has no repeated roots in its splitting field. It is evident that we need to know exactly\\nwhen a polynomial factors into distinct linear factors in its splitting field. Let E be the\\nsplitting field of a polynomial f(x) in F [x]. Suppose that f(x) factors over E as\\n\\nf(x) = (x− α1)\\nn1(x− α2)\\n\\nn2 · · · (x− αr)\\nnr =\\n\\nr∏\\ni=1\\n\\n(x− αi)\\nni .\\n\\nWe define the multiplicity of a root αi of f(x) to be ni. A root with multiplicity 1 is\\ncalled a simple root. Recall that a polynomial f(x) ∈ F [x] of degree n is separable if it\\nhas n distinct roots in its splitting field E. Equivalently, f(x) is separable if it factors into\\ndistinct linear factors over E[x]. An extension E of F is a separable extension of F if\\nevery element in E is the root of a separable polynomial in F [x]. Also recall that f(x) is\\nseparable if and only if gcd(f(x), f ′(x)) = 1 (Lemma 22.5).\\n\\nProposition 23.11. Let f(x) be an irreducible polynomial over F . If the characteristic of\\nF is 0, then f(x) is separable. If the characteristic of F is p and f(x) ̸= g(xp) for some\\ng(x) in F [x], then f(x) is also separable.\\n\\nProof. First assume that charF = 0. Since deg f ′(x) < deg f(x) and f(x) is irreducible,\\nthe only way gcd(f(x), f ′(x)) ̸= 1 is if f ′(x) is the zero polynomial; however, this is impos-\\nsible in a field of characteristic zero. If charF = p, then f ′(x) can be the zero polynomial\\nif every coefficient of f(x) is a multiple of p. This can happen only if we have a polynomial\\nof the form f(x) = a0 + a1x\\n\\np + a2x\\n2p + · · ·+ anx\\n\\nnp.\\n\\nCertainly extensions of a field F of the form F (α) are some of the easiest to study\\nand understand. Given a field extension E of F , the obvious question to ask is when it\\nis possible to find an element α ∈ E such that E = F (α). In this case, α is called a\\nprimitive element. We already know that primitive elements exist for certain extensions.\\nFor example,\\n\\nQ(\\n√\\n3,\\n√\\n5 ) = Q(\\n\\n√\\n3 +\\n\\n√\\n5 )\\n\\nand\\nQ(\\n\\n3\\n√\\n5,\\n√\\n5 i) = Q(\\n\\n6\\n√\\n5 i).\\n\\nCorollary 22.12 tells us that there exists a primitive element for any finite extension of a\\nfinite field. The next theorem tells us that we can often find a primitive element.\\n\\n\\n\\n23.2. THE FUNDAMENTAL THEOREM 401\\n\\nTheorem 23.12 (Primitive Element Theorem). Let E be a finite separable extension of a\\nfield F . Then there exists an α ∈ E such that E = F (α).\\n\\nProof. We already know that there is no problem if F is a finite field. Suppose that E\\nis a finite extension of an infinite field. We will prove the result for F (α, β). The general\\ncase easily follows when we use mathematical induction. Let f(x) and g(x) be the minimal\\npolynomials of α and β, respectively. Let K be the field in which both f(x) and g(x) split.\\nSuppose that f(x) has zeros α = α1, . . . , αn in K and g(x) has zeros β = β1, . . . , βm in K.\\nAll of these zeros have multiplicity 1, since E is separable over F . Since F is infinite, we\\ncan find an a in F such that\\n\\na ̸= αi − α\\n\\nβ − βj\\n\\nfor all i and j with j ̸= 1. Therefore, a(β − βj) ̸= αi − α. Let γ = α+ aβ. Then\\n\\nγ = α+ aβ ̸= αi + aβj ;\\n\\nhence, γ − aβj ̸= αi for all i, j with j ̸= 1. Define h(x) ∈ F (γ)[x] by h(x) = f(γ − ax).\\nThen h(β) = f(α) = 0. However, h(βj) ̸= 0 for j ̸= 1. Hence, h(x) and g(x) have a single\\ncommon factor in F (γ)[x]; that is, the irreducible polynomial of β over F (γ) must be linear,\\nsince β is the only zero common to both g(x) and h(x). So β ∈ F (γ) and α = γ − aβ is in\\nF (γ). Hence, F (α, β) = F (γ).\\n\\n23.2 The Fundamental Theorem\\nThe goal of this section is to prove the Fundamental Theorem of Galois Theory. This\\ntheorem explains the connection between the subgroups of G(E/F ) and the intermediate\\nfields between E and F .\\n\\nProposition 23.13. Let {σi : i ∈ I} be a collection of automorphisms of a field F . Then\\n\\nF{σi} = {a ∈ F : σi(a) = a for all σi}\\n\\nis a subfield of F .\\n\\nProof. Let σi(a) = a and σi(b) = b. Then\\n\\nσi(a± b) = σi(a)± σi(b) = a± b\\n\\nand\\nσi(ab) = σi(a)σi(b) = ab.\\n\\nIf a ̸= 0, then σi(a\\n−1) = [σi(a)]\\n\\n−1 = a−1. Finally, σi(0) = 0 and σi(1) = 1 since σi is an\\nautomorphism.\\n\\nCorollary 23.14. Let F be a field and let G be a subgroup of Aut(F ). Then\\n\\nFG = {α ∈ F : σ(α) = α for all σ ∈ G}\\n\\nis a subfield of F .\\n\\nThe subfield F{σi} of F is called the fixed field of {σi}. The field fixed for a subgroup\\nG of Aut(F ) will be denoted by FG.\\n\\nExample 23.15. Let σ : Q(\\n√\\n3,\\n√\\n5 ) → Q(\\n\\n√\\n3,\\n√\\n5 ) be the automorphism that maps\\n\\n√\\n3 to\\n\\n−\\n√\\n3. Then Q(\\n\\n√\\n5 ) is the subfield of Q(\\n\\n√\\n3,\\n√\\n5 ) left fixed by σ.\\n\\n\\n\\n402 CHAPTER 23. GALOIS THEORY\\n\\nProposition 23.16. Let E be a splitting field over F of a separable polynomial. Then\\nEG(E/F ) = F .\\n\\nProof. Let G = G(E/F ). Clearly, F ⊂ EG ⊂ E. Also, E must be a splitting field of EG\\n\\nand G(E/F ) = G(E/EG). By Theorem 23.7,\\n\\n|G| = [E : EG] = [E : F ].\\n\\nTherefore, [EG : F ] = 1. Consequently, EG = F .\\n\\nA large number of mathematicians first learned Galois theory from Emil Artin’s mono-\\ngraph on the subject [1]. The very clever proof of the following lemma is due to Artin.\\n\\nLemma 23.17. Let G be a finite group of automorphisms of E and let F = EG. Then\\n[E : F ] ≤ |G|.\\n\\nProof. Let |G| = n. We must show that any set of n + 1 elements α1, . . . , αn+1 in E is\\nlinearly dependent over F ; that is, we need to find elements ai ∈ F , not all zero, such that\\n\\na1α1 + a2α2 + · · ·+ an+1αn+1 = 0.\\n\\nSuppose that σ1 = id, σ2, . . . , σn are the automorphisms in G. The homogeneous system of\\nlinear equations\\n\\nσ1(α1)x1 + σ1(α2)x2 + · · ·+ σ1(αn+1)xn+1 = 0\\n\\nσ2(α1)x1 + σ2(α2)x2 + · · ·+ σ2(αn+1)xn+1 = 0\\n\\n...\\nσn(α1)x1 + σn(α2)x2 + · · ·+ σn(αn+1)xn+1 = 0\\n\\nhas more unknowns than equations. From linear algebra we know that this system has a\\nnontrivial solution, say xi = ai for i = 1, 2, . . . , n + 1. Since σ1 is the identity, the first\\nequation translates to\\n\\na1α1 + a2α2 + · · ·+ an+1αn+1 = 0.\\n\\nThe problem is that some of the ai’s may be in E but not in F . We must show that this is\\nimpossible.\\n\\nSuppose that at least one of the ai’s is in E but not in F . By rearranging the αi’s we\\nmay assume that a1 is nonzero. Since any nonzero multiple of a solution is also a solution,\\nwe can also assume that a1 = 1. Of all possible solutions fitting this description, we choose\\nthe one with the smallest number of nonzero terms. Again, by rearranging α2, . . . , αn+1 if\\nnecessary, we can assume that a2 is in E but not in F . Since F is the subfield of E that\\nis fixed elementwise by G, there exists a σi in G such that σi(a2) ̸= a2. Applying σi to\\neach equation in the system, we end up with the same homogeneous system, since G is a\\ngroup. Therefore, x1 = σi(a1) = 1, x2 = σi(a2), . . ., xn+1 = σi(an+1) is also a solution of\\nthe original system. We know that a linear combination of two solutions of a homogeneous\\nsystem is also a solution; consequently,\\n\\nx1 = 1− 1 = 0\\n\\nx2 = a2 − σi(a2)\\n\\n...\\nxn+1 = an+1 − σi(an+1)\\n\\n\\n\\n23.2. THE FUNDAMENTAL THEOREM 403\\n\\nmust be another solution of the system. This is a nontrivial solution because σi(a2) ̸= a2,\\nand has fewer nonzero entries than our original solution. This is a contradiction, since the\\nnumber of nonzero solutions to our original solution was assumed to be minimal. We can\\ntherefore conclude that a1, . . . , an+1 ∈ F .\\n\\nLet E be an algebraic extension of F . If every irreducible polynomial in F [x] with a\\nroot in E has all of its roots in E, then E is called a normal extension of F ; that is,\\nevery irreducible polynomial in F [x] containing a root in E is the product of linear factors\\nin E[x].\\n\\nTheorem 23.18. Let E be a field extension of F . Then the following statements are\\nequivalent.\\n\\n1. E is a finite, normal, separable extension of F .\\n\\n2. E is a splitting field over F of a separable polynomial.\\n\\n3. F = EG for some finite group G of automorphisms of E.\\n\\nProof. (1) ⇒ (2). Let E be a finite, normal, separable extension of F . By the Primitive\\nElement Theorem, we can find an α in E such that E = F (α). Let f(x) be the minimal\\npolynomial of α over F . The field E must contain all of the roots of f(x) since it is a normal\\nextension F ; hence, E is a splitting field for f(x).\\n\\n(2) ⇒ (3). Let E be the splitting field over F of a separable polynomial. By Proposi-\\ntion 23.16, EG(E/F ) = F . Since |G(E/F )| = [E : F ], this is a finite group.\\n\\n(3) ⇒ (1). Let F = EG for some finite group of automorphisms G of E. Since [E : F ] ≤\\n|G|, E is a finite extension of F . To show that E is a finite, normal extension of F , let\\nf(x) ∈ F [x] be an irreducible monic polynomial that has a root α in E. We must show that\\nf(x) is the product of distinct linear factors in E[x]. By Proposition 23.5, automorphisms in\\nG permute the roots of f(x) lying in E. Hence, if we let G act on α, we can obtain distinct\\nroots α1 = α, α2, . . . , αn in E. Let g(x) =\\n\\n∏n\\ni=1(x − αi). Then g(x) is separable over F\\n\\nand g(α) = 0. Any automorphism σ in G permutes the factors of g(x) since it permutes\\nthese roots; hence, when σ acts on g(x), it must fix the coefficients of g(x). Therefore,\\nthe coefficients of g(x) must be in F . Since deg g(x) ≤ deg f(x) and f(x) is the minimal\\npolynomial of α, f(x) = g(x).\\n\\nCorollary 23.19. Let K be a field extension of F such that F = KG for some finite group\\nof automorphisms G of K. Then G = G(K/F ).\\n\\nProof. Since F = KG, G is a subgroup of G(K/F ). Hence,\\n\\n[K : F ] ≤ |G| ≤ |G(K/F )| = [K : F ].\\n\\nIt follows that G = G(K/F ), since they must have the same order.\\n\\nBefore we determine the exact correspondence between field extensionsand automor-\\nphisms of fields, let us return to a familiar example.\\n\\nExample 23.20. In Example 23.4 we examined the automorphisms of Q(\\n√\\n3,\\n√\\n5 ) fixing\\n\\nQ. Figure 23.21 compares the lattice of field extensions of Q with the lattice of subgroups\\nof G(Q(\\n\\n√\\n3,\\n√\\n5 )/Q). The Fundamental Theorem of Galois Theory tells us what the rela-\\n\\ntionship is between the two lattices.\\n\\n\\n\\n404 CHAPTER 23. GALOIS THEORY\\n\\n{id, σ} {id, τ} {id, µ}\\n\\n{id, σ, τ, µ}\\n\\n{id}\\n\\nQ(\\n√\\n3 ) Q(\\n\\n√\\n5 ) Q(\\n\\n√\\n15 )\\n\\nQ(\\n√\\n3,\\n√\\n5 )\\n\\nQ\\n\\nFigure 23.21: G(Q(\\n√\\n3,\\n√\\n5 )/Q)\\n\\nWe are now ready to state and prove the Fundamental Theorem of Galois Theory.\\n\\nTheorem 23.22 (Fundamental Theorem of Galois Theory). Let F be a finite field or a field\\nof characteristic zero. If E is a finite normal extension of F with Galois group G(E/F ),\\nthen the following statements are true.\\n\\n1. The map K 7→ G(E/K) is a bijection of subfields K of E containing F with the\\nsubgroups of G(E/F ).\\n\\n2. If F ⊂ K ⊂ E, then\\n\\n[E : K] = |G(E/K)| and [K : F ] = [G(E/F ) : G(E/K)].\\n\\n3. F ⊂ K ⊂ L ⊂ E if and only if {id} ⊂ G(E/L) ⊂ G(E/K) ⊂ G(E/F ).\\n\\n4. K is a normal extension of F if and only if G(E/K) is a normal subgroup of G(E/F ).\\nIn this case\\n\\nG(K/F ) ∼= G(E/F )/G(E/K).\\n\\nProof. (1) Suppose that G(E/K) = G(E/L) = G. Both K and L are fixed fields of\\nG; hence, K = L and the map defined by K 7→ G(E/K) is one-to-one. To show that\\nthe map is onto, let G be a subgroup of G(E/F ) and K be the field fixed by G. Then\\nF ⊂ K ⊂ E; consequently, E is a normal extension of K. Thus, G(E/K) = G and the map\\nK 7→ G(E/K) is a bijection.\\n\\n(2) By Theorem 23.7, |G(E/K)| = [E : K]; therefore,\\n\\n|G(E/F )| = [G(E/F ) : G(E/K)] · |G(E/K)| = [E : F ] = [E : K][K : F ].\\n\\nThus, [K : F ] = [G(E/F ) : G(E/K)].\\n(3) Statement (3) is illustrated in Figure 23.23. We leave the proof of this property as\\n\\nan exercise.\\n(4) This part takes a little more work. Let K be a normal extension of F . If σ is in\\n\\nG(E/F ) and τ is in G(E/K), we need to show that σ−1τσ is in G(E/K); that is, we need\\nto show that σ−1τσ(α) = α for all α ∈ K. Suppose that f(x) is the minimal polynomial of\\nα over F . Then σ(α) is also a root of f(x) lying in K, since K is a normal extension of F .\\nHence, τ(σ(α)) = σ(α) or σ−1τσ(α) = α.\\n\\nConversely, let G(E/K) be a normal subgroup of G(E/F ). We need to show that\\nF = KG(K/F ). Let τ ∈ G(E/K). For all σ ∈ G(E/F ) there exists a τ ∈ G(E/K) such that\\nτσ = στ . Consequently, for all α ∈ K\\n\\nτ(σ(α)) = σ(τ(α)) = σ(α);\\n\\n\\n\\n23.2. THE FUNDAMENTAL THEOREM 405\\n\\nhence, σ(α) must be in the fixed field of G(E/K). Let σ be the restriction of σ to K. Then\\nσ is an automorphism of K fixing F , since σ(α) ∈ K for all α ∈ K; hence, σ ∈ G(K/F ).\\nNext, we will show that the fixed field of G(K/F ) is F . Let β be an element in K that\\nis fixed by all automorphisms in G(K/F ). In particular, σ(β) = β for all σ ∈ G(E/F ).\\nTherefore, β belongs to the fixed field F of G(E/F ).\\n\\nFinally, we must show that when K is a normal extension of F ,\\n\\nG(K/F ) ∼= G(E/F )/G(E/K).\\n\\nFor σ ∈ G(E/F ), let σK be the automorphism of K obtained by restricting σ to K. Since K\\nis a normal extension, the argument in the preceding paragraph shows that σK ∈ G(K/F ).\\nConsequently, we have a map ϕ : G(E/F ) → G(K/F ) defined by σ 7→ σK . This map is a\\ngroup homomorphism since\\n\\nϕ(στ) = (στ)K = σKτK = ϕ(σ)ϕ(τ).\\n\\nThe kernel of ϕ is G(E/K). By (2),\\n\\n|G(E/F )|/|G(E/K)| = [K : F ] = |G(K/F )|.\\n\\nHence, the image of ϕ is G(K/F ) and ϕ is onto. Applying the First Isomorphism Theorem,\\nwe have\\n\\nG(K/F ) ∼= G(E/F )/G(E/K).\\n\\nF G(E/F )\\n\\nK G(E/K)\\n\\nL G(E/L)\\n\\nE {id}\\n\\nFigure 23.23: Subgroups of G(E/F ) and subfields of E\\n\\nExample 23.24. In this example we will illustrate the Fundamental Theorem of Galois\\nTheory by determining the lattice of subgroups of the Galois group of f(x) = x4 − 2. We\\nwill compare this lattice to the lattice of field extensions of Q that are contained in the\\nsplitting field of x4 − 2. The splitting field of f(x) is Q( 4\\n\\n√\\n2, i). To see this, notice that\\n\\nf(x) factors as (x2 +\\n√\\n2 )(x2 −\\n\\n√\\n2 ); hence, the roots of f(x) are ± 4\\n\\n√\\n2 and ± 4\\n\\n√\\n2 i. We first\\n\\nadjoin the root 4\\n√\\n2 to Q and then adjoin the root i of x2 + 1 to Q( 4\\n\\n√\\n2 ). The splitting field\\n\\nof f(x) is then Q( 4\\n√\\n2 )(i) = Q( 4\\n\\n√\\n2, i).\\n\\nSince [Q( 4\\n√\\n2 ) : Q] = 4 and i is not in Q( 4\\n\\n√\\n2 ), it must be the case that [Q( 4\\n\\n√\\n2, i) :\\n\\nQ( 4\\n√\\n2 )] = 2. Hence, [Q( 4\\n\\n√\\n2, i) : Q] = 8. The set\\n\\n{1, 4\\n√\\n2, (\\n\\n4\\n√\\n2 )2, (\\n\\n4\\n√\\n2 )3, i, i\\n\\n4\\n√\\n2, i(\\n\\n4\\n√\\n2 )2, i(\\n\\n4\\n√\\n2 )3}\\n\\n\\n\\n406 CHAPTER 23. GALOIS THEORY\\n\\nis a basis of Q( 4\\n√\\n2, i) over Q. The lattice of field extensions of Q contained in Q( 4\\n\\n√\\n2, i) is\\n\\nillustrated in Figure 23.25(a).\\nThe Galois group G of f(x) must be of order 8. Let σ be the automorphism defined by\\n\\nσ( 4\\n√\\n2 ) = i 4\\n\\n√\\n2 and σ(i) = i, and τ be the automorphism defined by complex conjugation;\\n\\nthat is, τ(i) = −i. Then G has an element of order 4 and an element of order 2. It is easy\\nto verify by direct computation that the elements of G are {id, σ, σ2, σ3, τ, στ, σ2τ, σ3τ}\\nand that the relations τ2 = id, σ4 = id, and τστ = σ−1 are satisfied; hence, G must be\\nisomorphic to D4. The lattice of subgroups of G is illustrated in Figure 23.25(b).\\n\\n{id}\\n\\n{id, τ} {id, σ2τ} {id, σ2} {id, στ} {id, σ3τ}\\n\\n{id, σ2, τ, σ2τ} {id, σ, σ2, σ3}{id, σ2, στ, σ3τ}\\n\\nD4\\n\\n(b)\\n\\nQ\\n\\nQ(\\n√\\n2 ) Q(i) Q(\\n\\n√\\n2 i)\\n\\nQ( 4\\n√\\n2 ) Q( 4\\n\\n√\\n2 i) Q(\\n\\n√\\n2, i) Q((1 + i) 4\\n\\n√\\n2 ) Q((1− i) 4\\n\\n√\\n2 )\\n\\nQ( 4\\n√\\n2, i)\\n\\n(a)\\n\\nFigure 23.25: Galois group of x4 − 2\\n\\nHistorical Note\\n\\nSolutions for the cubic and quartic equations were discovered in the 1500s. Attempts to\\nfind solutions for the quintic equations puzzled some of history’s best mathematicians. In\\n1798, P. Ruffini submitted a paper that claimed no such solution could be found; however,\\nthe paper was not well received. In 1826, Niels Henrik Abel (1802–1829) finally offered the\\nfirst correct proof that quintics are not always solvable by radicals.\\n\\nAbel inspired the work of Évariste Galois. Born in 1811, Galois began to display ex-\\ntraordinary mathematical talent at the age of 14. He applied for entrance to the École\\nPolytechnique several times; however, he had great difficulty meeting the formal entrance\\nrequirements, and the examiners failed to recognize his mathematical genius. He was finally\\naccepted at the École Normale in 1829.\\n\\n\\n\\n23.3. APPLICATIONS 407\\n\\nGalois worked to develop a theory of solvability for polynomials. In 1829, at the age of\\n17, Galois presented two papers on the solution of algebraic equations to the Académie des\\nSciences de Paris. These papers were sent to Cauchy, who subsequently lost them. A third\\npaper was submitted to Fourier, who died before he could read the paper. Another paper\\nwas presented, but was not published until 1846.\\n\\nGalois’ democratic sympathies led him into the Revolution of 1830. He was expelled\\nfrom school and sent to prison for his part in the turmoil. After his release in 1832, he was\\ndrawn into a duel possibly over a love affair. Certain that he would be killed, he spent the\\nevening before his death outlining his work and his basic ideas for research in a long letter\\nto his friend Chevalier. He was indeed dead the next day, at the age of 20.\\n\\n23.3 Applications\\nSolvability by Radicals\\nThroughout this section we shall assume that all fields have characteristic zero to ensure\\nthat irreducible polynomials do not have multiple roots. The immediate goal of this section\\nis to determine when the roots of a polynomial f(x) can be computed with a finite number of\\noperations on the coefficients of f(x). The allowable operations are addition, subtraction,\\nmultiplication, division, and the extraction of nth roots. Certainly the solution to the\\nquadratic equation, ax2 + bx+ c = 0, illustrates this process:\\n\\nx =\\n−b±\\n\\n√\\nb2 − 4ac\\n\\n2a\\n.\\n\\nThe only one of these operations that might demand a larger field is the taking of nth roots.\\nWe are led to the following definition.\\n\\nAn extension field E of a field F is an extension by radicals if there exists a chain of\\nsubfields\\n\\nF = F0 ⊆ F1 ⊆ F2 ⊆ · · · ⊆ Fr = E\\n\\nsuch for i = 1, 2, . . . , r, we have Fi = Fi−1(αi) and αni\\ni ∈ Fi−1 for some positive integer ni.\\n\\nA polynomial f(x) is solvable by radicals over F if the splitting field K of f(x) over F\\nis contained in an extension of F by radicals. Our goal is to arrive at criteria that will tell\\nus whether or not a polynomial f(x) is solvable by radicals by examining the Galois group\\nf(x).\\n\\nThe easiest polynomial to solve by radicals is one of the form xn−a. As we discussed in\\nChapter 4, the roots of xn − 1 are called the nth roots of unity. These roots are a finite\\nsubgroup of the splitting field of xn − 1. By Corollary 22.11, the nth roots of unity form a\\ncyclic group. Any generator of this group is called a primitive nth root of unity.\\n\\nExample 23.26. The polynomial xn − 1 is solvable by radicals over Q. The roots of this\\npolynomial are 1, ω, ω2, . . . , ωn−1, where\\n\\nω = cos\\n(\\n2π\\n\\nn\\n\\n)\\n+ i sin\\n\\n(\\n2π\\n\\nn\\n\\n)\\n.\\n\\nThe splitting field of xn − 1 over Q is Q(ω).\\n\\nWe shall prove that a polynomial is solvable by radicals if its Galois group is solvable.\\nRecall that a subnormal series of a group G is a finite sequence of subgroups\\n\\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e},\\n\\n\\n\\n408 CHAPTER 23. GALOIS THEORY\\n\\nwhere Hi is normal in Hi+1. A group G is solvable if it has a subnormal series {Hi} such\\nthat all of the factor groups Hi+1/Hi are abelian. For example, if we examine the series\\n{id} ⊂ A3 ⊂ S3, we see that S3 is solvable. On the other hand, S5 is not solvable, by\\nTheorem 10.11.\\n\\nLemma 23.27. Let F be a field of characteristic zero and E be the splitting field of xn − a\\nover F with a ∈ F . Then G(E/F ) is a solvable group.\\n\\nProof. The roots of xn − a are n\\n√\\na, ω n\\n\\n√\\na, . . . , ωn−1 n\\n\\n√\\na, where ω is a primitive nth root\\n\\nof unity. Suppose that F contains all of its nth roots of unity. If ζ is one of the roots of\\nxn − a, then distinct roots of xn − a are ζ, ωζ, . . . , ωn−1ζ, and E = F (ζ). Since G(E/F )\\npermutes the roots xn − a, the elements in G(E/F ) must be determined by their action on\\nthese roots. Let σ and τ be in G(E/F ) and suppose that σ(ζ) = ωiζ and τ(ζ) = ωjζ. If F\\ncontains the roots of unity, then\\n\\nστ(ζ) = σ(ωjζ) = ωjσ(ζ) = ωi+jζ = ωiτ(ζ) = τ(ωiζ) = τσ(ζ).\\n\\nTherefore, στ = τσ and G(E/F ) is abelian, and G(E/F ) must be solvable.\\nNow suppose that F does not contain a primitive nth root of unity. Let ω be a generator\\n\\nof the cyclic group of the nth roots of unity. Let α be a zero of xn − a. Since α and ωα\\nare both in the splitting field of xn − a, ω = (ωα)/α is also in E. Let K = F (ω). Then\\nF ⊂ K ⊂ E. Since K is the splitting field of xn − 1, K is a normal extension of F .\\nTherefore, any automorphism σ in G(F (ω)/F ) is determined by σ(ω). It must be the case\\nthat σ(ω) = ωi for some integer i since all of the zeros of xn−1 are powers of ω. If τ(ω) = ωj\\n\\nis in G(F (ω)/F ), then\\n\\nστ(ω) = σ(ωj) = [σ(ω)]j = ωij = [τ(ω)]i = τ(ωi) = τσ(ω).\\n\\nTherefore, G(F (ω)/F ) is abelian. By the Fundamental Theorem of Galois Theory the series\\n\\n{id} ⊂ G(E/F (ω)) ⊂ G(E/F )\\n\\nis a normal series. By our previous argument, G(E/F (ω)) is abelian. Since\\n\\nG(E/F )/G(E/F (ω)) ∼= G(F (ω)/F )\\n\\nis also abelian, G(E/F ) is solvable.\\n\\nLemma 23.28. Let F be a field of characteristic zero and let\\n\\nF = F0 ⊆ F1 ⊆ F2 ⊆ · · · ⊆ Fr = E\\n\\na radical extension of F . Then there exists a normal radical extension\\n\\nF = K0 ⊆ K1 ⊆ K2 ⊆ · · · ⊆ Kr = K\\n\\nsuch that K that contains E and Ki is a normal extension of Ki−1.\\n\\nProof. Since E is a radical extension of F , there exists a chain of subfields\\n\\nF = F0 ⊆ F1 ⊆ F2 ⊆ · · · ⊆ Fr = E\\n\\nsuch for i = 1, 2, . . . , r, we have Fi = Fi−1(αi) and αni\\ni ∈ Fi−1 for some positive integer ni.\\n\\nWe will build a normal radical extension of F ,\\n\\nF = K0 ⊆ K1 ⊆ K2 ⊆ · · · ⊆ Kr = K\\n\\n\\n\\n23.3. APPLICATIONS 409\\n\\nsuch that K ⊇ E. Define K1 for be the splitting field of xn1 − αn1\\n1 . The roots of this\\n\\npolynomial are α1, α1ω, α1ω\\n2, . . . , α1ω\\n\\nn1−1, where ω is a primitive n1th root of unity. If F\\ncontains all of its n1 roots of unity, then K1 = F (α!). On the other hand, suppose that\\nF does not contain a primitive n1th root of unity. If β is a root of xn1 − αn1\\n\\n1 , then all of\\nthe roots of xn1 − αn1\\n\\n1 must be β, ωβ, . . . , ωn1−1, where ω is a primitive n1th root of unity.\\nIn this case, K1 = F (ωβ). Thus, K1 is a normal radical extension of F containing F1.\\nContinuing in this manner, we obtain\\n\\nF = K0 ⊆ K1 ⊆ K2 ⊆ · · · ⊆ Kr = K\\n\\nsuch that Ki is a normal extension of Ki−1 and Ki ⊇ Fi for i = 1, 2, . . . , r.\\n\\nWe will now prove the main theorem about solvability by radicals.\\n\\nTheorem 23.29. Let f(x) be in F [x], where charF = 0. If f(x) is solvable by radicals,\\nthen the Galois group of f(x) over F is solvable.\\n\\nProof. Since f(x) is solvable by radicals there exists an extension E of F by radicals\\nF = F0 ⊆ F1 ⊆ · · · ⊆ Fn = E. By Lemma 23.28, we can assume that E is a splitting field\\nf(x) and Fi is normal over Fi−1. By the Fundamental Theorem of Galois Theory, G(E/Fi)\\nis a normal subgroup of G(E/Fi−1). Therefore, we have a subnormal series of subgroups of\\nG(E/F ):\\n\\n{id} ⊂ G(E/Fn−1) ⊂ · · · ⊂ G(E/F1) ⊂ G(E/F ).\\n\\nAgain by the Fundamental Theorem of Galois Theory, we know that\\n\\nG(E/Fi−1)/G(E/Fi) ∼= G(Fi/Fi−1).\\n\\nBy Lemma 23.27, G(Fi/Fi−1) is solvable; hence, G(E/F ) is also solvable.\\n\\nThe converse of Theorem 23.29 is also true. For a proof, see any of the references at the\\nend of this chapter.\\n\\nInsolvability of the Quintic\\n\\nWe are now in a position to find a fifth-degree polynomial that is not solvable by radicals.\\nWe merely need to find a polynomial whose Galois group is S5. We begin by proving a\\nlemma.\\n\\nLemma 23.30. If p is prime, then any subgroup of Sp that contains a transposition and a\\ncycle of length p must be all of Sp.\\n\\nProof. Let G be a subgroup of Sp that contains a transposition σ and τ a cycle of length\\np. We may assume that σ = (12). The order of τ is p and τn must be a cycle of length\\np for 1 ≤ n < p. Therefore, we may assume that µ = τn = (12i3 . . . ip) for some n, where\\n1 ≤ n < p (see Exercise 5.3.13 in Chapter 5). Noting that (12)(12i3 . . . ip) = (2i3 . . . ip)\\nand (2i3 . . . ip)\\n\\nk(12)(2i3 . . . ip)\\n−k = (1ik), we can obtain all the transpositions of the form\\n\\n(1n) for 1 ≤ n < p. However, these transpositions generate all transpositions in Sp, since\\n(1j)(1i)(1j) = (ij). The transpositions generate Sp.\\n\\n\\n\\n410 CHAPTER 23. GALOIS THEORY\\n\\n-3 -2 -1 1 2 3\\nx\\n\\n-60\\n\\n-40\\n\\n-20\\n\\n20\\n\\n40\\n\\n60\\n\\ny\\n\\nf(x) =x5 −6x3 −27x−3\\n\\nFigure 23.31: The graph of f(x) = x5 − 6x3 − 27x− 3\\n\\nExample 23.32. We will show that f(x) = x5 − 6x3 − 27x − 3 ∈ Q[x] is not solvable.\\nWe claim that the Galois group of f(x) over Q is S5. By Eisenstein’s Criterion, f(x) is\\nirreducible and, therefore, must be separable. The derivative of f(x) is f ′(x) = 5x4−18x2−\\n27; hence, setting f ′(x) = 0 and solving, we find that the only real roots of f ′(x) are\\n\\nx = ±\\n\\n√\\n6\\n√\\n6 + 9\\n\\n5\\n.\\n\\nTherefore, f(x) can have at most one maximum and one minimum. It is easy to show that\\nf(x) changes sign between −3 and −2, between −2 and 0, and once again between 0 and\\n4 (Figure 23.31). Therefore, f(x) has exactly three distinct real roots. The remaining two\\nroots of f(x) must be complex conjugates. Let K be the splitting field of f(x). Since f(x)\\nhas five distinct roots in K and every automorphism of K fixing Q is determined by the\\nway it permutes the roots of f(x), we know that G(K/Q) is a subgroup of S5. Since f\\nis irreducible, there is an element in σ ∈ G(K/Q) such that σ(a) = b for two roots a and\\nb of f(x). The automorphism of C that takes a + bi 7→ a − bi leaves the real roots fixed\\nand interchanges the complex roots; consequently, G(K/Q) ⊂ S5. By Lemma 23.30, S5 is\\ngenerated by a transposition and an element of order 5; therefore, G(K/Q) must be all of\\nS5. By Theorem 10.11, S5 is not solvable. Consequently, f(x) cannot be solved by radicals.\\n\\n\\n\\n23.4. EXERCISES 411\\n\\nThe Fundamental Theorem of Algebra\\nIt seems fitting that the last theorem that we will state and prove is the Fundamental\\nTheorem of Algebra. This theorem was first proven by Gauss in his doctoral thesis. Prior\\nto Gauss’s proof, mathematicians suspected that there might exist polynomials over the real\\nand complex numbers having no solutions. The Fundamental Theorem of Algebra states\\nthat every polynomial over the complex numbers factors into distinct linear factors.\\n\\nTheorem 23.33 (Fundamental Theorem of Algebra). The field of complex numbers is\\nalgebraically closed; that is, every polynomial in C[x] has a root in C.\\n\\nProof. Suppose that E is a proper finite field extension of the complex numbers. Since\\nany finite extension of a field of characteristic zero is a simple extension, there exists an\\nα ∈ E such that E = C(α) with α the root of an irreducible polynomial f(x) in C[x]. The\\nsplitting field L of f(x) is a finite normal separable extension of C that contains E. We\\nmust show that it is impossible for L to be a proper extension of C.\\n\\nSuppose that L is a proper extension of C. Since L is the splitting field of f(x)(x2 + 1)\\nover R, L is a finite normal separable extension of R. Let K be the fixed field of a Sylow\\n2-subgroup G of G(L/R). Then L ⊃ K ⊃ R and |G(L/K)| = [L : K]. Since [L : R] = [L :\\nK][K : R], we know that [K : R] must be odd. Consequently, K = R(β) with β having a\\nminimal polynomial f(x) of odd degree. Therefore, K = R.\\n\\nWe now know that G(L/R) must be a 2-group. It follows that G(L/C) is a 2-group. We\\nhave assumed that L ̸= C; therefore, |G(L/C)| ≥ 2. By the first Sylow Theorem and the\\nFundamental Theorem of Galois Theory, there exists a subgroup G of G(L/C) of index 2\\nand a field E fixed elementwise by G. Then [E : C] = 2 and there exists an element γ ∈ E\\nwith minimal polynomial x2+ bx+ c in C[x]. This polynomial has roots (−b±\\n\\n√\\nb2 − 4c )/2\\n\\nthat are in C, since b2 − 4c is in C. This is impossible; hence, L = C.\\n\\nAlthough our proof was strictly algebraic, we were forced to rely on results from cal-\\nculus. It is necessary to assume the completeness axiom from analysis to show that every\\npolynomial of odd degree has a real root and that every positive real number has a square\\nroot. It seems that there is no possible way to avoid this difficulty and formulate a purely\\nalgebraic argument. It is somewhat amazing that there are several elegant proofs of the\\nFundamental Theorem of Algebra that use complex analysis. It is also interesting to note\\nthat we can obtain a proof of such an important theorem from two very different fields of\\nmathematics.\\n\\n23.4 Exercises\\n1. Compute each of the following Galois groups. Which of these field extensions are normal\\nfield extensions? If the extension is not normal, find a normal extension of Q in which the\\nextension field is contained.\\n\\n(a) G(Q(\\n√\\n30 )/Q)\\n\\n(b) G(Q( 4\\n√\\n5 )/Q)\\n\\n(c) G(Q(\\n√\\n2,\\n√\\n3,\\n√\\n5 )/Q)\\n\\n(d) G(Q(\\n√\\n2, 3\\n\\n√\\n2, i)/Q)\\n\\n(e) G(Q(\\n√\\n6, i)/Q)\\n\\n2. Determine the separability of each of the following polynomials.\\n\\n\\n\\n412 CHAPTER 23. GALOIS THEORY\\n\\n(a) x3 + 2x2 − x− 2 over Q\\n(b) x4 + 2x2 + 1 over Q\\n\\n(c) x4 + x2 + 1 over Z3\\n\\n(d) x3 + x2 + 1 over Z2\\n\\n3. Give the order and describe a generator of the Galois group of GF(729) over GF(9).\\n\\n4. Determine the Galois groups of each of the following polynomials in Q[x]; hence, deter-\\nmine the solvability by radicals of each of the polynomials.\\n\\n(a) x5 − 12x2 + 2\\n\\n(b) x5 − 4x4 + 2x+ 2\\n\\n(c) x3 − 5\\n\\n(d) x4 − x2 − 6\\n\\n(e) x5 + 1\\n\\n(f) (x2 − 2)(x2 + 2)\\n\\n(g) x8 − 1\\n\\n(h) x8 + 1\\n\\n(i) x4 − 3x2 − 10\\n\\n5. Find a primitive element in the splitting field of each of the following polynomials in\\nQ[x].\\n\\n(a) x4 − 1\\n\\n(b) x4 − 8x2 + 15\\n\\n(c) x4 − 2x2 − 15\\n\\n(d) x3 − 2\\n\\n6. Prove that the Galois group of an irreducible quadratic polynomial is isomorphic to Z2.\\n\\n7. Prove that the Galois group of an irreducible cubic polynomial is isomorphic to S3 or\\nZ3.\\n\\n8. Let F ⊂ K ⊂ E be fields. If E is a normal extension of F , show that E must also be a\\nnormal extension of K.\\n\\n9. Let G be the Galois group of a polynomial of degree n. Prove that |G| divides n!.\\n\\n10. Let F ⊂ E. If f(x) is solvable over F , show that f(x) is also solvable over E.\\n\\n11. Construct a polynomial f(x) in Q[x] of degree 7 that is not solvable by radicals.\\n\\n12. Let p be prime. Prove that there exists a polynomial f(x) ∈ Q[x] of degree p with\\nGalois group isomorphic to Sp. Conclude that for each prime p with p ≥ 5 there exists a\\npolynomial of degree p that is not solvable by radicals.\\n\\n13. Let p be a prime and Zp(t) be the field of rational functions over Zp. Prove that\\nf(x) = xp − t is an irreducible polynomial in Zp(t)[x]. Show that f(x) is not separable.\\n\\n14. Let E be an extension field of F . Suppose that K and L are two intermediate fields.\\nIf there exists an element σ ∈ G(E/F ) such that σ(K) = L, then K and L are said to be\\nconjugate fields. Prove that K and L are conjugate if and only if G(E/K) and G(E/L)\\nare conjugate subgroups of G(E/F ).\\n\\n15. Let σ ∈ Aut(R). If a is a positive real number, show that σ(a) > 0.\\n\\n16. Let K be the splitting field of x3 + x2 + 1 ∈ Z2[x]. Prove or disprove that K is an\\nextension by radicals.\\n\\n17. Let F be a field such that charF ̸= 2. Prove that the splitting field of f(x) = ax2+bx+c\\nis F (\\n\\n√\\nα ), where α = b2 − 4ac.\\n\\n\\n\\n23.5. REFERENCES AND SUGGESTED READINGS 413\\n\\n18. Prove or disprove: Two different subgroups of a Galois group will have different fixed\\nfields.\\n\\n19. Let K be the splitting field of a polynomial over F . If E is a field extension of F\\ncontained in K and [E : F ] = 2, then E is the splitting field of some polynomial in F [x].\\n\\n20. We know that the cyclotomic polynomial\\n\\nΦp(x) =\\nxp − 1\\n\\nx− 1\\n= xp−1 + xp−2 + · · ·+ x+ 1\\n\\nis irreducible over Q for every prime p. Let ω be a zero of Φp(x), and consider the field\\nQ(ω).\\n(a) Show that ω, ω2, . . . , ωp−1 are distinct zeros of Φp(x), and conclude that they are all\\n\\nthe zeros of Φp(x).\\n(b) Show that G(Q(ω)/Q) is abelian of order p− 1.\\n(c) Show that the fixed field of G(Q(ω)/Q) is Q.\\n\\n21. Let F be a finite field or a field of characteristic zero. Let E be a finite normal\\nextension of F with Galois group G(E/F ). Prove that F ⊂ K ⊂ L ⊂ E if and only if\\n{id} ⊂ G(E/L) ⊂ G(E/K) ⊂ G(E/F ).\\n\\n22. Let F be a field of characteristic zero and let f(x) ∈ F [x] be a separable polynomial\\nof degree n. If E is the splitting field of f(x), let α1, . . . , αn be the roots of f(x) in E. Let\\n∆ =\\n\\n∏\\ni<j(αi − αj). We define the discriminant of f(x) to be ∆2.\\n\\n(a) If f(x) = x2 + bx+ c, show that ∆2 = b2 − 4c.\\n(b) If f(x) = x3 + px+ q, show that ∆2 = −4p3 − 27q2.\\n(c) Prove that ∆2 is in F .\\n(d) If σ ∈ G(E/F ) is a transposition of two roots of f(x), show that σ(∆) = −∆.\\n(e) If σ ∈ G(E/F ) is an even permutation of the roots of f(x), show that σ(∆) = ∆.\\n(f) Prove that G(E/F ) is isomorphic to a subgroup of An if and only if ∆ ∈ F .\\n(g) Determine the Galois groups of x3 + 2x− 4 and x3 + x− 3.\\n\\n23.5 References and Suggested Readings\\n[1] Artin, E. Theory: Lectures Delivered at the University of Notre Dame (Notre Dame\\n\\nMathematical Lectures, Number 2). Dover, Mineola, NY, 1997.\\n[2] Edwards, H. M. Galois Theory. Springer-Verlag, New York, 1984.\\n[3] Fraleigh, J. B. A First Course in Abstract Algebra. 7th ed. Pearson, Upper Saddle\\n\\nRiver, NJ, 2003.\\n[4] Gaal, L. Classical Galois Theory with Examples. American Mathematical Society,\\n\\nProvidence, 1979.\\n[5] Garling, D. J. H. A Course in Galois Theory. Cambridge University Press, Cambridge,\\n\\n1986.\\n[6] Kaplansky, I. Fields and Rings. 2nd ed. University of Chicago Press, Chicago, 1972.\\n[7] Rothman, T. “The Short Life of Évariste Galois,” Scientific American, April 1982,\\n\\n136–49.\\n\\n\\n\\n414 CHAPTER 23. GALOIS THEORY\\n\\n23.6 Sage\\nAgain, our competence at examining fields with Sage will allow us to study the main con-\\ncepts of Galois Theory easily. We will thoroughly examine Example 7 carefully using our\\ncomputational tools.\\n\\nGalois Groups\\nWe will repeat Example 23.24 and analyze carefully the splitting field of the polynomial\\np(x) = x4 − 2. We begin with an initial field extension containing at least one root.\\n\\nx = polygen(QQ, \' x \' )\\nN.<a> = NumberField(x^4 - 2); N\\n\\nNumber Field in a with defining polynomial x^4 - 2\\n\\nThe .galois_closure() method will create an extension containing all of the roots of the\\ndefining polynomial of a number field.\\n\\nL.<b> = N.galois_closure (); L\\n\\nNumber Field in b with defining polynomial x^8 + 28*x^4 + 2500\\n\\nL.degree ()\\n\\n8\\n\\ny = polygen(L, \' y \' )\\n(y^4 - 2).factor ()\\n\\n(y - 1/120*b^5 - 19/60*b) *\\n(y - 1/240*b^5 + 41/120*b) *\\n(y + 1/240*b^5 - 41/120*b) *\\n(y + 1/120*b^5 + 19/60*b)\\n\\nFrom the factorization, it is clear that L is the splitting field of the polynomial, even\\nif the factorization is not pretty. It is easy to then obtain the Galois group of this field\\nextension.\\n\\nG = L.galois_group (); G\\n\\nGalois group of Number Field in b with\\ndefining polynomial x^8 + 28*x^4 + 2500\\n\\nWe can examine this group, and identify it. Notice that since the field is a degree\\n8 extension, the group is described as a permutation group on 8 symbols. (It is just a\\ncoincidence that the group has 8 elements.) With a paucity of nonabelian groups of order\\n8, it is not hard to guess the nature of the group.\\n\\nG.is_abelian ()\\n\\nFalse\\n\\nG.order()\\n\\n8\\n\\n\\n\\n23.6. SAGE 415\\n\\nG.list()\\n\\n[(), (1,2,8,7)(3,4,6,5),\\n(1,3)(2,5)(4,7)(6,8), (1,4)(2,3)(5,8)(6,7),\\n(1,5)(2,6)(3,7)(4,8), (1,6)(2,4)(3,8)(5,7),\\n(1,7,8,2)(3,5,6,4), (1,8)(2,7)(3,6)(4,5)]\\n\\nG.is_isomorphic(DihedralGroup (4))\\n\\nTrue\\n\\nThat’s it. But maybe not very satisfying. Let us dig deeper for more understanding.\\nWe will start over and create the splitting field of p(x) = x4 − 2 again, but the primary\\ndifference is that we will make the roots extremely obvious so we can work more carefully\\nwith the Galois group and the fixed fields. Along the way, we will see another example of\\nlinear algebra enabling certain computations. The following construction should be familiar\\nby now.\\n\\nx = polygen(QQ, \' x \' )\\np = x^4 - 2\\nN.<a> = NumberField(p); N\\n\\nNumber Field in a with defining polynomial x^4 - 2\\n\\ny = polygen(N, \' y \' )\\np = p.subs(x=y)\\np.factor ()\\n\\n(y - a) * (y + a) * (y^2 + a^2)\\n\\nM.<b> = NumberField(y^2 + a^2); M\\n\\nNumber Field in b with defining polynomial y^2 + a^2 over\\nits base field\\n\\nz = polygen(M, \' z \' )\\n(z^4 - 2).factor ()\\n\\n(z - b) * (z - a) * (z + a) * (z + b)\\n\\nThe important thing to notice here is that we have arranged the splitting field so that\\nthe four roots, a, -a, b, -b, are very simple functions of the generators. In more traditional\\nnotation, a is 2\\n\\n1\\n4 = 4\\n\\n√\\n2, and b is 2\\n\\n1\\n4 i = 4\\n\\n√\\n2i (or their negatives).\\n\\nWe will find it easier to compute in the flattened tower, a now familiar construction.\\nL.<c> = M.absolute_field (); L\\n\\nNumber Field in c with defining polynomial x^8 + 28*x^4 + 2500\\n\\nfromL , toL = L.structure ()\\n\\nWe can return to our original polynomial (over the rationals), and ask for its roots in\\nthe flattened tower, custom-designed to contain these roots.\\n\\nroots = p.roots(ring=L, multiplicities=False); roots\\n\\n\\n\\n416 CHAPTER 23. GALOIS THEORY\\n\\n[1/120*c^5 + 19/60*c,\\n1/240*c^5 - 41/120*c,\\n\\n-1/240*c^5 + 41/120*c,\\n-1/120*c^5 - 19/60*c]\\n\\nHmmm. Do those look right? If you look back at the factorization obtained in the\\nfield constructed with the .galois_closure() method, then they look right. But we can do\\nbetter.\\n\\n[fromL(r) for r in roots]\\n\\n[b, a, -a, -b]\\n\\nYes, those are the roots.\\nThe End() command will create the group of automorphisms of the field L.\\nG = End(L); G\\n\\nAutomorphism group of Number Field in c with\\ndefining polynomial x^8 + 28*x^4 + 2500\\n\\nWe can check that each of these automorphisms fixes the rational numbers elementwise.\\nIf a field homomorphism fixes 1, then it will fix the integers, and thus fix all fractions of\\nintegers.\\n\\n[tau(1) for tau in G]\\n\\n[1, 1, 1, 1, 1, 1, 1, 1]\\n\\nSo each element of G fixes the rationals elementwise and thus G is the Galois group of\\nthe splitting field L over the rationals.\\n\\nProposition 23.5 is fundamental. It says every automorphism in the Galois group of a\\nfield extension creates a permutation of the roots of a polynomial with coefficients in the\\nbase field. We have all of those ingredients here. So we will evaluate each automorphism of\\nthe Galois group at each of the four roots of our polynomial, which in each case should be\\nanother root. (We use the Sequence() constructor just to get nicely-aligned output.)\\n\\nSequence ([[ fromL(tau(r)) for r in roots] for tau in G], cr=True)\\n\\n[\\n[b, a, -a, -b],\\n[-b, -a, a, b],\\n[a, -b, b, -a],\\n[b, -a, a, -b],\\n[-a, -b, b, a],\\n[a, b, -b, -a],\\n[-b, a, -a, b],\\n[-a, b, -b, a]\\n]\\n\\nEach row of the output is a list of the roots, but permuted, and so corresponds to a\\npermutation of four objects (the roots). For example, the second row shows the second\\nautomorphism interchanging a with -a, and b with -b. (Notice that the first row is the\\nresult of the identity automorphism, so we can mentally comine the first row with any other\\nrow to imagine a “two-row” form of a permutation.) We can number the roots, 1 through\\n4, and create each permutation as an element of S4. It is overkill, but we can then build\\nthe permutation group by letting all of these elements generate a group.\\n\\n\\n\\n23.6. SAGE 417\\n\\nS4 = SymmetricGroup (4)\\nelements = [S4([1, 2, 3, 4]),\\n\\nS4([4, 3, 2, 1]),\\nS4([2, 4, 1, 3]),\\nS4([1, 3, 2, 4]),\\nS4([3, 4, 1, 2]),\\nS4([2, 1, 4, 3]),\\nS4([4, 2, 3, 1]),\\nS4([3, 1, 4, 2])]\\n\\nelements\\n\\n[(), (1,4)(2,3), (1,2,4,3), (2,3), (1,3)(2,4),\\n(1,2)(3,4), (1,4), (1,3,4,2)]\\n\\nP = S4.subgroup(elements)\\nP.is_isomorphic(DihedralGroup (4))\\n\\nTrue\\n\\nNotice that we now have built an isomorphism from the Galois group to a group of\\npermutations using just four symbols, rather than the eight used previously.\\n\\nFixed Fields\\nIn a previous Sage exercise, we computed the fixed fields of single field automorphisms for\\nfinite fields. This was “easy” in the sense that we could just test every element of the field\\nto see if it was fixed, since the field was finite. Now we have an infinite field extension.\\nHow are we going to determine which elements are fixed by individual automorphisms, or\\nsubgroups of automorphisms?\\n\\nThe answer is to use the vector space structure of the flattened tower. As a degree 8\\nextension of the rationals, the first 8 powers of the primitive element c form a basis when\\nthe field is viewed as a vector space with the rationals as the scalars. It is sufficient to\\nknow how each field automorphism behaves on this basis to fully specify the definition of\\nthe automorphism. To wit,\\n\\nτ(x) = τ\\n\\n(\\n7∑\\n\\ni=0\\n\\nqic\\ni\\n\\n)\\nqi ∈ Q\\n\\n=\\n\\n7∑\\ni=0\\n\\nτ(qi)τ(c\\ni) τ is a field automorphism\\n\\n=\\n7∑\\n\\ni=0\\n\\nqiτ(c\\ni) rationals are fixed\\n\\nSo we can compute the value of a field automorphism at any linear combination of powers\\nof the primitive element as a linear combination of the values of the field automorphism at\\njust the powers of the primitive element. This is known as the “power basis”, which we can\\nobtain simply with the .power_basis() method. We will begin with an example of how we\\ncan use this basis. We will illustrate with the fourth automorphism of the Galois group.\\nNotice that the .vector() method is a convenience that strips a linear combination of the\\npowers of c into a vector of just the coefficients. (Notice too that τ is totally defined by\\nthe value of τ(c), since as a field automorphism τ(ck) = (τ(c))k. However, we still need to\\nwork with the entire power basis to exploit the vector space structure.)\\n\\n\\n\\n418 CHAPTER 23. GALOIS THEORY\\n\\nbasis = L.power_basis (); basis\\n\\n[1, c, c^2, c^3, c^4, c^5, c^6, c^7]\\n\\ntau = G[3]\\nz = 4 + 5*c+ 6*c^3-7*c^6\\ntz = tau(4 + 5*c+ 6*c^3-7*c^6); tz\\n\\n11/250*c^7 - 98/25*c^6 + 1/12*c^5 + 779/125*c^3 +\\n6006/25*c^2 - 11/6*c + 4\\n\\ntz.vector ()\\n\\n(4, -11/6, 6006/25 , 779/125 , 0, 1/12, -98/25, 11/250)\\n\\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\\ntau_matrix\\n\\n[ 1 0 0 0 -28 0 0 0]\\n[ 0 -11/30 0 0 0 779/15 0 0]\\n[ 0 0 -14/25 0 0 0 -858/25 0]\\n[ 0 0 0 779/750 0 0 0 -4031/375]\\n[ 0 0 0 0 -1 0 0 0]\\n[ 0 1/60 0 0 0 11/30 0 0]\\n[ 0 0 -1/50 0 0 0 14/25 0]\\n[ 0 0 0 11/1500 0 0 0 -779/750]\\n\\ntau_matrix*z.vector ()\\n\\n(4, -11/6, 6006/25 , 779/125 , 0, 1/12, -98/25, 11/250)\\n\\ntau_matrix *(z.vector ()) == (tau(z)).vector ()\\n\\nTrue\\n\\nThe last line expresses the fact that tau_matrix is a matrix representation of the field\\nautomorphism, viewed as a linear transformation of the vector space structure. As a rep-\\nresentation of an invertible field homomorphism, the matrix is invertible. As an order\\n2 permutation of the roots, the inverse of the matrix is itself. But these facts are just\\nverifications that we have the right thing, we are interested in other properties.\\n\\nTo construct fixed fields, we want to find elements fixed by automorphisms. Continuing\\nwith tau from above, we seek elements z (written as vectors) such that tau_matrix*z=z. These\\nare eigenvectors for the eigenvalue 1, or elements of the null space of (tau_matrix - I) (null\\nspaces are obtained with .right_kernel() in Sage).\\n\\nK = (tau_matrix -identity_matrix (8)).right_kernel (); K\\n\\nVector space of degree 8 and dimension 4 over Rational Field\\nBasis matrix:\\n[ 1 0 0 0 0 0 0 0]\\n[ 0 1 0 0 0 1/38 0 0]\\n[ 0 0 1 0 0 0 -1/22 0]\\n[ 0 0 0 1 0 0 0 1/278]\\n\\n\\n\\n23.6. SAGE 419\\n\\nEach row of the basis matrix is a vector representing an element of the field, specifically\\n1, c + (1/38)*c^5, c^2 - (1/22)*c^6, c^3 + (1/278)*c^7. Let’s take a closer look at these\\nfixed elements, in terms we recognize.\\n\\nfromL (1)\\n\\n1\\n\\nfromL(c + (1/38)*c^5)\\n\\n60/19*b\\n\\nfromL(c^2 - (1/22)*c^6)\\n\\n150/11*a^2\\n\\nfromL(c^3 + (1/278)*c^7)\\n\\n1500/139*a^2*b\\n\\nAny element fixed by tau will be a linear combination of these four elements. We can\\nignore any rational multiples present, the first element is just saying the rationals are fixed,\\nand the last element is just a product of the middle two. So fundamentally tau is fixing\\nrationals, b (which is 4\\n\\n√\\n2i) and a^2 (which is\\n\\n√\\n2). Furthermore, b^2 = -a^2 (the check\\n\\nfollows), so we can create any fixed element of tau by just adjoining b= 4\\n√\\n2i to the rationals.\\n\\nSo the elements fixed by tau are Q( 4\\n√\\n2i).\\n\\na^2 + b^2\\n\\n0\\n\\nGalois Correspondence\\nThe entire subfield structure of our splitting field is determined by the subgroup structure\\nof the Galois group (Theorem 23.22), which is isomorphic to a group we know well. What\\nare the subgroups of our Galois group, expressed as permutation groups? (For brevity, we\\njust list the generators of each subgroup.)\\n\\nsg = P.subgroups ();\\n[H.gens() for H in sg]\\n\\n[[()],\\n[(2,3)],\\n[(1,4)],\\n[(1,4)(2,3)],\\n[(1,2)(3,4)],\\n[(1,3)(2,4)],\\n[(2,3), (1,4)],\\n[(1,2)(3,4), (1,4)(2,3)],\\n[(1,3,4,2), (1,4)(2,3)],\\n[(2,3), (1,2)(3,4), (1,4)]]\\n\\n[H.order () for H in sg]\\n\\n[1, 2, 2, 2, 2, 2, 4, 4, 4, 8]\\n\\n\\n\\n420 CHAPTER 23. GALOIS THEORY\\n\\ntau above is the fourth element of the automorphism group, and the fourth permutation\\nin elements is the permutation (2,3), the generator (of order 2) for the second subgroup.\\nSo as the only nontrivial element of this subgroup, we know that the corresponding fixed\\nfield is Q( 4\\n\\n√\\n2i).\\n\\nLet us analyze another subgroup of order 2, without all the explanation, and starting\\nwith the subgroup. The sixth subgroup is generated by the fifth automorphism, so let us\\ndetermine the elements that are fixed.\\n\\ntau = G[4]\\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\\n(tau_matrix -identity_matrix (8)).right_kernel ()\\n\\nVector space of degree 8 and dimension 4 over Rational Field\\nBasis matrix:\\n[ 1 0 0 0 0 0 0 0]\\n[ 0 1 0 0 0 1/158 0 0]\\n[ 0 0 1 0 0 0 1/78 0]\\n[ 0 0 0 1 0 0 0 13/614]\\n\\nfromL(tau(1))\\n\\n1\\n\\nfromL(tau(c+(1/158)*c^5))\\n\\n120/79*b - 120/79*a\\n\\nfromL(tau(c^2+(1/78)*c^6))\\n\\n-200/39*a*b\\n\\nfromL(tau(c^3+(13/614)*c^7))\\n\\n3000/307*a^2*b + 3000/307*a^3\\n\\nThe first element indicates that the rationals are fixed (we knew that). Scaling the\\nsecond element gives b - a as a fixed element. Scaling the third and fourth fixed elements,\\nwe recognize that they can be obtained from powers of b - a.\\n\\n(b-a)^2\\n\\n-2*a*b\\n\\n(b-a)^3\\n\\n2*a^2*b + 2*a^3\\n\\nSo the fixed field of this subgroup can be formed by adjoining b - a to the rationals,\\nwhich in mathematical notation is 4\\n\\n√\\n2i− 4\\n\\n√\\n2 = (1− i) 4\\n\\n√\\n2, so the fixed field is Q( 4\\n\\n√\\n2i− 4\\n\\n√\\n2 =\\n\\n(1− i) 4\\n√\\n2).\\n\\nWe can create this fixed field, though as created here it is not strictly a subfield of L.\\nWe will use an expression for b - a that is a linear combination of powers of c.\\n\\nsubinfo = L.subfield ((79/120) *(c+(1/158)*c^5)); subinfo\\n\\n\\n\\n23.6. SAGE 421\\n\\n(Number Field in c0 with defining polynomial x^4 + 8, Ring morphism:\\nFrom: Number Field in c0 with defining polynomial x^4 + 8\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: c0 |--> 1/240*c^5 + 79/120*c)\\n\\nThe .subfield() method returns a pair. The first item is a new number field, isomorphic\\nto a subfield of L. The second item is an injective mapping from the new number field into\\nL. In this case, the image of the primitive element c0 is the element we have specified as the\\ngenerator of the subfield. The primitive element of the new field will satisfy the defining\\npolynomial x4+8 — you can check that (1− i) 4\\n\\n√\\n2 is indeed a root of the polynomial x4+8.\\n\\nThere are five subgroups of order 2, we have found fixed fields for two of them. The other\\nthree are similar, so it would be a good exercise to work through them. Our automorphism\\ngroup has three subgroups of order 4, and at least one of each possible type (cyclic versus\\nnon-cyclic). Fixed fields of larger subgroups require that we find elements fixed by all of the\\nautomorphisms in the subgroup. (We were conveniently ignoring the identity automorphism\\nabove.) This will require more computation, but will restrict the possibilities (smaller fields)\\nto where it will be easier to deduce a primitive element for each field.\\n\\nThe seventh subgroup is generated by two elements of order 2 and is composed entirely\\nof elements of order 2 (except the identity), so is isomorphic to Z2 ×Z2. The permutations\\ncorrespond to automorphisms number 0, 1, 3, and 6. To determine the elements fixed by\\nall four automorphisms, we will build the kernel for each one and as we go, we form the\\nintersection of all four kernels. We will work via a loop over the four automorphisms.\\n\\nV = QQ^8\\nfor tau in [G[0], G[1], G[3], G[6]]:\\n\\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\\nK = (tau_matrix -identity_matrix (8)).right_kernel ()\\nV = V.intersection(K)\\n\\nV\\n\\nVector space of degree 8 and dimension 2 over Rational Field\\nBasis matrix:\\n[ 1 0 0 0 0 0 0 0]\\n[ 0 0 1 0 0 0 -1/22 0]\\n\\nOutside of the rationals, there is a single fixed element.\\nfromL(tau(c^2 - (1/22)*c^6))\\n\\n150/11*a^2\\n\\nRemoving a scalar multiple, our primitive element is a^2, which mathematically is\\n√\\n2,\\n\\nso the fixed field is Q(\\n√\\n2). Again, we can build this fixed field, but ignore the mapping.\\n\\nF, mapping = L.subfield ((11/150) *(c^2 - (1/22)*c^6))\\nF\\n\\nNumber Field in c0 with defining polynomial x^2 - 2\\n\\nOne more subgroup. The penultimate subgroup has a permutation of order 4 as a\\ngenerator, so is a cyclic group of order 4. The individual permutations of the subgroup\\ncorrespond to automorphisms 0, 1, 2, 7.\\n\\nV = QQ^8\\nfor tau in [G[0], G[1], G[2], G[7]]:\\n\\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\\n\\n\\n\\n422 CHAPTER 23. GALOIS THEORY\\n\\nK = (tau_matrix -identity_matrix (8)).right_kernel ()\\nV = V.intersection(K)\\n\\nV\\n\\nVector space of degree 8 and dimension 2 over Rational Field\\nBasis matrix:\\n[1 0 0 0 0 0 0 0]\\n[0 0 0 0 1 0 0 0]\\n\\nSo we compute the primitive element.\\nfromL(tau(c^4))\\n\\n-24*a^3*b - 14\\n\\nSince rationals are fixed, we can remove the −14 and the multiple and take a^3*b as\\nthe primitive element. Mathematically, this is 2i, so we might as well use just i as the\\nprimitive element and the fixed field is Q(i). We can then build the fixed field (and ignore\\nthe mapping also returned).\\n\\nF, mapping = L.subfield ((c^4+14) /-48)\\nF\\n\\nNumber Field in c0 with defining polynomial x^2 + 1\\n\\nThere is one more subgroup of order 4, which we will leave as an exercise to analyze.\\nThere are also two trivial subgroups (the identity and the full group) which are not very\\ninteresting or surprising.\\n\\nIf the above seems like too much work, you can always just have Sage do it all with the\\n.subfields() method.\\n\\nL.subfields ()\\n\\n[\\n(Number Field in c0 with defining polynomial x,\\nRing morphism:\\n\\nFrom: Number Field in c0 with defining polynomial x\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: 0 |--> 0,\\n\\nNone),\\n(Number Field in c1 with defining polynomial x^2 + 112*x + 40000,\\nRing morphism:\\n\\nFrom: Number Field in c1 with defining polynomial x^2 + 112*x +\\n40000\\n\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n2500\\n\\nDefn: c1 |--> 4*c^4,\\nNone),\\n\\n(Number Field in c2 with defining polynomial x^2 + 512,\\nRing morphism:\\n\\nFrom: Number Field in c2 with defining polynomial x^2 + 512\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: c2 |--> 1/25*c^6 + 78/25*c^2,\\n\\nNone),\\n(Number Field in c3 with defining polynomial x^2 - 288,\\nRing morphism:\\n\\n\\n\\n23.6. SAGE 423\\n\\nFrom: Number Field in c3 with defining polynomial x^2 - 288\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: c3 |--> -1/25*c^6 + 22/25*c^2,\\n\\nNone),\\n(Number Field in c4 with defining polynomial x^4 + 112*x^2 + 40000,\\nRing morphism:\\n\\nFrom: Number Field in c4 with defining polynomial x^4 + 112*x^2 +\\n40000\\n\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n2500\\n\\nDefn: c4 |--> 2*c^2,\\nNone),\\n\\n(Number Field in c5 with defining polynomial x^4 + 648,\\nRing morphism:\\n\\nFrom: Number Field in c5 with defining polynomial x^4 + 648\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: c5 |--> 1/80*c^5 + 79/40*c,\\n\\nNone),\\n(Number Field in c6 with defining polynomial x^4 + 8,\\nRing morphism:\\n\\nFrom: Number Field in c6 with defining polynomial x^4 + 8\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: c6 |--> -1/80*c^5 + 1/40*c,\\nNone),\\n\\n(Number Field in c7 with defining polynomial x^4 - 512,\\nRing morphism:\\n\\nFrom: Number Field in c7 with defining polynomial x^4 - 512\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: c7 |--> -1/60*c^5 + 41/30*c,\\n\\nNone),\\n(Number Field in c8 with defining polynomial x^4 - 32,\\nRing morphism:\\n\\nFrom: Number Field in c8 with defining polynomial x^4 - 32\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n\\n2500\\nDefn: c8 |--> 1/60*c^5 + 19/30*c,\\n\\nNone),\\n(Number Field in c9 with defining polynomial x^8 + 28*x^4 + 2500,\\nRing morphism:\\n\\nFrom: Number Field in c9 with defining polynomial x^8 + 28*x^4 +\\n2500\\n\\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n2500\\n\\nDefn: c9 |--> c,\\nRing morphism:\\n\\nFrom: Number Field in c with defining polynomial x^8 + 28*x^4 +\\n2500\\n\\nTo: Number Field in c9 with defining polynomial x^8 + 28*x^4 +\\n2500\\n\\nDefn: c |--> c9)\\n]\\n\\nTen subfields are described, which is what we would expect, given the 10 subgroups\\n\\n\\n\\n424 CHAPTER 23. GALOIS THEORY\\n\\nof the Galois group. Each begins with a new number field that is a subfield. Technically,\\neach is not a subset of L, but the second item returned for each subfield is an injective\\nhomomorphism, also known generally as an “embedding.” Each embedding describes how\\na primitive element of the subfield translates to an element of L. Some of these primitive\\nelements could be manipulated (as we have done above) to yield slightly simpler minimal\\npolynomials, but the results are quite impressive nonetheless. Each item in the list has a\\nthird component, which is almost always None, except when the subfield is the whole field,\\nand then the third component is an injective homomorphism “in the other direction.”\\n\\nNormal Extensions\\nConsider the third subgroup in the list above, generated by the permutation (1,4). As\\na subgroup of order 2, it only has one nontrivial element, which here corresponds to the\\nseventh automorphism. We determine the fixed elements as before.\\n\\ntau = G[6]\\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\\n(tau_matrix -identity_matrix (8)).right_kernel ()\\n\\nVector space of degree 8 and dimension 4 over Rational Field\\nBasis matrix:\\n[ 1 0 0 0 0 0 0 0]\\n[ 0 1 0 0 0 -1/82 0 0]\\n[ 0 0 1 0 0 0 -1/22 0]\\n[ 0 0 0 1 0 0 0 11/58]\\n\\nfromL(tau(1))\\n\\n1\\n\\nfromL(tau(c+( -1/82)*c^5))\\n\\n-120/41*a\\n\\nfromL(tau(c^2+( -1/22)*c^6))\\n\\n150/11*a^2\\n\\nfromL(tau(c^3+(11/58)*c^7))\\n\\n3000/29*a^3\\n\\nAs usual, ignoring rational multiples, we see powers of a and recognize that a alone will\\nbe a primitive element for the fixed field, which is thus Q( 4\\n\\n√\\n2). Recognize that a was our\\n\\nfirst root of x4 − 2, and was used to create the first part of original tower, N. So N is both\\nQ( 4\\n\\n√\\n2) and the fixed field of H = ⟨(1, 4)⟩.\\n\\nQ( 4\\n√\\n2) contains at least one root of the irreducible x4 − 2, but not all of the roots\\n\\n(witness the factorization above) and therefore does not qualify as a normal extension. By\\npart (4) of Theorem 23.22 the automorphism group of the extension is not normal in the\\nfull Galois group.\\n\\nsg[2]. is_normal(P)\\n\\nFalse\\n\\nAs expected.\\n\\n\\n\\n23.7. SAGE EXERCISES 425\\n\\n23.7 Sage Exercises\\n1. In the analysis of Example 23.24 with Sage, two subgroups of order 2 and one subgroup\\nof order 4 were not analyzed. Determine the fixed fields of these three subgroups.\\n\\n2. Build the splitting field of p(x) = x3 − 6x2 + 12x − 10 and then determine the Galois\\ngroup of p(x) as a concrete group of explicit permutations. Build the lattice of subgroups of\\nthe Galois group, again using the same explicit permutations. Using the Fundamental The-\\norem of Galois Theory, construct the subfields of the splitting field. Include your supporting\\ndocumentation in your submitted Sage worksheet. Also, submit a written component of\\nthis assignment containing a complete layout of the subgroups and subfields, written en-\\ntirely with mathematical notation and with no Sage commands, designed to illustrate the\\ncorrespondence between the two. All you need here is the graphical layout, suitably labeled\\n— the Sage worksheet will substantiate your work.\\n\\n3. The polynomial x5−x−1 has all of the symmetric group S5 as its Galois group. Because\\nS5 is not solvable, we know this polynomial to be an example of a quintic polynomial that\\nis not solvable by radicals. Unfortunately, asking Sage to compute this Galois group takes\\nfar too long. So this exercise will simulate that experience with a slightly smaller example.\\nConsider the polynomial p(x) = x4 + x+ 1.\\n(a) Build the splitting field of p(x) one root at a time. Create an extension, factor there,\\n\\ndiscard linear factors, use the remaining irreducible factor to extend once more. Repeat\\nuntil p(x) factors completely. Be sure to do a final extension via just a linear factor.\\nThis is a little silly, and Sage will seem to ignore your final generator (so you will want\\nto setermine what it is equivalent to in terms of the previous gfenerators). Directions\\nbelow depend on taking this extra step.\\n\\n(b) Factor the original polynomial over the final extension field in the tower. What is\\nboring about this factorization in comparison to some other examples we have done?\\n\\n(c) Construct the full tower as an absolute field over Q. From the degree of this extension\\nand the degree of the original polynomial, infer the Galois group of the polynomial.\\n\\n(d) Using the mappings that allow you to translate between the tower and the absolute\\nfield (obtained from the .structure() method), choose one of the roots (any one) and\\nexpress it in terms of the single generator of the absolute field. Then reverse the\\nprocedure and express the single generator of the absolute field in terms of the roots\\nin the tower.\\n\\n(e) Compute the group of automorphisms of the absolute field (but don’t display the whole\\ngroup in what you submit). Take all four roots (including your silly one from the last\\nstep of the tower construction) and apply each field automorphism to the four roots\\n(creating the guaranteed permutations of the roots). Comment on what you see.\\n\\n(f) There is one nontrivial automorphism that has an especially simple form (it is the\\nsecond one for me) when applied to the generator of the absolute field. What does\\nthis automorphism do to the roots of p(x)?\\n\\n(g) Consider the extension of Q formed by adjoining just one of the roots. This is a subfield\\nof the splitting field of the polynomial, so is the fixed field of a subgroup of the Galois\\ngroup. Give a simple description of the corresponding subgroup using language we\\ntypically only apply to permutation groups.\\n\\n4. Return to the splitting field of the quintic discussed in the introduction to the previous\\nproblem (x5 − x − 1). Create the first two intermediate fields by adjoining two roots (one\\nat a time). But instead of factoring at each step to get a new irreducible polynomial, divide\\n\\n\\n\\n426 CHAPTER 23. GALOIS THEORY\\n\\nby the linear factor you know is a factor. In general, the quotient might factor further, but\\nin this exercise presume it does not. In other words, act as if your quotient by the linear\\nfactor is irreducible. If it is not, then the NumberField() command should complain (which\\nit will not).\\nAfter adjoining two roots, create the extension producing a third root, and do the division.\\nYou should now have a quadratic factor. Assuming the quadratic is irreducible (it is) argue\\nthat you have enough evidence to establish the order of the Galois group, and hence can\\ndetermine exactly which group it is.\\nYou can try to use this quadratic factor to create one more step in the extensions, and you\\nwill arrive at the splitting field, as can be seen with logic or division. However, this could take\\na long time to complete (save your work beforehand!). You can try passing the check=False\\n\\nargument to the NumberField() command — this will bypass checking irreducibility.\\n\\n5. Create the finite field of order 36, letting Sage supply the default polynomial for its\\nconstruction. The polynomial x6 + x2 + 2 ∗ x+ 1 is irreducible over this finite field. Check\\nthat this polynomial splits in the finite field, and then use the .roots() method to collect\\nthe roots of the polynomial. Get the group of automorphisms of the field with the End()\\n\\ncommand.\\nYou now have all of the pieces to associate each field automorphism with a permutation of\\nthe roots. From this, identify the Galois group and all of its subgroups. For each subgroup,\\ndetermine the fixed field. You might find the roots easier to work with if you use the .log()\\n\\nmethod to identify them as powers of the field’s multiplicative generator.\\nYour Galois group in this example will be abelian. So every subgroup is normal, and\\nhence any extension is also normal. Can you extend this example by choosing a nontrivial\\nintermediate field with a nontrivial irreducible polynomial that has all of its roots in the\\nintermediate field and a nontrivial irreducible polynomial with none of its roots in the\\nintermediate field?\\nYour results here are “typical” in the sense that the particular field or irreducible polynomial\\nmakes little difference in the qualitative nature of the results.\\n\\n6. The splitting field for the irreducible polynomial p(x) = x7 − 7x + 3 has degree 168\\n(hence this is the order of the Galois group). This polynomial is derived from an “Elkies\\ntrinomial curve,” a hyperelliptic curve (below) that produces polynomials with interesting\\nGalois groups:\\n\\ny2 = x(81x5 + 396x4 + 738x3 + 660x2 + 269x+ 48)\\n\\nFor p(x) the resulting Galois group is PSL(2, 7), a simple group. If SL(2, 7) is all 2 ×\\n2 matrices over Z7 with determinant 1, then PSL(2, 7) is the quotient by the subgroup\\n{I2,−I2}. It is the second-smallest non-abelian simple group (after A5).\\nSee how far you can get in using Sage to build this splitting field. A degree 7 extension\\nwill yield one linear factor, and a subsequent degree 6 extension will yield two linear fac-\\ntors, leaving a quartic factor. Here is where the computations begin to slow down. If we\\nbelieve that the splitting field has degree 168, then we know that adding a root from this\\ndegree 4 factor will get us to the splitting field. Creating this extension may be possible\\ncomputationally, but verifying that the quartic splits into linear factors here seems to be\\ninfeasible.\\n\\n7. Return to Example 23.24, and the complete list of subfields obtainable from the .subfields()\\n\\nmethod applied to the flattened tower. As mentioned, these are technically not subfields,\\nbut do have embeddings into the tower. Given two subfields, their respective primitive ele-\\nments are embedded into the tower, with an image that is a linear combination of powers\\nof the primitive element for the tower.\\n\\n\\n\\n23.7. SAGE EXERCISES 427\\n\\nIf one subfield is contained in the other, then the image of the primitive element for the\\nsmaller field should be a linear combination of the (appropriate) powers of the image of the\\nprimitive element for the larger field. This is a linear algebra computation that should be\\npossible in the tower, relative to the power basis for the whole tower.\\nWrite a procedure to determine if two subfields are related by one being a subset of the\\nother. Then use this procedure to create the lattice of subfields. The eventual goal would be\\na graphical display of the lattice, using the existing plotting facilities available for lattices,\\nsimilar to the top half of Figure 23.25. This is a “challenging” exercise, which is code for\\n“it is speculative and has not been tested.”\\n\\n\\n\\nA\\n\\nGNU Free Documentation License\\n\\nVersion 1.3, 3 November 2008\\nCopyright © 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. <http://www.\\n\\nfsf.org/>\\nEveryone is permitted to copy and distribute verbatim copies of this license document,\\n\\nbut changing it is not allowed.\\n\\n0. PREAMBLE The purpose of this License is to make a manual, textbook, or other\\nfunctional and useful document “free” in the sense of freedom: to assure everyone the effec-\\ntive freedom to copy and redistribute it, with or without modifying it, either commercially\\nor noncommercially. Secondarily, this License preserves for the author and publisher a way\\nto get credit for their work, while not being considered responsible for modifications made\\nby others.\\n\\nThis License is a kind of “copyleft”, which means that derivative works of the document\\nmust themselves be free in the same sense. It complements the GNU General Public License,\\nwhich is a copyleft license designed for free software.\\n\\nWe have designed this License in order to use it for manuals for free software, because free\\nsoftware needs free documentation: a free program should come with manuals providing the\\nsame freedoms that the software does. But this License is not limited to software manuals;\\nit can be used for any textual work, regardless of subject matter or whether it is published\\nas a printed book. We recommend this License principally for works whose purpose is\\ninstruction or reference.\\n\\n1. APPLICABILITY AND DEFINITIONS This License applies to any manual or\\nother work, in any medium, that contains a notice placed by the copyright holder saying\\nit can be distributed under the terms of this License. Such a notice grants a world-wide,\\nroyalty-free license, unlimited in duration, to use that work under the conditions stated\\nherein. The “Document”, below, refers to any such manual or work. Any member of the\\npublic is a licensee, and is addressed as “you”. You accept the license if you copy, modify\\nor distribute the work in a way requiring permission under copyright law.\\n\\nA “Modified Version” of the Document means any work containing the Document or a\\nportion of it, either copied verbatim, or with modifications and/or translated into another\\nlanguage.\\n\\nA “Secondary Section” is a named appendix or a front-matter section of the Document\\nthat deals exclusively with the relationship of the publishers or authors of the Document\\nto the Document’s overall subject (or to related matters) and contains nothing that could\\nfall directly within that overall subject. (Thus, if the Document is in part a textbook of\\nmathematics, a Secondary Section may not explain any mathematics.) The relationship\\n\\n428\\n\\nhttp://www.fsf.org/\\nhttp://www.fsf.org/\\n\\n\\n429\\n\\ncould be a matter of historical connection with the subject or with related matters, or of\\nlegal, commercial, philosophical, ethical or political position regarding them.\\n\\nThe “Invariant Sections” are certain Secondary Sections whose titles are designated, as\\nbeing those of Invariant Sections, in the notice that says that the Document is released\\nunder this License. If a section does not fit the above definition of Secondary then it is not\\nallowed to be designated as Invariant. The Document may contain zero Invariant Sections.\\nIf the Document does not identify any Invariant Sections then there are none.\\n\\nThe “Cover Texts” are certain short passages of text that are listed, as Front-Cover\\nTexts or Back-Cover Texts, in the notice that says that the Document is released under this\\nLicense. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at\\nmost 25 words.\\n\\nA “Transparent” copy of the Document means a machine-readable copy, represented in\\na format whose specification is available to the general public, that is suitable for revising\\nthe document straightforwardly with generic text editors or (for images composed of pixels)\\ngeneric paint programs or (for drawings) some widely available drawing editor, and that is\\nsuitable for input to text formatters or for automatic translation to a variety of formats\\nsuitable for input to text formatters. A copy made in an otherwise Transparent file format\\nwhose markup, or absence of markup, has been arranged to thwart or discourage subsequent\\nmodification by readers is not Transparent. An image format is not Transparent if used for\\nany substantial amount of text. A copy that is not “Transparent” is called “Opaque”.\\n\\nExamples of suitable formats for Transparent copies include plain ASCII without markup,\\nTexinfo input format, LaTeX input format, SGML or XML using a publicly available DTD,\\nand standard-conforming simple HTML, PostScript or PDF designed for human modifica-\\ntion. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats\\ninclude proprietary formats that can be read and edited only by proprietary word proces-\\nsors, SGML or XML for which the DTD and/or processing tools are not generally available,\\nand the machine-generated HTML, PostScript or PDF produced by some word processors\\nfor output purposes only.\\n\\nThe “Title Page” means, for a printed book, the title page itself, plus such following\\npages as are needed to hold, legibly, the material this License requires to appear in the title\\npage. For works in formats which do not have any title page as such, “Title Page” means\\nthe text near the most prominent appearance of the work’s title, preceding the beginning\\nof the body of the text.\\n\\nThe “publisher” means any person or entity that distributes copies of the Document to\\nthe public.\\n\\nA section “Entitled XYZ” means a named subunit of the Document whose title either\\nis precisely XYZ or contains XYZ in parentheses following text that translates XYZ in\\nanother language. (Here XYZ stands for a specific section name mentioned below, such\\nas “Acknowledgements”, “Dedications”, “Endorsements”, or “History”.) To “Preserve the\\nTitle” of such a section when you modify the Document means that it remains a section\\n“Entitled XYZ” according to this definition.\\n\\nThe Document may include Warranty Disclaimers next to the notice which states that\\nthis License applies to the Document. These Warranty Disclaimers are considered to be\\nincluded by reference in this License, but only as regards disclaiming warranties: any other\\nimplication that these Warranty Disclaimers may have is void and has no effect on the\\nmeaning of this License.\\n\\n2. VERBATIM COPYING You may copy and distribute the Document in any\\nmedium, either commercially or noncommercially, provided that this License, the copyright\\nnotices, and the license notice saying this License applies to the Document are reproduced\\n\\n\\n\\n430 APPENDIX A. GNU FREE DOCUMENTATION LICENSE\\n\\nin all copies, and that you add no other conditions whatsoever to those of this License. You\\nmay not use technical measures to obstruct or control the reading or further copying of\\nthe copies you make or distribute. However, you may accept compensation in exchange for\\ncopies. If you distribute a large enough number of copies you must also follow the conditions\\nin section 3.\\n\\nYou may also lend copies, under the same conditions stated above, and you may publicly\\ndisplay copies.\\n\\n3. COPYING IN QUANTITY If you publish printed copies (or copies in media\\nthat commonly have printed covers) of the Document, numbering more than 100, and the\\nDocument’s license notice requires Cover Texts, you must enclose the copies in covers that\\ncarry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and\\nBack-Cover Texts on the back cover. Both covers must also clearly and legibly identify you\\nas the publisher of these copies. The front cover must present the full title with all words\\nof the title equally prominent and visible. You may add other material on the covers in\\naddition. Copying with changes limited to the covers, as long as they preserve the title of\\nthe Document and satisfy these conditions, can be treated as verbatim copying in other\\nrespects.\\n\\nIf the required texts for either cover are too voluminous to fit legibly, you should put\\nthe first ones listed (as many as fit reasonably) on the actual cover, and continue the rest\\nonto adjacent pages.\\n\\nIf you publish or distribute Opaque copies of the Document numbering more than 100,\\nyou must either include a machine-readable Transparent copy along with each Opaque copy,\\nor state in or with each Opaque copy a computer-network location from which the general\\nnetwork-using public has access to download using public-standard network protocols a\\ncomplete Transparent copy of the Document, free of added material. If you use the latter\\noption, you must take reasonably prudent steps, when you begin distribution of Opaque\\ncopies in quantity, to ensure that this Transparent copy will remain thus accessible at the\\nstated location until at least one year after the last time you distribute an Opaque copy\\n(directly or through your agents or retailers) of that edition to the public.\\n\\nIt is requested, but not required, that you contact the authors of the Document well\\nbefore redistributing any large number of copies, to give them a chance to provide you with\\nan updated version of the Document.\\n\\n4. MODIFICATIONS You may copy and distribute a Modified Version of the Docu-\\nment under the conditions of sections 2 and 3 above, provided that you release the Modified\\nVersion under precisely this License, with the Modified Version filling the role of the Doc-\\nument, thus licensing distribution and modification of the Modified Version to whoever\\npossesses a copy of it. In addition, you must do these things in the Modified Version:\\n\\nA. Use in the Title Page (and on the covers, if any) a title distinct from that of the\\nDocument, and from those of previous versions (which should, if there were any, be\\nlisted in the History section of the Document). You may use the same title as a\\nprevious version if the original publisher of that version gives permission.\\n\\nB. List on the Title Page, as authors, one or more persons or entities responsible for\\nauthorship of the modifications in the Modified Version, together with at least five\\nof the principal authors of the Document (all of its principal authors, if it has fewer\\nthan five), unless they release you from this requirement.\\n\\nC. State on the Title page the name of the publisher of the Modified Version, as the\\npublisher.\\n\\n\\n\\n431\\n\\nD. Preserve all the copyright notices of the Document.\\n\\nE. Add an appropriate copyright notice for your modifications adjacent to the other\\ncopyright notices.\\n\\nF. Include, immediately after the copyright notices, a license notice giving the public\\npermission to use the Modified Version under the terms of this License, in the form\\nshown in the Addendum below.\\n\\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover\\nTexts given in the Document’s license notice.\\n\\nH. Include an unaltered copy of this License.\\n\\nI. Preserve the section Entitled “History”, Preserve its Title, and add to it an item\\nstating at least the title, year, new authors, and publisher of the Modified Version as\\ngiven on the Title Page. If there is no section Entitled “History” in the Document,\\ncreate one stating the title, year, authors, and publisher of the Document as given\\non its Title Page, then add an item describing the Modified Version as stated in the\\nprevious sentence.\\n\\nJ. Preserve the network location, if any, given in the Document for public access to a\\nTransparent copy of the Document, and likewise the network locations given in the\\nDocument for previous versions it was based on. These may be placed in the “History”\\nsection. You may omit a network location for a work that was published at least four\\nyears before the Document itself, or if the original publisher of the version it refers to\\ngives permission.\\n\\nK. For any section Entitled “Acknowledgements” or “Dedications”, Preserve the Title\\nof the section, and preserve in the section all the substance and tone of each of the\\ncontributor acknowledgements and/or dedications given therein.\\n\\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in\\ntheir titles. Section numbers or the equivalent are not considered part of the section\\ntitles.\\n\\nM. Delete any section Entitled “Endorsements”. Such a section may not be included in\\nthe Modified Version.\\n\\nN. Do not retitle any existing section to be Entitled “Endorsements” or to conflict in title\\nwith any Invariant Section.\\n\\nO. Preserve any Warranty Disclaimers.\\n\\nIf the Modified Version includes new front-matter sections or appendices that qualify as\\nSecondary Sections and contain no material copied from the Document, you may at your\\noption designate some or all of these sections as invariant. To do this, add their titles to\\nthe list of Invariant Sections in the Modified Version’s license notice. These titles must be\\ndistinct from any other section titles.\\n\\nYou may add a section Entitled “Endorsements”, provided it contains nothing but en-\\ndorsements of your Modified Version by various parties — for example, statements of peer\\nreview or that the text has been approved by an organization as the authoritative definition\\nof a standard.\\n\\nYou may add a passage of up to five words as a Front-Cover Text, and a passage of up\\nto 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified\\n\\n\\n\\n432 APPENDIX A. GNU FREE DOCUMENTATION LICENSE\\n\\nVersion. Only one passage of Front-Cover Text and one of Back-Cover Text may be added\\nby (or through arrangements made by) any one entity. If the Document already includes\\na cover text for the same cover, previously added by you or by arrangement made by the\\nsame entity you are acting on behalf of, you may not add another; but you may replace the\\nold one, on explicit permission from the previous publisher that added the old one.\\n\\nThe author(s) and publisher(s) of the Document do not by this License give permission\\nto use their names for publicity for or to assert or imply endorsement of any Modified\\nVersion.\\n\\n5. COMBINING DOCUMENTS You may combine the Document with other docu-\\nments released under this License, under the terms defined in section 4 above for modified\\nversions, provided that you include in the combination all of the Invariant Sections of all of\\nthe original documents, unmodified, and list them all as Invariant Sections of your combined\\nwork in its license notice, and that you preserve all their Warranty Disclaimers.\\n\\nThe combined work need only contain one copy of this License, and multiple identical\\nInvariant Sections may be replaced with a single copy. If there are multiple Invariant\\nSections with the same name but different contents, make the title of each such section\\nunique by adding at the end of it, in parentheses, the name of the original author or\\npublisher of that section if known, or else a unique number. Make the same adjustment to\\nthe section titles in the list of Invariant Sections in the license notice of the combined work.\\n\\nIn the combination, you must combine any sections Entitled “History” in the various\\noriginal documents, forming one section Entitled “History”; likewise combine any sections\\nEntitled “Acknowledgements”, and any sections Entitled “Dedications”. You must delete\\nall sections Entitled “Endorsements”.\\n\\n6. COLLECTIONS OF DOCUMENTS You may make a collection consisting of\\nthe Document and other documents released under this License, and replace the individual\\ncopies of this License in the various documents with a single copy that is included in the\\ncollection, provided that you follow the rules of this License for verbatim copying of each\\nof the documents in all other respects.\\n\\nYou may extract a single document from such a collection, and distribute it individually\\nunder this License, provided you insert a copy of this License into the extracted document,\\nand follow this License in all other respects regarding verbatim copying of that document.\\n\\n7. AGGREGATION WITH INDEPENDENT WORKS A compilation of the\\nDocument or its derivatives with other separate and independent documents or works, in or\\non a volume of a storage or distribution medium, is called an “aggregate” if the copyright\\nresulting from the compilation is not used to limit the legal rights of the compilation’s users\\nbeyond what the individual works permit. When the Document is included in an aggregate,\\nthis License does not apply to the other works in the aggregate which are not themselves\\nderivative works of the Document.\\n\\nIf the Cover Text requirement of section 3 is applicable to these copies of the Document,\\nthen if the Document is less than one half of the entire aggregate, the Document’s Cover\\nTexts may be placed on covers that bracket the Document within the aggregate, or the\\nelectronic equivalent of covers if the Document is in electronic form. Otherwise they must\\nappear on printed covers that bracket the whole aggregate.\\n\\n8. TRANSLATION Translation is considered a kind of modification, so you may dis-\\ntribute translations of the Document under the terms of section 4. Replacing Invariant\\nSections with translations requires special permission from their copyright holders, but you\\n\\n\\n\\n433\\n\\nmay include translations of some or all Invariant Sections in addition to the original ver-\\nsions of these Invariant Sections. You may include a translation of this License, and all\\nthe license notices in the Document, and any Warranty Disclaimers, provided that you also\\ninclude the original English version of this License and the original versions of those notices\\nand disclaimers. In case of a disagreement between the translation and the original version\\nof this License or a notice or disclaimer, the original version will prevail.\\n\\nIf a section in the Document is Entitled “Acknowledgements”, “Dedications”, or “His-\\ntory”, the requirement (section 4) to Preserve its Title (section 1) will typically require\\nchanging the actual title.\\n\\n9. TERMINATION You may not copy, modify, sublicense, or distribute the Document\\nexcept as expressly provided under this License. Any attempt otherwise to copy, modify,\\nsublicense, or distribute it is void, and will automatically terminate your rights under this\\nLicense.\\n\\nHowever, if you cease all violation of this License, then your license from a particular\\ncopyright holder is reinstated (a) provisionally, unless and until the copyright holder explic-\\nitly and finally terminates your license, and (b) permanently, if the copyright holder fails to\\nnotify you of the violation by some reasonable means prior to 60 days after the cessation.\\n\\nMoreover, your license from a particular copyright holder is reinstated permanently if\\nthe copyright holder notifies you of the violation by some reasonable means, this is the first\\ntime you have received notice of violation of this License (for any work) from that copyright\\nholder, and you cure the violation prior to 30 days after your receipt of the notice.\\n\\nTermination of your rights under this section does not terminate the licenses of parties\\nwho have received copies or rights from you under this License. If your rights have been\\nterminated and not permanently reinstated, receipt of a copy of some or all of the same\\nmaterial does not give you any rights to use it.\\n\\n10. FUTURE REVISIONS OF THIS LICENSE The Free Software Foundation\\nmay publish new, revised versions of the GNU Free Documentation License from time to\\ntime. Such new versions will be similar in spirit to the present version, but may differ in\\ndetail to address new problems or concerns. See http://www.gnu.org/copyleft/.\\n\\nEach version of the License is given a distinguishing version number. If the Document\\nspecifies that a particular numbered version of this License “or any later version” applies\\nto it, you have the option of following the terms and conditions either of that specified\\nversion or of any later version that has been published (not as a draft) by the Free Software\\nFoundation. If the Document does not specify a version number of this License, you may\\nchoose any version ever published (not as a draft) by the Free Software Foundation. If the\\nDocument specifies that a proxy can decide which future versions of this License can be\\nused, that proxy’s public statement of acceptance of a version permanently authorizes you\\nto choose that version for the Document.\\n\\n11. RELICENSING “Massive Multiauthor Collaboration Site” (or “MMC Site”) means\\nany World Wide Web server that publishes copyrightable works and also provides prominent\\nfacilities for anybody to edit those works. A public wiki that anybody can edit is an example\\nof such a server. A “Massive Multiauthor Collaboration” (or “MMC”) contained in the site\\nmeans any set of copyrightable works thus published on the MMC site.\\n\\n“CC-BY-SA” means the Creative Commons Attribution-Share Alike 3.0 license pub-\\nlished by Creative Commons Corporation, a not-for-profit corporation with a principal\\nplace of business in San Francisco, California, as well as future copyleft versions of that\\nlicense published by that same organization.\\n\\nhttp://www.gnu.org/copyleft/\\n\\n\\n434 APPENDIX A. GNU FREE DOCUMENTATION LICENSE\\n\\n“Incorporate” means to publish or republish a Document, in whole or in part, as part\\nof another Document.\\n\\nAn MMC is “eligible for relicensing” if it is licensed under this License, and if all works\\nthat were first published under this License somewhere other than this MMC, and subse-\\nquently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant\\nsections, and (2) were thus incorporated prior to November 1, 2008.\\n\\nThe operator of an MMC Site may republish an MMC contained in the site under CC-\\nBY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible\\nfor relicensing.\\n\\nADDENDUM: How to use this License for your documents To use this License\\nin a document you have written, include a copy of the License in the document and put the\\nfollowing copyright and license notices just after the title page:\\n\\nCopyright (C) YEAR YOUR NAME.\\nPermission is granted to copy, distribute and/or modify this document\\nunder the terms of the GNU Free Documentation License, Version 1.3\\nor any later version published by the Free Software Foundation;\\nwith no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.\\nA copy of the license is included in the section entitled \\"GNU\\nFree Documentation License\\".\\n\\nIf you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the\\n“with… Texts.” line with this:\\n\\nwith the Invariant Sections being LIST THEIR TITLES, with the\\nFront-Cover Texts being LIST, and with the Back-Cover Texts being LIST.\\n\\nIf you have Invariant Sections without Cover Texts, or some other combination of the\\nthree, merge those two alternatives to suit the situation.\\n\\nIf your document contains nontrivial examples of program code, we recommend releasing\\nthese examples in parallel under your choice of free software license, such as the GNU\\nGeneral Public License, to permit their use in free software.\\n\\n\\n\\nB\\n\\nHints and Solutions to Selected\\nExercises\\n\\n1.3 Exercises\\n\\n1. (a) A ∩B = {2}; (b) B ∩ C = {5}.\\n\\n2. (a) A×B = {(a, 1), (a, 2), (a, 3), (b, 1), (b, 2), (b, 3), (c, 1), (c, 2), (c, 3)}; (d) A×D = ∅.\\n\\n6. If x ∈ A ∪ (B ∩ C), then either x ∈ A or x ∈ B ∩ C. Thus, x ∈ A ∪ B and A ∪ C.\\nHence, x ∈ (A ∪ B) ∩ (A ∪ C). Therefore, A ∪ (B ∩ C) ⊂ (A ∪ B) ∩ (A ∪ C). Conversely,\\nif x ∈ (A ∪ B) ∩ (A ∪ C), then x ∈ A ∪ B and A ∪ C. Thus, x ∈ A or x is in both B\\nand C. So x ∈ A ∪ (B ∩ C) and therefore (A ∪ B) ∩ (A ∪ C) ⊂ A ∪ (B ∩ C). Hence,\\nA ∪ (B ∩ C) = (A ∪B) ∩ (A ∪ C).\\n\\n10. (A∩B)∪ (A\\\\B)∪ (B \\\\A) = (A∩B)∪ (A∩B′)∪ (B∩A′) = [A∩ (B∪B′)]∪ (B∩A′) =\\nA ∪ (B ∩A′) = (A ∪B) ∩ (A ∪A′) = A ∪B.\\n\\n14. A\\\\(B∪C) = A∩(B∪C)′ = (A∩A)∩(B′∩C ′) = (A∩B′)∩(A∩C ′) = (A\\\\B)∩(A\\\\C).\\n\\n17. (a) Not a map since f(2/3) is undefined; (b) this is a map; (c) not a map, since\\nf(1/2) = 3/4 but f(2/4) = 3/8; (d) this is a map.\\n\\n18. (a) f is one-to-one but not onto. f(R) = {x ∈ R : x > 0}. (c) f is neither one-to-one\\nnor onto. f(R) = {x : −1 ≤ x ≤ 1}.\\n\\n20. (a) f(n) = n+ 1.\\n\\n22. (a) Let x, y ∈ A. Then g(f(x)) = (g ◦f)(x) = (g ◦f)(y) = g(f(y)). Thus, f(x) = f(y)\\nand x = y, so g ◦ f is one-to-one. (b) Let c ∈ C, then c = (g ◦ f)(x) = g(f(x)) for some\\nx ∈ A. Since f(x) ∈ B, g is onto.\\n\\n23. f−1(x) = (x+ 1)/(x− 1).\\n\\n24. (a) Let y ∈ f(A1 ∪ A2). Then there exists an x ∈ A1 ∪ A2 such that f(x) = y.\\nHence, y ∈ f(A1) or f(A2). Therefore, y ∈ f(A1) ∪ f(A2). Consequently, f(A1 ∪ A2) ⊂\\nf(A1) ∪ f(A2). Conversely, if y ∈ f(A1) ∪ f(A2), then y ∈ f(A1) or f(A2). Hence, there\\nexists an x in A1 or A2 such that f(x) = y. Thus, there exists an x ∈ A1 ∪ A2 such that\\nf(x) = y. Therefore, f(A1) ∪ f(A2) ⊂ f(A1 ∪A2), and f(A1 ∪A2) = f(A1) ∪ f(A2).\\n\\n25. (a) NThe relation fails to be symmetric. (b) The relation is not reflexive, since 0 is\\nnot equivalent to itself. (c) The relation is not transitive.\\n\\n28. Let X = N ∪ {\\n√\\n2 } and define x ∼ y if x+ y ∈ N.\\n\\n435\\n\\n\\n\\n436 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\\n\\n2.3 Exercises\\n1. The base case, S(1) : [1(1 + 1)(2(1) + 1)]/6 = 1 = 12 is true. Assume that S(k) :\\n12 + 22 + · · ·+ k2 = [k(k + 1)(2k + 1)]/6 is true. Then\\n\\n12 + 22 + · · ·+ k2 + (k + 1)2 = [k(k + 1)(2k + 1)]/6 + (k + 1)2\\n\\n= [(k + 1)((k + 1) + 1)(2(k + 1) + 1)]/6,\\n\\nand so S(k + 1) is true. Thus, S(n) is true for all positive integers n.\\n3. The base case, S(4) : 4! = 24 > 16 = 24 is true. Assume S(k) : k! > 2k is true. Then\\n(k + 1)! = k!(k + 1) > 2k · 2 = 2k+1, so S(k + 1) is true. Thus, S(n) is true for all positive\\nintegers n.\\n8. Follow the proof in Example 2.4.\\n11. The base case, S(0) : (1+x)0−1 = 0 ≥ 0 = 0·x is true. Assume S(k) : (1+x)k−1 ≥ kx\\nis true. Then\\n\\n(1 + x)k+1 − 1 = (1 + x)(1 + x)k − 1\\n\\n= (1 + x)k + x(1 + x)k − 1\\n\\n≥ kx+ x(1 + x)k\\n\\n≥ kx+ x\\n\\n= (k + 1)x,\\n\\nso S(k + 1) is true. Therefore, S(n) is true for all positive integers n.\\n17. For (a) and (b) use mathematical induction. (c) Show that f1 = 1, f2 = 1, and\\nfn+2 = fn+1 + fn. (d) Use part (c). (e) Use part (b) and Exercise 2.3.16.\\n19. Use the Fundamental Theorem of Arithmetic.\\n23. Use the Principle of Well-Ordering and the division algorithm.\\n27. Since gcd(a, b) = 1, there exist integers r and s such that ar+bs = 1. Thus, acr+bcs =\\nc. Since a divides both bc and itself, a must divide c.\\n29. Every prime must be of the form 2, 3, 6n + 1, or 6n + 5. Suppose there are only\\nfinitely many primes of the form 6k + 5.\\n\\n3.4 Exercises\\n1. (a) 3 + 7Z = {. . . ,−4, 3, 10, . . .}; (c) 18 + 26Z; (e) 5 + 6Z.\\n2. (a) Not a group; (c) a group.\\n6.\\n\\n· 1 5 7 11\\n\\n1 1 5 7 11\\n\\n5 5 1 11 7\\n\\n7 7 11 1 5\\n\\n11 11 7 5 1\\n\\n8. Pick two matrices. Almost any pair will work.\\n15. There is a nonabelian group containing six elements.\\n16. Look at the symmetry group of an equilateral triangle or a square.\\n17. The are five different groups of order 8.\\n\\n\\n\\n437\\n\\n18. Let\\nσ =\\n\\n(\\n1 2 · · · n\\n\\na1 a2 · · · an\\n\\n)\\nbe in Sn. All of the ais must be distinct. There are n ways to choose a1, n − 1 ways to\\nchoose a2, . . ., 2 ways to choose an−1, and only one way to choose an. Therefore, we can\\nform σ in n(n− 1) · · · 2 · 1 = n! ways.\\n25.\\n\\n(aba−1)n = (aba−1)(aba−1) · · · (aba−1)\\n\\n= ab(aa−1)b(aa−1)b · · · b(aa−1)ba−1\\n\\n= abna−1.\\n\\n31. Since abab = (ab)2 = e = a2b2 = aabb, we know that ba = ab.\\n35. H1 = {id}, H2 = {id, ρ1, ρ2}, H3 = {id, µ1}, H4 = {id, µ2}, H5 = {id, µ3}, S3.\\n41. The identity of G is 1 = 1+0\\n\\n√\\n2. Since (a+b\\n\\n√\\n2 )(c+d\\n\\n√\\n2 ) = (ac+2bd)+(ad+bc)\\n\\n√\\n2,\\n\\nG is closed under multiplication. Finally, (a+ b\\n√\\n2 )−1 = a/(a2 − 2b2)− b\\n\\n√\\n2/(a2 − 2b2).\\n\\n46. Look at S3.\\n49. ba = a4b = a3ab = ab\\n\\n4.4 Exercises\\n1. (a) False; (c) false; (e) true.\\n2. (a) 12; (c) infinite; (e) 10.\\n3. (a) 7Z = {. . . ,−7, 0, 7, 14, . . .}; (b) {0, 3, 6, 9, 12, 15, 18, 21}; (c) {0}, {0, 6}, {0, 4, 8},\\n{0, 3, 6, 9}, {0, 2, 4, 6, 8, 10}; (g) {1, 3, 7, 9}; (j) {1,−1, i,−i}.\\n4. (a) (\\n\\n1 0\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n−1 0\\n\\n0 −1\\n\\n)\\n,\\n\\n(\\n0 −1\\n\\n1 0\\n\\n)\\n,\\n\\n(\\n0 1\\n\\n−1 0\\n\\n)\\n.\\n\\n(c) (\\n1 0\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n1 −1\\n\\n1 0\\n\\n)\\n,\\n\\n(\\n−1 1\\n\\n−1 0\\n\\n)\\n,\\n\\n(\\n0 1\\n\\n−1 1\\n\\n)\\n,\\n\\n(\\n0 −1\\n\\n1 −1\\n\\n)\\n,\\n\\n(\\n−1 0\\n\\n0 −1\\n\\n)\\n.\\n\\n10. (a) 0; (b) 1,−1.\\n11. 1, 2, 3, 4, 6, 8, 12, 24.\\n15. (a) −3 + 3i; (c) 43− 18i; (e) i\\n16. (a)\\n\\n√\\n3 + i; (c) −3.\\n\\n17. (a)\\n√\\n2 cis(7π/4); (c) 2\\n\\n√\\n2 cis(π/4); (e) 3 cis(3π/2).\\n\\n18. (a) (1− i)/2; (c) 16(i−\\n√\\n3 ); (e) −1/4.\\n\\n22. (a) 292; (c) 1523.\\n27. |⟨g⟩ ∩ ⟨h⟩| = 1.\\n31. The identity element in any group has finite order. Let g, h ∈ G have orders m and\\nn, respectively. Since (g−1)m = e and (gh)mn = e, the elements of finite order in G form a\\nsubgroup of G.\\n37. If g is an element distinct from the identity in G, g must generate G; otherwise, ⟨g⟩\\nis a nontrivial proper subgroup of G.\\n\\n\\n\\n438 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\\n\\n5.3 Exercises\\n\\n1. (a) (12453); (c) (13)(25).\\n2. (a) (135)(24); (c) (14)(23); (e) (1324); (g) (134)(25); (n) (17352).\\n3. (a) (16)(15)(13)(14); (c) (16)(14)(12).\\n4. (a1, a2, . . . , an)\\n\\n−1 = (a1, an, an−1, . . . , a2)\\n\\n5. (a) {(13), (13)(24), (132), (134), (1324), (1342)} is not a subgroup.\\n8. (12345)(678).\\n11. Permutations of the form\\n\\n(1), (a1, a2)(a3, a4), (a1, a2, a3), (a1, a2, a3, a4, a5)\\n\\nare possible for A5.\\n17. Calculate (123)(12) and (12)(123).\\n25. Consider the cases (ab)(bc) and (ab)(cd).\\n30. For (a), show that στσ−1(σ(ai)) = σ(ai+1).\\n\\n6.4 Exercises\\n\\n1. The order of g and the order h must both divide the order of G.\\n2. The possible orders must divide 60.\\n3. This is true for every proper nontrivial subgroup.\\n4. False.\\n5. (a) ⟨8⟩, 1 + ⟨8⟩, 2 + ⟨8⟩, 3 + ⟨8⟩, 4 + ⟨8⟩, 5 + ⟨8⟩, 6 + ⟨8⟩, and 7 + ⟨8⟩; (c) 3Z, 1 + 3Z,\\nand 2 + 3Z.\\n7. 4ϕ(15) ≡ 48 ≡ 1 (mod 15).\\n12. Let g1 ∈ gH. Show that g1 ∈ Hg and thus gH ⊂ Hg.\\n19. Show that g(H ∩K) = gH ∩ gK.\\n22. If gcd(m,n) = 1, then ϕ(mn) = ϕ(m)ϕ(n) (Exercise 2.3.26 in Chapter 2).\\n\\n7.3 Exercises\\n\\n1. LAORYHAPDWK\\n\\n3. Hint: V = E, E = X (also used for spaces and punctuation), K = R.\\n4. 26!− 1\\n\\n7. (a) 2791; (c) 112135 25032 442.\\n9. (a) 31; (c) 14.\\n10. (a) n = 11 · 41; (c) n = 8779 · 4327.\\n\\n\\n\\n439\\n\\n8.5 Exercises\\n2. This cannot be a group code since (0000) /∈ C.\\n3. (a) 2; (c) 2.\\n4. (a) 3; (c) 4.\\n6. (a) dmin = 2; (c) dmin = 1.\\n7.\\n\\n(a) (00000), (00101), (10011), (10110)\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n0 1\\n\\n0 0\\n\\n1 0\\n\\n0 1\\n\\n1 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\n(b) (000000), (010111), (101101), (111010)\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n\\n1 0\\n\\n0 1\\n\\n1 0\\n\\n1 1\\n\\n0 1\\n\\n1 1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\n9. Multiple errors occur in one of the received words.\\n11. (a) A canonical parity-check matrix with standard generator matrix\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\n(c) A canonical parity-check matrix with standard generator matrix\\n\\nG =\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\\n1 0\\n\\n0 1\\n\\n1 1\\n\\n1 0\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\\n\\n12. (a) All possible syndromes occur.\\n15. (a) C, (10000)+C, (01000)+C, (00100)+C, (00010)+C, (11000)+C, (01100)+C,\\n(01010)+C. A decoding table does not exist for C since this is only a single error-detecting\\ncode.\\n19. Let x ∈ C have odd weight and define a map from the set of odd codewords to the\\nset of even codewords by y 7→ x + y. Show that this map is a bijection.\\n23. For 20 information positions, at least 6 check bits are needed to ensure an error-\\ncorrecting code.\\n\\n\\n\\n440 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\\n\\n9.3 Exercises\\n\\n1. Every infinite cyclic group is isomorphic to Z by Theorem 9.7.\\n2. Define ϕ : C∗ → GL2(R) by\\n\\nϕ(a+ bi) =\\n\\n(\\na b\\n\\n−b a\\n\\n)\\n.\\n\\n3. False.\\n6. Define a map from Zn into the nth roots of unity by k 7→ cis(2kπ/n).\\n8. Assume that Q is cyclic and try to find a generator.\\n11. There are two nonabelian and three abelian groups that are not isomorphic.\\n16. (a) 12; (c) 5.\\n19. Draw the picture.\\n20. True.\\n25. True.\\n27. Let a be a generator for G. If ϕ : G → H is an isomorphism, show that ϕ(a) is a\\ngenerator for H.\\n38. Any automorphism of Z6 must send 1 to another generator of Z6.\\n45. To show that ϕ is one-to-one, let g1 = h1k1 and g2 = h2k2 and consider ϕ(g1) = ϕ(g2).\\n\\n10.3 Exercises\\n\\n1. (a)\\nA4 (12)A4\\n\\nA4 A4 (12)A4\\n\\n(12)A4 (12)A4 A4\\n\\n(c) D4 is not normal in S4.\\n8. If a ∈ G is a generator for G, then aH is a generator for G/H.\\n11. For any g ∈ G, show that the map ig : G → G defined by ig : x 7→ gxg−1 is an\\nisomorphism of G with itself. Then consider ig(H).\\n12. Suppose that ⟨g⟩ is normal in G and let y be an arbitrary element of G. If x ∈ C(g),\\nwe must show that yxy−1 is also in C(g). Show that (yxy−1)g = g(yxy−1).\\n14. (a) Let g ∈ G and h ∈ G′. If h = aba−1b−1, then\\n\\nghg−1 = gaba−1b−1g−1\\n\\n= (gag−1)(gbg−1)(ga−1g−1)(gb−1g−1)\\n\\n= (gag−1)(gbg−1)(gag−1)−1(gbg−1)−1.\\n\\nWe also need to show that if h = h1 · · ·hn with hi = aibia\\n−1\\ni b−1\\n\\ni , then ghg−1 is a product of\\nelements of the same type. However, ghg−1 = gh1 · · ·hng−1 = (gh1g\\n\\n−1)(gh2g\\n−1) · · · (ghng−1).\\n\\n\\n\\n441\\n\\n11.3 Exercises\\n2. (a) is a homomorphism with kernel {1}; (c) is not a homomorphism.\\n4. Since ϕ(m+ n) = 7(m+ n) = 7m+ 7n = ϕ(m) + ϕ(n), ϕ is a homomorphism.\\n5. For any homomorphism ϕ : Z24 → Z18, the kernel of ϕ must be a subgroup of Z24 and\\nthe image of ϕ must be a subgroup of Z18. Now use the fact that a generator must map to\\na generator.\\n9. Let a, b ∈ G. Then ϕ(a)ϕ(b) = ϕ(ab) = ϕ(ba) = ϕ(b)ϕ(a).\\n17. Find a counterexample.\\n\\n12.3 Exercises\\n1.\\n\\n1\\n\\n2\\n\\n[\\n∥x + y∥2 + ∥x∥2 − ∥y∥2\\n\\n]\\n=\\n\\n1\\n\\n2\\n\\n[\\n⟨x+ y, x+ y⟩ − ∥x∥2 − ∥y∥2\\n\\n]\\n=\\n\\n1\\n\\n2\\n\\n[\\n∥x∥2 + 2⟨x, y⟩+ ∥y∥2 − ∥x∥2 − ∥y∥2\\n\\n]\\n= ⟨x,y⟩.\\n\\n3. (a) is in SO(2); (c) is not in O(3).\\n5. (a) ⟨x,y⟩ = ⟨y,x⟩.\\n7. Use the unimodular matrix (\\n\\n5 2\\n\\n2 1\\n\\n)\\n.\\n\\n10. Show that the kernel of the map det : O(n) → R∗ is SO(n).\\n13. True.\\n17. p6m\\n\\n13.3 Exercises\\n1. There are three possible groups.\\n4. (a) {0} ⊂ ⟨6⟩ ⊂ ⟨3⟩ ⊂ Z12; (e) {(1)} × {0} ⊂ {(1), (123), (132)} × {0} ⊂ S3 × {0} ⊂\\nS3 × ⟨2⟩ ⊂ S3 × Z4.\\n7. Use the Fundamental Theorem of Finitely Generated Abelian Groups.\\n12. If N and G/N are solvable, then they have solvable series\\n\\nN = Nn ⊃ Nn−1 ⊃ · · · ⊃ N1 ⊃ N0 = {e}\\nG/N = Gn/N ⊃ Gn−1/N ⊃ · · ·G1/N ⊃ G0/N = {N}.\\n\\n16. Use the fact that Dn has a cyclic subgroup of index 2.\\n21. G/G′ is abelian.\\n\\n\\n\\n442 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\\n\\n14.4 Exercises\\n1. Example 14.1: 0, R2 \\\\ {0}. Example 14.2: X = {1, 2, 3, 4}.\\n2. (a) X(1) = {1, 2, 3}, X(12) = {3}, X(13) = {2}, X(23) = {1}, X(123) = X(132) = ∅.\\nG1 = {(1), (23)}, G2 = {(1), (13)}, G3 = {(1), (12)}.\\n3. (a) O1 = O2 = O3 = {1, 2, 3}.\\n6. The conjugacy classes for S4 are\\n\\nO(1) = {(1)},\\nO(12) = {(12), (13), (14), (23), (24), (34)},\\nO(12)(34) = {(12)(34), (13)(24), (14)(23)},\\n\\nO(123) = {(123), (132), (124), (142), (134), (143), (234), (243)},\\nO(1234) = {(1234), (1243), (1324), (1342), (1423), (1432)}.\\n\\nThe class equation is 1 + 3 + 6 + 6 + 8 = 24.\\n8. (34 + 31 + 32 + 31 + 32 + 32 + 33 + 33)/8 = 21.\\n11. The group of rigid motions of the cube can be described by the allowable permutations\\nof the six faces and is isomorphic to S4. There are the identity cycle, 6 permutations with\\nthe structure (abcd) that correspond to the quarter turns, 3 permutations with the structure\\n(ab)(cd) that correspond to the half turns, 6 permutations with the structure (ab)(cd)(ef)\\nthat correspond to rotating the cube about the centers of opposite edges, and 8 permutations\\nwith the structure (abc)(def) that correspond to rotating the cube about opposite vertices.\\n15. (1 · 26 + 3 · 24 + 4 · 23 + 2 · 22 + 2 · 21)/12 = 13.\\n17. (1 · 28 + 3 · 26 + 2 · 24)/6 = 80.\\n22. Use the fact that x ∈ gC(a)g−1 if and only if g−1xg ∈ C(a).\\n\\n15.3 Exercises\\n1. If |G| = 18 = 2 · 32, then the order of a Sylow 2-subgroup is 2, and the order of a Sylow\\n3-subgroup is 9.\\n2. The four Sylow 3-subgroups of S4 are P1 = {(1), (123), (132)}, P2 = {(1), (124), (142)},\\nP3 = {(1), (134), (143)}, P4 = {(1), (234), (243)}.\\n5. Since |G| = 96 = 25 · 3, G has either one or three Sylow 2-subgroups by the Third\\nSylow Theorem. If there is only one subgroup, we are done. If there are three Sylow 2-\\nsubgroups, let H and K be two of them. Therefore, |H ∩K| ≥ 16; otherwise, HK would\\nhave (32 ·32)/8 = 128 elements, which is impossible. Thus, H ∩K is normal in both H and\\nK since it has index 2 in both groups.\\n8. Show that G has a normal Sylow p-subgroup of order p2 and a normal Sylow q-subgroup\\nof order q2.\\n10. False.\\n17. If G is abelian, then G is cyclic, since |G| = 3 · 5 · 17. Now look at Example 15.14.\\n23. Define a mapping between the right cosets of N(H) in G and the conjugates of H in\\nG by N(H)g 7→ g−1Hg. Prove that this map is a bijection.\\n26. Let aG′, bG′ ∈ G/G′. Then (aG′)(bG′) = abG′ = ab(b−1a−1ba)G′ = (abb−1a−1)baG′ =\\nbaG′.\\n\\n\\n\\n443\\n\\n16.6 Exercises\\n1. (a) 7Z is a ring but not a field; (c) Q(\\n\\n√\\n2 ) is a field; (f) R is not a ring.\\n\\n3. (a) {1, 3, 7, 9}; (c) {1, 2, 3, 4, 5, 6}; (e){(\\n1 0\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n1 1\\n\\n0 1\\n\\n)\\n,\\n\\n(\\n1 0\\n\\n1 1\\n\\n)\\n,\\n\\n(\\n0 1\\n\\n1 0\\n\\n)\\n,\\n\\n(\\n1 1\\n\\n1 0\\n\\n)\\n,\\n\\n(\\n0 1\\n\\n1 1\\n\\n)\\n,\\n\\n}\\n.\\n\\n4. (a) {0}, {0, 9}, {0, 6, 12}, {0, 3, 6, 9, 12, 15}, {0, 2, 4, 6, 8, 10, 12, 14, 16}; (c) there are no\\nnontrivial ideals.\\n7. Assume there is an isomorphism ϕ : C → R with ϕ(i) = a.\\n8. False. Assume there is an isomorphism ϕ : Q(\\n\\n√\\n2 ) → Q(\\n\\n√\\n3 ) such that ϕ(\\n\\n√\\n2 ) = a.\\n\\n13. (a) x ≡ 17 (mod 55); (c) x ≡ 214 (mod 2772).\\n16. If I ̸= {0}, show that 1 ∈ I.\\n18. (a) ϕ(a)ϕ(b) = ϕ(ab) = ϕ(ba) = ϕ(b)ϕ(a).\\n26. Let a ∈ R with a ̸= 0. Then the principal ideal generated by a is R. Thus, there\\nexists a b ∈ R such that ab = 1.\\n28. Compute (a+ b)2 and (−ab)2.\\n34. Let a/b, c/d ∈ Z(p). Then a/b+ c/d = (ad+ bc)/bd and (a/b) · (c/d) = (ac)/(bd) are\\nboth in Z(p), since gcd(bd, p) = 1.\\n38. Suppose that x2 = x and x ̸= 0. Since R is an integral domain, x = 1. To find a\\nnontrivial idempotent, look in M2(R).\\n\\n17.4 Exercises\\n2. (a) 9x2 + 2x+ 5; (b) 8x4 + 7x3 + 2x2 + 7x.\\n3. (a) 5x3 + 6x2 − 3x + 4 = (5x2 + 2x + 1)(x − 2) + 6; (c) 4x5 − x3 + x2 + 4 = (4x2 +\\n4)(x3 + 3) + 4x2 + 2.\\n5. (a) No zeros in Z12; (c) 3, 4.\\n7. Look at (2x+ 1).\\n8. (a) Reducible; (c) irreducible.\\n10. One factorization is x2 + x+ 8 = (x+ 2)(x+ 9).\\n13. The integers Z do not form a field.\\n14. False.\\n16. Let ϕ : R→ S be an isomorphism. Define ϕ : R[x] → S[x] by ϕ(a0+a1x+· · ·+anxn) =\\nϕ(a0) + ϕ(a1)x+ · · ·+ ϕ(an)x\\n\\nn.\\n20. The polynomial\\n\\nΦn(x) =\\nxn − 1\\n\\nx− 1\\n= xn−1 + xn−2 + · · ·+ x+ 1\\n\\nis called the cyclotomic polynomial. Show that Φp(x) is irreducible over Q for any prime\\np.\\n26. Find a nontrivial proper ideal in F [x].\\n\\n\\n\\n444 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\\n\\n18.3 Exercises\\n1. Note that z−1 = 1/(a + b\\n\\n√\\n3 i) = (a − b\\n\\n√\\n3 i)/(a2 + 3b2) is in Z[\\n\\n√\\n3 i] if and only if\\n\\na2 + 3b2 = 1. The only integer solutions to the equation are a = ±1, b = 0.\\n2. (a) 5 = −i(1 + 2i)(2 + i); (c) 6 + 8i = −i(1 + i)2(2 + i)2.\\n4. True.\\n9. Let z = a+ bi and w = c+ di ̸= 0 be in Z[i]. Prove that z/w ∈ Q(i).\\n15. Let a = ub with u a unit. Then ν(b) ≤ ν(ub) ≤ ν(a). Similarly, ν(a) ≤ ν(b).\\n16. Show that 21 can be factored in two different ways.\\n\\n19.4 Exercises\\n2.\\n\\n1\\n\\n5\\n\\n10\\n\\n30\\n\\n2 3\\n\\n15\\n\\n5. False.\\n6. (a) (a ∨ b ∨ a′) ∧ a\\n\\na′\\n\\nb\\n\\na\\n\\na\\n\\n(c) a ∨ (a ∧ b)\\n\\na\\n\\na b\\n\\n8. Not equivalent.\\n10. (a) a′ ∧ [(a ∧ b′) ∨ b] = a ∧ (a ∨ b).\\n14. Let I, J be ideals in R. We need to show that I + J = {r + s : r ∈ I and s ∈ J}\\nis the smallest ideal in R containing both I and J . If r1, r2 ∈ I and s1, s2 ∈ J , then\\n(r1+s1)+(r2+s2) = (r1+r2)+(s1+s2) is in I+J . For a ∈ R, a(r1+s1) = ar1+as1 ∈ I+J ;\\nhence, I + J is an ideal in R.\\n18. (a) No.\\n\\n\\n\\n445\\n\\n20. (⇒). a = b⇒ (a∧b′)∨(a′∧b) = (a∧a′)∨(a′∧a) = O∨O = O. (⇐). (a∧b′)∨(a′∧b) =\\nO ⇒ a ∨ b = (a ∨ a) ∨ b = a ∨ (a ∨ b) = a ∨ [I ∧ (a ∨ b)] = a ∨ [(a ∨ a′) ∧ (a ∨ b)] =\\n[a ∨ (a ∧ b′)] ∨ [a ∨ (a′ ∧ b)] = a ∨ [(a ∧ b′) ∨ (a′ ∧ b)] = a ∨ 0 = a. A symmetric argument\\nshows that a ∨ b = b.\\n\\n20.4 Exercises\\n3. Q(\\n\\n√\\n2,\\n√\\n3 ) has basis {1,\\n\\n√\\n2,\\n√\\n3,\\n√\\n6 } over Q.\\n\\n5. The set {1, x, x2, . . . , xn−1} is a basis for Pn.\\n7. (a) Subspace of dimension 2 with basis {(1, 0,−3), (0, 1, 2)}; (d) not a subspace\\n10. Since 0 = α0 = α(−v + v) = α(−v) + αv, it follows that −αv = α(−v).\\n12. Let v0 = 0, v1, . . . , vn ∈ V and α0 ̸= 0, α1, . . . , αn ∈ F . Then α0v0 + · · ·+ αnvn = 0.\\n15. (a) Let u, v ∈ ker(T ) and α ∈ F . Then\\n\\nT (u+ v) = T (u) + T (v) = 0\\n\\nT (αv) = αT (v) = α0 = 0.\\n\\nHence, u+ v, αv ∈ ker(T ), and ker(T ) is a subspace of V .\\n(c) The statement that T (u) = T (v) is equivalent to T (u− v) = T (u)−T (v) = 0, which\\n\\nis true if and only if u− v = 0 or u = v.\\n17. (a) Let u, u′ ∈ U and v, v′ ∈ V . Then\\n\\n(u+ v) + (u′ + v′) = (u+ u′) + (v + v′) ∈ U + V\\n\\nα(u+ v) = αu+ αv ∈ U + V.\\n\\n21.4 Exercises\\n1. (a) x4 − (2/3)x2 − 62/9; (c) x4 − 2x2 + 25.\\n2. (a) {1,\\n\\n√\\n2,\\n√\\n3,\\n√\\n6 }; (c) {1, i,\\n\\n√\\n2,\\n√\\n2 i}; (e) {1, 21/6, 21/3, 21/2, 22/3, 25/6}.\\n\\n3. (a) Q(\\n√\\n3,\\n√\\n7 ).\\n\\n5. Use the fact that the elements of Z2[x]/⟨x3+x+1⟩ are 0, 1, α, 1+α, α2, 1+α2, α+α2,\\n1 + α+ α2 and the fact that α3 + α+ 1 = 0.\\n8. False.\\n14. Suppose that E is algebraic over F and K is algebraic over E. Let α ∈ K. It suffices\\nto show that α is algebraic over some finite extension of F . Since α is algebraic over E,\\nit must be the zero of some polynomial p(x) = β0 + β1x + · · · + βnx\\n\\nn in E[x]. Hence α is\\nalgebraic over F (β0, . . . , βn).\\n22. Since {1,\\n\\n√\\n3,\\n√\\n7,\\n√\\n21 } is a basis for Q(\\n\\n√\\n3,\\n√\\n7 ) over Q, Q(\\n\\n√\\n3,\\n√\\n7 ) ⊃ Q(\\n\\n√\\n3 +\\n\\n√\\n7 ).\\n\\nSince [Q(\\n√\\n3,\\n√\\n7 ) : Q] = 4, [Q(\\n\\n√\\n3 +\\n\\n√\\n7 ) : Q] = 2 or 4. Since the degree of the minimal\\n\\npolynomial of\\n√\\n3 +\\n\\n√\\n7 is 4, Q(\\n\\n√\\n3,\\n√\\n7 ) = Q(\\n\\n√\\n3 +\\n\\n√\\n7 ).\\n\\n27. Let β ∈ F (α) not in F . Then β = p(α)/q(α), where p and q are polynomials in α\\nwith q(α) ̸= 0 and coefficients in F . If β is algebraic over F , then there exists a polynomial\\nf(x) ∈ F [x] such that f(β) = 0. Let f(x) = a0 + a1x+ · · ·+ anx\\n\\nn. Then\\n\\n0 = f(β) = f\\n\\n(\\np(α)\\n\\nq(α)\\n\\n)\\n= a0 + a1\\n\\n(\\np(α)\\n\\nq(α)\\n\\n)\\n+ · · ·+ an\\n\\n(\\np(α)\\n\\nq(α)\\n\\n)n\\n\\n.\\n\\nNow multiply both sides by q(α)n to show that there is a polynomial in F [x] that has α as\\na zero.\\n\\n\\n\\n446 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\\n\\n22.3 Exercises\\n1. Make sure that you have a field extension.\\n4. There are eight elements in Z2(α). Exhibit two more zeros of x3 + x2 +1 other than α\\nin these eight elements.\\n5. Find an irreducible polynomial p(x) in Z3[x] of degree 3 and show that Z3[x]/⟨p(x)⟩\\nhas 27 elements.\\n7. (a) x5− 1 = (x+1)(x4+x3+x2+x+1); (c) x9− 1 = (x+1)(x2+x+1)(x6+x3+1).\\n8. True.\\n11. (a) Use the fact that x7 − 1 = (x+ 1)(x3 + x+ 1)(x3 + x2 + 1).\\n12. False.\\n17. If p(x) ∈ F [x], then p(x) ∈ E[x].\\n18. Since α is algebraic over F of degree n, we can write any element β ∈ F (α) uniquely as\\nβ = a0+a1α+ · · ·+an−1α\\n\\nn−1 with ai ∈ F . There are qn possible n-tuples (a0, a1, . . . , an−1).\\n24. Factor xp−1 − 1 over Zp.\\n\\n23.4 Exercises\\n1. (a) Z2; (c) Z2 × Z2 × Z2.\\n2. (a) Separable over Q since x3 + 2x2 − x− 2 = (x− 1)(x+ 1)(x+ 2); (c) not separable\\nover Z3 since x4 + x2 + 1 = (x+ 1)2(x+ 2)2.\\n3. If\\n\\n[GF(729) : GF(9)] = [GF(729) : GF(3)]/[GF(9) : GF(3)] = 6/2 = 3,\\n\\nthen G(GF(729)/GF(9)) ∼= Z3. A generator for G(GF(729)/GF(9)) is σ, where σ36(α) =\\nα36 = α729 for α ∈ GF(729).\\n4. (a) S5; (c) S3; (g) see Example 23.10.\\n5. (a) Q(i)\\n\\n7. Let E be the splitting field of a cubic polynomial in F [x]. Show that [E : F ] is less\\nthan or equal to 6 and is divisible by 3. Since G(E/F ) is a subgroup of S3 whose order is\\ndivisible by 3, conclude that this group must be isomorphic to Z3 or S3.\\n9. G is a subgroup of Sn.\\n16. True.\\n20.\\n\\n(a) Clearly ω, ω2, . . . , ωp−1 are distinct since ω ̸= 1 or 0. To show that ωi is a zero of Φp,\\ncalculate Φp(ω\\n\\ni).\\n\\n(b) The conjugates of ω are ω, ω2, . . . , ωp−1. Define a map ϕi : Q(ω) → Q(ωi) by\\n\\nϕi(a0 + a1ω + · · ·+ ap−2ω\\np−2) = a0 + a1ω\\n\\ni + · · ·+ cp−2(ω\\ni)p−2,\\n\\nwhere ai ∈ Q. Prove that ϕi is an isomorphism of fields. Show that ϕ2 generates\\nG(Q(ω)/Q).\\n\\n(c) Show that {ω, ω2, . . . , ωp−1} is a basis for Q(ω) over Q, and consider which linear\\ncombinations of ω, ω2, . . . , ωp−1 are left fixed by all elements of G(Q(ω)/Q).\\n\\n\\n\\nC\\n\\nNotation\\n\\nThe following table defines the notation used in this book. Page numbers or references refer\\nto the first appearance of each symbol.\\n\\nSymbol Description Page\\n\\na ∈ A a is in the set A 3\\nN the natural numbers 4\\nZ the integers 4\\nQ the rational numbers 4\\nR the real numbers 4\\nC the complex numbers 4\\nA ⊂ B A is a subset of B 4\\n∅ the empty set 4\\nA ∪B the union of sets A and B 4\\nA ∩B the intersection of sets A and B 4\\nA′ complement of the set A 4\\nA \\\\B difference between sets A and B 5\\nA×B Cartesian product of sets A and B 6\\nAn A× · · · ×A (n times) 6\\nid identity mapping 9\\nf−1 inverse of the function f 9\\na ≡ b (mod n) a is congruent to b modulo n 12\\nn! n factorial 23(\\nn\\nk\\n\\n)\\nbinomial coefficient n!/(k!(n− k)!) 23\\n\\na | b a divides b 25\\ngcd(a, b) greatest common divisor of a and b 25\\nP(X) power set of X 29\\nlcm(m,n) the least common multiple of m and n 30\\nZn the integers modulo n 36\\nU(n) group of units in Zn 41\\nMn(R) the n× n matrices with entries in R 42\\ndetA the determinant of A 42\\nGLn(R) the general linear group 42\\nQ8 the group of quaternions 42\\nC∗ the multiplicative group of complex numbers 42\\n|G| the order of a group 43\\n\\n(Continued on next page)\\n\\n447\\n\\n\\n\\n448 APPENDIX C. NOTATION\\n\\nSymbol Description Page\\n\\nR∗ the multiplicative group of real numbers 45\\nQ∗ the multiplicative group of rational numbers 45\\nSLn(R) the special linear group 45\\nZ(G) the center of a group 50\\n⟨a⟩ cyclic group generated by a 59\\n|a| the order of an element a 60\\ncis θ cos θ + i sin θ 63\\nT the circle group 64\\nSn the symmetric group on n letters 80\\n(a1, a2, . . . , ak) cycle of length k 82\\nAn the alternating group on n letters 85\\nDn the dihedral group 86\\n[G : H] index of a subgroup H in a group G 102\\nLH the set of left cosets of a subgroup H in a group G 103\\nRH the set of right cosets of a subgroup H in a group G 103\\nd(x,y) Hamming distance between x and y 130\\ndmin the minimum distance of a code 130\\nw(x) the weight of x 130\\nMm×n(Z2) the set of m× n matrices with entries in Z2 134\\nNull(H) null space of a matrix H 134\\nδij Kronecker delta 138\\nG ∼= H G is isomorphic to a group H 152\\nAut(G) automorphism group of a group G 161\\nig ig(x) = gxg−1 161\\nInn(G) inner automorphism group of a group G 161\\nρg right regular representation 161\\nG/N factor group of G mod N 169\\nG′ commutator subgroup of G 174\\nkerϕ kernel of ϕ 180\\n(aij) matrix 193\\nO(n) orthogonal group 195\\n∥x∥ length of a vector x 195\\nSO(n) special orthogonal group 198\\nE(n) Euclidean group 198\\nOx orbit of x 221\\nXg fixed point set of g 221\\nGx isotropy subgroup of x 221\\nN(H) normalizer of s subgroup H 240\\nH the ring of quaternions 257\\nZ[i] the Gaussian integers 258\\ncharR characteristic of a ring R 259\\nZ(p) ring of integers localized at p 272\\ndeg f(x) degree of a polynomial 283\\nR[x] ring of polynomials over a ring R 284\\n\\n(Continued on next page)\\n\\n\\n\\n449\\n\\nSymbol Description Page\\n\\nR[x1, x2, . . . , xn] ring of polynomials in n indeterminants 286\\nϕα evaluation homomorphism at α 286\\nQ(x) field of rational functions over Q 307\\nν(a) Euclidean valuation of a 310\\nF (x) field of rational functions in x 314\\nF (x1, . . . , xn) field of rational functions in x1, . . . , xn 314\\na ⪯ b a is less than b 320\\na ∨ b join of a and b 322\\na ∧ b meet of a and b 322\\nI largest element in a lattice 323\\nO smallest element in a lattice 323\\na′ complement of a in a lattice 323\\ndimV dimension of a vector space V 344\\nU ⊕ V direct sum of vector spaces U and V 346\\nHom(V,W ) set of all linear transformations from U into V 346\\nV ∗ dual of a vector space V 346\\nF (α1, . . . , αn) smallest field containing F and α1, . . . , αn 356\\n[E : F ] dimension of a field extension of E over F 359\\nGF(pn) Galois field of order pn 382\\nF ∗ multiplicative group of a field F 383\\nG(E/F ) Galois group of E over F 398\\nF{σi} field fixed by the automorphism σi 401\\nFG field fixed by the automorphism group G 401\\n∆2 discriminant of a polynomial 413\\n\\n\\n\\n450 APPENDIX C. NOTATION\\n\\n\\n\\nIndex\\n\\nG-equivalent, 221\\nG-set, 220\\nnth root of unity, 64, 407\\nrsa cryptosystem, 116\\n\\nAbel, Niels Henrik, 406\\nAbelian group, 40\\nAdleman, L., 116\\nAlgebraic closure, 361\\nAlgebraic extension, 356\\nAlgebraic number, 357\\nAlgorithm\\n\\ndivision, 286\\nEuclidean, 27\\n\\nAscending chain condition, 309\\nAssociate elements, 307\\nAtom, 326\\nAutomorphism\\n\\ninner, 185\\n\\nBasis of a lattice, 201\\nBieberbach, L., 204\\nBinary operation, 40\\nBinary symmetric channel, 129\\nBoole, George, 330\\nBoolean algebra\\n\\natom in a, 326\\ndefinition of, 324\\nfinite, 326\\nisomorphism, 326\\n\\nBoolean function, 228, 333\\nBurnside’s Counting Theorem, 225\\nBurnside, William, 44, 173, 230\\n\\nCancellation law\\nfor groups, 44\\nfor integral domains, 259\\n\\nCardano, Gerolamo, 293\\nCarmichael numbers, 121\\n\\nCauchy’s Theorem, 239\\nCauchy, Augustin-Louis, 86\\nCayley table, 41\\nCayley’s Theorem, 155\\nCayley, Arthur, 155\\nCentralizer\\n\\nof a subgroup, 223\\nCharacteristic of a ring, 259\\nChinese Remainder Theorem\\n\\nfor integers, 266\\nCipher, 113\\nCiphertext, 113\\nCircuit\\n\\nparallel, 328\\nseries, 328\\nseries-parallel, 329\\n\\nClass equation, 223\\nCode\\n\\nbch, 389\\ncyclic, 383\\ngroup, 132\\nlinear, 135\\nminimum distance of, 130\\npolynomial, 384\\n\\nCommutative diagrams, 182\\nCommutative rings, 255\\nComposite integer, 27\\nComposition series, 213\\nCongruence modulo n, 12\\nConjugacy classes, 223\\nConjugate elements, 398\\nConjugate, complex, 62\\nConjugation, 221\\nConstructible number, 365\\nCorrespondence Theorem\\n\\nfor groups, 183\\nfor rings, 263\\n\\nCoset\\n\\n451\\n\\n\\n\\n452 INDEX\\n\\nleader, 142\\nleft, 101\\nrepresentative, 101\\nright, 101\\n\\nCoset decoding, 141\\nCryptanalysis, 114\\nCryptosystem\\n\\nrsa, 116\\naffine, 115\\ndefinition of, 113\\nmonoalphabetic, 114\\npolyalphabetic, 115\\nprivate key, 113\\npublic key, 113\\nsingle key, 113\\n\\nCycle\\ndefinition of, 82\\ndisjoint, 82\\n\\nDe Morgan’s laws\\nfor Boolean algebras, 325\\nfor sets, 5\\n\\nDe Morgan, Augustus, 330\\nDecoding table, 142\\nDeligne, Pierre, 369\\nDeMoivre’s Theorem, 64\\nDerivative, 381\\nDeterminant, Vandermonde, 387\\nDickson, L. E., 173\\nDiffie, W., 115\\nDirect product of groups\\n\\nexternal, 156\\ninternal, 158\\n\\nDiscriminant\\nof the cubic equation, 297\\nof the quadratic equation, 296\\n\\nDivision algorithm\\nfor integers, 25\\nfor polynomials, 286\\n\\nDivision ring, 255\\nDomain\\n\\nEuclidean, 310\\nprincipal ideal, 308\\nunique factorization, 307\\n\\nDoubling the cube, 368\\n\\nEisenstein’s Criterion, 291\\nElement\\n\\nassociate, 307\\nidentity, 40\\ninverse, 40\\n\\nirreducible, 307\\norder of, 60\\nprime, 307\\nprimitive, 400\\ntranscendental, 356\\n\\nEquivalence class, 12\\nEquivalence relation, 11\\nEuclidean algorithm, 27\\nEuclidean domain, 310\\nEuclidean group, 198\\nEuclidean inner product, 195\\nEuclidean valuation, 310\\nEuler ϕ-function, 104\\nEuler, Leonhard, 105, 369\\nExtension\\n\\nalgebraic, 356\\nfield, 354\\nfinite, 359\\nnormal, 403\\nradical, 407\\nseparable, 381, 400\\nsimple, 356\\n\\nExternal direct product, 156\\n\\nFaltings, Gerd, 369\\nFeit, W., 173, 230\\nFermat’s factorizationalgorithm, 120\\nFermat’s Little Theorem, 105\\nFermat, Pierre de, 105, 368\\nFerrari, Ludovico, 293\\nFerro, Scipione del, 293\\nField, 255\\n\\nalgebraically closed, 361\\nbase, 354\\nextension, 354\\nfixed, 401\\nGalois, 382\\nof fractions, 306\\nof quotients, 306\\nsplitting, 362\\n\\nFinitely generated group, 208\\nFior, Antonio, 293\\nFirst Isomorphism Theorem\\n\\nfor groups, 181\\nfor rings, 262\\n\\nFixed point set, 221\\nFreshman’s Dream, 380\\nFunction\\n\\nbijective, 7\\nBoolean, 228, 333\\ncomposition of, 7\\n\\n\\n\\nINDEX 453\\n\\ndefinition of, 6\\ndomain of, 6\\nidentity, 9\\ninjective, 7\\ninvertible, 9\\none-to-one, 7\\nonto, 7\\nrange of, 6\\nsurjective, 7\\nswitching, 228, 333\\n\\nFundamental Theorem\\nof Algebra, 362, 411\\nof Arithmetic, 27\\nof Finite Abelian Groups, 209\\n\\nFundamental Theorem of Galois Theory,\\n404\\n\\nGalois field, 382\\nGalois group, 398\\nGalois, Évariste, 44, 406\\nGauss’s Lemma, 312\\nGauss, Karl Friedrich, 313\\nGaussian integers, 258\\nGenerator of a cyclic subgroup, 60\\nGenerators for a group, 208\\nGlide reflection, 199\\nGorenstein, Daniel, 173\\nGreatest common divisor\\n\\nof two integers, 25\\nof two polynomials, 288\\n\\nGreatest lower bound, 321\\nGreiss, R., 173\\nGrothendieck, Alexander, 369\\nGroup\\n\\np-group, 209, 239\\nabelian, 40\\naction, 220\\nalternating, 85\\ncenter of, 223\\ncircle, 64\\ncommutative, 40\\ncyclic, 60\\ndefinition of, 40\\ndihedral, 86\\nEuclidean, 198\\nfactor, 169\\nfinite, 43\\nfinitely generated, 208\\nGalois, 398\\ngeneral linear, 42, 194\\ngenerators of, 208\\n\\nhomomorphism of, 179\\ninfinite, 43\\nisomorphic, 152\\nisomorphism of, 152\\nnonabelian, 40\\nnoncommutative, 40\\nof units, 41\\norder of, 43\\northogonal, 195\\npermutation, 81\\npoint, 202\\nquaternion, 42\\nquotient, 169\\nsimple, 170, 173\\nsolvable, 215\\nspace, 202\\nspecial linear, 45, 194\\nspecial orthogonal, 198\\nsymmetric, 80\\nsymmetry, 200\\n\\nGödel, Kurt, 330\\n\\nHamming distance, 130\\nHamming, R., 132\\nHellman, M., 115\\nHilbert, David, 204, 264, 330, 369\\nHomomorphic image, 179\\nHomomorphism\\n\\ncanonical, 181, 262\\nevaluation, 260, 286\\nkernel of a group, 180\\nkernel of a ring, 260\\nnatural, 181, 262\\nof groups, 179\\nring, 260\\n\\nIdeal\\ndefinition of, 261\\nmaximal, 263\\none-sided, 262\\nprime, 264\\nprincipal, 261\\ntrivial, 261\\ntwo-sided, 262\\n\\nIndeterminate, 283\\nIndex of a subgroup, 102\\nInduction\\n\\nfirst principle of, 22\\nsecond principle of, 24\\n\\nInfimum, 321\\nInner product, 133\\n\\n\\n\\n454 INDEX\\n\\nIntegral domain, 255\\nInternal direct product, 158\\nInternational standard book number, 51\\nIrreducible element, 307\\nIrreducible polynomial, 289\\nIsometry, 198\\nIsomorphism\\n\\nof Boolean algebras, 326\\nof groups, 152\\nring, 260\\n\\nJoin, 322\\nJordan, C., 173\\nJordan-Hölder Theorem, 214\\n\\nKernel\\nof a group homomorphism, 180\\nof a ring homomorphism, 260\\n\\nKey\\ndefinition of, 113\\nprivate, 113\\npublic, 113\\nsingle, 113\\n\\nKlein, Felix, 44, 192, 264\\nKronecker delta, 138, 196\\nKronecker, Leopold, 369\\nKummer, Ernst, 369\\n\\nLagrange’s Theorem, 103\\nLagrange, Joseph-Louis, 44, 86, 105\\nLaplace, Pierre-Simon, 86\\nLattice\\n\\ncompleted, 323\\ndefinition of, 322\\ndistributive, 324\\n\\nLattice of points, 201\\nLattices, Principle of Duality for, 322\\nLeast upper bound, 321\\nLeft regular representation, 155\\nLie, Sophus, 44, 242\\nLinear combination, 342\\nLinear dependence, 342\\nLinear independence, 342\\nLinear map, 192\\nLinear transformation\\n\\ndefinition of, 8, 192\\nLower bound, 321\\n\\nMapping, see Function\\nMatrix\\n\\ndistance-preserving, 196\\ngenerator, 135\\n\\ninner product-preserving, 196\\ninvertible, 193\\nlength-preserving, 196\\nnonsingular, 193\\nnull space of, 134\\northogonal, 195\\nparity-check, 135\\nsimilar, 12\\nunimodular, 202\\n\\nMatrix, Vandermonde, 387\\nMaximal ideal, 263\\nMaximum-likelihood decoding, 129\\nMeet, 322\\nMinimal generator polynomial, 386\\nMinimal polynomial, 357\\nMinkowski, Hermann, 369\\nMonic polynomial, 283\\nMordell-Weil conjecture, 369\\nMultiplicity of a root, 400\\n\\nNoether, A. Emmy, 264\\nNoether, Max, 264\\nNormal extension, 403\\nNormal series of a group, 212\\nNormal subgroup, 168\\nNormalizer, 240\\nNull space\\n\\nof a matrix, 134\\n\\nOdd Order Theorem, 244\\nOrbit, 221\\nOrthogonal group, 195\\nOrthogonal matrix, 195\\nOrthonormal set, 196\\n\\nPartial order, 320\\nPartially ordered set, 320\\nPartitions, 12\\nPermutation\\n\\ndefinition of, 9, 80\\neven, 85\\nodd, 85\\n\\nPermutation group, 81\\nPlaintext, 113\\nPolynomial\\n\\ncode, 384\\ncontent of, 312\\ndefinition of, 283\\ndegree of, 283\\nerror, 392\\nerror-locator, 392\\n\\n\\n\\nINDEX 455\\n\\ngreatest common divisor of, 288\\nin n indeterminates, 286\\nirreducible, 289\\nleading coefficient of, 283\\nminimal, 357\\nminimal generator, 386\\nmonic, 283\\nprimitive, 312\\nroot of, 288\\nseparable, 400\\nzero of, 288\\n\\nPolynomial separable, 381\\nPoset\\n\\ndefinition of, 320\\nlargest element in, 323\\nsmallest element in, 323\\n\\nPower set, 320\\nPrime element, 307\\nPrime ideal, 264\\nPrime integer, 27\\nPrimitive nth root of unity, 65, 407\\nPrimitive element, 400\\nPrimitive Element Theorem, 401\\nPrimitive polynomial, 312\\nPrincipal ideal, 261\\nPrincipal ideal domain (pid), 308\\nPrincipal series, 213\\nPseudoprime, 120\\n\\nQuaternions, 42, 257\\n\\nResolvent cubic equation, 297\\nRigid motion, 38, 198\\nRing\\n\\ncharacteristic of, 259\\ncommutative, 255\\ndefinition of, 255\\ndivision, 255\\nfactor, 262\\nhomomorphism, 260\\nisomorphism, 260\\nNoetherian, 309\\nquotient, 262\\nwith identity, 255\\nwith unity, 255\\n\\nRivest, R., 116\\nRuffini, P., 406\\nRussell, Bertrand, 330\\n\\nScalar product, 340\\nSecond Isomorphism Theorem\\n\\nfor groups, 182\\nfor rings, 263\\n\\nShamir, A., 116\\nShannon, C.., 132\\nSimple extension, 356\\nSimple group, 170\\nSimple root, 400\\nSolvability by radicals, 407\\nSpanning set, 342\\nSplitting field, 362\\nSquaring the circle is impossible, 368\\nStandard decoding, 141\\nSubgroup\\n\\np-subgroup, 239\\ncentralizer, 223\\ncommutator, 243\\ncyclic, 60\\ndefinition of, 45\\nindex of, 102\\nisotropy, 221\\nnormal, 168\\nnormalizer of, 240\\nproper, 45\\nstabilizer, 221\\nSylowp-subgroup, 240\\ntranslation, 202\\ntrivial, 45\\n\\nSubnormal series of a group, 212\\nSubring, 257\\nSupremum, 321\\nSwitch\\n\\nclosed, 328\\ndefinition of, 328\\nopen, 328\\n\\nSwitching function, 228, 333\\nSylow p-subgroup, 240\\nSylow, Ludvig, 242\\nSyndrome of a code, 140, 392\\n\\nTartaglia, 293\\nThird Isomorphism Theorem\\n\\nfor groups, 183\\nfor rings, 263\\n\\nThompson, J., 173, 230\\nTranscendental element, 356\\nTranscendental number, 357\\nTransposition, 84\\nTrisection of an angle, 368\\n\\nUnique factorization domain (ufd), 307\\nUnit, 255, 307\\n\\n\\n\\n456 INDEX\\n\\nUniversal Product Code, 50\\nUpper bound, 321\\n\\nVandermonde determinant, 387\\nVandermonde matrix, 387\\nVector space\\n\\nbasis of, 343\\ndefinition of, 340\\ndimension of, 344\\nsubspace of, 341\\n\\nWeight of a codeword, 130\\nWeil, André, 369\\nWell-defined map, 7\\nWell-ordered set, 24\\nWhitehead, Alfred North, 330\\n\\nZero\\nmultiplicity of, 400\\nof a polynomial, 288\\n\\nZero divisor, 256\\n\\n\\n\\nThis book was authored and produced with MathBook XML.\\n\\nhttps://mathbook.pugetsound.edu\\n\\n\\tAcknowledgements\\n\\tPreface\\n\\tPreliminaries\\n\\tA Short Note on Proofs\\n\\tSets and Equivalence Relations\\n\\tExercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tThe Integers\\n\\tMathematical Induction\\n\\tThe Division Algorithm\\n\\tExercises\\n\\tProgramming Exercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tGroups\\n\\tInteger Equivalence Classes and Symmetries\\n\\tDefinitions and Examples\\n\\tSubgroups\\n\\tExercises\\n\\tAdditional Exercises: Detecting Errors\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tCyclic Groups\\n\\tCyclic Subgroups\\n\\tMultiplicative Group of Complex Numbers\\n\\tThe Method of Repeated Squares\\n\\tExercises\\n\\tProgramming Exercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tPermutation Groups\\n\\tDefinitions and Notation\\n\\tDihedral Groups\\n\\tExercises\\n\\tSage\\n\\tSage Exercises\\n\\n\\tCosets and Lagrange\'s Theorem\\n\\tCosets\\n\\tLagrange\'s Theorem\\n\\tFermat\'s and Euler\'s Theorems\\n\\tExercises\\n\\tSage\\n\\tSage Exercises\\n\\n\\tIntroduction to Cryptography\\n\\tPrivate Key Cryptography\\n\\tPublic Key Cryptography\\n\\tExercises\\n\\tAdditional Exercises: Primality and Factoring\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tAlgebraic Coding Theory\\n\\tError-Detecting and Correcting Codes\\n\\tLinear Codes\\n\\tParity-Check and Generator Matrices\\n\\tEfficient Decoding\\n\\tExercises\\n\\tProgramming Exercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tIsomorphisms\\n\\tDefinition and Examples\\n\\tDirect Products\\n\\tExercises\\n\\tSage\\n\\tSage Exercises\\n\\n\\tNormal Subgroups and Factor Groups\\n\\tFactor Groups and Normal Subgroups\\n\\tThe Simplicity of the Alternating Group\\n\\tExercises\\n\\tSage\\n\\tSage Exercises\\n\\n\\tHomomorphisms\\n\\tGroup Homomorphisms\\n\\tThe Isomorphism Theorems\\n\\tExercises\\n\\tAdditional Exercises: Automorphisms\\n\\tSage\\n\\tSage Exercises\\n\\n\\tMatrix Groups and Symmetry\\n\\tMatrix Groups\\n\\tSymmetry\\n\\tExercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tThe Structure of Groups\\n\\tFinite Abelian Groups\\n\\tSolvable Groups\\n\\tExercises\\n\\tProgramming Exercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tGroup Actions\\n\\tGroups Acting on Sets\\n\\tThe Class Equation\\n\\tBurnside\'s Counting Theorem\\n\\tExercises\\n\\tProgramming Exercise\\n\\tReferences and Suggested Reading\\n\\tSage\\n\\tSage Exercises\\n\\n\\tThe Sylow Theorems\\n\\tThe Sylow Theorems\\n\\tExamples and Applications\\n\\tExercises\\n\\tA Project\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tRings\\n\\tRings\\n\\tIntegral Domains and Fields\\n\\tRing Homomorphisms and Ideals\\n\\tMaximal and Prime Ideals\\n\\tAn Application to Software Design\\n\\tExercises\\n\\tProgramming Exercise\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tPolynomials\\n\\tPolynomial Rings\\n\\tThe Division Algorithm\\n\\tIrreducible Polynomials\\n\\tExercises\\n\\tAdditional Exercises: Solving the Cubic and Quartic Equations\\n\\tSage\\n\\tSage Exercises\\n\\n\\tIntegral Domains\\n\\tFields of Fractions\\n\\tFactorization in Integral Domains\\n\\tExercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tLattices and Boolean Algebras\\n\\tLattices\\n\\tBoolean Algebras\\n\\tThe Algebra of Electrical Circuits\\n\\tExercises\\n\\tProgramming Exercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tVector Spaces\\n\\tDefinitions and Examples\\n\\tSubspaces\\n\\tLinear Independence\\n\\tExercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tFields\\n\\tExtension Fields\\n\\tSplitting Fields\\n\\tGeometric Constructions\\n\\tExercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tFinite Fields\\n\\tStructure of a Finite Field\\n\\tPolynomial Codes\\n\\tExercises\\n\\tAdditional Exercises: Error Correction for BCH Codes\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tGalois Theory\\n\\tField Automorphisms\\n\\tThe Fundamental Theorem\\n\\tApplications\\n\\tExercises\\n\\tReferences and Suggested Readings\\n\\tSage\\n\\tSage Exercises\\n\\n\\tGNU Free Documentation License\\n\\tHints and Solutions to Selected Exercises\\n\\tNotation\\n\\tIndex\\n\\n", "metadata"=>{"pdf:docinfo:title"=>"Abstract Algebra", "resourceName"=>"b\'Abstract Algebra - 2016 (aata-20160809-sage-7.3).pdf\'", "pdf:docinfo:created"=>"2016-08-15T23:32:12Z"}, "filename"=>"Abstract Algebra - 2016 (aata-20160809-sage-7.3).pdf"}', 'filename': 'Abstract Algebra - 2016 (aata-20160809-sage-7.3).pdf', 'metadata_pdf:docinfo:created': '2016-08-15T23:32:12Z', 'mongo_id': '63cffa2978b994746c729ce7', '@version': '1', 'host': 'bdvm', 'logdate': '2023-01-24T15:32:57+00:00', '@timestamp': '2023-01-24T15:32:58.538898129Z', 'content': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract Algebra\n\n\nAbstract Algebra\n\nTheory and Applications\n\n\n\n\n\nAbstract Algebra\nTheory and Applications\n\nThomas W. Judson\nStephen F. Austin State University\n\nSage Exercises for Abstract Algebra\nRobert A. Beezer\n\nUniversity of Puget Sound\n\nAugust 9, 2016\n\n\n\nWebsite: abstract.pugetsound.edu\n\n© 1997–2016 Thomas W. Judson, Robert A. Beezer\n\nPermission is granted to copy, distribute and/or modify this document under the terms\nof the GNU Free Documentation License, Version 1.2 or any later version published by\nthe Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no\nBack-Cover Texts. A copy of the license is included in the appendix entitled “GNU Free\nDocumentation License.”\n\nhttp://abstract.pugetsound.edu\n\n\nAcknowledgements\n\nI would like to acknowledge the following reviewers for their helpful comments and sugges-\ntions.\n\n• David Anderson, University of Tennessee, Knoxville\n\n• Robert Beezer, University of Puget Sound\n\n• Myron Hood, California Polytechnic State University\n\n• Herbert Kasube, Bradley University\n\n• John Kurtzke, University of Portland\n\n• Inessa Levi, University of Louisville\n\n• Geoffrey Mason, University of California, Santa Cruz\n\n• Bruce Mericle, Mankato State University\n\n• Kimmo Rosenthal, Union College\n\n• Mark Teply, University of Wisconsin\n\nI would also like to thank Steve Quigley, Marnie Pommett, Cathie Griffin, Kelle Karshick,\nand the rest of the staff at PWS Publishing for their guidance throughout this project. It\nhas been a pleasure to work with them.\n\nRobert Beezer encouraged me to make Abstract Algebra: Theory and Applications avail-\nable as an open source textbook, a decision that I have never regretted. With his assistance,\nthe book has been rewritten in MathBook XML (http://mathbook.pugetsound.edu), mak-\ning it possible to quickly output print, web, pdf versions and more from the same source.\nThe open source version of this book has received support from the National Science Foun-\ndation (Award #DUE-1020957).\n\nv\n\nhttp://mathbook.pugetsound.edu\n\n\nPreface\n\nThis text is intended for a one or two-semester undergraduate course in abstract algebra.\nTraditionally, these courses have covered the theoretical aspects of groups, rings, and fields.\nHowever, with the development of computing in the last several decades, applications that\ninvolve abstract algebra and discrete mathematics have become increasingly important,\nand many science, engineering, and computer science students are now electing to minor in\nmathematics. Though theory still occupies a central role in the subject of abstract algebra\nand no student should go through such a course without a good notion of what a proof is, the\nimportance of applications such as coding theory and cryptography has grown significantly.\n\nUntil recently most abstract algebra texts included few if any applications. However,\none of the major problems in teaching an abstract algebra course is that for many students it\nis their first encounter with an environment that requires them to do rigorous proofs. Such\nstudents often find it hard to see the use of learning to prove theorems and propositions;\napplied examples help the instructor provide motivation.\n\nThis text contains more material than can possibly be covered in a single semester.\nCertainly there is adequate material for a two-semester course, and perhaps more; however,\nfor a one-semester course it would be quite easy to omit selected chapters and still have a\nuseful text. The order of presentation of topics is standard: groups, then rings, and finally\nfields. Emphasis can be placed either on theory or on applications. A typical one-semester\ncourse might cover groups and rings while briefly touching on field theory, using Chapters 1\nthrough 6, 9, 10, 11, 13 (the first part), 16, 17, 18 (the first part), 20, and 21. Parts of\nthese chapters could be deleted and applications substituted according to the interests of\nthe students and the instructor. A two-semester course emphasizing theory might cover\nChapters 1 through 6, 9, 10, 11, 13 through 18, 20, 21, 22 (the first part), and 23. On\nthe other hand, if applications are to be emphasized, the course might cover Chapters 1\nthrough 14, and 16 through 22. In an applied course, some of the more theoretical results\ncould be assumed or omitted. A chapter dependency chart appears below. (A broken line\nindicates a partial dependency.)\n\nvi\n\n\n\nvii\n\nChapter 23\n\nChapter 22\n\nChapter 21\n\nChapter 18 Chapter 20 Chapter 19\n\nChapter 17 Chapter 15\n\nChapter 13 Chapter 16 Chapter 12 Chapter 14\n\nChapter 11\n\nChapter 10\n\nChapter 8 Chapter 9 Chapter 7\n\nChapters 1–6\n\nThough there are no specific prerequisites for a course in abstract algebra, students\nwho have had other higher-level courses in mathematics will generally be more prepared\nthan those who have not, because they will possess a bit more mathematical sophistication.\nOccasionally, we shall assume some basic linear algebra; that is, we shall take for granted an\nelementary knowledge of matrices and determinants. This should present no great problem,\nsince most students taking a course in abstract algebra have been introduced to matrices\nand determinants elsewhere in their career, if they have not already taken a sophomore or\njunior-level course in linear algebra.\n\nExercise sections are the heart of any mathematics text. An exercise set appears at the\nend of each chapter. The nature of the exercises ranges over several categories; computa-\ntional, conceptual, and theoretical problems are included. A section presenting hints and\nsolutions to many of the exercises appears at the end of the text. Often in the solutions\na proof is only sketched, and it is up to the student to provide the details. The exercises\nrange in difficulty from very easy to very challenging. Many of the more substantial prob-\nlems require careful thought, so the student should not be discouraged if the solution is not\nforthcoming after a few minutes of work.\n\nThere are additional exercises or computer projects at the ends of many of the chapters.\nThe computer projects usually require a knowledge of programming. All of these exercises\nand projects are more substantial in nature and allow the exploration of new results and\ntheory.\n\nSage (sagemath.org) is a free, open source, software system for advanced mathematics,\nwhich is ideal for assisting with a study of abstract algebra. Sage can be used either on\nyour own computer, a local server, or on SageMathCloud (https://cloud.sagemath.com).\nRobert Beezer has written a comprehensive introduction to Sage and a selection of relevant\nexercises that appear at the end of each chapter, including live Sage cells in the web version\n\nhttp://sagemath.org\nhttps://cloud.sagemath.com\n\n\nviii\n\nof the book. The Sage code has been tested for accuracy with the most recent version\navailable at this time: Sage Version 7.3 (released 2016-08-04).\n\nThomas W. Judson\nNacogdoches, Texas 2015\n\n\n\nContents\n\nAcknowledgements v\n\nPreface vi\n\n1 Preliminaries 1\n1.1 A Short Note on Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Sets and Equivalence Relations . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.4 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 15\n1.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n2 The Integers 22\n2.1 Mathematical Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.2 The Division Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.4 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n2.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 31\n2.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n3 Groups 36\n3.1 Integer Equivalence Classes and Symmetries . . . . . . . . . . . . . . . . . . 36\n3.2 Definitions and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.3 Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.5 Additional Exercises: Detecting Errors . . . . . . . . . . . . . . . . . . . . . 50\n3.6 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 52\n3.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n3.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n\n4 Cyclic Groups 59\n4.1 Cyclic Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Multiplicative Group of Complex Numbers . . . . . . . . . . . . . . . . . . 62\n4.3 The Method of Repeated Squares . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.5 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n4.6 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 70\n4.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\nix\n\n\n\nx CONTENTS\n\n4.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n5 Permutation Groups 80\n5.1 Definitions and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2 Dihedral Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n5.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n5.4 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n5.5 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n6 Cosets and Lagrange’s Theorem 101\n6.1 Cosets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6.2 Lagrange’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n6.3 Fermat’s and Euler’s Theorems . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n\n7 Introduction to Cryptography 113\n7.1 Private Key Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n7.2 Public Key Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n7.4 Additional Exercises: Primality and Factoring . . . . . . . . . . . . . . . . . 119\n7.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 121\n7.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n\n8 Algebraic Coding Theory 126\n8.1 Error-Detecting and Correcting Codes . . . . . . . . . . . . . . . . . . . . . 126\n8.2 Linear Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n8.3 Parity-Check and Generator Matrices . . . . . . . . . . . . . . . . . . . . . 135\n8.4 Efficient Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n8.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n8.6 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n8.7 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 147\n8.8 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n8.9 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n\n9 Isomorphisms 152\n9.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n9.2 Direct Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n9.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n9.4 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n9.5 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n\n10 Normal Subgroups and Factor Groups 168\n10.1 Factor Groups and Normal Subgroups . . . . . . . . . . . . . . . . . . . . . 168\n10.2 The Simplicity of the Alternating Group . . . . . . . . . . . . . . . . . . . . 170\n10.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n10.4 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n10.5 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n\n\n\nCONTENTS xi\n\n11 Homomorphisms 179\n11.1 Group Homomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n11.2 The Isomorphism Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n11.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n11.4 Additional Exercises: Automorphisms . . . . . . . . . . . . . . . . . . . . . 185\n11.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n11.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\n\n12 Matrix Groups and Symmetry 192\n12.1 Matrix Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n12.2 Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n12.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n12.4 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 207\n12.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n12.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n\n13 The Structure of Groups 208\n13.1 Finite Abelian Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n13.2 Solvable Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n13.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n13.4 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n13.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 216\n13.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n13.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n\n14 Group Actions 220\n14.1 Groups Acting on Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n14.2 The Class Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n14.3 Burnside’s Counting Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 224\n14.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\n14.5 Programming Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n14.6 References and Suggested Reading . . . . . . . . . . . . . . . . . . . . . . . 232\n14.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n14.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n\n15 The Sylow Theorems 239\n15.1 The Sylow Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n15.2 Examples and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\n15.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n15.4 A Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\n15.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 247\n15.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n15.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n\n16 Rings 255\n16.1 Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\n16.2 Integral Domains and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n16.3 Ring Homomorphisms and Ideals . . . . . . . . . . . . . . . . . . . . . . . . 260\n16.4 Maximal and Prime Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n16.5 An Application to Software Design . . . . . . . . . . . . . . . . . . . . . . . 265\n16.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n\n\n\nxii CONTENTS\n\n16.7 Programming Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\n16.8 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 273\n16.9 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\n16.10Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n\n17 Polynomials 283\n17.1 Polynomial Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\n17.2 The Division Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\n17.3 Irreducible Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n17.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n17.5 Additional Exercises: Solving the Cubic and Quartic Equations . . . . . . . 296\n17.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n17.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\n\n18 Integral Domains 304\n18.1 Fields of Fractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\n18.2 Factorization in Integral Domains . . . . . . . . . . . . . . . . . . . . . . . . 307\n18.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\n18.4 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 316\n18.5 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\n18.6 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\n\n19 Lattices and Boolean Algebras 320\n19.1 Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320\n19.2 Boolean Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n19.3 The Algebra of Electrical Circuits . . . . . . . . . . . . . . . . . . . . . . . . 328\n19.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\n19.5 Programming Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\n19.6 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 333\n19.7 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\n19.8 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n\n20 Vector Spaces 340\n20.1 Definitions and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\n20.2 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n20.3 Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\n20.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n20.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 346\n20.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\n20.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n\n21 Fields 354\n21.1 Extension Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\n21.2 Splitting Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n21.3 Geometric Constructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n21.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\n21.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 371\n21.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n21.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n\n\n\nCONTENTS xiii\n\n22 Finite Fields 380\n22.1 Structure of a Finite Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\n22.2 Polynomial Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n22.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390\n22.4 Additional Exercises: Error Correction for BCH Codes . . . . . . . . . . . . 392\n22.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 393\n22.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n22.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\n\n23 Galois Theory 397\n23.1 Field Automorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397\n23.2 The Fundamental Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . 401\n23.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407\n23.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n23.5 References and Suggested Readings . . . . . . . . . . . . . . . . . . . . . . . 413\n23.6 Sage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414\n23.7 Sage Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\n\nA GNU Free Documentation License 428\n\nB Hints and Solutions to Selected Exercises 435\n\nC Notation 447\n\nIndex 451\n\n\n\nxiv CONTENTS\n\n\n\n1\n\nPreliminaries\n\nA certain amount of mathematical maturity is necessary to find and study applications\nof abstract algebra. A basic knowledge of set theory, mathematical induction, equivalence\nrelations, and matrices is a must. Even more important is the ability to read and understand\nmathematical proofs. In this chapter we will outline the background needed for a course in\nabstract algebra.\n\n1.1 A Short Note on Proofs\nAbstract mathematics is different from other sciences. In laboratory sciences such as chem-\nistry and physics, scientists perform experiments to discover new principles and verify theo-\nries. Although mathematics is often motivated by physical experimentation or by computer\nsimulations, it is made rigorous through the use of logical arguments. In studying abstract\nmathematics, we take what is called an axiomatic approach; that is, we take a collection\nof objects S and assume some rules about their structure. These rules are called axioms.\nUsing the axioms for S, we wish to derive other information about S by using logical argu-\nments. We require that our axioms be consistent; that is, they should not contradict one\nanother. We also demand that there not be too many axioms. If a system of axioms is too\nrestrictive, there will be few examples of the mathematical structure.\n\nA statement in logic or mathematics is an assertion that is either true or false. Consider\nthe following examples:\n\n• 3 + 56− 13 + 8/2.\n\n• All cats are black.\n\n• 2 + 3 = 5.\n\n• 2x = 6 exactly when x = 4.\n\n• If ax2 + bx+ c = 0 and a ̸= 0, then\n\nx =\n−b±\n\n√\nb2 − 4ac\n\n2a\n.\n\n• x3 − 4x2 + 5x− 6.\n\nAll but the first and last examples are statements, and must be either true or false.\nA mathematical proof is nothing more than a convincing argument about the accuracy\n\nof a statement. Such an argument should contain enough detail to convince the audience; for\ninstance, we can see that the statement “2x = 6 exactly when x = 4” is false by evaluating\n2 · 4 and noting that 6 ̸= 8, an argument that would satisfy anyone. Of course, audiences\n\n1\n\n\n\n2 CHAPTER 1. PRELIMINARIES\n\nmay vary widely: proofs can be addressed to another student, to a professor, or to the\nreader of a text. If more detail than needed is presented in the proof, then the explanation\nwill be either long-winded or poorly written. If too much detail is omitted, then the proof\nmay not be convincing. Again it is important to keep the audience in mind. High school\nstudents require much more detail than do graduate students. A good rule of thumb for an\nargument in an introductory abstract algebra course is that it should be written to convince\none’s peers, whether those peers be other students or other readers of the text.\n\nLet us examine different types of statements. A statement could be as simple as “10/5 =\n2;” however, mathematicians are usually interested in more complex statements such as “If\np, then q,” where p and q are both statements. If certain statements are known or assumed\nto be true, we wish to know what we can say about other statements. Here p is called\nthe hypothesis and q is known as the conclusion. Consider the following statement: If\nax2 + bx+ c = 0 and a ̸= 0, then\n\nx =\n−b±\n\n√\nb2 − 4ac\n\n2a\n.\n\nThe hypothesis is ax2 + bx+ c = 0 and a ̸= 0; the conclusion is\n\nx =\n−b±\n\n√\nb2 − 4ac\n\n2a\n.\n\nNotice that the statement says nothing about whether or not the hypothesis is true. How-\never, if this entire statement is true and we can show that ax2 + bx + c = 0 with a ̸= 0 is\ntrue, then the conclusion must be true. A proof of this statement might simply be a series\nof equations:\n\nax2 + bx+ c = 0\n\nx2 +\nb\n\na\nx = − c\n\na\n\nx2 +\nb\n\na\nx+\n\n(\nb\n\n2a\n\n)2\n\n=\n\n(\nb\n\n2a\n\n)2\n\n− c\n\na(\nx+\n\nb\n\n2a\n\n)2\n\n=\nb2 − 4ac\n\n4a2\n\nx+\nb\n\n2a\n=\n\n±\n√\nb2 − 4ac\n\n2a\n\nx =\n−b±\n\n√\nb2 − 4ac\n\n2a\n.\n\nIf we can prove a statement true, then that statement is called a proposition. A\nproposition of major importance is called a theorem. Sometimes instead of proving a\ntheorem or proposition all at once, we break the proof down into modules; that is, we\nprove several supporting propositions, which are called lemmas, and use the results of\nthese propositions to prove the main result. If we can prove a proposition or a theorem,\nwe will often, with very little effort, be able to derive other related propositions called\ncorollaries.\n\nSome Cautions and Suggestions\nThere are several different strategies for proving propositions. In addition to using different\nmethods of proof, students often make some common mistakes when they are first learning\nhow to prove theorems. To aid students who are studying abstract mathematics for the\n\n\n\n1.2. SETS AND EQUIVALENCE RELATIONS 3\n\nfirst time, we list here some of the difficulties that they may encounter and some of the\nstrategies of proof available to them. It is a good idea to keep referring back to this list as\na reminder. (Other techniques of proof will become apparent throughout this chapter and\nthe remainder of the text.)\n\n• A theorem cannot be proved by example; however, the standard way to show that a\nstatement is not a theorem is to provide a counterexample.\n\n• Quantifiers are important. Words and phrases such as only, for all, for every, and for\nsome possess different meanings.\n\n• Never assume any hypothesis that is not explicitly stated in the theorem. You cannot\ntake things for granted.\n\n• Suppose you wish to show that an object exists and is unique. First show that there\nactually is such an object. To show that it is unique, assume that there are two such\nobjects, say r and s, and then show that r = s.\n\n• Sometimes it is easier to prove the contrapositive of a statement. Proving the state-\nment “If p, then q” is exactly the same as proving the statement “If not q, then not\np.”\n\n• Although it is usually better to find a direct proof of a theorem, this task can some-\ntimes be difficult. It may be easier to assume that the theorem that you are trying\nto prove is false, and to hope that in the course of your argument you are forced to\nmake some statement that cannot possibly be true.\n\nRemember that one of the main objectives of higher mathematics is proving theorems.\nTheorems are tools that make new and productive applications of mathematics possible. We\nuse examples to give insight into existing theorems and to foster intuitions as to what new\ntheorems might be true. Applications, examples, and proofs are tightly interconnected—\nmuch more so than they may seem at first appearance.\n\n1.2 Sets and Equivalence Relations\nSet Theory\nA set is a well-defined collection of objects; that is, it is defined in such a manner that we\ncan determine for any given object x whether or not x belongs to the set. The objects that\nbelong to a set are called its elements or members. We will denote sets by capital letters,\nsuch as A or X; if a is an element of the set A, we write a ∈ A.\n\nA set is usually specified either by listing all of its elements inside a pair of braces or\nby stating the property that determines whether or not an object x belongs to the set. We\nmight write\n\nX = {x1, x2, . . . , xn}\n\nfor a set containing elements x1, x2, . . . , xn or\n\nX = {x : x satisfies P}\n\nif each x in X satisfies a certain property P. For example, if E is the set of even positive\nintegers, we can describe E by writing either\n\nE = {2, 4, 6, . . .} or E = {x : x is an even integer and x > 0}.\n\n\n\n4 CHAPTER 1. PRELIMINARIES\n\nWe write 2 ∈ E when we want to say that 2 is in the set E, and −3 /∈ E to say that −3 is\nnot in the set E.\n\nSome of the more important sets that we will consider are the following:\n\nN = {n : n is a natural number} = {1, 2, 3, . . .};\nZ = {n : n is an integer} = {. . . ,−1, 0, 1, 2, . . .};\n\nQ = {r : r is a rational number} = {p/q : p, q ∈ Z where q ̸= 0};\nR = {x : x is a real number};\n\nC = {z : z is a complex number}.\n\nWe can find various relations between sets as well as perform operations on sets. A set\nA is a subset of B, written A ⊂ B or B ⊃ A, if every element of A is also an element of B.\nFor example,\n\n{4, 5, 8} ⊂ {2, 3, 4, 5, 6, 7, 8, 9}\n\nand\nN ⊂ Z ⊂ Q ⊂ R ⊂ C.\n\nTrivially, every set is a subset of itself. A set B is a proper subset of a set A if B ⊂ A but\nB ̸= A. If A is not a subset of B, we write A ̸⊂ B; for example, {4, 7, 9} ̸⊂ {2, 4, 5, 8, 9}.\nTwo sets are equal, written A = B, if we can show that A ⊂ B and B ⊂ A.\n\nIt is convenient to have a set with no elements in it. This set is called the empty set\nand is denoted by ∅. Note that the empty set is a subset of every set.\n\nTo construct new sets out of old sets, we can perform certain operations: the union\nA ∪B of two sets A and B is defined as\n\nA ∪B = {x : x ∈ A or x ∈ B};\n\nthe intersection of A and B is defined by\n\nA ∩B = {x : x ∈ A and x ∈ B}.\n\nIf A = {1, 3, 5} and B = {1, 2, 3, 9}, then\n\nA ∪B = {1, 2, 3, 5, 9} and A ∩B = {1, 3}.\n\nWe can consider the union and the intersection of more than two sets. In this case we write\nn∪\n\ni=1\n\nAi = A1 ∪ . . . ∪An\n\nand\nn∩\n\ni=1\n\nAi = A1 ∩ . . . ∩An\n\nfor the union and intersection, respectively, of the sets A1, . . . , An.\nWhen two sets have no elements in common, they are said to be disjoint; for example,\n\nif E is the set of even integers and O is the set of odd integers, then E and O are disjoint.\nTwo sets A and B are disjoint exactly when A ∩B = ∅.\n\nSometimes we will work within one fixed set U , called the universal set. For any set\nA ⊂ U , we define the complement of A, denoted by A′, to be the set\n\nA′ = {x : x ∈ U and x /∈ A}.\n\n\n\n1.2. SETS AND EQUIVALENCE RELATIONS 5\n\nWe define the difference of two sets A and B to be\n\nA \\B = A ∩B′ = {x : x ∈ A and x /∈ B}.\n\nExample 1.1. Let R be the universal set and suppose that\n\nA = {x ∈ R : 0 < x ≤ 3} and B = {x ∈ R : 2 ≤ x < 4}.\n\nThen\n\nA ∩B = {x ∈ R : 2 ≤ x ≤ 3}\nA ∪B = {x ∈ R : 0 < x < 4}\nA \\B = {x ∈ R : 0 < x < 2}\n\nA′ = {x ∈ R : x ≤ 0 or x > 3}.\n\nProposition 1.2. Let A, B, and C be sets. Then\n\n1. A ∪A = A, A ∩A = A, and A \\A = ∅;\n\n2. A ∪ ∅ = A and A ∩ ∅ = ∅;\n\n3. A ∪ (B ∪ C) = (A ∪B) ∪ C and A ∩ (B ∩ C) = (A ∩B) ∩ C;\n\n4. A ∪B = B ∪A and A ∩B = B ∩A;\n\n5. A ∪ (B ∩ C) = (A ∪B) ∩ (A ∪ C);\n\n6. A ∩ (B ∪ C) = (A ∩B) ∪ (A ∩ C).\n\nProof. We will prove (1) and (3) and leave the remaining results to be proven in the\nexercises.\n\n(1) Observe that\n\nA ∪A = {x : x ∈ A or x ∈ A}\n= {x : x ∈ A}\n= A\n\nand\n\nA ∩A = {x : x ∈ A and x ∈ A}\n= {x : x ∈ A}\n= A.\n\nAlso, A \\A = A ∩A′ = ∅.\n(3) For sets A, B, and C,\n\nA ∪ (B ∪ C) = A ∪ {x : x ∈ B or x ∈ C}\n= {x : x ∈ A or x ∈ B, or x ∈ C}\n= {x : x ∈ A or x ∈ B} ∪ C\n= (A ∪B) ∪ C.\n\nA similar argument proves that A ∩ (B ∩ C) = (A ∩B) ∩ C.\n\nTheorem 1.3 (De Morgan’s Laws). Let A and B be sets. Then\n\n\n\n6 CHAPTER 1. PRELIMINARIES\n\n1. (A ∪B)′ = A′ ∩B′;\n\n2. (A ∩B)′ = A′ ∪B′.\n\nProof. (1) If A∪B = ∅, then the theorem follows immediately since both A and B are the\nempty set. Otherwise, we must show that (A ∪B)′ ⊂ A′ ∩B′ and (A ∪B)′ ⊃ A′ ∩B′. Let\nx ∈ (A∪B)′. Then x /∈ A∪B. So x is neither in A nor in B, by the definition of the union\nof sets. By the definition of the complement, x ∈ A′ and x ∈ B′. Therefore, x ∈ A′ ∩ B′\n\nand we have (A ∪B)′ ⊂ A′ ∩B′.\nTo show the reverse inclusion, suppose that x ∈ A′ ∩B′. Then x ∈ A′ and x ∈ B′, and\n\nso x /∈ A and x /∈ B. Thus x /∈ A ∪B and so x ∈ (A ∪B)′. Hence, (A ∪B)′ ⊃ A′ ∩B′ and\nso (A ∪B)′ = A′ ∩B′.\n\nThe proof of (2) is left as an exercise.\n\nExample 1.4. Other relations between sets often hold true. For example,\n\n(A \\B) ∩ (B \\A) = ∅.\n\nTo see that this is true, observe that\n\n(A \\B) ∩ (B \\A) = (A ∩B′) ∩ (B ∩A′)\n\n= A ∩A′ ∩B ∩B′\n\n= ∅.\n\nCartesian Products and Mappings\nGiven sets A and B, we can define a new set A × B, called the Cartesian product of A\nand B, as a set of ordered pairs. That is,\n\nA×B = {(a, b) : a ∈ A and b ∈ B}.\n\nExample 1.5. If A = {x, y}, B = {1, 2, 3}, and C = ∅, then A×B is the set\n\n{(x, 1), (x, 2), (x, 3), (y, 1), (y, 2), (y, 3)}\n\nand\nA× C = ∅.\n\nWe define the Cartesian product of n sets to be\n\nA1 × · · · ×An = {(a1, . . . , an) : ai ∈ Ai for i = 1, . . . , n}.\n\nIf A = A1 = A2 = · · · = An, we often write An for A× · · · × A (where A would be written\nn times). For example, the set R3 consists of all of 3-tuples of real numbers.\n\nSubsets of A×B are called relations. We will define a mapping or function f ⊂ A×B\nfrom a set A to a set B to be the special type of relation where (a, b) ∈ f if for every element\na ∈ A there exists a unique element b ∈ B. Another way of saying this is that for every\nelement in A, f assigns a unique element in B. We usually write f : A → B or A f→ B.\nInstead of writing down ordered pairs (a, b) ∈ A× B, we write f(a) = b or f : a 7→ b. The\nset A is called the domain of f and\n\nf(A) = {f(a) : a ∈ A} ⊂ B\n\nis called the range or image of f . We can think of the elements in the function’s domain\nas input values and the elements in the function’s range as output values.\n\n\n\n1.2. SETS AND EQUIVALENCE RELATIONS 7\n\n1\n\n2\n\n3\n\na\n\nb\n\nc\n\n1\n\n2\n\n3\n\na\n\nb\n\nc\n\nA B\n\nA Bg\n\nf\n\nFigure 1.6: Mappings and relations\n\nExample 1.7. Suppose A = {1, 2, 3} and B = {a, b, c}. In Figure 1.6 we define relations f\nand g from A to B. The relation f is a mapping, but g is not because 1 ∈ A is not assigned\nto a unique element in B; that is, g(1) = a and g(1) = b.\n\nGiven a function f : A → B, it is often possible to write a list describing what the\nfunction does to each specific element in the domain. However, not all functions can be\ndescribed in this manner. For example, the function f : R → R that sends each real number\nto its cube is a mapping that must be described by writing f(x) = x3 or f : x 7→ x3.\n\nConsider the relation f : Q → Z given by f(p/q) = p. We know that 1/2 = 2/4, but\nis f(1/2) = 1 or 2? This relation cannot be a mapping because it is not well-defined. A\nrelation is well-defined if each element in the domain is assigned to a unique element in\nthe range.\n\nIf f : A→ B is a map and the image of f is B, i.e., f(A) = B, then f is said to be onto\nor surjective. In other words, if there exists an a ∈ A for each b ∈ B such that f(a) = b,\nthen f is onto. A map is one-to-one or injective if a1 ̸= a2 implies f(a1) ̸= f(a2).\nEquivalently, a function is one-to-one if f(a1) = f(a2) implies a1 = a2. A map that is both\none-to-one and onto is called bijective.\n\nExample 1.8. Let f : Z → Q be defined by f(n) = n/1. Then f is one-to-one but not\nonto. Define g : Q → Z by g(p/q) = p where p/q is a rational number expressed in its lowest\nterms with a positive denominator. The function g is onto but not one-to-one.\n\nGiven two functions, we can construct a new function by using the range of the first\nfunction as the domain of the second function. Let f : A→ B and g : B → C be mappings.\nDefine a new map, the composition of f and g from A to C, by (g ◦ f)(x) = g(f(x)).\n\n\n\n8 CHAPTER 1. PRELIMINARIES\n\nA B C\n\n1\n\n2\n\n3\n\na\n\nb\n\nc\n\nX\n\nY\n\nZ\n\nf g\n\nA C\n\n1\n\n2\n\n3\n\nX\n\nY\n\nZ\n\ng ◦ f\n\nFigure 1.9: Composition of maps\n\nExample 1.10. Consider the functions f : A → B and g : B → C that are defined in\nFigure 1.9 (top). The composition of these functions, g ◦f : A→ C, is defined in Figure 1.9\n(bottom).\n\nExample 1.11. Let f(x) = x2 and g(x) = 2x+ 5. Then\n\n(f ◦ g)(x) = f(g(x)) = (2x+ 5)2 = 4x2 + 20x+ 25\n\nand\n(g ◦ f)(x) = g(f(x)) = 2x2 + 5.\n\nIn general, order makes a difference; that is, in most cases f ◦ g ̸= g ◦ f .\n\nExample 1.12. Sometimes it is the case that f ◦ g = g ◦ f . Let f(x) = x3 and g(x) = 3\n√\nx.\n\nThen\n(f ◦ g)(x) = f(g(x)) = f( 3\n\n√\nx ) = ( 3\n\n√\nx )3 = x\n\nand\n(g ◦ f)(x) = g(f(x)) = g(x3) =\n\n3\n√\nx3 = x.\n\nExample 1.13. Given a 2× 2 matrix\n\nA =\n\n(\na b\n\nc d\n\n)\n,\n\nwe can define a map TA : R2 → R2 by\n\nTA(x, y) = (ax+ by, cx+ dy)\n\nfor (x, y) in R2. This is actually matrix multiplication; that is,(\na b\n\nc d\n\n)(\nx\n\ny\n\n)\n=\n\n(\nax+ by\n\ncx+ dy\n\n)\n.\n\nMaps from Rn to Rm given by matrices are called linear maps or linear transforma-\ntions.\n\n\n\n1.2. SETS AND EQUIVALENCE RELATIONS 9\n\nExample 1.14. Suppose that S = {1, 2, 3}. Define a map π : S → S by\n\nπ(1) = 2, π(2) = 1, π(3) = 3.\n\nThis is a bijective map. An alternative way to write π is(\n1 2 3\n\nπ(1) π(2) π(3)\n\n)\n=\n\n(\n1 2 3\n\n2 1 3\n\n)\n.\n\nFor any set S, a one-to-one and onto mapping π : S → S is called a permutation of S.\n\nTheorem 1.15. Let f : A→ B, g : B → C, and h : C → D. Then\n\n1. The composition of mappings is associative; that is, (h ◦ g) ◦ f = h ◦ (g ◦ f);\n\n2. If f and g are both one-to-one, then the mapping g ◦ f is one-to-one;\n\n3. If f and g are both onto, then the mapping g ◦ f is onto;\n\n4. If f and g are bijective, then so is g ◦ f .\n\nProof. We will prove (1) and (3). Part (2) is left as an exercise. Part (4) follows directly\nfrom (2) and (3).\n\n(1) We must show that\nh ◦ (g ◦ f) = (h ◦ g) ◦ f.\n\nFor a ∈ A we have\n\n(h ◦ (g ◦ f))(a) = h((g ◦ f)(a))\n= h(g(f(a)))\n\n= (h ◦ g)(f(a))\n= ((h ◦ g) ◦ f)(a).\n\n(3) Assume that f and g are both onto functions. Given c ∈ C, we must show that\nthere exists an a ∈ A such that (g ◦ f)(a) = g(f(a)) = c. However, since g is onto, there\nis an element b ∈ B such that g(b) = c. Similarly, there is an a ∈ A such that f(a) = b.\nAccordingly,\n\n(g ◦ f)(a) = g(f(a)) = g(b) = c.\n\nIf S is any set, we will use idS or id to denote the identity mapping from S to itself.\nDefine this map by id(s) = s for all s ∈ S. A map g : B → A is an inverse mapping\nof f : A → B if g ◦ f = idA and f ◦ g = idB; in other words, the inverse function of a\nfunction simply “undoes” the function. A map is said to be invertible if it has an inverse.\nWe usually write f−1 for the inverse of f .\n\nExample 1.16. The function f(x) = x3 has inverse f−1(x) = 3\n√\nx by Example 1.12.\n\nExample 1.17. The natural logarithm and the exponential functions, f(x) = lnx and\nf−1(x) = ex, are inverses of each other provided that we are careful about choosing domains.\nObserve that\n\nf(f−1(x)) = f(ex) = ln ex = x\n\nand\nf−1(f(x)) = f−1(lnx) = elnx = x\n\nwhenever composition makes sense.\n\n\n\n10 CHAPTER 1. PRELIMINARIES\n\nExample 1.18. Suppose that\n\nA =\n\n(\n3 1\n\n5 2\n\n)\n.\n\nThen A defines a map from R2 to R2 by\n\nTA(x, y) = (3x+ y, 5x+ 2y).\n\nWe can find an inverse map of TA by simply inverting the matrix A; that is, T−1\nA = TA−1 .\n\nIn this example,\n\nA−1 =\n\n(\n2 −1\n\n−5 3\n\n)\n;\n\nhence, the inverse map is given by\n\nT−1\nA (x, y) = (2x− y,−5x+ 3y).\n\nIt is easy to check that\n\nT−1\nA ◦ TA(x, y) = TA ◦ T−1\n\nA (x, y) = (x, y).\n\nNot every map has an inverse. If we consider the map\n\nTB(x, y) = (3x, 0)\n\ngiven by the matrix\n\nB =\n\n(\n3 0\n\n0 0\n\n)\n,\n\nthen an inverse map would have to be of the form\n\nT−1\nB (x, y) = (ax+ by, cx+ dy)\n\nand\n(x, y) = T ◦ T−1\n\nB (x, y) = (3ax+ 3by, 0)\n\nfor all x and y. Clearly this is impossible because y might not be 0.\n\nExample 1.19. Given the permutation\n\nπ =\n\n(\n1 2 3\n\n2 3 1\n\n)\non S = {1, 2, 3}, it is easy to see that the permutation defined by\n\nπ−1 =\n\n(\n1 2 3\n\n3 1 2\n\n)\nis the inverse of π. In fact, any bijective mapping possesses an inverse, as we will see in the\nnext theorem.\n\nTheorem 1.20. A mapping is invertible if and only if it is both one-to-one and onto.\n\nProof. Suppose first that f : A → B is invertible with inverse g : B → A. Then g ◦ f =\nidA is the identity map; that is, g(f(a)) = a. If a1, a2 ∈ A with f(a1) = f(a2), then\na1 = g(f(a1)) = g(f(a2)) = a2. Consequently, f is one-to-one. Now suppose that b ∈ B.\nTo show that f is onto, it is necessary to find an a ∈ A such that f(a) = b, but f(g(b)) = b\nwith g(b) ∈ A. Let a = g(b).\n\nConversely, let f be bijective and let b ∈ B. Since f is onto, there exists an a ∈ A such\nthat f(a) = b. Because f is one-to-one, a must be unique. Define g by letting g(b) = a. We\nhave now constructed the inverse of f .\n\n\n\n1.2. SETS AND EQUIVALENCE RELATIONS 11\n\nEquivalence Relations and Partitions\nA fundamental notion in mathematics is that of equality. We can generalize equality with\nequivalence relations and equivalence classes. An equivalence relation on a set X is a\nrelation R ⊂ X ×X such that\n\n• (x, x) ∈ R for all x ∈ X (reflexive property);\n\n• (x, y) ∈ R implies (y, x) ∈ R (symmetric property);\n\n• (x, y) and (y, z) ∈ R imply (x, z) ∈ R (transitive property).\n\nGiven an equivalence relation R on a set X, we usually write x ∼ y instead of (x, y) ∈ R.\nIf the equivalence relation already has an associated notation such as =, ≡, or ∼=, we will\nuse that notation.\n\nExample 1.21. Let p, q, r, and s be integers, where q and s are nonzero. Define p/q ∼ r/s\nif ps = qr. Clearly ∼ is reflexive and symmetric. To show that it is also transitive, suppose\nthat p/q ∼ r/s and r/s ∼ t/u, with q, s, and u all nonzero. Then ps = qr and ru = st.\nTherefore,\n\npsu = qru = qst.\n\nSince s ̸= 0, pu = qt. Consequently, p/q ∼ t/u.\n\nExample 1.22. Suppose that f and g are differentiable functions on R. We can define an\nequivalence relation on such functions by letting f(x) ∼ g(x) if f ′(x) = g′(x). It is clear that\n∼ is both reflexive and symmetric. To demonstrate transitivity, suppose that f(x) ∼ g(x)\nand g(x) ∼ h(x). From calculus we know that f(x)−g(x) = c1 and g(x)−h(x) = c2, where\nc1 and c2 are both constants. Hence,\n\nf(x)− h(x) = (f(x)− g(x)) + (g(x)− h(x)) = c1 − c2\n\nand f ′(x)− h′(x) = 0. Therefore, f(x) ∼ h(x).\n\nExample 1.23. For (x1, y1) and (x2, y2) in R2, define (x1, y1) ∼ (x2, y2) if x21+y21 = x22+y\n2\n2.\n\nThen ∼ is an equivalence relation on R2.\n\nExample 1.24. Let A and B be 2× 2 matrices with entries in the real numbers. We can\ndefine an equivalence relation on the set of 2× 2 matrices, by saying A ∼ B if there exists\nan invertible matrix P such that PAP−1 = B. For example, if\n\nA =\n\n(\n1 2\n\n−1 1\n\n)\nand B =\n\n(\n−18 33\n\n−11 20\n\n)\n,\n\nthen A ∼ B since PAP−1 = B for\n\nP =\n\n(\n2 5\n\n1 3\n\n)\n.\n\nLet I be the 2× 2 identity matrix; that is,\n\nI =\n\n(\n1 0\n\n0 1\n\n)\n.\n\nThen IAI−1 = IAI = A; therefore, the relation is reflexive. To show symmetry, suppose\nthat A ∼ B. Then there exists an invertible matrix P such that PAP−1 = B. So\n\nA = P−1BP = P−1B(P−1)−1.\n\n\n\n12 CHAPTER 1. PRELIMINARIES\n\nFinally, suppose that A ∼ B and B ∼ C. Then there exist invertible matrices P and Q\nsuch that PAP−1 = B and QBQ−1 = C. Since\n\nC = QBQ−1 = QPAP−1Q−1 = (QP )A(QP )−1,\n\nthe relation is transitive. Two matrices that are equivalent in this manner are said to be\nsimilar.\n\nA partition P of a set X is a collection of nonempty sets X1, X2, . . . such that Xi∩Xj =\n∅ for i ̸= j and\n\n∪\nkXk = X. Let ∼ be an equivalence relation on a set X and let x ∈ X.\n\nThen [x] = {y ∈ X : y ∼ x} is called the equivalence class of x. We will see that\nan equivalence relation gives rise to a partition via equivalence classes. Also, whenever\na partition of a set exists, there is some natural underlying equivalence relation, as the\nfollowing theorem demonstrates.\n\nTheorem 1.25. Given an equivalence relation ∼ on a set X, the equivalence classes of X\nform a partition of X. Conversely, if P = {Xi} is a partition of a set X, then there is an\nequivalence relation on X with equivalence classes Xi.\n\nProof. Suppose there exists an equivalence relation ∼ on the set X. For any x ∈ X, the\nreflexive property shows that x ∈ [x] and so [x] is nonempty. Clearly X =\n\n∪\nx∈X [x]. Now\n\nlet x, y ∈ X. We need to show that either [x] = [y] or [x] ∩ [y] = ∅. Suppose that the\nintersection of [x] and [y] is not empty and that z ∈ [x] ∩ [y]. Then z ∼ x and z ∼ y. By\nsymmetry and transitivity x ∼ y; hence, [x] ⊂ [y]. Similarly, [y] ⊂ [x] and so [x] = [y].\nTherefore, any two equivalence classes are either disjoint or exactly the same.\n\nConversely, suppose that P = {Xi} is a partition of a set X. Let two elements be\nequivalent if they are in the same partition. Clearly, the relation is reflexive. If x is in the\nsame partition as y, then y is in the same partition as x, so x ∼ y implies y ∼ x. Finally,\nif x is in the same partition as y and y is in the same partition as z, then x must be in the\nsame partition as z, and transitivity holds.\n\nCorollary 1.26. Two equivalence classes of an equivalence relation are either disjoint or\nequal.\n\nLet us examine some of the partitions given by the equivalence classes in the last set of\nexamples.\n\nExample 1.27. In the equivalence relation in Example 1.21, two pairs of integers, (p, q)\nand (r, s), are in the same equivalence class when they reduce to the same fraction in its\nlowest terms.\n\nExample 1.28. In the equivalence relation in Example 1.22, two functions f(x) and g(x)\nare in the same partition when they differ by a constant.\n\nExample 1.29. We defined an equivalence class on R2 by (x1, y1) ∼ (x2, y2) if x21 + y21 =\nx22 + y22. Two pairs of real numbers are in the same partition when they lie on the same\ncircle about the origin.\n\nExample 1.30. Let r and s be two integers and suppose that n ∈ N. We say that r is\ncongruent to s modulo n, or r is congruent to s mod n, if r − s is evenly divisible by n;\nthat is, r − s = nk for some k ∈ Z. In this case we write r ≡ s (mod n). For example,\n41 ≡ 17 (mod 8) since 41 − 17 = 24 is divisible by 8. We claim that congruence modulo\nn forms an equivalence relation of Z. Certainly any integer r is equivalent to itself since\nr − r = 0 is divisible by n. We will now show that the relation is symmetric. If r ≡ s\n\n\n\n1.3. EXERCISES 13\n\n(mod n), then r − s = −(s − r) is divisible by n. So s − r is divisible by n and s ≡ r\n(mod n). Now suppose that r ≡ s (mod n) and s ≡ t (mod n). Then there exist integers\nk and l such that r − s = kn and s− t = ln. To show transitivity, it is necessary to prove\nthat r − t is divisible by n. However,\n\nr − t = r − s+ s− t = kn+ ln = (k + l)n,\n\nand so r − t is divisible by n.\nIf we consider the equivalence relation established by the integers modulo 3, then\n\n[0] = {. . . ,−3, 0, 3, 6, . . .},\n[1] = {. . . ,−2, 1, 4, 7, . . .},\n[2] = {. . . ,−1, 2, 5, 8, . . .}.\n\nNotice that [0] ∪ [1] ∪ [2] = Z and also that the sets are disjoint. The sets [0], [1], and [2]\nform a partition of the integers.\n\nThe integers modulo n are a very important example in the study of abstract algebra\nand will become quite useful in our investigation of various algebraic structures such as\ngroups and rings. In our discussion of the integers modulo n we have actually assumed a\nresult known as the division algorithm, which will be stated and proved in Chapter 2.\n\n1.3 Exercises\n1. Suppose that\n\nA = {x : x ∈ N and x is even},\nB = {x : x ∈ N and x is prime},\nC = {x : x ∈ N and x is a multiple of 5}.\n\nDescribe each of the following sets.\n\n(a) A ∩B\n(b) B ∩ C\n\n(c) A ∪B\n(d) A ∩ (B ∪ C)\n\n2. If A = {a, b, c}, B = {1, 2, 3}, C = {x}, and D = ∅, list all of the elements in each of the\nfollowing sets.\n\n(a) A×B\n\n(b) B ×A\n\n(c) A×B × C\n\n(d) A×D\n\n3. Find an example of two nonempty sets A and B for which A×B = B ×A is true.\n\n4. Prove A ∪ ∅ = A and A ∩ ∅ = ∅.\n\n5. Prove A ∪B = B ∪A and A ∩B = B ∩A.\n\n6. Prove A ∪ (B ∩ C) = (A ∪B) ∩ (A ∪ C).\n\n7. Prove A ∩ (B ∪ C) = (A ∩B) ∪ (A ∩ C).\n\n8. Prove A ⊂ B if and only if A ∩B = A.\n\n\n\n14 CHAPTER 1. PRELIMINARIES\n\n9. Prove (A ∩B)′ = A′ ∪B′.\n\n10. Prove A ∪B = (A ∩B) ∪ (A \\B) ∪ (B \\A).\n\n11. Prove (A ∪B)× C = (A× C) ∪ (B × C).\n\n12. Prove (A ∩B) \\B = ∅.\n\n13. Prove (A ∪B) \\B = A \\B.\n\n14. Prove A \\ (B ∪ C) = (A \\B) ∩ (A \\ C).\n\n15. Prove A ∩ (B \\ C) = (A ∩B) \\ (A ∩ C).\n\n16. Prove (A \\B) ∪ (B \\A) = (A ∪B) \\ (A ∩B).\n\n17. Which of the following relations f : Q → Q define a mapping? In each case, supply a\nreason why f is or is not a mapping.\n\n(a) f(p/q) =\np+ 1\n\np− 2\n\n(b) f(p/q) =\n3p\n\n3q\n\n(c) f(p/q) =\np+ q\n\nq2\n\n(d) f(p/q) =\n3p2\n\n7q2\n− p\n\nq\n\n18. Determine which of the following functions are one-to-one and which are onto. If the\nfunction is not onto, determine its range.\n(a) f : R → R defined by f(x) = ex\n\n(b) f : Z → Z defined by f(n) = n2 + 3\n\n(c) f : R → R defined by f(x) = sinx\n(d) f : Z → Z defined by f(x) = x2\n\n19. Let f : A→ B and g : B → C be invertible mappings; that is, mappings such that f−1\n\nand g−1 exist. Show that (g ◦ f)−1 = f−1 ◦ g−1.\n\n20.\n(a) Define a function f : N → N that is one-to-one but not onto.\n(b) Define a function f : N → N that is onto but not one-to-one.\n\n21. Prove the relation defined on R2 by (x1, y1) ∼ (x2, y2) if x21 + y21 = x22 + y22 is an\nequivalence relation.\n\n22. Let f : A→ B and g : B → C be maps.\n(a) If f and g are both one-to-one functions, show that g ◦ f is one-to-one.\n(b) If g ◦ f is onto, show that g is onto.\n(c) If g ◦ f is one-to-one, show that f is one-to-one.\n(d) If g ◦ f is one-to-one and f is onto, show that g is one-to-one.\n(e) If g ◦ f is onto and g is one-to-one, show that f is onto.\n\n23. Define a function on the real numbers by\n\nf(x) =\nx+ 1\n\nx− 1\n.\n\nWhat are the domain and range of f? What is the inverse of f? Compute f ◦ f−1 and\nf−1 ◦ f .\n\n\n\n1.4. REFERENCES AND SUGGESTED READINGS 15\n\n24. Let f : X → Y be a map with A1, A2 ⊂ X and B1, B2 ⊂ Y .\n(a) Prove f(A1 ∪A2) = f(A1) ∪ f(A2).\n(b) Prove f(A1 ∩A2) ⊂ f(A1) ∩ f(A2). Give an example in which equality fails.\n(c) Prove f−1(B1 ∪B2) = f−1(B1) ∪ f−1(B2), where\n\nf−1(B) = {x ∈ X : f(x) ∈ B}.\n\n(d) Prove f−1(B1 ∩B2) = f−1(B1) ∩ f−1(B2).\n(e) Prove f−1(Y \\B1) = X \\ f−1(B1).\n\n25. Determine whether or not the following relations are equivalence relations on the given\nset. If the relation is an equivalence relation, describe the partition given by it. If the\nrelation is not an equivalence relation, state why it fails to be one.\n\n(a) x ∼ y in R if x ≥ y\n\n(b) m ∼ n in Z if mn > 0\n\n(c) x ∼ y in R if |x− y| ≤ 4\n\n(d) m ∼ n in Z if m ≡ n (mod 6)\n\n26. Define a relation ∼ on R2 by stating that (a, b) ∼ (c, d) if and only if a2 + b2 ≤ c2 + d2.\nShow that ∼ is reflexive and transitive but not symmetric.\n\n27. Show that an m× n matrix gives rise to a well-defined map from Rn to Rm.\n\n28. Find the error in the following argument by providing a counterexample. “The reflexive\nproperty is redundant in the axioms for an equivalence relation. If x ∼ y, then y ∼ x by\nthe symmetric property. Using the transitive property, we can deduce that x ∼ x.”\n\n29. (Projective Real Line) Define a relation on R2 \\ {(0, 0)} by letting (x1, y1) ∼ (x2, y2) if\nthere exists a nonzero real number λ such that (x1, y1) = (λx2, λy2). Prove that ∼ defines\nan equivalence relation on R2 \\(0, 0). What are the corresponding equivalence classes? This\nequivalence relation defines the projective line, denoted by P(R), which is very important\nin geometry.\n\n1.4 References and Suggested Readings\n[1] Artin, M. Abstract Algebra. 2nd ed. Pearson, Upper Saddle River, NJ, 2011.\n[2] Childs, L. A Concrete Introduction to Higher Algebra. 2nd ed. Springer-Verlag, New\n\nYork, 1995.\n[3] Dummit, D. and Foote, R. Abstract Algebra. 3rd ed. Wiley, New York, 2003.\n[4] Ehrlich, G. Fundamental Concepts of Algebra. PWS-KENT, Boston, 1991.\n[5] Fraleigh, J. B. A First Course in Abstract Algebra. 7th ed. Pearson, Upper Saddle\n\nRiver, NJ, 2003.\n[6] Gallian, J. A. Contemporary Abstract Algebra. 7th ed. Brooks/Cole, Belmont, CA,\n\n2009.\n[7] Halmos, P. Naive Set Theory. Springer, New York, 1991. One of the best references\n\nfor set theory.\n[8] Herstein, I. N. Abstract Algebra. 3rd ed. Wiley, New York, 1996.\n[9] Hungerford, T. W. Algebra. Springer, New York, 1974. One of the standard graduate\n\nalgebra texts.\n\n\n\n16 CHAPTER 1. PRELIMINARIES\n\n[10] Lang, S. Algebra. 3rd ed. Springer, New York, 2002. Another standard graduate text.\n[11] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\n[12] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\n[13] Nickelson, W. K. Introduction to Abstract Algebra. 3rd ed. Wiley, New York, 2006.\n[14] Solow, D. How to Read and Do Proofs. 5th ed. Wiley, New York, 2009.\n[15] van der Waerden, B. L. A History of Algebra. Springer-Verlag, New York, 1985. An\n\naccount of the historical development of algebra.\n\n1.5 Sage\nSage is a powerful system for studying and exploring many different areas of mathematics.\nIn this textbook, you will study a variety of algebraic structures, such as groups, rings and\nfields. Sage does an excellent job of implementing many features of these objects as we will\nsee in the chapters ahead. But here and now, in this initial chapter, we will concentrate on\na few general ways of getting the most out of working with Sage.\n\nYou may use Sage several different ways. It may be used as a command-line program\nwhen installed on your own computer. Or it might be a web application such as the\nSageMathCloud. Our writing will assume that you are reading this as a worksheet within\nthe Sage Notebook (a web browser interface), or this is a section of the entire book presented\nas web pages, and you are employing the Sage Cell Server via those pages. After the first\nfew chapters the explanations should work equally well for whatever vehicle you use to\nexecute Sage commands.\n\nExecuting Sage Commands\n\nMost of your interaction will be by typing commands into a compute cell. If you are reading\nthis in the Sage Notebook or as a webpage version of the book, then you will see a compute\ncell just below this paragraph. Click once inside the compute cell and if you are in the Sage\nNotebook, you will get a more distinctive border around it, a blinking cursor inside, plus a\ncute little “evaluate” link below.\n\nAt the cursor, type 2+2 and then click on the evaluate link. Did a 4 appear below the\ncell? If so, you have successfully sent a command off for Sage to evaluate and you have\nreceived back the (correct) answer.\n\nHere is another compute cell. Try evaluating the command factorial(300) here.\nHmmmmm. That is quite a big integer! If you see slashes at the end of each line, this\n\nmeans the result is continued onto the next line, since there are 615 total digits in the result.\nTo make new compute cells in the Sage Notebook (only), hover your mouse just above\n\nanother compute cell, or just below some output from a compute cell. When you see a\nskinny blue bar across the width of your worksheet, click and you will open up a new\ncompute cell, ready for input. Note that your worksheet will remember any calculations\nyou make, in the order you make them, no matter where you put the cells, so it is best to\nstay organized and add new cells at the bottom.\n\nTry placing your cursor just below the monstrous value of 300! that you have. Click on\nthe blue bar and try another factorial computation in the new compute cell.\n\nEach compute cell will show output due to only the very last command in the cell. Try\nto predict the following output before evaluating the cell.\n\na = 10\nb = 6\n\n\n\n1.5. SAGE 17\n\nb = b - 10\na = a + 20\na\n\n30\n\nThe following compute cell will not print anything since the one command does not\ncreate output. But it will have an effect, as you can see when you execute the subsequent\ncell. Notice how this uses the value of b from above. Execute this compute cell once.\nExactly once. Even if it appears to do nothing. If you execute the cell twice, your credit\ncard may be charged twice.\n\nb = b + 50\n\nNow execute this cell, which will produce some output.\nb + 20\n\n66\n\nSo b came into existence as 6. We subtracted 10 immediately afterward. Then a subse-\nquent cell added 50. This assumes you executed this cell exactly once! In the last cell we\ncreate b+20 (but do not save it) and it is this value (66) that is output, while b is still 46.\n\nYou can combine several commands on one line with a semi-colon. This is a great way\nto get multiple outputs from a compute cell. The syntax for building a matrix should be\nsomewhat obvious when you see the output, but if not, it is not particularly important to\nunderstand now.\n\nA = matrix ([[3, 1], [5 ,2]]); A\n\n[3 1]\n[5 2]\n\nprint A; print ; A.inverse ()\n\n[3 1]\n[5 2]\n<BLANKLINE >\n[ 2 -1]\n[-5 3]\n\nImmediate Help\nSome commands in Sage are “functions,” an example is factorial() above. Other commands\nare “methods” of an object and are like characteristics of objects, an example is .inverse()\n\nas a method of a matrix. Once you know how to create an object (such as a matrix), then\nit is easy to see all the available methods. Write the name of the object, place a period\n(“dot”) and hit the TAB key. If you have A defined from above, then the compute cell below\nis ready to go, click into it and then hit TAB (not “evaluate”!). You should get a long list\nof possible methods.\n\nA.\n\nTo get some help on how to use a method with an object, write its name after a dot (with\nno parentheses) and then use a question-mark and hit TAB. (Hit the escape key “ESC” to\nremove the list, or click on the text for a method.)\n\n\n\n18 CHAPTER 1. PRELIMINARIES\n\nA.inverse?\n\nWith one more question-mark and a TAB you can see the actual computer instructions\nthat were programmed into Sage to make the method work, once you scoll down past the\ndocumentation delimited by the triple quotes ("""):\n\nA.inverse ??\n\nIt is worthwhile to see what Sage does when there is an error. You will probably see a\nlot of these at first, and initially they will be a bit intimidating. But with time, you will\nlearn how to use them effectively and you will also become more proficient with Sage and\nsee them less often. Execute the compute cell below, it asks for the inverse of a matrix that\nhas no inverse. Then reread the commentary.\n\nB = matrix ([[2, 20], [5, 50]])\nB.inverse ()\n\nTraceback (most recent call last):\n...\nZeroDivisionError: Matrix is singular\n\nClick just to the left of the error message to expand it fully (another click hides it totally,\nand a third click brings back the abbreviated form). Read the bottom of an error message\nfirst, it is your best explanation. Here a ZeroDivisionError is not 100% accurate, but is\nclose. The matrix is not invertible, not dissimilar to how we cannot divide scalars by zero.\nThe remainder of the message begins at the top showing were the error first happened in\nyour code and then the various places where intermediate functions were called, until the\nactual piece of Sage where the problem occurred. Sometimes this information will give you\nsome clues, sometimes it is totally undecipherable. So do not let it scare you if it seems\nmysterious, but do remember to always read the last line first, then go back and read the\nfirst few lines for something that looks like your code.\n\nAnnotating Your Work\nIt is easy to comment on your work when you use the Sage Notebook. (The following only\napplies if you are reading this within a Sage Notebook. If you are not, then perhaps you\ncan go open up a worksheet in the Sage Notebook and experiment there.) You can open up\na small word-processor by hovering your mouse until you get a skinny blue bar again, but\nnow when you click, also hold the SHIFT key at the same time. Experiment with fonts,\ncolors, bullet lists, etc and then click the “Save changes” button to exit. Double-click on\nyour text if you need to go back and edit it later.\n\nOpen the word-processor again to create a new bit of text (maybe next to the empty\ncompute cell just below). Type all of the following exactly:\n\nPythagorean Theorem: $c^2=a^2+b^2$\n\nand save your changes. The symbols between the dollar signs are written according to\nthe mathematical typesetting language known as TEX — cruise the internet to learn more\nabout this very popular tool. (Well, it is extremely popular among mathematicians and\nphysical scientists.)\n\nLists\nMuch of our interaction with sets will be through Sage lists. These are not really sets — they\nallow duplicates, and order matters. But they are so close to sets, and so easy and powerful\n\n\n\n1.5. SAGE 19\n\nto use that we will use them regularly. We will use a fun made-up list for practice, the\nquote marks mean the items are just text, with no special mathematical meaning. Execute\nthese compute cells as we work through them.\n\nzoo = [ \' snake \' , \' parrot \' , \' elephant \' , \' baboon \' , \' beetle \' ]\nzoo\n\n[ \' snake \' , \' parrot \' , \' elephant \' , \' baboon \' , \' beetle \' ]\n\nSo the square brackets define the boundaries of our list, commas separate items, and we\ncan give the list a name. To work with just one element of the list, we use the name and\na pair of brackets with an index. Notice that lists have indices that begin counting at zero.\nThis will seem odd at first and will seem very natural later.\n\nzoo [2]\n\n\' elephant \'\n\nWe can add a new creature to the zoo, it is joined up at the far right end.\nzoo.append( \' ostrich \' ); zoo\n\n[ \' snake \' , \' parrot \' , \' elephant \' , \' baboon \' , \' beetle \' , \' ostrich \' ]\n\nWe can remove a creature.\nzoo.remove( \' parrot \' )\nzoo\n\n[ \' snake \' , \' elephant \' , \' baboon \' , \' beetle \' , \' ostrich \' ]\n\nWe can extract a sublist. Here we start with element 1 (the elephant) and go all the\nway up to, but not including, element 3 (the beetle). Again a bit odd, but it will feel natural\nlater. For now, notice that we are extracting two elements of the lists, exactly 3 − 1 = 2\nelements.\n\nmammals = zoo [1:3]\nmammals\n\n[ \' elephant \' , \' baboon \' ]\n\nOften we will want to see if two lists are equal. To do that we will need to sort a list\nfirst. A function creates a new, sorted list, leaving the original alone. So we need to save\nthe new one with a new name.\n\nnewzoo = sorted(zoo)\nnewzoo\n\n[ \' baboon \' , \' beetle \' , \' elephant \' , \' ostrich \' , \' snake \' ]\n\nzoo.sort()\nzoo\n\n[ \' baboon \' , \' beetle \' , \' elephant \' , \' ostrich \' , \' snake \' ]\n\nNotice that if you run this last compute cell your zoo has changed and some commands\nabove will not necessarily execute the same way. If you want to experiment, go all the way\nback to the first creation of the zoo and start executing cells again from there with a fresh\nzoo.\n\n\n\n20 CHAPTER 1. PRELIMINARIES\n\nA construction called a list comprehension is especially powerful, especially since it\nalmost exactly mirrors notation we use to describe sets. Suppose we want to form the plural\nof the names of the creatures in our zoo. We build a new list, based on all of the elements\nof our old list.\n\nplurality_zoo = [animal+ \' s \' for animal in zoo]\nplurality_zoo\n\n[ \' baboons \' , \' beetles \' , \' elephants \' , \' ostrichs \' , \' snakes \' ]\n\nAlmost like it says: we add an “s” to each animal name, for each animal in the zoo, and\nplace them in a new list. Perfect. (Except for getting the plural of “ostrich” wrong.)\n\nLists of Integers\nOne final type of list, with numbers this time. The srange() function will create lists of\nintegers. (The “s” in the name stands for “Sage” and so will produce integers that Sage\nunderstands best. Many early difficulties with Sage and group theory can be alleviated by\nusing only this command to create lists of integers.) In its simplest form an invocation\nlike srange(12) will create a list of 12 integers, starting at zero and working up to, but not\nincluding, 12. Does this sound familiar?\n\ndozen = srange (12); dozen\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\nHere are two other forms, that you should be able to understand by studying the exam-\nples.\n\nteens = srange (13, 20); teens\n\n[13, 14, 15, 16, 17, 18, 19]\n\ndecades = srange (1900, 2000, 10); decades\n\n[1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990]\n\nSaving and Sharing Your Work\nThere is a “Save” button in the upper-right corner of the Sage Notebook. This will save a\ncurrent copy of your worksheet that you can retrieve your work from within your notebook\nagain later, though you have to re-execute all the cells when you re-open the worksheet.\n\nThere is also a “File” drop-down list, on the left, just above your very top compute cell\n(not be confused with your browser’s File menu item!). You will see a choice here labeled\n“Save worksheet to a file...” When you do this, you are creating a copy of your worksheet\nin the sws format (short for “Sage WorkSheet”). You can email this file, or post it on a\nwebsite, for other Sage users and they can use the “Upload” link on the homepage of their\nnotebook to incorporate a copy of your worksheet into their notebook.\n\nThere are other ways to share worksheets that you can experiment with, but this gives\nyou one way to share any worksheet with anybody almost anywhere.\n\nWe have covered a lot here in this section, so come back later to pick up tidbits you\nmight have missed. There are also many more features in the Sage Notebook that we have\nnot covered.\n\n\n\n1.6. SAGE EXERCISES 21\n\n1.6 Sage Exercises\n1. This exercise is just about making sure you know how to use Sage. Login to a Sage\nNotebook server and create a new worksheet. Do some non-trivial computation, maybe\na pretty plot or some gruesome numerical computation to an insane precision. Create an\ninteresting list and experiment with it some. Maybe include some nicely formatted text or\nTEX using the included mini-word-processor of the Sage Notebook (hover until a blue bar\nappears between cells and then shift-click).\nUse whatever mechanism your instructor has in place for submitting your work. Or save\nyour worksheet and then trade worksheets via email (or another electronic method) with a\nclassmate.\n\n\n\n2\n\nThe Integers\n\nThe integers are the building blocks of mathematics. In this chapter we will investigate\nthe fundamental properties of the integers, including mathematical induction, the division\nalgorithm, and the Fundamental Theorem of Arithmetic.\n\n2.1 Mathematical Induction\nSuppose we wish to show that\n\n1 + 2 + · · ·+ n =\nn(n+ 1)\n\n2\n\nfor any natural number n. This formula is easily verified for small numbers such as n = 1,\n2, 3, or 4, but it is impossible to verify for all natural numbers on a case-by-case basis. To\nprove the formula true in general, a more generic method is required.\n\nSuppose we have verified the equation for the first n cases. We will attempt to show\nthat we can generate the formula for the (n+ 1)th case from this knowledge. The formula\nis true for n = 1 since\n\n1 =\n1(1 + 1)\n\n2\n.\n\nIf we have verified the first n cases, then\n\n1 + 2 + · · ·+ n+ (n+ 1) =\nn(n+ 1)\n\n2\n+ n+ 1\n\n=\nn2 + 3n+ 2\n\n2\n\n=\n(n+ 1)[(n+ 1) + 1]\n\n2\n.\n\nThis is exactly the formula for the (n+ 1)th case.\nThis method of proof is known as mathematical induction. Instead of attempting to\n\nverify a statement about some subset S of the positive integers N on a case-by-case basis, an\nimpossible task if S is an infinite set, we give a specific proof for the smallest integer being\nconsidered, followed by a generic argument showing that if the statement holds for a given\ncase, then it must also hold for the next case in the sequence. We summarize mathematical\ninduction in the following axiom.\n\nPrinciple 2.1 (First Principle of Mathematical Induction). Let S(n) be a statement about\nintegers for n ∈ N and suppose S(n0) is true for some integer n0. If for all integers k with\nk ≥ n0, S(k) implies that S(k + 1) is true, then S(n) is true for all integers n greater than\nor equal to n0.\n\n22\n\n\n\n2.1. MATHEMATICAL INDUCTION 23\n\nExample 2.2. For all integers n ≥ 3, 2n > n+ 4. Since\n\n8 = 23 > 3 + 4 = 7,\n\nthe statement is true for n0 = 3. Assume that 2k > k + 4 for k ≥ 3. Then 2k+1 = 2 · 2k >\n2(k + 4). But\n\n2(k + 4) = 2k + 8 > k + 5 = (k + 1) + 4\n\nsince k is positive. Hence, by induction, the statement holds for all integers n ≥ 3.\n\nExample 2.3. Every integer 10n+1 + 3 · 10n + 5 is divisible by 9 for n ∈ N. For n = 1,\n\n101+1 + 3 · 10 + 5 = 135 = 9 · 15\n\nis divisible by 9. Suppose that 10k+1 + 3 · 10k + 5 is divisible by 9 for k ≥ 1. Then\n\n10(k+1)+1 + 3 · 10k+1 + 5 = 10k+2 + 3 · 10k+1 + 50− 45\n\n= 10(10k+1 + 3 · 10k + 5)− 45\n\nis divisible by 9.\n\nExample 2.4. We will prove the binomial theorem using mathematical induction; that is,\n\n(a+ b)n =\n\nn∑\nk=0\n\n(\nn\n\nk\n\n)\nakbn−k,\n\nwhere a and b are real numbers, n ∈ N, and\n\n(\nn\n\nk\n\n)\n=\n\nn!\n\nk!(n− k)!\n\nis the binomial coefficient. We first show that(\nn+ 1\n\nk\n\n)\n=\n\n(\nn\n\nk\n\n)\n+\n\n(\nn\n\nk − 1\n\n)\n.\n\nThis result follows from(\nn\n\nk\n\n)\n+\n\n(\nn\n\nk − 1\n\n)\n=\n\nn!\n\nk!(n− k)!\n+\n\nn!\n\n(k − 1)!(n− k + 1)!\n\n=\n(n+ 1)!\n\nk!(n+ 1− k)!\n\n=\n\n(\nn+ 1\n\nk\n\n)\n.\n\nIf n = 1, the binomial theorem is easy to verify. Now assume that the result is true for n\n\n\n\n24 CHAPTER 2. THE INTEGERS\n\ngreater than or equal to 1. Then\n\n(a+ b)n+1 = (a+ b)(a+ b)n\n\n= (a+ b)\n\n(\nn∑\n\nk=0\n\n(\nn\n\nk\n\n)\nakbn−k\n\n)\n\n=\nn∑\n\nk=0\n\n(\nn\n\nk\n\n)\nak+1bn−k +\n\nn∑\nk=0\n\n(\nn\n\nk\n\n)\nakbn+1−k\n\n= an+1 +\nn∑\n\nk=1\n\n(\nn\n\nk − 1\n\n)\nakbn+1−k +\n\nn∑\nk=1\n\n(\nn\n\nk\n\n)\nakbn+1−k + bn+1\n\n= an+1 +\n\nn∑\nk=1\n\n[(\nn\n\nk − 1\n\n)\n+\n\n(\nn\n\nk\n\n)]\nakbn+1−k + bn+1\n\n=\nn+1∑\nk=0\n\n(\nn+ 1\n\nk\n\n)\nakbn+1−k.\n\nWe have an equivalent statement of the Principle of Mathematical Induction that is\noften very useful.\n\nPrinciple 2.5 (Second Principle of Mathematical Induction). Let S(n) be a statement\nabout integers for n ∈ N and suppose S(n0) is true for some integer n0. If S(n0), S(n0 +\n1), . . . , S(k) imply that S(k+1) for k ≥ n0, then the statement S(n) is true for all integers\nn ≥ n0.\n\nA nonempty subset S of Z is well-ordered if S contains a least element. Notice that\nthe set Z is not well-ordered since it does not contain a smallest element. However, the\nnatural numbers are well-ordered.\n\nPrinciple 2.6 (Principle of Well-Ordering). Every nonempty subset of the natural numbers\nis well-ordered.\n\nThe Principle of Well-Ordering is equivalent to the Principle of Mathematical Induction.\n\nLemma 2.7. The Principle of Mathematical Induction implies that 1 is the least positive\nnatural number.\n\nProof. Let S = {n ∈ N : n ≥ 1}. Then 1 ∈ S. Now assume that n ∈ S; that is, n ≥ 1.\nSince n + 1 ≥ 1, n + 1 ∈ S; hence, by induction, every natural number is greater than or\nequal to 1.\n\nTheorem 2.8. The Principle of Mathematical Induction implies the Principle of Well-\nOrdering. That is, every nonempty subset of N contains a least element.\n\nProof. We must show that if S is a nonempty subset of the natural numbers, then S\ncontains a least element. If S contains 1, then the theorem is true by Lemma 2.7. Assume\nthat if S contains an integer k such that 1 ≤ k ≤ n, then S contains a least element. We\nwill show that if a set S contains an integer less than or equal to n+ 1, then S has a least\nelement. If S does not contain an integer less than n+ 1, then n+ 1 is the smallest integer\nin S. Otherwise, since S is nonempty, S must contain an integer less than or equal to n. In\nthis case, by induction, S contains a least element.\n\nInduction can also be very useful in formulating definitions. For instance, there are two\nways to define n!, the factorial of a positive integer n.\n\n\n\n2.2. THE DIVISION ALGORITHM 25\n\n• The explicit definition: n! = 1 · 2 · 3 · · · (n− 1) · n.\n\n• The inductive or recursive definition: 1! = 1 and n! = n(n− 1)! for n > 1.\n\nEvery good mathematician or computer scientist knows that looking at problems recursively,\nas opposed to explicitly, often results in better understanding of complex issues.\n\n2.2 The Division Algorithm\nAn application of the Principle of Well-Ordering that we will use often is the division\nalgorithm.\n\nTheorem 2.9 (Division Algorithm). Let a and b be integers, with b > 0. Then there exist\nunique integers q and r such that\n\na = bq + r\n\nwhere 0 ≤ r < b.\n\nProof. This is a perfect example of the existence-and-uniqueness type of proof. We must\nfirst prove that the numbers q and r actually exist. Then we must show that if q′ and r′\n\nare two other such numbers, then q = q′ and r = r′.\nExistence of q and r. Let\n\nS = {a− bk : k ∈ Z and a− bk ≥ 0}.\n\nIf 0 ∈ S, then b divides a, and we can let q = a/b and r = 0. If 0 /∈ S, we can use the Well-\nOrdering Principle. We must first show that S is nonempty. If a > 0, then a − b · 0 ∈ S.\nIf a < 0, then a − b(2a) = a(1 − 2b) ∈ S. In either case S ̸= ∅. By the Well-Ordering\nPrinciple, S must have a smallest member, say r = a − bq. Therefore, a = bq + r, r ≥ 0.\nWe now show that r < b. Suppose that r > b. Then\n\na− b(q + 1) = a− bq − b = r − b > 0.\n\nIn this case we would have a− b(q + 1) in the set S. But then a− b(q + 1) < a− bq, which\nwould contradict the fact that r = a − bq is the smallest member of S. So r ≤ b. Since\n0 /∈ S, r ̸= b and so r < b.\n\nUniqueness of q and r. Suppose there exist integers r, r′, q, and q′ such that\n\na = bq + r, 0 ≤ r < b and a = bq′ + r′, 0 ≤ r′ < b.\n\nThen bq+r = bq′+r′. Assume that r′ ≥ r. From the last equation we have b(q−q′) = r′−r;\ntherefore, b must divide r′ − r and 0 ≤ r′ − r ≤ r′ < b. This is possible only if r′ − r = 0.\nHence, r = r′ and q = q′.\n\nLet a and b be integers. If b = ak for some integer k, we write a | b. An integer d is\ncalled a common divisor of a and b if d | a and d | b. The greatest common divisor of\nintegers a and b is a positive integer d such that d is a common divisor of a and b and if d′\nis any other common divisor of a and b, then d′ | d. We write d = gcd(a, b); for example,\ngcd(24, 36) = 12 and gcd(120, 102) = 6. We say that two integers a and b are relatively\nprime if gcd(a, b) = 1.\n\nTheorem 2.10. Let a and b be nonzero integers. Then there exist integers r and s such\nthat\n\ngcd(a, b) = ar + bs.\n\nFurthermore, the greatest common divisor of a and b is unique.\n\n\n\n26 CHAPTER 2. THE INTEGERS\n\nProof. Let\nS = {am+ bn : m,n ∈ Z and am+ bn > 0}.\n\nClearly, the set S is nonempty; hence, by the Well-Ordering Principle S must have a smallest\nmember, say d = ar+ bs. We claim that d = gcd(a, b). Write a = dq+ r′ where 0 ≤ r′ < d.\nIf r′ > 0, then\n\nr′ = a− dq\n\n= a− (ar + bs)q\n\n= a− arq − bsq\n\n= a(1− rq) + b(−sq),\n\nwhich is in S. But this would contradict the fact that d is the smallest member of S. Hence,\nr′ = 0 and d divides a. A similar argument shows that d divides b. Therefore, d is a common\ndivisor of a and b.\n\nSuppose that d′ is another common divisor of a and b, and we want to show that d′ | d.\nIf we let a = d′h and b = d′k, then\n\nd = ar + bs = d′hr + d′ks = d′(hr + ks).\n\nSo d′ must divide d. Hence, d must be the unique greatest common divisor of a and b.\n\nCorollary 2.11. Let a and b be two integers that are relatively prime. Then there exist\nintegers r and s such that ar + bs = 1.\n\nThe Euclidean Algorithm\nAmong other things, Theorem 2.10 allows us to compute the greatest common divisor of\ntwo integers.\n\nExample 2.12. Let us compute the greatest common divisor of 945 and 2415. First observe\nthat\n\n2415 = 945 · 2 + 525\n\n945 = 525 · 1 + 420\n\n525 = 420 · 1 + 105\n\n420 = 105 · 4 + 0.\n\nReversing our steps, 105 divides 420, 105 divides 525, 105 divides 945, and 105 divides 2415.\nHence, 105 divides both 945 and 2415. If d were another common divisor of 945 and 2415,\nthen d would also have to divide 105. Therefore, gcd(945, 2415) = 105.\n\nIf we work backward through the above sequence of equations, we can also obtain\nnumbers r and s such that 945r + 2415s = 105. Observe that\n\n105 = 525 + (−1) · 420\n= 525 + (−1) · [945 + (−1) · 525]\n= 2 · 525 + (−1) · 945\n= 2 · [2415 + (−2) · 945] + (−1) · 945\n= 2 · 2415 + (−5) · 945.\n\nSo r = −5 and s = 2. Notice that r and s are not unique, since r = 41 and s = −16 would\nalso work.\n\n\n\n2.2. THE DIVISION ALGORITHM 27\n\nTo compute gcd(a, b) = d, we are using repeated divisions to obtain a decreasing se-\nquence of positive integers r1 > r2 > · · · > rn = d; that is,\n\nb = aq1 + r1\n\na = r1q2 + r2\n\nr1 = r2q3 + r3\n...\n\nrn−2 = rn−1qn + rn\n\nrn−1 = rnqn+1.\n\nTo find r and s such that ar + bs = d, we begin with this last equation and substitute\nresults obtained from the previous equations:\n\nd = rn\n\n= rn−2 − rn−1qn\n\n= rn−2 − qn(rn−3 − qn−1rn−2)\n\n= −qnrn−3 + (1 + qnqn−1)rn−2\n\n...\n= ra+ sb.\n\nThe algorithm that we have just used to find the greatest common divisor d of two integers\na and b and to write d as the linear combination of a and b is known as the Euclidean\nalgorithm.\n\nPrime Numbers\nLet p be an integer such that p > 1. We say that p is a prime number, or simply p is\nprime, if the only positive numbers that divide p are 1 and p itself. An integer n > 1 that\nis not prime is said to be composite.\n\nLemma 2.13 (Euclid). Let a and b be integers and p be a prime number. If p | ab, then\neither p | a or p | b.\n\nProof. Suppose that p does not divide a. We must show that p | b. Since gcd(a, p) = 1,\nthere exist integers r and s such that ar + ps = 1. So\n\nb = b(ar + ps) = (ab)r + p(bs).\n\nSince p divides both ab and itself, p must divide b = (ab)r + p(bs).\n\nTheorem 2.14 (Euclid). There exist an infinite number of primes.\n\nProof. We will prove this theorem by contradiction. Suppose that there are only a finite\nnumber of primes, say p1, p2, . . . , pn. Let P = p1p2 · · · pn + 1. Then P must be divisible\nby some pi for 1 ≤ i ≤ n. In this case, pi must divide P − p1p2 · · · pn = 1, which is a\ncontradiction. Hence, either P is prime or there exists an additional prime number p ̸= pi\nthat divides P .\n\nTheorem 2.15 (Fundamental Theorem of Arithmetic). Let n be an integer such that n > 1.\nThen\n\nn = p1p2 · · · pk,\n\n\n\n28 CHAPTER 2. THE INTEGERS\n\nwhere p1, . . . , pk are primes (not necessarily distinct). Furthermore, this factorization is\nunique; that is, if\n\nn = q1q2 · · · ql,\n\nthen k = l and the qi’s are just the pi’s rearranged.\n\nProof. Uniqueness. To show uniqueness we will use induction on n. The theorem is\ncertainly true for n = 2 since in this case n is prime. Now assume that the result holds for\nall integers m such that 1 ≤ m < n, and\n\nn = p1p2 · · · pk = q1q2 · · · ql,\n\nwhere p1 ≤ p2 ≤ · · · ≤ pk and q1 ≤ q2 ≤ · · · ≤ ql. By Lemma 2.13, p1 | qi for some\ni = 1, . . . , l and q1 | pj for some j = 1, . . . , k. Since all of the pi’s and qi’s are prime, p1 = qi\nand q1 = pj . Hence, p1 = q1 since p1 ≤ pj = q1 ≤ qi = p1. By the induction hypothesis,\n\nn′ = p2 · · · pk = q2 · · · ql\n\nhas a unique factorization. Hence, k = l and qi = pi for i = 1, . . . , k.\nExistence. To show existence, suppose that there is some integer that cannot be written\n\nas the product of primes. Let S be the set of all such numbers. By the Principle of Well-\nOrdering, S has a smallest number, say a. If the only positive factors of a are a and 1, then\na is prime, which is a contradiction. Hence, a = a1a2 where 1 < a1 < a and 1 < a2 < a.\nNeither a1 ∈ S nor a2 ∈ S, since a is the smallest element in S. So\n\na1 = p1 · · · pr\na2 = q1 · · · qs.\n\nTherefore,\na = a1a2 = p1 · · · prq1 · · · qs.\n\nSo a /∈ S, which is a contradiction.\n\nHistorical Note\n\nPrime numbers were first studied by the ancient Greeks. Two important results from\nantiquity are Euclid’s proof that an infinite number of primes exist and the Sieve of Eratos-\nthenes, a method of computing all of the prime numbers less than a fixed positive integer\nn. One problem in number theory is to find a function f such that f(n) is prime for each\ninteger n. Pierre Fermat (1601?–1665) conjectured that 22\n\nn\n+ 1 was prime for all n, but\n\nlater it was shown by Leonhard Euler (1707–1783) that\n\n22\n5\n+ 1 = 4,294,967,297\n\nis a composite number. One of the many unproven conjectures about prime numbers is\nGoldbach’s Conjecture. In a letter to Euler in 1742, Christian Goldbach stated the conjec-\nture that every even integer with the exception of 2 seemed to be the sum of two primes:\n4 = 2 + 2, 6 = 3 + 3, 8 = 3 + 5, . . .. Although the conjecture has been verified for the\nnumbers up through 4× 1018, it has yet to be proven in general. Since prime numbers play\nan important role in public key cryptography, there is currently a great deal of interest in\ndetermining whether or not a large number is prime.\n\n\n\n2.3. EXERCISES 29\n\n2.3 Exercises\n1. Prove that\n\n12 + 22 + · · ·+ n2 =\nn(n+ 1)(2n+ 1)\n\n6\nfor n ∈ N.\n\n2. Prove that\n13 + 23 + · · ·+ n3 =\n\nn2(n+ 1)2\n\n4\nfor n ∈ N.\n\n3. Prove that n! > 2n for n ≥ 4.\n\n4. Prove that\nx+ 4x+ 7x+ · · ·+ (3n− 2)x =\n\nn(3n− 1)x\n\n2\nfor n ∈ N.\n\n5. Prove that 10n+1 + 10n + 1 is divisible by 3 for n ∈ N.\n\n6. Prove that 4 · 102n + 9 · 102n−1 + 5 is divisible by 99 for n ∈ N.\n\n7. Show that\nn\n√\na1a2 · · · an ≤ 1\n\nn\n\nn∑\nk=1\n\nak.\n\n8. Prove the Leibniz rule for f (n)(x), where f (n) is the nth derivative of f ; that is, show\nthat\n\n(fg)(n)(x) =\nn∑\n\nk=0\n\n(\nn\n\nk\n\n)\nf (k)(x)g(n−k)(x).\n\n9. Use induction to prove that 1 + 2 + 22 + · · ·+ 2n = 2n+1 − 1 for n ∈ N.\n\n10. Prove that\n1\n\n2\n+\n\n1\n\n6\n+ · · ·+ 1\n\nn(n+ 1)\n=\n\nn\n\nn+ 1\n\nfor n ∈ N.\n\n11. If x is a nonnegative real number, then show that (1 + x)n − 1 ≥ nx for n = 0, 1, 2, . . ..\n\n12. (Power Sets) Let X be a set. Define the power set of X, denoted P(X), to be the\nset of all subsets of X. For example,\n\nP({a, b}) = {∅, {a}, {b}, {a, b}}.\n\nFor every positive integer n, show that a set with exactly n elements has a power set with\nexactly 2n elements.\n\n13. Prove that the two principles of mathematical induction stated in Section 2.1 are equiv-\nalent.\n\n14. Show that the Principle of Well-Ordering for the natural numbers implies that 1 is the\nsmallest natural number. Use this result to show that the Principle of Well-Ordering implies\nthe Principle of Mathematical Induction; that is, show that if S ⊂ N such that 1 ∈ S and\nn+ 1 ∈ S whenever n ∈ S, then S = N.\n\n15. For each of the following pairs of numbers a and b, calculate gcd(a, b) and find integers\nr and s such that gcd(a, b) = ra+ sb.\n\n\n\n30 CHAPTER 2. THE INTEGERS\n\n(a) 14 and 39\n(b) 234 and 165\n(c) 1739 and 9923\n\n(d) 471 and 562\n(e) 23,771 and 19,945\n(f) −4357 and 3754\n\n16. Let a and b be nonzero integers. If there exist integers r and s such that ar + bs = 1,\nshow that a and b are relatively prime.\n\n17. (Fibonacci Numbers) The Fibonacci numbers are\n\n1, 1, 2, 3, 5, 8, 13, 21, . . . .\n\nWe can define them inductively by f1 = 1, f2 = 1, and fn+2 = fn+1 + fn for n ∈ N.\n(a) Prove that fn < 2n.\n(b) Prove that fn+1fn−1 = f2n + (−1)n, n ≥ 2.\n(c) Prove that fn = [(1 +\n\n√\n5 )n − (1−\n\n√\n5 )n]/2n\n\n√\n5.\n\n(d) Show that limn→∞ fn/fn+1 = (\n√\n5− 1)/2.\n\n(e) Prove that fn and fn+1 are relatively prime.\n\n18. Let a and b be integers such that gcd(a, b) = 1. Let r and s be integers such that\nar + bs = 1. Prove that\n\ngcd(a, s) = gcd(r, b) = gcd(r, s) = 1.\n\n19. Let x, y ∈ N be relatively prime. If xy is a perfect square, prove that x and y must\nboth be perfect squares.\n\n20. Using the division algorithm, show that every perfect square is of the form 4k or 4k+1\nfor some nonnegative integer k.\n\n21. Suppose that a, b, r, s are pairwise relatively prime and that\n\na2 + b2 = r2\n\na2 − b2 = s2.\n\nProve that a, r, and s are odd and b is even.\n\n22. Let n ∈ N. Use the division algorithm to prove that every integer is congruent mod n\nto precisely one of the integers 0, 1, . . . , n − 1. Conclude that if r is an integer, then there\nis exactly one s in Z such that 0 ≤ s < n and [r] = [s]. Hence, the integers are indeed\npartitioned by congruence mod n.\n\n23. Define the least common multiple of two nonzero integers a and b, denoted by\nlcm(a, b), to be the nonnegative integer m such that both a and b divide m, and if a and b\ndivide any other integer n, then m also divides n. Prove there exists a unique least common\nmultiple for any two integers a and b.\n\n24. If d = gcd(a, b) and m = lcm(a, b), prove that dm = |ab|.\n\n25. Show that lcm(a, b) = ab if and only if gcd(a, b) = 1.\n\n26. Prove that gcd(a, c) = gcd(b, c) = 1 if and only if gcd(ab, c) = 1 for integers a, b, and c.\n\n\n\n2.4. PROGRAMMING EXERCISES 31\n\n27. Let a, b, c ∈ Z. Prove that if gcd(a, b) = 1 and a | bc, then a | c.\n\n28. Let p ≥ 2. Prove that if 2p − 1 is prime, then p must also be prime.\n\n29. Prove that there are an infinite number of primes of the form 6n+ 5.\n\n30. Prove that there are an infinite number of primes of the form 4n− 1.\n\n31. Using the fact that 2 is prime, show that there do not exist integers p and q such that\np2 = 2q2. Demonstrate that therefore\n\n√\n2 cannot be a rational number.\n\n2.4 Programming Exercises\n1. (The Sieve of Eratosthenes) One method of computing all of the prime numbers less\nthan a certain fixed positive integer N is to list all of the numbers n such that 1 < n < N .\nBegin by eliminating all of the multiples of 2. Next eliminate all of the multiples of 3. Now\neliminate all of the multiples of 5. Notice that 4 has already been crossed out. Continue in\nthis manner, noticing that we do not have to go all the way to N ; it suffices to stop at\n\n√\nN .\n\nUsing this method, compute all of the prime numbers less than N = 250. We can also use\nthis method to find all of the integers that are relatively prime to an integer N . Simply\neliminate the prime factors of N and all of their multiples. Using this method, find all of\nthe numbers that are relatively prime to N = 120. Using the Sieve of Eratosthenes, write\na program that will compute all of the primes less than an integer N .\n\n2. Let N0 = N ∪ {0}. Ackermann’s function is the function A : N0 × N0 → N0 defined by\nthe equations\n\nA(0, y) = y + 1,\n\nA(x+ 1, 0) = A(x, 1),\n\nA(x+ 1, y + 1) = A(x,A(x+ 1, y)).\n\nUse this definition to compute A(3, 1). Write a program to evaluate Ackermann’s function.\nModify the program to count the number of statements executed in the program when\nAckermann’s function is evaluated. How many statements are executed in the evaluation\nof A(4, 1)? What about A(5, 1)?\n\n3. Write a computer program that will implement the Euclidean algorithm. The program\nshould accept two positive integers a and b as input and should output gcd(a, b) as well as\nintegers r and s such that\n\ngcd(a, b) = ra+ sb.\n\n2.5 References and Suggested Readings\n[1] Brookshear, J. G. Theory of Computation: Formal Languages, Automata, and Com-\n\nplexity. Benjamin/Cummings, Redwood City, CA, 1989. Shows the relationships of\nthe theoretical aspects of computer science to set theory and the integers.\n\n[2] Hardy, G. H. and Wright, E. M. An Introduction to the Theory of Numbers. 6th ed.\nOxford University Press, New York, 2008.\n\n[3] Niven, I. and Zuckerman, H. S. An Introduction to the Theory of Numbers. 5th ed.\nWiley, New York, 1991.\n\n[4] Vanden Eynden, C. Elementary Number Theory. 2nd ed. Waveland Press, Long\nGrove IL, 2001.\n\n\n\n32 CHAPTER 2. THE INTEGERS\n\n2.6 Sage\nMany properties of the algebraic objects we will study can be determined from properties\nof associated integers. And Sage has many powerful functions for analyzing integers.\n\nDivision Algorithm\n\nThe code a % b will return the remainder upon division of a by b. In other words, the result\nis the unique integer r such that (1) 0 ≤ r < b, and (2) a = bq + r for some integer q (the\nquotient), as guaranteed by the Division Algorithm (Theorem 2.9). Then (a − r)/b will\nequal q. For example,\n\nr = 14 % 3\nr\n\n2\n\nq = (14 - r)/3\nq\n\n4\n\nIt is also possible to get both the quotient and remainder at the same time with the\n.quo_rem() method (quotient and remainder).\n\na = 14\nb = 3\na.quo_rem(b)\n\n(4, 2)\n\nA remainder of zero indicates divisibility. So (a % b)== 0 will return True if b divides a,\nand will otherwise return False.\n\n(20 % 5) == 0\n\nTrue\n\n(17 % 4) == 0\n\nFalse\n\nThe .divides() method is another option.\n\nc = 5\nc.divides (20)\n\nTrue\n\nd = 4\nd.divides (17)\n\nFalse\n\n\n\n2.6. SAGE 33\n\nGreatest Common Divisor\nThe greatest common divisor of a and b is obtained with the command gcd(a, b), where\nin our first uses, a and b are integers. Later, a and b can be other objects with a notion of\ndivisibility and “greatness,” such as polynomials. For example,\n\ngcd (2776 , 2452)\n\n4\n\nWe can use the gcd command to determine if a pair of integers are relatively prime.\na = 31049\nb = 2105\ngcd(a, b) == 1\n\nTrue\n\na = 3563\nb = 2947\ngcd(a, b) == 1\n\nFalse\n\nThe command xgcd(a,b) (“eXtended GCD”) returns a triple where the first element is\nthe greatest common divisor of a and b (as with the gcd(a,b) command above), but the\nnext two elements are values of r and s such that ra+ sb = gcd(a, b).\n\nxgcd (633 ,331)\n\n(1, -137, 262)\n\nPortions of the triple can be extracted using [ ] (“indexing”) to access the entries of the\ntriple, starting with the first as number 0. For example, the following should always return\nthe result True, even if you change the values of a and b. Try changing the values of a and\nb below, to see that the result is always True.\n\na = 633\nb = 331\nextended = xgcd(a, b)\ng = extended [0]\nr = extended [1]\ns = extended [2]\ng == r*a + s*b\n\nTrue\n\nStudying this block of code will go a long way towards helping you get the most out of\nSage’s output. Note that = is how a value is assigned to a variable, while as in the last line,\n== is how we compare two items for equality.\n\nPrimes and Factoring\nThe method .is_prime() will determine if an integer is prime or not.\n\na = 117371\na.is_prime ()\n\n\n\n34 CHAPTER 2. THE INTEGERS\n\nTrue\n\nb = 14547073\nb.is_prime ()\n\nFalse\n\nb == 1597 * 9109\n\nTrue\n\nThe command random_prime(a, proof=True) will generate a random prime number be-\ntween 2 and a. Experiment by executing the following two compute cells several times.\n(Replacing proof=True by proof=False will speed up the search, but there will be a very,\nvery, very small probability the result will not be prime.)\n\na = random_prime (10^21 , proof=True)\na\n\n424729101793542195193\n\na.is_prime ()\n\nTrue\n\nThe command prime_range(a, b) returns an ordered list of all the primes from a to b−1,\ninclusive. For example,\n\nprime_range (500, 550)\n\n[503, 509, 521, 523, 541, 547]\n\nThe commands next_prime(a) and previous_prime(a) are other ways to get a single prime\nnumber of a desired size. Give them a try below if you have an empty compute cell there\n(as you will if you are reading in the Sage Notebook, or are reading the online version).\n(The hash symbol, #, is used to indicate a “comment” line, which will not be evaluated by\nSage. So erase this line, or start on the one below it.)\n\nIn addition to checking if integers are prime or not, or generating prime numbers, Sage\ncan also decompose any integer into its prime factors, as described by the Fundamental\nTheorem of Arithmetic (Theorem 2.15).\n\na = 2600\na.factor ()\n\n2^3 * 5^2 * 13\n\nSo 2600 = 23 × 52 × 13 and this is the unique way to write 2600 as a product of prime\nnumbers (other than rearranging the order of the primes themselves in the product).\n\nWhile Sage will print a factorization nicely, it is carried internally as a list of pairs of\nintegers, with each pair being a base (a prime number) and an exponent (a positive integer).\nStudy the following carefully, as it is another good exercise in working with Sage output in\nthe form of lists.\n\na = 2600\nfactored = a.factor ()\nfirst_term = factored [0]\nfirst_term\n\n\n\n2.7. SAGE EXERCISES 35\n\n(2, 3)\n\nsecond_term = factored [1]\nsecond_term\n\n(5, 2)\n\nthird_term = factored [2]\nthird_term\n\n(13, 1)\n\nfirst_prime = first_term [0]\nfirst_prime\n\n2\n\nfirst_exponent = first_term [1]\nfirst_exponent\n\n3\n\nThe next compute cell reveals the internal version of the factorization by asking for the\nactual list. And we show how you could determine exactly how many terms the factorization\nhas by using the length command, len().\n\nlist(factored)\n\n[(2, 3), (5, 2), (13, 1)]\n\nlen(factored)\n\n3\n\nCan you extract the next two primes, and their exponents, from a?\n\n2.7 Sage Exercises\nThese exercises are about investigating basic properties of the integers, something we will\nfrequently do when investigating groups. Use the editing capabilities of a Sage worksheet\nto annotate and explain your work.\n1. Use the next_prime() command to construct two different 8-digit prime numbers and\nsave them in variables named a and b.\n\n2. Use the .is_prime() method to verify that your primes a and b are really prime.\n\n3. Verify that 1 is the greatest common divisor of your two primes from the previous\nexercises.\n\n4. Find two integers that make a “linear combination” of your two primes equal to 1.\nInclude a verification of your result.\n\n5. Determine a factorization into powers of primes for c = 4598 037 234.\n\n6. Write a compute cell that defines the same value of c again, and then defines a candidate\ndivisor of c named d. The third line of the cell should return True if and only if d is a divisor\nof c. Illustrate the use of your cell by testing your code with d = 7 and in a new copy of\nthe cell, testing your code with d = 11.\n\n\n\n3\n\nGroups\n\nWe begin our study of algebraic structures by investigating sets associated with single\noperations that satisfy certain reasonable axioms; that is, we want to define an operation\non a set in a way that will generalize such familiar structures as the integers Z together\nwith the single operation of addition, or invertible 2 × 2 matrices together with the single\noperation of matrix multiplication. The integers and the 2×2 matrices, together with their\nrespective single operations, are examples of algebraic structures known as groups.\n\nThe theory of groups occupies a central position in mathematics. Modern group theory\narose from an attempt to find the roots of a polynomial in terms of its coefficients. Groups\nnow play a central role in such areas as coding theory, counting, and the study of symmetries;\nmany areas of biology, chemistry, and physics have benefited from group theory.\n\n3.1 Integer Equivalence Classes and Symmetries\n\nLet us now investigate some mathematical structures that can be viewed as sets with single\noperations.\n\nThe Integers mod n\n\nThe integers mod n have become indispensable in the theory and applications of algebra.\nIn mathematics they are used in cryptography, coding theory, and the detection of errors\nin identification codes.\n\nWe have already seen that two integers a and b are equivalent mod n if n divides\na − b. The integers mod n also partition Z into n different equivalence classes; we will\ndenote the set of these equivalence classes by Zn. Consider the integers modulo 12 and the\ncorresponding partition of the integers:\n\n[0] = {. . . ,−12, 0, 12, 24, . . .},\n[1] = {. . . ,−11, 1, 13, 25, . . .},\n\n...\n[11] = {. . . ,−1, 11, 23, 35, . . .}.\n\nWhen no confusion can arise, we will use 0, 1, . . . , 11 to indicate the equivalence classes\n[0], [1], . . . , [11] respectively. We can do arithmetic on Zn. For two integers a and b, define\naddition modulo n to be (a + b) (mod n); that is, the remainder when a + b is divided by\nn. Similarly, multiplication modulo n is defined as (ab) (mod n), the remainder when ab is\ndivided by n.\n\n36\n\n\n\n3.1. INTEGER EQUIVALENCE CLASSES AND SYMMETRIES 37\n\nExample 3.1. The following examples illustrate integer arithmetic modulo n:\n\n7 + 4 ≡ 1 (mod 5) 7 · 3 ≡ 1 (mod 5)\n\n3 + 5 ≡ 0 (mod 8) 3 · 5 ≡ 7 (mod 8)\n\n3 + 4 ≡ 7 (mod 12) 3 · 4 ≡ 0 (mod 12)\n\nIn particular, notice that it is possible that the product of two nonzero numbers modulo n\ncan be equivalent to 0 modulo n.\n\nExample 3.2. Most, but not all, of the usual laws of arithmetic hold for addition and\nmultiplication in Zn. For instance, it is not necessarily true that there is a multiplicative\ninverse. Consider the multiplication table for Z8 in Table 3.3. Notice that 2, 4, and 6 do\nnot have multiplicative inverses; that is, for n = 2, 4, or 6, there is no integer k such that\nkn ≡ 1 (mod 8).\n\n· 0 1 2 3 4 5 6 7\n\n0 0 0 0 0 0 0 0 0\n\n1 0 1 2 3 4 5 6 7\n\n2 0 2 4 6 0 2 4 6\n\n3 0 3 6 1 4 7 2 5\n\n4 0 4 0 4 0 4 0 4\n\n5 0 5 2 7 4 1 6 3\n\n6 0 6 4 2 0 6 4 2\n\n7 0 7 6 5 4 3 2 1\n\nTable 3.3: Multiplication table for Z8\n\nProposition 3.4. Let Zn be the set of equivalence classes of the integers mod n and\na, b, c ∈ Zn.\n\n1. Addition and multiplication are commutative:\n\na+ b ≡ b+ a (mod n)\n\nab ≡ ba (mod n).\n\n2. Addition and multiplication are associative:\n\n(a+ b) + c ≡ a+ (b+ c) (mod n)\n\n(ab)c ≡ a(bc) (mod n).\n\n3. There are both additive and multiplicative identities:\n\na+ 0 ≡ a (mod n)\n\na · 1 ≡ a (mod n).\n\n4. Multiplication distributes over addition:\n\na(b+ c) ≡ ab+ ac (mod n).\n\n\n\n38 CHAPTER 3. GROUPS\n\n5. For every integer a there is an additive inverse −a:\n\na+ (−a) ≡ 0 (mod n).\n\n6. Let a be a nonzero integer. Then gcd(a, n) = 1 if and only if there exists a multiplicative\ninverse b for a (mod n); that is, a nonzero integer b such that\n\nab ≡ 1 (mod n).\n\nProof. We will prove (1) and (6) and leave the remaining properties to be proven in the\nexercises.\n\n(1) Addition and multiplication are commutative modulo n since the remainder of a+ b\ndivided by n is the same as the remainder of b+ a divided by n.\n\n(6) Suppose that gcd(a, n) = 1. Then there exist integers r and s such that ar+ns = 1.\nSince ns = 1 − ar, it must be the case that ar ≡ 1 (mod n). Letting b be the equivalence\nclass of r, ab ≡ 1 (mod n).\n\nConversely, suppose that there exists an integer b such that ab ≡ 1 (mod n). Then n\ndivides ab − 1, so there is an integer k such that ab − nk = 1. Let d = gcd(a, n). Since d\ndivides ab− nk, d must also divide 1; hence, d = 1.\n\nSymmetries\n\nreflection\nhorizontal axis\n\nA\n\nD\n\nB\n\nC\n\nC\n\nB\n\nD\n\nA\n\nreflection\nvertical axis\n\nA\n\nD\n\nB\n\nC\n\nA\n\nD\n\nB\n\nC\n\n180◦\n\nrotation\n\nA\n\nD\n\nB\n\nC\n\nD\n\nA\n\nC\n\nB\n\nidentityA\n\nD\n\nB\n\nC\n\nB\n\nC\n\nA\n\nD\n\nFigure 3.5: Rigid motions of a rectangle\n\nA symmetry of a geometric figure is a rearrangement of the figure preserving the\narrangement of its sides and vertices as well as its distances and angles. A map from the\nplane to itself preserving the symmetry of an object is called a rigid motion. For example,\nif we look at the rectangle in Figure 3.5, it is easy to see that a rotation of 180◦ or 360◦\n\nreturns a rectangle in the plane with the same orientation as the original rectangle and the\nsame relationship among the vertices. A reflection of the rectangle across either the vertical\n\n\n\n3.1. INTEGER EQUIVALENCE CLASSES AND SYMMETRIES 39\n\naxis or the horizontal axis can also be seen to be a symmetry. However, a 90◦ rotation in\neither direction cannot be a symmetry unless the rectangle is a square.\n\nA\n\nB\n\nC\n\nreflection\n\nB C\n\nA\n\nµ3 =\n\n(\nA B C\n\nB A C\n\n)\nA\n\nB\n\nC\n\nreflection\n\nC A\n\nB\n\nµ2 =\n\n(\nA B C\n\nC B A\n\n)\nA\n\nB\n\nC\n\nreflection\n\nA B\n\nC\n\nµ1 =\n\n(\nA B C\n\nA C B\n\n)\nA\n\nB\n\nC\n\nrotation\n\nB A\n\nC\n\nρ2 =\n\n(\nA B C\n\nC A B\n\n)\nA\n\nB\n\nC\n\nrotation\n\nC B\n\nA\n\nρ1 =\n\n(\nA B C\n\nB C A\n\n)\nA\n\nB\n\nC\n\nidentity\n\nA C\n\nB\n\nid =\n\n(\nA B C\n\nA B C\n\n)\n\nFigure 3.6: Symmetries of a triangle\n\nLet us find the symmetries of the equilateral triangle △ABC. To find a symmetry of\n△ABC, we must first examine the permutations of the vertices A, B, and C and then ask\nif a permutation extends to a symmetry of the triangle. Recall that a permutation of a\nset S is a one-to-one and onto map π : S → S. The three vertices have 3! = 6 permutations,\nso the triangle has at most six symmetries. To see that there are six permutations, observe\nthere are three different possibilities for the first vertex, and two for the second, and the\nremaining vertex is determined by the placement of the first two. So we have 3 ·2 ·1 = 3! = 6\ndifferent arrangements. To denote the permutation of the vertices of an equilateral triangle\nthat sends A to B, B to C, and C to A, we write the array(\n\nA B C\n\nB C A\n\n)\n.\n\nNotice that this particular permutation corresponds to the rigid motion of rotating the\ntriangle by 120◦ in a clockwise direction. In fact, every permutation gives rise to a symmetry\nof the triangle. All of these symmetries are shown in Figure 3.6.\n\nA natural question to ask is what happens if one motion of the triangle △ABC is\nfollowed by another. Which symmetry is µ1ρ1; that is, what happens when we do the\npermutation ρ1 and then the permutation µ1? Remember that we are composing functions\n\n\n\n40 CHAPTER 3. GROUPS\n\nhere. Although we usually multiply left to right, we compose functions right to left. We have\n\n(µ1ρ1)(A) = µ1(ρ1(A)) = µ1(B) = C\n\n(µ1ρ1)(B) = µ1(ρ1(B)) = µ1(C) = B\n\n(µ1ρ1)(C) = µ1(ρ1(C)) = µ1(A) = A.\n\nThis is the same symmetry as µ2. Suppose we do these motions in the opposite order,\nρ1 then µ1. It is easy to determine that this is the same as the symmetry µ3; hence,\nρ1µ1 ̸= µ1ρ1. A multiplication table for the symmetries of an equilateral triangle △ABC is\ngiven in Table 3.7.\n\nNotice that in the multiplication table for the symmetries of an equilateral triangle, for\nevery motion of the triangle α there is another motion β such that αβ = id; that is, for\nevery motion there is another motion that takes the triangle back to its original orientation.\n\n◦ id ρ1 ρ2 µ1 µ2 µ3\nid id ρ1 ρ2 µ1 µ2 µ3\nρ1 ρ1 ρ2 id µ3 µ1 µ2\nρ2 ρ2 id ρ1 µ2 µ3 µ1\nµ1 µ1 µ2 µ3 id ρ1 ρ2\nµ2 µ2 µ3 µ1 ρ2 id ρ1\nµ3 µ3 µ1 µ2 ρ1 ρ2 id\n\nTable 3.7: Symmetries of an equilateral triangle\n\n3.2 Definitions and Examples\nThe integers mod n and the symmetries of a triangle or a rectangle are examples of groups.\nA binary operation or law of composition on a set G is a function G × G → G that\nassigns to each pair (a, b) ∈ G×G a unique element a◦ b, or ab in G, called the composition\nof a and b. A group (G, ◦) is a set G together with a law of composition (a, b) 7→ a ◦ b that\nsatisfies the following axioms.\n\n• The law of composition is associative. That is,\n\n(a ◦ b) ◦ c = a ◦ (b ◦ c)\n\nfor a, b, c ∈ G.\n\n• There exists an element e ∈ G, called the identity element, such that for any element\na ∈ G\n\ne ◦ a = a ◦ e = a.\n\n• For each element a ∈ G, there exists an inverse element in G, denoted by a−1, such\nthat\n\na ◦ a−1 = a−1 ◦ a = e.\n\nA group G with the property that a ◦ b = b ◦ a for all a, b ∈ G is called abelian or\ncommutative. Groups not satisfying this property are said to be nonabelian or non-\ncommutative.\n\n\n\n3.2. DEFINITIONS AND EXAMPLES 41\n\nExample 3.8. The integers Z = {. . . ,−1, 0, 1, 2, . . .} form a group under the operation\nof addition. The binary operation on two integers m,n ∈ Z is just their sum. Since the\nintegers under addition already have a well-established notation, we will use the operator +\ninstead of ◦; that is, we shall write m+n instead of m◦n. The identity is 0, and the inverse\nof n ∈ Z is written as −n instead of n−1. Notice that the set of integers under addition\nhave the additional property that m+ n = n+m and therefore form an abelian group.\n\nMost of the time we will write ab instead of a ◦ b; however, if the group already has a\nnatural operation such as addition in the integers, we will use that operation. That is, if\nwe are adding two integers, we still write m+ n, −n for the inverse, and 0 for the identity\nas usual. We also write m− n instead of m+ (−n).\n\nIt is often convenient to describe a group in terms of an addition or multiplication table.\nSuch a table is called a Cayley table.\n\nExample 3.9. The integers mod n form a group under addition modulo n. Consider\nZ5, consisting of the equivalence classes of the integers 0, 1, 2, 3, and 4. We define the\ngroup operation on Z5 by modular addition. We write the binary operation on the group\nadditively; that is, we write m + n. The element 0 is the identity of the group and each\nelement in Z5 has an inverse. For instance, 2 + 3 = 3 + 2 = 0. Table 3.10 is a Cayley table\nfor Z5. By Proposition 3.4, Zn = {0, 1, . . . , n− 1} is a group under the binary operation of\naddition mod n.\n\n+ 0 1 2 3 4\n\n0 0 1 2 3 4\n\n1 1 2 3 4 0\n\n2 2 3 4 0 1\n\n3 3 4 0 1 2\n\n4 4 0 1 2 3\n\nTable 3.10: Cayley table for (Z5,+)\n\nExample 3.11. Not every set with a binary operation is a group. For example, if we let\nmodular multiplication be the binary operation on Zn, then Zn fails to be a group. The\nelement 1 acts as a group identity since 1 · k = k · 1 = k for any k ∈ Zn; however, a\nmultiplicative inverse for 0 does not exist since 0 · k = k · 0 = 0 for every k in Zn. Even if\nwe consider the set Zn \\ {0}, we still may not have a group. For instance, let 2 ∈ Z6. Then\n2 has no multiplicative inverse since\n\n0 · 2 = 0 1 · 2 = 2\n\n2 · 2 = 4 3 · 2 = 0\n\n4 · 2 = 2 5 · 2 = 4.\n\nBy Proposition 3.4, every nonzero k does have an inverse in Zn if k is relatively prime to\nn. Denote the set of all such nonzero elements in Zn by U(n). Then U(n) is a group called\nthe group of units of Zn. Table 3.12 is a Cayley table for the group U(8).\n\n\n\n42 CHAPTER 3. GROUPS\n\n· 1 3 5 7\n\n1 1 3 5 7\n\n3 3 1 7 5\n\n5 5 7 1 3\n\n7 7 5 3 1\n\nTable 3.12: Multiplication table for U(8)\n\nExample 3.13. The symmetries of an equilateral triangle described in Section 3.1 form\na nonabelian group. As we observed, it is not necessarily true that αβ = βα for two\nsymmetries α and β. Using Table 3.7, which is a Cayley table for this group, we can easily\ncheck that the symmetries of an equilateral triangle are indeed a group. We will denote this\ngroup by either S3 or D3, for reasons that will be explained later.\nExample 3.14. We use M2(R) to denote the set of all 2× 2 matrices. Let GL2(R) be the\nsubset of M2(R) consisting of invertible matrices; that is, a matrix\n\nA =\n\n(\na b\n\nc d\n\n)\nis in GL2(R) if there exists a matrix A−1 such that AA−1 = A−1A = I, where I is the 2×2\nidentity matrix. For A to have an inverse is equivalent to requiring that the determinant\nof A be nonzero; that is, detA = ad− bc ̸= 0. The set of invertible matrices forms a group\ncalled the general linear group. The identity of the group is the identity matrix\n\nI =\n\n(\n1 0\n\n0 1\n\n)\n.\n\nThe inverse of A ∈ GL2(R) is\n\nA−1 =\n1\n\nad− bc\n\n(\nd −b\n−c a\n\n)\n.\n\nThe product of two invertible matrices is again invertible. Matrix multiplication is associa-\ntive, satisfying the other group axiom. For matrices it is not true in general that AB = BA;\nhence, GL2(R) is another example of a nonabelian group.\nExample 3.15. Let\n\n1 =\n\n(\n1 0\n\n0 1\n\n)\nI =\n\n(\n0 1\n\n−1 0\n\n)\nJ =\n\n(\n0 i\n\ni 0\n\n)\nK =\n\n(\ni 0\n\n0 −i\n\n)\n,\n\nwhere i2 = −1. Then the relations I2 = J2 = K2 = −1, IJ = K, JK = I, KI = J ,\nJI = −K, KJ = −I, and IK = −J hold. The set Q8 = {±1,±I,±J,±K} is a group\ncalled the quaternion group. Notice that Q8 is noncommutative.\nExample 3.16. Let C∗be the set of nonzero complex numbers. Under the operation of\nmultiplication C∗ forms a group. The identity is 1. If z = a + bi is a nonzero complex\nnumber, then\n\nz−1 =\na− bi\n\na2 + b2\n\nis the inverse of z. It is easy to see that the remaining group axioms hold.\n\n\n\n3.2. DEFINITIONS AND EXAMPLES 43\n\nA group is finite, or has finite order, if it contains a finite number of elements;\notherwise, the group is said to be infinite or to have infinite order. The order of a finite\ngroup is the number of elements that it contains. If G is a group containing n elements,\nwe write |G| = n. The group Z5 is a finite group of order 5; the integers Z form an infinite\ngroup under addition, and we sometimes write |Z| = ∞.\n\nBasic Properties of Groups\n\nProposition 3.17. The identity element in a group G is unique; that is, there exists only\none element e ∈ G such that eg = ge = g for all g ∈ G.\n\nProof. Suppose that e and e′ are both identities in G. Then eg = ge = g and e′g = ge′ = g\nfor all g ∈ G. We need to show that e = e′. If we think of e as the identity, then ee′ = e′; but\nif e′ is the identity, then ee′ = e. Combining these two equations, we have e = ee′ = e′.\n\nInverses in a group are also unique. If g′ and g′′ are both inverses of an element g\nin a group G, then gg′ = g′g = e and gg′′ = g′′g = e. We want to show that g′ = g′′,\nbut g′ = g′e = g′(gg′′) = (g′g)g′′ = eg′′ = g′′. We summarize this fact in the following\nproposition.\n\nProposition 3.18. If g is any element in a group G, then the inverse of g, denoted by g−1,\nis unique.\n\nProposition 3.19. Let G be a group. If a, b ∈ G, then (ab)−1 = b−1a−1.\n\nProof. Let a, b ∈ G. Then abb−1a−1 = aea−1 = aa−1 = e. Similarly, b−1a−1ab = e. But\nby the previous proposition, inverses are unique; hence, (ab)−1 = b−1a−1.\n\nProposition 3.20. Let G be a group. For any a ∈ G, (a−1)−1 = a.\n\nProof. Observe that a−1(a−1)−1 = e. Consequently, multiplying both sides of this equa-\ntion by a, we have\n\n(a−1)−1 = e(a−1)−1 = aa−1(a−1)−1 = ae = a.\n\nIt makes sense to write equations with group elements and group operations. If a and b\nare two elements in a group G, does there exist an element x ∈ G such that ax = b? If such\nan x does exist, is it unique? The following proposition answers both of these questions\npositively.\n\nProposition 3.21. Let G be a group and a and b be any two elements in G. Then the\nequations ax = b and xa = b have unique solutions in G.\n\nProof. Suppose that ax = b. We must show that such an x exists. We can multiply both\nsides of ax = b by a−1 to find x = ex = a−1ax = a−1b.\n\nTo show uniqueness, suppose that x1 and x2 are both solutions of ax = b; then ax1 =\nb = ax2. So x1 = a−1ax1 = a−1ax2 = x2. The proof for the existence and uniqueness of\nthe solution of xa = b is similar.\n\nProposition 3.22. If G is a group and a, b, c ∈ G, then ba = ca implies b = c and ab = ac\nimplies b = c.\n\n\n\n44 CHAPTER 3. GROUPS\n\nThis proposition tells us that the right and left cancellation laws are true in groups.\nWe leave the proof as an exercise.\n\nWe can use exponential notation for groups just as we do in ordinary algebra. If G is a\ngroup and g ∈ G, then we define g0 = e. For n ∈ N, we define\n\ngn = g · g · · · g︸ ︷︷ ︸\nn times\n\nand\ng−n = g−1 · g−1 · · · g−1︸ ︷︷ ︸\n\nn times\n\n.\n\nTheorem 3.23. In a group, the usual laws of exponents hold; that is, for all g, h ∈ G,\n\n1. gmgn = gm+n for all m,n ∈ Z;\n\n2. (gm)n = gmn for all m,n ∈ Z;\n\n3. (gh)n = (h−1g−1)−n for all n ∈ Z. Furthermore, if G is abelian, then (gh)n = gnhn.\n\nWe will leave the proof of this theorem as an exercise. Notice that (gh)n ̸= gnhn in\ngeneral, since the group may not be abelian. If the group is Z or Zn, we write the group\noperation additively and the exponential operation multiplicatively; that is, we write ng\ninstead of gn. The laws of exponents now become\n\n1. mg + ng = (m+ n)g for all m,n ∈ Z;\n\n2. m(ng) = (mn)g for all m,n ∈ Z;\n\n3. m(g + h) = mg +mh for all n ∈ Z.\n\nIt is important to realize that the last statement can be made only because Z and Zn\n\nare commutative groups.\n\nHistorical Note\n\nAlthough the first clear axiomatic definition of a group was not given until the late\n1800s, group-theoretic methods had been employed before this time in the development of\nmany areas of mathematics, including geometry and the theory of algebraic equations.\n\nJoseph-Louis Lagrange used group-theoretic methods in a 1770–1771 memoir to study\nmethods of solving polynomial equations. Later, Évariste Galois (1811–1832) succeeded\nin developing the mathematics necessary to determine exactly which polynomial equations\ncould be solved in terms of the polynomials’coefficients. Galois’ primary tool was group\ntheory.\n\nThe study of geometry was revolutionized in 1872 when Felix Klein proposed that ge-\nometric spaces should be studied by examining those properties that are invariant under\na transformation of the space. Sophus Lie, a contemporary of Klein, used group theory\nto study solutions of partial differential equations. One of the first modern treatments of\ngroup theory appeared in William Burnside’s The Theory of Groups of Finite Order [1],\nfirst published in 1897.\n\n\n\n3.3. SUBGROUPS 45\n\n3.3 Subgroups\nDefinitions and Examples\nSometimes we wish to investigate smaller groups sitting inside a larger group. The set of\neven integers 2Z = {. . . ,−2, 0, 2, 4, . . .} is a group under the operation of addition. This\nsmaller group sits naturally inside of the group of integers under addition. We define a\nsubgroup H of a group G to be a subset H of G such that when the group operation of\nG is restricted to H, H is a group in its own right. Observe that every group G with at\nleast two elements will always have at least two subgroups, the subgroup consisting of the\nidentity element alone and the entire group itself. The subgroup H = {e} of a group G is\ncalled the trivial subgroup. A subgroup that is a proper subset of G is called a proper\nsubgroup. In many of the examples that we have investigated up to this point, there exist\nother subgroups besides the trivial and improper subgroups.\nExample 3.24. Consider the set of nonzero real numbers, R∗, with the group operation of\nmultiplication. The identity of this group is 1 and the inverse of any element a ∈ R∗ is just\n1/a. We will show that\n\nQ∗ = {p/q : p and q are nonzero integers}\n\nis a subgroup of R∗. The identity of R∗ is 1; however, 1 = 1/1 is the quotient of two nonzero\nintegers. Hence, the identity of R∗ is in Q∗. Given two elements in Q∗, say p/q and r/s,\ntheir product pr/qs is also in Q∗. The inverse of any element p/q ∈ Q∗ is again in Q∗ since\n(p/q)−1 = q/p. Since multiplication in R∗ is associative, multiplication in Q∗ is associative.\nExample 3.25. Recall that C∗ is the multiplicative group of nonzero complex numbers.\nLet H = {1,−1, i,−i}. Then H is a subgroup of C∗. It is quite easy to verify that H is a\ngroup under multiplication and that H ⊂ C∗.\nExample 3.26. Let SL2(R) be the subset of GL2(R)consisting of matrices of determinant\none; that is, a matrix\n\nA =\n\n(\na b\n\nc d\n\n)\nis in SL2(R) exactly when ad− bc = 1. To show that SL2(R) is a subgroup of the general\nlinear group, we must show that it is a group under matrix multiplication. The 2×2 identity\nmatrix is in SL2(R), as is the inverse of the matrix A:\n\nA−1 =\n\n(\nd −b\n−c a\n\n)\n.\n\nIt remains to show that multiplication is closed; that is, that the product of two matrices\nof determinant one also has determinant one. We will leave this task as an exercise. The\ngroup SL2(R) is called the special linear group.\nExample 3.27. It is important to realize that a subset H of a group G can be a group\nwithout being a subgroup of G. For H to be a subgroup of G it must inherit G’s binary\noperation. The set of all 2 × 2 matrices, M2(R), forms a group under the operation of\naddition. The 2× 2 general linear group is a subset of M2(R) and is a group under matrix\nmultiplication, but it is not a subgroup of M2(R). If we add two invertible matrices, we do\nnot necessarily obtain another invertible matrix. Observe that(\n\n1 0\n\n0 1\n\n)\n+\n\n(\n−1 0\n\n0 −1\n\n)\n=\n\n(\n0 0\n\n0 0\n\n)\n,\n\nbut the zero matrix is not in GL2(R).\n\n\n\n46 CHAPTER 3. GROUPS\n\nExample 3.28. One way of telling whether or not two groups are the same is by examining\ntheir subgroups. Other than the trivial subgroup and the group itself, the group Z4 has\na single subgroup consisting of the elements 0 and 2. From the group Z2, we can form\nanother group of four elements as follows. As a set this group is Z2 × Z2. We perform the\ngroup operation coordinatewise; that is, (a, b) + (c, d) = (a + c, b + d). Table 3.29 is an\naddition table for Z2 × Z2. Since there are three nontrivial proper subgroups of Z2 × Z2,\nH1 = {(0, 0), (0, 1)}, H2 = {(0, 0), (1, 0)}, and H3 = {(0, 0), (1, 1)}, Z4 and Z2 ×Z2 must be\ndifferent groups.\n\n+ (0, 0) (0, 1) (1, 0) (1, 1)\n\n(0, 0) (0, 0) (0, 1) (1, 0) (1, 1)\n\n(0, 1) (0, 1) (0, 0) (1, 1) (1, 0)\n\n(1, 0) (1, 0) (1, 1) (0, 0) (0, 1)\n\n(1, 1) (1, 1) (1, 0) (0, 1) (0, 0)\n\nTable 3.29: Addition table for Z2 × Z2\n\nSome Subgroup Theorems\nLet us examine some criteria for determining exactly when a subset of a group is a subgroup.\n\nProposition 3.30. A subset H of G is a subgroup if and only if it satisfies the following\nconditions.\n\n1. The identity e of G is in H.\n\n2. If h1, h2 ∈ H, then h1h2 ∈ H.\n\n3. If h ∈ H, then h−1 ∈ H.\n\nProof. First suppose that H is a subgroup of G. We must show that the three conditions\nhold. Since H is a group, it must have an identity eH . We must show that eH = e, where\ne is the identity of G. We know that eHeH = eH and that eeH = eHe = eH ; hence,\neeH = eHeH . By right-hand cancellation, e = eH . The second condition holds since a\nsubgroup H is a group. To prove the third condition, let h ∈ H. Since H is a group, there\nis an element h′ ∈ H such that hh′ = h′h = e. By the uniqueness of the inverse in G,\nh′ = h−1.\n\nConversely, if the three conditions hold, we must show that H is a group under the same\noperation as G; however, these conditions plus the associativity of the binary operation are\nexactly the axioms stated in the definition of a group.\n\nProposition 3.31. Let H be a subset of a group G. Then H is a subgroup of G if and only\nif H ̸= ∅, and whenever g, h ∈ H then gh−1 is in H.\n\nProof. First assume that H is a subgroup of G. We wish to show that gh−1 ∈ H whenever\ng and h are in H. Since h is in H, its inverse h−1 must also be in H. Because of the closure\nof the group operation, gh−1 ∈ H.\n\nConversely, suppose that H ⊂ G such that H ̸= ∅ and gh−1 ∈ H whenever g, h ∈ H. If\ng ∈ H, then gg−1 = e is in H. If g ∈ H, then eg−1 = g−1 is also in H. Now let h1, h2 ∈ H.\nWe must show that their product is also in H. However, h1(h−1\n\n2 )−1 = h1h2 ∈ H. Hence,\nH is a subgroup of G.\n\n\n\n3.4. EXERCISES 47\n\n3.4 Exercises\n1. Find all x ∈ Z satisfying each of the following equations.\n\n(a) 3x ≡ 2 (mod 7)\n\n(b) 5x+ 1 ≡ 13 (mod 23)\n\n(c) 5x+ 1 ≡ 13 (mod 26)\n\n(d) 9x ≡ 3 (mod 5)\n\n(e) 5x ≡ 1 (mod 6)\n\n(f) 3x ≡ 1 (mod 6)\n\n2. Which of the following multiplication tables defined on the set G = {a, b, c, d} form a\ngroup? Support your answer in each case.\n\n(a)\n◦ a b c d\n\na a c d a\n\nb b b c d\n\nc c d a b\n\nd d a b c\n\n(b)\n◦ a b c d\n\na a b c d\n\nb b a d c\n\nc c d a b\n\nd d c b a\n\n(c)\n◦ a b c d\n\na a b c d\n\nb b c d a\n\nc c d a b\n\nd d a b c\n\n(d)\n◦ a b c d\n\na a b c d\n\nb b a c d\n\nc c b a d\n\nd d d b c\n\n3. Write out Cayley tables for groups formed by the symmetries of a rectangle and for\n(Z4,+). How many elements are in each group? Are the groups the same? Why or why\nnot?\n\n4. Describe the symmetries of a rhombus and prove that the set of symmetries forms a\ngroup. Give Cayley tables for both the symmetries of a rectangle and the symmetries of a\nrhombus. Are the symmetries of a rectangle and those of a rhombus the same?\n\n5. Describe the symmetries of a square and prove that the set of symmetries is a group.\nGive a Cayley table for the symmetries. How many ways can the vertices of a square be\npermuted? Is each permutation necessarily a symmetry of the square? The symmetry group\nof the square is denoted by D4.\n\n6. Give a multiplication table for the group U(12).\n\n7. Let S = R \\ {−1} and define a binary operation on S by a ∗ b = a+ b+ ab. Prove that\n(S, ∗) is an abelian group.\n\n8. Give an example of two elements A and B in GL2(R) with AB ̸= BA.\n\n9. Prove that the product of two matrices in SL2(R) has determinant one.\n\n10. Prove that the set of matrices of the form\uf8eb\uf8ed1 x y\n\n0 1 z\n\n0 0 1\n\n\uf8f6\uf8f8\n\n\n\n48 CHAPTER 3. GROUPS\n\nis a group under matrix multiplication. This group, known as the Heisenberg group, is\nimportant in quantum physics. Matrix multiplication in the Heisenberg group is defined by\uf8eb\uf8ed1 x y\n\n0 1 z\n\n0 0 1\n\n\uf8f6\uf8f8\uf8eb\uf8ed1 x′ y′\n\n0 1 z′\n\n0 0 1\n\n\uf8f6\uf8f8 =\n\n\uf8eb\uf8ed1 x+ x′ y + y′ + xz′\n\n0 1 z + z′\n\n0 0 1\n\n\uf8f6\uf8f8 .\n\n11. Prove that det(AB) = det(A)det(B) in GL2(R). Use this result to show that the\nbinary operation in the group GL2(R) is closed; that is, if A and B are in GL2(R), then\nAB ∈ GL2(R).\n\n12. Let Zn\n2 = {(a1, a2, . . . , an) : ai ∈ Z2}. Define a binary operation on Zn\n\n2 by\n\n(a1, a2, . . . , an) + (b1, b2, . . . , bn) = (a1 + b1, a2 + b2, . . . , an + bn).\n\nProve that Zn\n2 is a group under this operation. This group is important in algebraic coding\n\ntheory.\n\n13. Show that R∗ = R \\ {0} is a group under the operation of multiplication.\n\n14. Given the groups R∗ and Z, let G = R∗ × Z. Define a binary operation ◦ on G by\n(a,m) ◦ (b, n) = (ab,m+ n). Show that G is a group under this operation.\n\n15. Prove or disprove that every group containing six elements is abelian.\n\n16. Give a specific example of some group G and elements g, h ∈ G where (gh)n ̸= gnhn.\n\n17. Give an example of three different groups with eight elements. Why are the groups\ndifferent?\n\n18. Show that there are n! permutations of a set containing n items.\n\n19. Show that\n0 + a ≡ a+ 0 ≡ a (mod n)\n\nfor all a ∈ Zn.\n\n20. Prove that there is a multiplicative identity for the integers modulo n:\n\na · 1 ≡ a (mod n).\n\n21. For each a ∈ Zn find an element b ∈ Zn such that\n\na+ b ≡ b+ a ≡ 0 (mod n).\n\n22. Show that addition and multiplication mod n are well defined operations. That is, show\nthat the operations do not depend on the choice of the representative from the equivalence\nclasses mod n.\n\n23. Show that addition and multiplication mod n are associative operations.\n\n24. Show that multiplication distributes over addition modulo n:\n\na(b+ c) ≡ ab+ ac (mod n).\n\n25. Let a and b be elements in a group G. Prove that abna−1 = (aba−1)n for n ∈ Z.\n\n\n\n3.4. EXERCISES 49\n\n26. Let U(n) be the group of units in Zn. If n > 2, prove that there is an element k ∈ U(n)\nsuch that k2 = 1 and k ̸= 1.\n\n27. Prove that the inverse of g1g2 · · · gn is g−1\nn g−1\n\nn−1 · · · g\n−1\n1 .\n\n28. Prove the remainder of Proposition 3.21: if G is a group and a, b ∈ G, then the equation\nxa = b has a unique solution in G.\n\n29. Prove Theorem 3.23.\n\n30. Prove the right and left cancellation laws for a group G; that is, show that in the group\nG, ba = ca implies b = c and ab = ac implies b = c for elements a, b, c ∈ G.\n\n31. Show that if a2 = e for all elements a in a group G, then G must be abelian.\n\n32. Show that if G is a finite group of even order, then there is an a ∈ G such that a is not\nthe identity and a2 = e.\n\n33. Let G be a group and suppose that (ab)2 = a2b2 for all a and b in G. Prove that G is\nan abelian group.\n\n34. Find all the subgroups of Z3 × Z3. Use this information to show that Z3 × Z3 is not\nthe same group as Z9. (See Example 3.28 for a short description of the product of groups.)\n\n35. Find all the subgroups of the symmetry group of an equilateral triangle.\n\n36. Compute the subgroups of the symmetry group of a square.\n\n37. Let H = {2k : k ∈ Z}. Show that H is a subgroup of Q∗.\n\n38. Let n = 0, 1, 2, . . . and nZ = {nk : k ∈ Z}. Prove that nZ is a subgroup of Z. Show\nthat these subgroups are the only subgroups of Z.\n\n39. Let T = {z ∈ C∗ : |z| = 1}. Prove that T is a subgroup of C∗.\n\n40. (\ncos θ − sin θ\nsin θ cos θ\n\n)\nwhere θ ∈ R. Prove that G is a subgroup of SL2(R).\n\n41. Prove that\n\nG = {a+ b\n√\n2 : a, b ∈ Q and a and b are not both zero}\n\nis a subgroup of R∗ under the group operation of multiplication.\n\n42. Let G be the group of 2× 2 matrices under addition and\n\nH =\n\n{(\na b\n\nc d\n\n)\n: a+ d = 0\n\n}\n.\n\nProve that H is a subgroup of G.\n\n43. Prove or disprove: SL2(Z), the set of 2×2 matrices with integer entries and determinant\none, is a subgroup of SL2(R).\n\n44. List the subgroups of the quaternion group, Q8.\n\n45. Prove that the intersection of two subgroups of a group G is also a subgroup of G.\n\n\n\n50 CHAPTER 3. GROUPS\n\n46. Prove or disprove: If H and K are subgroups of a group G, then H ∪K is a subgroup\nof G.\n\n47. Prove or disprove: If H and K are subgroups of a group G, then HK = {hk : h ∈\nH and k ∈ K} is a subgroup of G. What if G is abelian?\n\n48. Let G be a group and g ∈ G. Show that\n\nZ(G) = {x ∈ G : gx = xg for all g ∈ G}\n\nis a subgroup of G. This subgroup is called the center of G.\n\n49. Let a and b be elements of a group G. If a4b = ba and a3 = e, prove that ab = ba.\n\n50. Give an example of an infinite group in which every nontrivial subgroup is infinite.\n\n51. If xy = x−1y−1 for all x and y in G, prove that G must be abelian.\n\n52. Prove or disprove: Every proper subgroup of an nonabelian group is nonabelian.\n\n53. Let H be a subgroup of G and\n\nC(H) = {g ∈ G : gh = hg for all h ∈ H}.\n\nProve C(H) is a subgroup of G. This subgroup is called the centralizer of H in G.\n\n54. Let H be a subgroup of G. If g ∈ G, show that gHg−1 = {ghg−1 : h ∈ H} is also a\nsubgroup of G.\n\n3.5 Additional Exercises: Detecting Errors\n\n1. (UPC Symbols) Universal Product Code (upc) symbols are found on most products in\ngrocery and retail stores. The upc symbol is a 12-digit code identifying the manufacturer\nof a product and the product itself (Figure 3.32). The first 11 digits contain information\nabout the product; the twelfth digit is used for error detection. If d1d2 · · · d12 is a valid upc\nnumber, then\n\n3 · d1 + 1 · d2 + 3 · d3 + · · ·+ 3 · d11 + 1 · d12 ≡ 0 (mod 10).\n\n(a) Show that the upc number 0-50000-30042-6, which appears in Figure 3.32, is a valid\nupc number.\n\n(b) Show that the number 0-50000-30043-6 is not a valid upc number.\n\n(c) Write a formula to calculate the check digit, d12, in the upc number.\n\n(d) The upc error detection scheme can detect most transposition errors; that is, it can\ndetermine if two digits have been interchanged. Show that the transposition error\n0-05000-30042-6 is not detected. Find a transposition error that is detected. Can you\nfind a general rule for the types of transposition errors that can be detected?\n\n(e) Write a program that will determine whether or not a upc number is valid.\n\n\n\n3.5. ADDITIONAL EXERCISES: DETECTING ERRORS 51\n\nFigure 3.32: A upc code\n\n2. It is often useful to use an inner product notation for this type of error detection scheme;\nhence, we will use the notion\n\n(d1, d2, . . . , dk) · (w1, w2, . . . , wk) ≡ 0 (mod n)\n\nto mean\nd1w1 + d2w2 + · · ·+ dkwk ≡ 0 (mod n).\n\nSuppose that (d1, d2, . . . , dk) · (w1, w2, . . . , wk) ≡ 0 (mod n) is an error detection scheme for\nthe k-digit identification number d1d2 · · · dk, where 0 ≤ di < n. Prove that all single-digit\nerrors are detected if and only if gcd(wi, n) = 1 for 1 ≤ i ≤ k.\n\n3. Let (d1, d2, . . . , dk) · (w1, w2, . . . , wk) ≡ 0 (mod n) be an error detection scheme for the\nk-digit identification number d1d2 · · · dk, where 0 ≤ di < n. Prove that all transposition\nerrors of two digits di and dj are detected if and only if gcd(wi − wj , n) = 1 for i and j\nbetween 1 and k.\n\n4. (ISBN Codes) Every book has an International Standard Book Number (isbn) code.\nThis is a 10-digit code indicating the book’s publisher and title. The tenth digit is a check\ndigit satisfying\n\n(d1, d2, . . . , d10) · (10, 9, . . . , 1) ≡ 0 (mod 11).\n\nOne problem is that d10 might have to be a 10 to make the inner product zero; in this case,\n11 digits would be needed to make this scheme work. Therefore, the character X is used for\nthe eleventh digit. So isbn 3-540-96035-X is a valid isbn code.\n(a) Is isbn 0-534-91500-0 a valid isbn code? What about isbn 0-534-91700-0 and isbn\n\n0-534-19500-0?\n(b) Does this method detect all single-digit errors? What about all transposition errors?\n(c) How many different isbn codes are there?\n(d) Write a computer program that will calculate the check digit for the first nine digits\n\nof an isbn code.\n(e) A publisher has houses in Germany and the United States. Its German prefix is 3-540.\n\nIf its United States prefix will be 0-abc, find abc such that the rest of the isbn code\nwill be the same for a book printed in Germany and in the United States. Under the\nisbn coding method the first digit identifies the language; German is 3 and English is\n\n\n\n52 CHAPTER 3. GROUPS\n\n0. The next group of numbers identifies the publisher, and the last group identifies\nthe specific book.\n\n3.6 References and Suggested Readings\n[1] Burnside, W. Theory of Groups of Finite Order. 2nd ed. Cambridge University Press,\n\nCambridge, 1911; Dover, New York, 1953. A classic. Also available at books.google.com.\n[2] Gallian, J. A. and Winters, S. “Modular Arithmetic in the Marketplace,” The Amer-\n\nican Mathematical Monthly 95 (1988): 548–51.\n[3] Gallian, J. A. Contemporary Abstract Algebra. 7th ed. Brooks/Cole, Belmont, CA,\n\n2009.\n[4] Hall, M. Theory of Groups. 2nd ed. American Mathematical Society, Providence,\n\n1959.\n[5] Kurosh, A. E. The Theory of Groups, vols. I and II. American Mathematical Society,\n\nProvidence, 1979.\n[6] Rotman, J. J. An Introduction to the Theory of Groups. 4th ed. Springer, New York,\n\n1995.\n\n3.7 Sage\nMany of the groups discussed in this chapter are available for study in Sage. It is important\nto understand that sets that form algebraic objects (groups in this chapter) are called\n“parents” in Sage, and elements of these objects are called, well, “elements.” So every\nelement belongs to a parent (in other words, is contained in some set). We can ask about\nproperties of parents (finite? order? abelian?), and we can ask about properties of individual\nelements (identity? inverse?). In the following we will show you how to create some of these\ncommon groups and begin to explore their properties with Sage.\n\nIntegers mod n\n\nZ8 = Integers (8)\nZ8\n\nRing of integers modulo 8\n\nZ8.list()\n\n[0, 1, 2, 3, 4, 5, 6, 7]\n\na = Z8.an_element (); a\n\n0\n\na.parent ()\n\nRing of integers modulo 8\n\n\n\n3.7. SAGE 53\n\nWe would like to work with elements of Z8. If you were to type a 6 into a compute cell\nright now, what would you mean? The integer 6, the rational number 6\n\n1 , the real number\n6.00000, or the complex number 6.00000 + 0.00000i? Or perhaps you really do want the\ninteger 6 mod 8? Sage really has no idea what you mean or want. To make this clear, you\ncan “coerce” 6 into Z8 with the syntax Z8(6). Without this, Sage will treat a input number\nlike 6 as an integer, the simplest possible interpretation in some sense. Study the following\ncarefully, where we first work with “normal” integers and then with integers mod 8.\n\na = 6\na\n\n6\n\na.parent ()\n\nInteger Ring\n\nb = 7\nc = a + b; c\n\n13\n\nd = Z8(6)\nd\n\n6\n\nd.parent ()\n\nRing of integers modulo 8\n\ne = Z8(7)\nf = d+e; f\n\n5\n\ng = Z8(85); g\n\n5\n\nf == g\n\nTrue\n\nZ8 is a bit unusual as a first example, since it has two operations defined, both addition\nand multiplication, with addition forming a group, and multiplication not forming a group.\nStill, we can work with the additive portion, here forming the Cayley table for the addition.\n\nZ8.addition_table(names= \' elements \' )\n\n\n\n54 CHAPTER 3. GROUPS\n\n+ 0 1 2 3 4 5 6 7\n+----------------\n\n0| 0 1 2 3 4 5 6 7\n1| 1 2 3 4 5 6 7 0\n2| 2 3 4 5 6 7 0 1\n3| 3 4 5 6 7 0 1 2\n4| 4 5 6 7 0 1 2 3\n5| 5 6 7 0 1 2 3 4\n6| 6 7 0 1 2 3 4 5\n7| 7 0 1 2 3 4 5 6\n\nWhen n is a prime number, the multipicative structure (excluding zero), will also form\na group.\n\nThe integers mod n are very important, so Sage implements both addition and mul-\ntiplication together. Groups of symmetries are a better example of how Sage implements\ngroups, since there is just one operation present.\n\nGroups of symmetries\n\nThe symmetries of some geometric shapes are already defined in Sage, albeit with different\nnames. They are implemented as “permutation groups” which we will begin to study\ncarefully in Chapter 5.\n\nSage uses integers to label vertices, starting the count at 1, instead of letters. Elements\nby default are printed using “cycle notation” which we will see described carefully in Chap-\nter 5. Here is an example, with both the mathematics and Sage. For the Sage part, we\ncreate the group of symmetries and then create the symmetry ρ2 with coercion, followed\nby outputting the element in cycle notation. Then we create just the bottom row of the\nnotation we are using for permutations.\n\nρ2 =\n\n(\nA B C\n\nC A B\n\n)\n=\n\n(\n1 2 3\n\n3 1 2\n\n)\n\ntriangle = SymmetricGroup (3)\nrho2 = triangle ([3,1 ,2])\nrho2\n\n(1,3,2)\n\n[rho2(x) for x in triangle.domain ()]\n\n[3, 1, 2]\n\nThe final list comprehension deserves comment. The .domain() method gives a lait of\nthe symbols used for the permutation group triangle and then rho2 is employed with syntax\nlike it is a function (it is a function) to create the images that would occupy the bottom\nrow.\n\nWith a double list comprehension we can list all six elements of the group in the “bottom\nrow” format. A good exercise would be to pair up each element with its name as given in\nFigure 3.6.\n\n[[a(x) for x in triangle.domain ()] for a in triangle]\n\n[[1, 2, 3], [2, 1, 3], [2, 3, 1], [3, 1, 2], [1, 3, 2], [3, 2, 1]]\n\n\n\n3.7. SAGE 55\n\nDifferent books, different authors, different software all have different ideas about the\norder in which to write multiplication of functions. This textbook builds on the idea of\ncomposition of functions, so that fg is the composition (fg)(x) = f(g(x)) and it is natural\nto apply g first. Sage takes the opposite view and since we write fg, Sage will understand\nthat we want to do f first. Neither approach is wrong, and neither is necessarily superior,\nthey are just different and there are good arguments for either one. When you consult other\nbooks that work with permutation groups, you want to first determine which approach it\ntakes. (Be aware that this discussion of Sage function composition is limited to permutations\nonly—“regular” functions in Sage compose in the order you might be familiar with from a\ncalculus course.)\n\nThe translation here between the text and Sage will be worthwhile practice. Here we\nwill reprise the discussion at the end of Section 3.1, but reverse the order on each product\nto compute Sage-style and exactly mirror what the text does.\n\nmu1 = triangle ([1 ,3,2])\nmu2 = triangle ([3 ,2,1])\nmu3 = triangle ([2 ,1,3])\nrho1 = triangle ([2,3 ,1])\nproduct = rho1*mu1\nproduct == mu2\n\nTrue\n\n[product(x) for x in triangle.domain ()]\n\n[3, 2, 1]\n\nrho1*mu1 == mu1*rho1\n\nFalse\n\nmu1*rho1 == mu3\n\nTrue\n\nNow that we understand that Sage does multiplication in reverse, we can compute the\nCayley table for this group. Default behavior is to just name elements of a group as letters,\na, b, c, … in the same order that the .list() command would produce the elements of\nthe group. But you can also print the elements in the table as themselves (that uses cycle\nnotation here), or you can give the elements names. We will use u as shorthand for µ and\nr as shorthand for ρ.\n\ntriangle.cayley_table ()\n\n* a b c d e f\n+------------\n\na| a b c d e f\nb| b a f e d c\nc| c e d a f b\nd| d f a c b e\ne| e c b f a d\nf| f d e b c a\n\ntriangle.cayley_table(names= \' elements \' )\n\n\n\n56 CHAPTER 3. GROUPS\n\n* () (1,2) (1,2,3) (1,3,2) (2,3) (1,3)\n+------------------------------------------------\n\n()| () (1,2) (1,2,3) (1,3,2) (2,3) (1,3)\n(1,2)| (1,2) () (1,3) (2,3) (1,3,2) (1,2,3)\n\n(1,2,3)| (1,2,3) (2,3) (1,3,2) () (1,3) (1,2)\n(1,3,2)| (1,3,2) (1,3) () (1,2,3) (1,2) (2,3)\n\n(2,3)| (2,3) (1,2,3) (1,2) (1,3) () (1,3,2)\n(1,3)| (1,3) (1,3,2) (2,3) (1,2) (1,2,3) ()\n\ntriangle.cayley_table(names=[ \' id \' , \' u1 \' , \' u3 \' , \' r1 \' , \' r2 \' , \' u2 \' ])\n\n* id u1 u3 r1 r2 u2\n+------------------\n\nid| id u1 u3 r1 r2 u2\nu1| u1 id u2 r2 r1 u3\nu3| u3 r2 r1 id u2 u1\nr1| r1 u2 id u3 u1 r2\nr2| r2 u3 u1 u2 id r1\nu2| u2 r1 r2 u1 u3 id\n\nYou should verify that the table above is correct, just like Table 3.2 is correct. Remember\nthat the convention is to multiply a row label times a column label, in that order. However,\nto do a check across the two tables, you will need to recall the difference in ordering between\nyour textbook and Sage.\n\nQuaternions\nSage implements the quaternions, but the elements are not matrices, but rather are permu-\ntations. Despite appearances the structure is identical. It should not matter which version\nyou have in mind (matrices or permutations) if you build the Cayley table and use the\ndefault behavior of using letters to name the elements. As permutations, or as letters, can\nyou identify −1, I, J and K?\n\nQ = QuaternionGroup ()\n[[a(x) for x in Q.domain ()] for a in Q]\n\n[[1, 2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 1, 6, 7, 8, 5],\n[5, 8, 7, 6, 3, 2, 1, 4], [3, 4, 1, 2, 7, 8, 5, 6],\n[6, 5, 8, 7, 4, 3, 2, 1], [8, 7, 6, 5, 2, 1, 4, 3],\n[4, 1, 2, 3, 8, 5, 6, 7], [7, 6, 5, 8, 1, 4, 3, 2]]\n\nQ.cayley_table ()\n\n* a b c d e f g h\n+----------------\n\na| a b c d e f g h\nb| b d f g c h a e\nc| c e d h g b f a\nd| d g h a f e b c\ne| e h b f d a c g\nf| f c g e a d h b\ng| g a e b h c d f\nh| h f a c b g e d\n\nIt should be fairly obvious that a is the identity element of the group (1), either from\nits behavior in the table, or from its “bottom row” representation in the list above. And if\nyou prefer, you can ask Sage.\n\n\n\n3.7. SAGE 57\n\nid = Q.identity ()\n[id(x) for x in Q.domain ()]\n\n[1, 2, 3, 4, 5, 6, 7, 8]\n\nNow −1 should have the property that −1 · −1 = 1. We see that the identity element a\n\nis on the diagonal of the Cayley table only when we compute c*c. We can verify this easily,\nborrowing the third “bottom row” element from the list above. With this information, once\nwe locate I, we can easily compute −I, and so on.\n\nminus_one = Q([3, 4, 1, 2, 7, 8, 5, 6])\nminus_one*minus_one == Q.identity ()\n\nTrue\n\nSee if you can pair up the letters with all eight elements of the quaternions. Be a bit\ncareful with your names, the symbol I is used by Sage for the imaginary number i (which\nwe will use below), but Sage will silently let you redefine it to be anything you like. Same\ngoes for lower-case i. So call your elements of the quaternions something like QI, QJ, QK to\navoid confusion.\n\nAs we begin to work with groups it is instructive to work with the actual elements. But\nmany properties of groups are totally independent of the order we use for multiplication, or\nthe names or representations we use for the elements. Here are facts about the quaternions\nwe can compute without any knowledge of just how the elements are written or multiplied.\n\nQ.is_finite ()\n\nTrue\n\nQ.order()\n\n8\n\nQ.is_abelian ()\n\nFalse\n\nSubgroups\nThe best techniques for creating subgroups will come in future chapters, but we can create\nsome groups that are naturally subgroups of other groups.\n\nElements of the quaternions were represented by certain permutations of the integers 1\nthrough 8. We can also build the group of all permutations of these eight integers. It gets\npretty big, so do not list it unless you want a lot of output! (I dare you.)\n\nS8 = SymmetricGroup (8)\na = S8.random_element ()\n[a(x) for x in S8.domain ()] # random\n\n[5, 2, 6, 4, 1, 8, 3, 7]\n\nS8.order ()\n\n40320\n\n\n\n58 CHAPTER 3. GROUPS\n\nThe quaternions, Q, is a subgroup of the full group of all permutations, the symmetric\ngroup S8 or S8, and Sage regards this as a property of Q.\n\nQ.is_subgroup(S8)\n\nTrue\n\nIn Sage the complex numbers are known by the name CC. We can create a list of the\nelements in the subgroup described in Example 3.16. Then we can verify that this set is a\nsubgroup by examining the Cayley table, using multiplication as the operation.\n\nH = [CC(1), CC(-1), CC(I), CC(-I)]\nCC.multiplication_table(elements=H,\n\nnames=[ \' 1 \' , \' -1 \' , \' i \' , \' -i \' ])\n\n* 1 -1 i -i\n+------------\n\n1| 1 -1 i -i\n-1| -1 1 -i i\ni| i -i -1 1\n\n-i| -i i 1 -1\n\n3.8 Sage Exercises\nThese exercises are about becoming comfortable working with groups in Sage.\n1. Create the groups CyclicPermutationGroup(8) and DihedralGroup(4) and name these groups\nC and D, respectively. We will understand these constructions better shortly, but for now\njust understand that both objects you create are actually groups.\n\n2. Check that C and D have the same size by using the .order() method. Determine which\ngroup is abelian, and which is not, by using the .is_abelian() method.\n\n3. Use the .cayley_table() method to create the Cayley table for each group.\n\n4. Write a nicely formatted discussion identifying differences between the two groups that\nare discernible in properties of their Cayley tables. In other words, what is different about\nthese two groups that you can “see” in the Cayley tables? (In the Sage notebook, a Shift-\nclick on a blue bar will bring up a mini-word-processor, and you can use use dollar signs to\nembed mathematics formatted using TEX syntax.)\n\n5. For C locate the one subgroup of order 4. The group D has three subgroups of order 4.\nSelect one of the three subgroups of D that has a different structure than the subgroup you\nobtained from C.\nThe .subgroups() method will give you a list of all of the subgroups to help you get started.\nA Cayley table will help you tell the difference between the two subgroups. What properties\nof these tables did you use to determine the difference in the structure of the subgroups?\n\n6. The .subgroup(elt_list) method of a group will create the smallest subgroup containing\nthe specified elements of the group, when given the elements as a list elt_list. Use this\ncommand to discover the shortest list of elements necessary to recreate the subgroups you\nfound in the previous exercise. The equality comparison, ==, can be used to test if two\nsubgroups are equal.\n\n\n\n4\n\nCyclic Groups\n\nThe groups Z and Zn, which are among the most familiar and easily understood groups, are\nboth examples of what are called cyclic groups. In this chapter we will study the properties\nof cyclic groups and cyclic subgroups, which play a fundamental part in the classification\nof all abelian groups.\n\n4.1 Cyclic Subgroups\n\nOften a subgroup will depend entirely on a single element of the group; that is, knowing\nthat particular element will allow us to compute any other element in the subgroup.\n\nExample 4.1. Suppose that we consider 3 ∈ Z and look at all multiples (both positive and\nnegative) of 3. As a set, this is\n\n3Z = {. . . ,−3, 0, 3, 6, . . .}.\n\nIt is easy to see that 3Z is a subgroup of the integers. This subgroup is completely deter-\nmined by the element 3 since we can obtain all of the other elements of the group by taking\nmultiples of 3. Every element in the subgroup is “generated” by 3.\n\nExample 4.2. If H = {2n : n ∈ Z}, then H is a subgroup of the multiplicative group of\nnonzero rational numbers, Q∗. If a = 2m and b = 2n are in H, then ab−1 = 2m2−n = 2m−n\n\nis also in H. By Proposition 3.31, H is a subgroup of Q∗ determined by the element 2.\n\nTheorem 4.3. Let G be a group and a be any element in G. Then the set\n\n⟨a⟩ = {ak : k ∈ Z}\n\nis a subgroup of G. Furthermore, ⟨a⟩ is the smallest subgroup of G that contains a.\n\nProof. The identity is in ⟨a⟩ since a0 = e. If g and h are any two elements in ⟨a⟩, then\nby the definition of ⟨a⟩ we can write g = am and h = an for some integers m and n. So\ngh = aman = am+n is again in ⟨a⟩. Finally, if g = an in ⟨a⟩, then the inverse g−1 = a−n\n\nis also in ⟨a⟩. Clearly, any subgroup H of G containing a must contain all the powers of a\nby closure; hence, H contains ⟨a⟩. Therefore, ⟨a⟩ is the smallest subgroup of G containing\na.\n\nRemark 4.4. If we are using the “+” notation, as in the case of the integers under addition,\nwe write ⟨a⟩ = {na : n ∈ Z}.\n\n59\n\n\n\n60 CHAPTER 4. CYCLIC GROUPS\n\nFor a ∈ G, we call ⟨a⟩ the cyclic subgroup generated by a. If G contains some element\na such that G = ⟨a⟩, then G is a cyclic group. In this case a is a generator of G. If a\nis an element of a group G, we define the order of a to be the smallest positive integer n\nsuch that an = e, and we write |a| = n. If there is no such integer n, we say that the order\nof a is infinite and write |a| = ∞ to denote the order of a.\n\nExample 4.5. Notice that a cyclic group can have more than a single generator. Both\n1 and 5 generate Z6; hence, Z6 is a cyclic group. Not every element in a cyclic group\nis necessarily a generator of the group. The order of 2 ∈ Z6 is 3. The cyclic subgroup\ngenerated by 2 is ⟨2⟩ = {0, 2, 4}.\n\nThe groups Z and Zn are cyclic groups. The elements 1 and −1 are generators for Z.\nWe can certainly generate Zn with 1 although there may be other generators of Zn, as in\nthe case of Z6.\n\nExample 4.6. The group of units, U(9), in Z9 is a cyclic group. As a set, U(9) is\n{1, 2, 4, 5, 7, 8}. The element 2 is a generator for U(9) since\n\n21 = 2 22 = 4\n\n23 = 8 24 = 7\n\n25 = 5 26 = 1.\n\nExample 4.7. Not every group is a cyclic group. Consider the symmetry group of an\nequilateral triangle S3. The multiplication table for this group is Table 3.7. The subgroups\nof S3 are shown in Figure 4.8. Notice that every subgroup is cyclic; however, no single\nelement generates the entire group.\n\n{id, ρ1, ρ2} {id, µ1} {id, µ2} {id, µ3}\n\nS3\n\n{id}\n\nFigure 4.8: Subgroups of S3\n\nTheorem 4.9. Every cyclic group is abelian.\n\nProof. Let G be a cyclic group and a ∈ G be a generator for G. If g and h are in G, then\nthey can be written as powers of a, say g = ar and h = as. Since\n\ngh = aras = ar+s = as+r = asar = hg,\n\nG is abelian.\n\nSubgroups of Cyclic Groups\nWe can ask some interesting questions about cyclic subgroups of a group and subgroups of\na cyclic group. If G is a group, which subgroups of G are cyclic? If G is a cyclic group,\nwhat type of subgroups does G possess?\n\n\n\n4.1. CYCLIC SUBGROUPS 61\n\nTheorem 4.10. Every subgroup of a cyclic group is cyclic.\nProof. The main tools used in this proof are the division algorithm and the Principle of\nWell-Ordering. Let G be a cyclic group generated by a and suppose that H is a subgroup\nof G. If H = {e}, then trivially H is cyclic. Suppose that H contains some other element\ng distinct from the identity. Then g can be written as an for some integer n. Since H is a\nsubgroup, g−1 = a−n must also be in H. Since either n or −n is postive, we can assume\nthat H contains positve powers of a and n > 0. Let m be the smallest natural number such\nthat am ∈ H. Such an m exists by the Principle of Well-Ordering.\n\nWe claim that h = am is a generator for H. We must show that every h′ ∈ H can be\nwritten as a power of h. Since h′ ∈ H and H is a subgroup of G, h′ = ak for some integer\nk. Using the division algorithm, we can find numbers q and r such that k = mq + r where\n0 ≤ r < m; hence,\n\nak = amq+r = (am)qar = hqar.\n\nSo ar = akh−q. Since ak and h−q are in H, ar must also be in H. However, m was the\nsmallest positive number such that am was in H; consequently, r = 0 and so k = mq.\nTherefore,\n\nh′ = ak = amq = hq\n\nand H is generated by h.\n\nCorollary 4.11. The subgroups of Z are exactly nZ for n = 0, 1, 2, . . ..\nProposition 4.12. Let G be a cyclic group of order n and suppose that a is a generator\nfor G. Then ak = e if and only if n divides k.\nProof. First suppose that ak = e. By the division algorithm, k = nq+ r where 0 ≤ r < n;\nhence,\n\ne = ak = anq+r = anqar = ear = ar.\n\nSince the smallest positive integer m such that am = e is n, r = 0.\nConversely, if n divides k, then k = ns for some integer s. Consequently,\n\nak = ans = (an)s = es = e.\n\nTheorem 4.13. Let G be a cyclic group of order n and suppose that a ∈ G is a generator\nof the group. If b = ak, then the order of b is n/d, where d = gcd(k, n).\nProof. We wish to find the smallest integer m such that e = bm = akm. By Proposi-\ntion 4.12, this is the smallest integer m such that n divides km or, equivalently, n/d divides\nm(k/d). Since d is the greatest common divisor of n and k, n/d and k/d are relatively\nprime. Hence, for n/d to divide m(k/d) it must divide m. The smallest such m is n/d.\n\nCorollary 4.14. The generators of Zn are the integers r such that 1 ≤ r < n and gcd(r, n) =\n1.\nExample 4.15. Let us examine the group Z16. The numbers 1, 3, 5, 7, 9, 11, 13, and 15\nare the elements of Z16 that are relatively prime to 16. Each of these elements generates\nZ16. For example,\n\n1 · 9 = 9 2 · 9 = 2 3 · 9 = 11\n\n4 · 9 = 4 5 · 9 = 13 6 · 9 = 6\n\n7 · 9 = 15 8 · 9 = 8 9 · 9 = 1\n\n10 · 9 = 10 11 · 9 = 3 12 · 9 = 12\n\n13 · 9 = 5 14 · 9 = 14 15 · 9 = 7.\n\n\n\n62 CHAPTER 4. CYCLIC GROUPS\n\n4.2 Multiplicative Group of Complex Numbers\nThe complex numbers are defined as\n\nC = {a+ bi : a, b ∈ R},\n\nwhere i2 = −1. If z = a+ bi, then a is the real part of z and b is the imaginary part of\nz.\n\nTo add two complex numbers z = a+ bi and w = c+ di, we just add the corresponding\nreal and imaginary parts:\n\nz + w = (a+ bi) + (c+ di) = (a+ c) + (b+ d)i.\n\nRemembering that i2 = −1, we multiply complex numbers just like polynomials. The\nproduct of z and w is\n\n(a+ bi)(c+ di) = ac+ bdi2 + adi+ bci = (ac− bd) + (ad+ bc)i.\n\nEvery nonzero complex number z = a + bi has a multiplicative inverse; that is, there\nexists a z−1 ∈ C∗ such that zz−1 = z−1z = 1. If z = a+ bi, then\n\nz−1 =\na− bi\n\na2 + b2\n.\n\nThe complex conjugate of a complex number z = a+ bi is defined to be z = a− bi. The\nabsolute value or modulus of z = a+ bi is |z| =\n\n√\na2 + b2.\n\nExample 4.16. Let z = 2 + 3i and w = 1− 2i. Then\n\nz + w = (2 + 3i) + (1− 2i) = 3 + i\n\nand\nzw = (2 + 3i)(1− 2i) = 8− i.\n\nAlso,\n\nz−1 =\n2\n\n13\n− 3\n\n13\ni\n\n|z| =\n√\n13\n\nz = 2− 3i.\n\ny\n\nx0\n\nz1 = 2 + 3i\n\nz3 = −3 + 2i\n\nz2 = 1− 2i\n\nFigure 4.17: Rectangular coordinates of a complex number\n\n\n\n4.2. MULTIPLICATIVE GROUP OF COMPLEX NUMBERS 63\n\nThere are several ways of graphically representing complex numbers. We can represent\na complex number z = a+ bi as an ordered pair on the xy plane where a is the x (or real)\ncoordinate and b is the y (or imaginary) coordinate. This is called the rectangular or\nCartesian representation. The rectangular representations of z1 = 2+3i, z2 = 1− 2i, and\nz3 = −3 + 2i are depicted in Figure 4.17.\n\ny\n\nx0\n\na+ bi\n\nr\n\nθ\n\nFigure 4.18: Polar coordinates of a complex number\n\nNonzero complex numbers can also be represented using polar coordinates. To specify\nany nonzero point on the plane, it suffices to give an angle θ from the positive x axis in the\ncounterclockwise direction and a distance r from the origin, as in Figure 4.18. We can see\nthat\n\nz = a+ bi = r(cos θ + i sin θ).\nHence,\n\nr = |z| =\n√\na2 + b2\n\nand\n\na = r cos θ\nb = r sin θ.\n\nWe sometimes abbreviate r(cos θ + i sin θ) as r cis θ. To assure that the representation of z\nis well-defined, we also require that 0◦ ≤ θ < 360◦. If the measurement is in radians, then\n0 ≤ θ < 2π.\n\nExample 4.19. Suppose that z = 2 cis 60◦. Then\n\na = 2 cos 60◦ = 1\n\nand\nb = 2 sin 60◦ =\n\n√\n3.\n\nHence, the rectangular representation is z = 1 +\n√\n3 i.\n\nConversely, if we are given a rectangular representation of a complex number, it is often\nuseful to know the number’s polar representation. If z = 3\n\n√\n2− 3\n\n√\n2 i, then\n\nr =\n√\na2 + b2 =\n\n√\n36 = 6\n\nand\nθ = arctan\n\n(\nb\n\na\n\n)\n= arctan(−1) = 315◦,\n\nso 3\n√\n2− 3\n\n√\n2 i = 6 cis 315◦.\n\n\n\n64 CHAPTER 4. CYCLIC GROUPS\n\nThe polar representation of a complex number makes it easy to find products and powers\nof complex numbers. The proof of the following proposition is straightforward and is left\nas an exercise.\n\nProposition 4.20. Let z = r cis θ and w = s cisϕ be two nonzero complex numbers. Then\n\nzw = rs cis(θ + ϕ).\n\nExample 4.21. If z = 3 cis(π/3) and w = 2 cis(π/6), then zw = 6 cis(π/2) = 6i.\n\nTheorem 4.22 (DeMoivre). Let z = r cis θ be a nonzero complex number. Then\n\n[r cis θ]n = rn cis(nθ)\n\nfor n = 1, 2, . . ..\n\nProof. We will use induction on n. For n = 1 the theorem is trivial. Assume that the\ntheorem is true for all k such that 1 ≤ k ≤ n. Then\n\nzn+1 = znz\n\n= rn(cosnθ + i sinnθ)r(cos θ + i sin θ)\n= rn+1[(cosnθ cos θ − sinnθ sin θ) + i(sinnθ cos θ + cosnθ sin θ)]\n= rn+1[cos(nθ + θ) + i sin(nθ + θ)]\n\n= rn+1[cos(n+ 1)θ + i sin(n+ 1)θ].\n\nExample 4.23. Suppose that z = 1+i and we wish to compute z10. Rather than computing\n(1 + i)10 directly, it is much easier to switch to polar coordinates and calculate z10 using\nDeMoivre’s Theorem:\n\nz10 = (1 + i)10\n\n=\n(√\n\n2 cis\n(π\n4\n\n))10\n= (\n\n√\n2 )10 cis\n\n(\n5π\n\n2\n\n)\n= 32 cis\n\n(π\n2\n\n)\n= 32i.\n\nThe Circle Group and the Roots of Unity\nThe multiplicative group of the complex numbers, C∗, possesses some interesting subgroups.\nWhereas Q∗ and R∗ have no interesting subgroups of finite order, C∗ has many. We first\nconsider the circle group,\n\nT = {z ∈ C : |z| = 1}.\n\nThe following proposition is a direct result of Proposition 4.20.\n\nProposition 4.24. The circle group is a subgroup of C∗.\n\nAlthough the circle group has infinite order, it has many interesting finite subgroups.\nSuppose that H = {1,−1, i,−i}. Then H is a subgroup of the circle group. Also, 1, −1, i,\nand −i are exactly those complex numbers that satisfy the equation z4 = 1. The complex\nnumbers satisfying the equation zn = 1 are called the nth roots of unity.\n\n\n\n4.3. THE METHOD OF REPEATED SQUARES 65\n\nTheorem 4.25. If zn = 1, then the nth roots of unity are\n\nz = cis\n(\n2kπ\n\nn\n\n)\n,\n\nwhere k = 0, 1, . . . , n − 1. Furthermore, the nth roots of unity form a cyclic subgroup of T\nof order n\n\nProof. By DeMoivre’s Theorem,\n\nzn = cis\n(\nn\n2kπ\n\nn\n\n)\n= cis(2kπ) = 1.\n\nThe z’s are distinct since the numbers 2kπ/n are all distinct and are greater than or equal\nto 0 but less than 2π. The fact that these are all of the roots of the equation zn = 1 follows\nfrom from Corollary 17.9, which states that a polynomial of degree n can have at most n\nroots. We will leave the proof that the nth roots of unity form a cyclic subgroup of T as an\nexercise.\n\nA generator for the group of the nth roots of unity is called a primitive nth root of\nunity.\n\nExample 4.26. The 8th roots of unity can be represented as eight equally spaced points\non the unit circle (Figure 4.27). The primitive 8th roots of unity are\n\nω =\n\n√\n2\n\n2\n+\n\n√\n2\n\n2\ni\n\nω3 = −\n√\n2\n\n2\n+\n\n√\n2\n\n2\ni\n\nω5 = −\n√\n2\n\n2\n−\n\n√\n2\n\n2\ni\n\nω7 =\n\n√\n2\n\n2\n−\n\n√\n2\n\n2\ni.\n\ny\n\nx0 1\n\nω\n\ni\n\nω3\n\n−1\n\nω5\n\n−i\n\nω7\n\nFigure 4.27: 8th roots of unity\n\n4.3 The Method of Repeated Squares\nComputing large powers can be very time-consuming. Just as anyone can compute 22 or\n28, everyone knows how to compute\n\n22\n1000000\n\n.\n\n\n\n66 CHAPTER 4. CYCLIC GROUPS\n\nHowever, such numbers are so large that we do not want to attempt the calculations;\nmoreover, past a certain point the computations would not be feasible even if we had every\ncomputer in the world at our disposal. Even writing down the decimal representation of a\nvery large number may not be reasonable. It could be thousands or even millions of digits\nlong. However, if we could compute something like\n\n237398332 (mod 46389),\n\nwe could very easily write the result down since it would be a number between 0 and 46,388.\nIf we want to compute powers modulo n quickly and efficiently, we will have to be clever.1\n\nThe first thing to notice is that any number a can be written as the sum of distinct\npowers of 2; that is, we can write\n\na = 2k1 + 2k2 + · · ·+ 2kn ,\n\nwhere k1 < k2 < · · · < kn. This is just the binary representation of a. For example, the\nbinary representation of 57 is 111001, since we can write 57 = 20 + 23 + 24 + 25.\n\nThe laws of exponents still work in Zn; that is, if b ≡ ax (mod n) and c ≡ ay (mod n),\nthen bc ≡ ax+y (mod n). We can compute a2k (mod n) in k multiplications by computing\n\na2\n0\n\n(mod n)\n\na2\n1\n\n(mod n)\n\n...\n\na2\nk\n\n(mod n).\n\nEach step involves squaring the answer obtained in the previous step, dividing by n, and\ntaking the remainder.\n\nExample 4.28. We will compute 271321 (mod 481). Notice that\n\n321 = 20 + 26 + 28;\n\nhence, computing 271321 (mod 481) is the same as computing\n\n2712\n0+26+28 ≡ 2712\n\n0 · 27126 · 27128 (mod 481).\n\nSo it will suffice to compute 2712\ni\n(mod 481) where i = 0, 6, 8. It is very easy to see that\n\n2712\n1\n= 73,441 ≡ 329 (mod 481).\n\nWe can square this result to obtain a value for 2712\n2\n(mod 481):\n\n2712\n2 ≡ (2712\n\n1\n)2 (mod 481)\n\n≡ (329)2 (mod 481)\n\n≡ 108,241 (mod 481)\n\n≡ 16 (mod 481).\n\nWe are using the fact that (a2\nn\n)2 ≡ a2·2\n\nn ≡ a2\nn+1\n\n(mod n). Continuing, we can calculate\n\n2712\n6 ≡ 419 (mod 481)\n\n1The results in this section are needed only in Chapter 7\n\n\n\n4.4. EXERCISES 67\n\nand\n2712\n\n8 ≡ 16 (mod 481).\n\nTherefore,\n\n271321 ≡ 2712\n0+26+28 (mod 481)\n\n≡ 2712\n0 · 27126 · 27128 (mod 481)\n\n≡ 271 · 419 · 16 (mod 481)\n\n≡ 1,816,784 (mod 481)\n\n≡ 47 (mod 481).\n\nThe method of repeated squares will prove to be a very useful tool when we explore rsa\ncryptography in Chapter 7. To encode and decode messages in a reasonable manner under\nthis scheme, it is necessary to be able to quickly compute large powers of integers mod n.\n\n4.4 Exercises\n1. Prove or disprove each of the following statements.\n(a) All of the generators of Z60 are prime.\n(b) U(8) is cyclic.\n(c) Q is cyclic.\n(d) If every proper subgroup of a group G is cyclic, then G is a cyclic group.\n(e) A group with a finite number of subgroups is finite.\n\n2. Find the order of each of the following elements.\n\n(a) 5 ∈ Z12\n\n(b)\n√\n3 ∈ R\n\n(c)\n√\n3 ∈ R∗\n\n(d) −i ∈ C∗\n\n(e) 72 in Z240\n\n(f) 312 in Z471\n\n3. List all of the elements in each of the following subgroups.\n(a) The subgroup of Z generated by 7\n(b) The subgroup of Z24 generated by 15\n(c) All subgroups of Z12\n\n(d) All subgroups of Z60\n\n(e) All subgroups of Z13\n\n(f) All subgroups of Z48\n\n(g) The subgroup generated by 3 in U(20)\n\n(h) The subgroup generated by 5 in U(18)\n\n(i) The subgroup of R∗ generated by 7\n(j) The subgroup of C∗ generated by i where i2 = −1\n\n(k) The subgroup of C∗ generated by 2i\n\n(l) The subgroup of C∗ generated by (1 + i)/\n√\n2\n\n(m) The subgroup of C∗ generated by (1 +\n√\n3 i)/2\n\n4. Find the subgroups of GL2(R) generated by each of the following matrices.\n\n\n\n68 CHAPTER 4. CYCLIC GROUPS\n\n(a)\n(\n\n0 1\n\n−1 0\n\n)\n(b)\n\n(\n0 1/3\n\n3 0\n\n) (c)\n(\n1 −1\n\n1 0\n\n)\n(d)\n\n(\n1 −1\n\n0 1\n\n) (e)\n(\n\n1 −1\n\n−1 0\n\n)\n(f)\n\n(√\n3/2 1/2\n\n−1/2\n√\n3/2\n\n)\n\n5. Find the order of every element in Z18.\n\n6. Find the order of every element in the symmetry group of the square, D4.\n\n7. What are all of the cyclic subgroups of the quaternion group, Q8?\n\n8. List all of the cyclic subgroups of U(30).\n\n9. List every generator of each subgroup of order 8 in Z32.\n\n10. Find all elements of finite order in each of the following groups. Here the “∗” indicates\nthe set with zero removed.\n\n(a) Z (b) Q∗ (c) R∗\n\n11. If a24 = e in a group G, what are the possible orders of a?\n\n12. Find a cyclic group with exactly one generator. Can you find cyclic groups with exactly\ntwo generators? Four generators? How about n generators?\n\n13. For n ≤ 20, which groups U(n) are cyclic? Make a conjecture as to what is true in\ngeneral. Can you prove your conjecture?\n\n14. Let\nA =\n\n(\n0 1\n\n−1 0\n\n)\nand B =\n\n(\n0 −1\n\n1 −1\n\n)\nbe elements in GL2(R). Show that A and B have finite orders but AB does not.\n\n15. Evaluate each of the following.\n\n(a) (3− 2i) + (5i− 6)\n\n(b) (4− 5i)− (4i− 4)\n\n(c) (5− 4i)(7 + 2i)\n\n(d) (9− i)(9− i)\n\n(e) i45\n\n(f) (1 + i) + (1 + i)\n\n16. Convert the following complex numbers to the form a+ bi.\n\n(a) 2 cis(π/6)\n(b) 5 cis(9π/4)\n\n(c) 3 cis(π)\n(d) cis(7π/4)/2\n\n17. Change the following complex numbers to polar representation.\n\n(a) 1− i\n\n(b) −5\n\n(c) 2 + 2i\n\n(d)\n√\n3 + i\n\n(e) −3i\n\n(f) 2i+ 2\n√\n3\n\n18. Calculate each of the following expressions.\n\n\n\n4.4. EXERCISES 69\n\n(a) (1 + i)−1\n\n(b) (1− i)6\n\n(c) (\n√\n3 + i)5\n\n(d) (−i)10\n\n(e) ((1− i)/2)4\n\n(f) (−\n√\n2−\n\n√\n2 i)12\n\n(g) (−2 + 2i)−5\n\n19. Prove each of the following statements.\n\n(a) |z| = |z|\n(b) zz = |z|2\n\n(c) z−1 = z/|z|2\n\n(d) |z + w| ≤ |z|+ |w|\n(e) |z − w| ≥ ||z| − |w||\n(f) |zw| = |z||w|\n\n20. List and graph the 6th roots of unity. What are the generators of this group? What\nare the primitive 6th roots of unity?\n\n21. List and graph the 5th roots of unity. What are the generators of this group? What\nare the primitive 5th roots of unity?\n\n22. Calculate each of the following.\n\n(a) 2923171 (mod 582)\n\n(b) 2557341 (mod 5681)\n\n(c) 20719521 (mod 4724)\n\n(d) 971321 (mod 765)\n\n23. Let a, b ∈ G. Prove the following statements.\n(a) The order of a is the same as the order of a−1.\n(b) For all g ∈ G, |a| = |g−1ag|.\n(c) The order of ab is the same as the order of ba.\n\n24. Let p and q be distinct primes. How many generators does Zpq have?\n\n25. Let p be prime and r be a positive integer. How many generators does Zpr have?\n\n26. Prove that Zp has no nontrivial subgroups if p is prime.\n\n27. If g and h have orders 15 and 16 respectively in a group G, what is the order of ⟨g⟩∩⟨h⟩?\n\n28. Let a be an element in a group G. What is a generator for the subgroup ⟨am⟩ ∩ ⟨an⟩?\n\n29. Prove that Zn has an even number of generators for n > 2.\n\n30. Suppose that G is a group and let a, b ∈ G. Prove that if |a| = m and |b| = n with\ngcd(m,n) = 1, then ⟨a⟩ ∩ ⟨b⟩ = {e}.\n\n31. Let G be an abelian group. Show that the elements of finite order in G form a subgroup.\nThis subgroup is called the torsion subgroup of G.\n\n32. Let G be a finite cyclic group of order n generated by x. Show that if y = xk where\ngcd(k, n) = 1, then y must be a generator of G.\n\n33. If G is an abelian group that contains a pair of cyclic subgroups of order 2, show that\nG must contain a subgroup of order 4. Does this subgroup have to be cyclic?\n\n34. Let G be an abelian group of order pq where gcd(p, q) = 1. If G contains elements a\nand b of order p and q respectively, then show that G is cyclic.\n\n\n\n70 CHAPTER 4. CYCLIC GROUPS\n\n35. Prove that the subgroups of Z are exactly nZ for n = 0, 1, 2, . . ..\n\n36. Prove that the generators of Zn are the integers r such that 1 ≤ r < n and gcd(r, n) = 1.\n\n37. Prove that if G has no proper nontrivial subgroups, then G is a cyclic group.\n\n38. Prove that the order of an element in a cyclic group G must divide the order of the\ngroup.\n\n39. Prove that if G is a cyclic group of order m and d | m, then G must have a subgroup\nof order d.\n\n40. For what integers n is −1 an nth root of unity?\n\n41. If z = r(cos θ + i sin θ) and w = s(cosϕ + i sinϕ) are two nonzero complex numbers,\nshow that\n\nzw = rs[cos(θ + ϕ) + i sin(θ + ϕ)].\n\n42. Prove that the circle group is a subgroup of C∗.\n\n43. Prove that the nth roots of unity form a cyclic subgroup of T of order n.\n\n44. Let α ∈ T. Prove that αm = 1 and αn = 1 if and only if αd = 1 for d = gcd(m,n).\n\n45. Let z ∈ C∗. If |z| ≠ 1, prove that the order of z is infinite.\n\n46. Let z = cos θ + i sin θ be in T where θ ∈ Q. Prove that the order of z is infinite.\n\n4.5 Programming Exercises\n1. Write a computer program that will write any decimal number as the sum of distinct\npowers of 2. What is the largest integer that yourprogram will handle?\n\n2. Write a computer program to calculate ax (mod n) by the method of repeated squares.\nWhat are the largest values of n and x that your program will accept?\n\n4.6 References and Suggested Readings\n[1] Koblitz, N. A Course in Number Theory and Cryptography. 2nd ed. Springer, New\n\nYork, 1994.\n[2] Pomerance, C. “Cryptology and Computational Number Theory—An Introduction,”\n\nin Cryptology and Computational Number Theory, Pomerance, C., ed. Proceedings of\nSymposia in Applied Mathematics, vol. 42, American Mathematical Society, Provi-\ndence, RI, 1990. Thisbook gives an excellent account of how the method of repeated\nsquares is used in cryptography.\n\n4.7 Sage\nCyclic groups are very important, so it is no surprise that they appear in many different\nforms in Sage. Each is slightly different, and no one implementation is ideal for an intro-\nduction, but together they can illustrate most of the important ideas. Here is a guide to\nthe various ways to construct, and study, a cyclic group in Sage.\n\n\n\n4.7. SAGE 71\n\nInfinite Cyclic Groups\nIn Sage, the integers Z are constructed with ZZ. To build the infinite cyclic group such as 3Z\nfrom Example 4.1, simply use 3*ZZ. As an infinite set, there is not a whole lot you can do\nwith this. You can test if integers are in this set, or not. You can also recall the generator\nwith the .gen() command.\n\nG = 3*ZZ\n-12 in G\n\nTrue\n\n37 in G\n\nFalse\n\nG.gen()\n\n3\n\nAdditive Cyclic Groups\nThe additive cyclic group Zn can be built as a special case of a more general Sage construc-\ntion. First we build Z14 and capture its generator. Throughout, pay close attention to the\nuse of parentheses and square brackets for when you experiment on your own.\n\nG = AdditiveAbelianGroup ([14])\nG.order()\n\n14\n\nG.list()\n\n[(0), (1), (2), (3), (4), (5), (6), (7),\n(8), (9), (10), (11), (12), (13)]\n\na = G.gen (0)\na\n\n(1)\n\nYou can compute in this group, by using the generator, or by using new elements formed\nby coercing integers into the group, or by taking the result of operations on other elements.\nAnd we can compute the order of elements in this group. Notice that we can perform\nrepeated additions with the shortcut of taking integer multiples of an element.\n\na + a\n\n(2)\n\na + a + a + a\n\n(4)\n\n4*a\n\n\n\n72 CHAPTER 4. CYCLIC GROUPS\n\n(4)\n\n37*a\n\n(9)\n\nWe can create, and then compute with, new elements of the group by coercing an integer\n(in a list of length 1) into the group. You may get a DeprecationWarning the first time you\nuse this syntax to create a new element. The mysterious warning can be safely ignored.\n\nG([2])\n\ndoctest :...: DeprecationWarning: The default behaviour changed! If\nyou\n\n*really* want a linear combination of smith generators , use\n.linear_combination_of_smith_form_gens.\nSee http :// trac.sagemath.org /16261 for details.\n(2)\n\nb = G([2]); b\n\n(2)\n\nb + b\n\n(4)\n\n2*b == 4*a\n\nTrue\n\n7*b\n\n(0)\n\nb.order()\n\n7\n\nc = a - 6*b; c\n\n(3)\n\nc + c + c + c\n\n(12)\n\nc.order()\n\n14\n\nIt is possible to create cyclic subgroups, from an element designated to be the new\ngenerator. Unfortunately, to do this requires the .submodule() method (which should be\nrenamed in Sage).\n\n\n\n4.7. SAGE 73\n\nH = G.submodule ([b]); H\n\nAdditive abelian group isomorphic to Z/7\n\nH.list()\n\n[(0), (2), (4), (6), (8), (10), (12)]\n\nH.order()\n\n7\n\ne = H.gen (0); e\n\n(2)\n\n3*e\n\n(6)\n\ne.order()\n\n7\n\nThe cyclic subgroup H just created has more than one generator. We can test this by\nbuilding a new subgroup and comparing the two subgroups.\n\nf = 12*a; f\n\n(12)\n\nf.order()\n\n7\n\nK = G.submodule ([f]); K\n\nAdditive abelian group isomorphic to Z/7\n\nK.order()\n\n7\n\nK.list()\n\n[(0), (2), (4), (6), (8), (10), (12)]\n\nK.gen (0)\n\n(2)\n\nH == K\n\nTrue\n\nCertainly the list of elements, and the common generator of (2) lead us to belive that H\n\nand K are the same, but the comparison in the last line leaves no doubt.\nResults in this section, especially Theorem 4.13 and Corollary 4.14, can be investigated\n\nby creating generators of subgroups from a generator of one additive cyclic group, creating\nthe subgroups, and computing the orders of both elements and orders of groups.\n\n\n\n74 CHAPTER 4. CYCLIC GROUPS\n\nAbstract Multiplicative Cyclic Groups\nWe can create an abstract cyclic group in the style of Theorems 4.3, 4.9, 4.10. In the\nsyntax below a is a name for the generator, and 14 is the order of the element. Notice that\nthe notation is now multiplicative, so we multiply elements, and repeated products can be\nwritten as powers.\n\nG.<a> = AbelianGroup ([14])\nG.order()\n\n14\n\nG.list()\n\n(1, a, a^2, a^3, a^4, a^5, a^6, a^7, a^8, a^9, a^10, a^11, a^12,\na^13)\n\na.order()\n\n14\n\nComputations in the group are similar to before, only with different notation. Now\nproducts, with repeated products written as exponentiation.\n\nb = a^2\nb.order()\n\n7\n\nb*b*b\n\na^6\n\nc = a^7\nc.order()\n\n2\n\nc^2\n\n1\n\nb*c\n\na^9\n\nb^37*c^42\n\na^4\n\nSubgroups can be formed with a .subgroup() command. But do not try to list the\ncontents of a subgroup, it’ll look strangely unfamiliar. Also, comparison of subgroups is not\nimplemented.\n\nH = G.subgroup ([a^2])\nH.order()\n\n\n\n4.7. SAGE 75\n\n7\n\nK = G.subgroup ([a^12])\nK.order()\n\n7\n\nL = G.subgroup ([a^4])\nH == L\n\nFalse\n\nOne advantage of this implementation is the possibility to create all possible subgroups.\nHere we create the list of subgroups, extract one in particular (the third), and check its\norder.\n\nallsg = G.subgroups (); allsg\n\n[Multiplicative Abelian subgroup isomorphic to C2 x C7 generated by\n{a},\n\nMultiplicative Abelian subgroup isomorphic to C7 generated by {a^2},\nMultiplicative Abelian subgroup isomorphic to C2 generated by {a^7},\nTrivial Abelian subgroup]\n\nsub = allsg [2]\nsub.order ()\n\n2\n\nCyclic Permutation Groups\nWe will learn more about permutation groups in the next chapter. But we will mention\nhere that it is easy to create cyclic groups as permutation groups, and a variety of methods\nare available for working with them, even if the actual elements get a bit cumbersome to\nwork with. As before, notice that the notation and syntax is multiplicative.\n\nG=CyclicPermutationGroup (14)\na = G.gen (0); a\n\n(1,2,3,4,5,6,7,8,9,10,11,12,13,14)\n\nb = a^2\nb = a^2; b\n\n(1,3,5,7,9,11,13)(2,4,6,8,10,12,14)\n\nb.order()\n\n7\n\na*a*b*b*b\n\n(1,9,3,11,5,13,7)(2,10,4,12,6,14,8)\n\nc = a^37*b^26; c\n\n\n\n76 CHAPTER 4. CYCLIC GROUPS\n\n(1,6,11,2,7,12,3,8,13,4,9,14,5,10)\n\nc.order()\n\n14\n\nWe can create subgroups, check their orders, and list their elements.\nH = G.subgroup ([a^2])\nH.order()\n\n7\n\nH.gen (0)\n\n(1,3,5,7,9,11,13)(2,4,6,8,10,12,14)\n\nH.list()\n\n[(),\n(1,3,5,7,9,11,13)(2,4,6,8,10,12,14),\n(1,5,9,13,3,7,11)(2,6,10,14,4,8,12),\n(1,7,13,5,11,3,9)(2,8,14,6,12,4,10),\n(1,9,3,11,5,13,7)(2,10,4,12,6,14,8),\n(1,11,7,3,13,9,5)(2,12,8,4,14,10,6),\n(1,13,11,9,7,5,3)(2,14,12,10,8,6,4)]\n\nIt could help to visualize this group, and the subgroup, as rotations of a regular 12-\ngon with the vertices labeled with the integers 1 through 12. This is not the full group of\nsymmetries, since it does not include reflections, just the 12 rotations.\n\nCayley Tables\nAs groups, each of the examples above (groups and subgroups) have Cayley tables imple-\nmented. Since the groups are cyclic, and their subgroups are therefore cyclic, the Cayley\ntables should have a similar “cyclic” pattern. Note that the letters used in the default table\nare generic, and are not related to the letters used above for specific elements — they just\nmatch up with the group elements in the order given by .list().\n\nG.<a> = AbelianGroup ([14])\nG.cayley_table ()\n\n* a b c d e f g h i j k l m n\n+----------------------------\n\na| a b c d e f g h i j k l m n\nb| b c d e f g h i j k l m n a\nc| c d e f g h i j k l m n a b\nd| d e f g h i j k l m n a b c\ne| e f g h i j k l m n a b c d\nf| f g h i j k l m n a b c d e\ng| g h i j k l m n a b c d e f\nh| h i j k l m n a b c d e f g\ni| i j k l m n a b c d e f g h\nj| j k l m n a b c d e f g h i\nk| k l m n a b c d e f g h i j\nl| l m n a b c d e f g h i j k\nm| m n a b c d e f g h i j k l\nn| n a b c d e f g h i j k l m\n\n\n\n4.7. SAGE 77\n\nIf the real names of the elements are not too complicated, the table could be more\ninformative using these names.\n\nK.<b> = AbelianGroup ([10])\nK.cayley_table(names= \' elements \' )\n\n* 1 b b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9\n+----------------------------------------\n\n1| 1 b b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9\nb| b b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9 1\n\nb^2| b^2 b^3 b^4 b^5 b^6 b^7 b^8 b^9 1 b\nb^3| b^3 b^4 b^5 b^6 b^7 b^8 b^9 1 b b^2\nb^4| b^4 b^5 b^6 b^7 b^8 b^9 1 b b^2 b^3\nb^5| b^5 b^6 b^7 b^8 b^9 1 b b^2 b^3 b^4\nb^6| b^6 b^7 b^8 b^9 1 b b^2 b^3 b^4 b^5\nb^7| b^7 b^8 b^9 1 b b^2 b^3 b^4 b^5 b^6\nb^8| b^8 b^9 1 b b^2 b^3 b^4 b^5 b^6 b^7\nb^9| b^9 1 b b^2 b^3 b^4 b^5 b^6 b^7 b^8\n\nComplex Roots of Unity\nThe finite cyclic subgroups of T, generated by a primitive nth root of unity are implemented\nas a more general construction in Sage, known as a cyclotomic field. If you concentrate\non just the multiplication of powers of a generator (and ignore the infinitely many other\nelements) then this is a finite cyclic group. Since this is not implemented directly in Sage\nas a group, per se, it is a bit harder to construct things like subgroups, but it is an excellent\nexercise to try. It is a nice example since the complex numbers are a concrete and familiar\nconstruction. Here are a few sample calculations to provide you with some exploratory\ntools. See the notes following the computations.\n\nG = CyclotomicField (14)\nw = G.gen (0); w\n\nzeta14\n\nwc = CDF(w)\nwc.abs()\n\n1.0\n\nwc.arg()/N(2*pi/14)\n\n1.0\n\nb = w^2\nb.multiplicative_order ()\n\n7\n\nbc = CDF(b); bc\n\n0.62348980185... + 0.781831482468...*I\n\nbc.abs()\n\n\n\n78 CHAPTER 4. CYCLIC GROUPS\n\n1.0\n\nbc.arg()/N(2*pi/14)\n\n2.0\n\nsg = [b^i for i in range (7)]; sg\n\n[1, zeta14^2, zeta14^4,\nzeta14 ^5 - zeta14 ^4 + zeta14 ^3 - zeta14 ^2 + zeta14 - 1,\n-zeta14 , -zeta14^3, -zeta14 ^5]\n\nc = sg[3]; d = sg[5]\nc*d\n\nzeta14 ^2\n\nc = sg[3]; d = sg[6]\nc*d in sg\n\nTrue\n\nc*d == sg[2]\n\nTrue\n\nsg[5]*sg[6] == sg[4]\n\nTrue\n\nG.multiplication_table(elements=sg)\n\n* a b c d e f g\n+--------------\n\na| a b c d e f g\nb| b c d e f g a\nc| c d e f g a b\nd| d e f g a b c\ne| e f g a b c d\nf| f g a b c d e\ng| g a b c d e f\n\nNotes:\n\n1. zeta14 is the name of the generator used for the cyclotomic field, it is a primitive root\nof unity (a 14th root of unity in this case). We have captured it as w.\n\n2. The syntax CDF(w) will convert the complex number w into the more familiar form\nwith real and imaginary parts.\n\n3. The method .abs() will return the modulus of a complex number, r as described in\nthe text. For elements of C∗ this should always equal 1.\n\n4. The method .arg() will return the argument of a complex number, θ as described in\nthe text. Every element of the cyclic group in this example should have an argument\nthat is an integer multiple of 2π\n\n14 . The N() syntax converts the symbolic value of pi to\na numerical approximation.\n\n\n\n4.8. SAGE EXERCISES 79\n\n5. sg is a list of elements that form a cyclic subgroup of order 7, composed of the first 7\npowers of b = w^2. So, for example, the last comparison multiplies the fifth power of\nb with the sixth power of b, which would be the eleventh power of b. But since b has\norder 7, this reduces to the fourth power.\n\n6. If you know a subset of an infinite group forms a subgroup, then you can produce its\nCayley table by specifying the list of elements you want to use. Here we ask for a\nmultiplication table, since that is the relevant operation.\n\n4.8 Sage Exercises\nThis group of exercises is about the group of units mod n, U(n), which is sometimes cyclic,\nsometimes not. There are some commands in Sage that will answer some of these questions\nvery quickly, but instead of using those now, just use the basic techniques described. The\nidea here is to just work with elements, and lists of elements, to discern the subgroup\nstructure of these groups.\n1. Execute the statement R = Integers(40) to create the set [0,1,2,...,39] This is a group\nunder addition mod 40, which we will ignore. Instead we are interested in the subset\nof elements which have an inverse under multiplication mod 40. Determine how big this\nsubgroup is by executing the command R.unit_group_order(), and then obtain a list of these\nelements with R.list_of_elements_of_multiplicative_group().\n\n2. You can create elements of this group by coercing regular integers into U, such as with\nthe statement a = U(7). (Don’t confuse this with our mathematical notation U(40).) This\nwill tell Sage that you want to view 7 as an element of U , subject to the corresponding\noperations. Determine the elements of the cyclic subgroup of U generated by 7 with a list\ncomprehension as follows:\n\nR = Integers (40)\na = R(7)\n[a^i for i in srange (16)]\n\nWhat is the order of 7 in U(40)?\n\n3. The group U(49) is cyclic. Using only the Sage commands described previously, use\nSage to find a generator for this group. Now using only theorems about the structure of\ncyclic groups, describe each of the subgroups of U(49) by specifying its order and by giving\nan explicit generator. Do not repeat any of the subgroups — in other words, present each\nsubgroup exactly once. You can use Sage to check your work on the subgroups, but your\nanswer about the subgroups should rely only on theorems and be a nicely written paragraph\nwith a table, etc.\n\n4. The group U(35) is not cyclic. Again, using only the Sage commands described pre-\nviously, use computations to provide irrefutable evidence of this. How many of the 16\ndifferent subgroups of U(35) can you list?\n\n5. Again, using only the Sage commands described previously, explore the structure of U(n)\nfor various values of n and see if you can formulate an interesting conjecture about some\nbasic property of this group. (Yes, this is a very open-ended question, but this is ultimately\nthe real power of exploring mathematics with Sage.)\n\n\n\n5\n\nPermutation Groups\n\nPermutation groups are central to the study of geometric symmetries and to Galois the-\nory, the study of finding solutions of polynomial equations. They also provide abundant\nexamples of nonabelian groups.\n\nLet us recall for a moment the symmetries of the equilateral triangle △ABC from\nChapter 3. The symmetries actually consist of permutations of the three vertices, where a\npermutation of the set S = {A,B,C} is a one-to-one and onto map π : S → S. The three\nvertices have the following six permutations.(\n\nA B C\n\nA B C\n\n) (\nA B C\n\nC A B\n\n) (\nA B C\n\nB C A\n\n)\n(\nA B C\n\nA C B\n\n) (\nA B C\n\nC B A\n\n) (\nA B C\n\nB A C\n\n)\nWe have used the array (\n\nA B C\n\nB C A\n\n)\nto denote the permutation that sends A to B, B to C, and C to A. That is,\n\nA 7→ B\n\nB 7→ C\n\nC 7→ A.\n\nThe symmetries of a triangle form a group. In this chapter we will study groups of this\ntype.\n\n5.1 Definitions and Notation\nIn general, the permutations of a set X form a group SX . If X is a finite set, we can assume\nX = {1, 2, . . . , n}. In this case we write Sn instead of SX . The following theorem says that\nSn is a group. We call this group the symmetric group on n letters.\n\nTheorem 5.1. The symmetric group on n letters, Sn, is a group with n! elements, where\nthe binary operation is the composition of maps.\n\nProof. The identity of Sn is just the identity map that sends 1 to 1, 2 to 2, . . ., n to n. If\nf : Sn → Sn is a permutation, then f−1 exists, since f is one-to-one and onto; hence, every\npermutation has an inverse. Composition of maps is associative, which makes the group\noperation associative. We leave the proof that |Sn| = n! as an exercise.\n\n80\n\n\n\n5.1. DEFINITIONS AND NOTATION 81\n\nA subgroup of Sn is called a permutation group.\n\nExample 5.2. Consider the subgroup G of S5 consisting of the identity permutation id\nand the permutations\n\nσ =\n\n(\n1 2 3 4 5\n\n1 2 3 5 4\n\n)\nτ =\n\n(\n1 2 3 4 5\n\n3 2 1 4 5\n\n)\nµ =\n\n(\n1 2 3 4 5\n\n3 2 1 5 4\n\n)\n.\n\nThe following table tells us how to multiply elements in the permutation group G.\n\n◦ id σ τ µ\n\nid id σ τ µ\n\nσ σ id µ τ\n\nτ τ µ id σ\n\nµ µ τ σ id\n\nRemark 5.3. Though it is natural to multiply elements in a group from left to right,\nfunctions are composed from right to left. Let σ and τ be permutations on a set X. To\ncompose σ and τ as functions, we calculate (σ◦τ)(x) = σ(τ(x)). That is, we do τ first, then\nσ. There are several ways to approach this inconsistency. We will adopt the convention of\nmultiplying permutations right to left. To compute στ , do τ first and then σ. That is, by\nστ(x) we mean σ(τ(x)). (Another way of solving this problem would be to write functions\non the right; that is, instead of writing σ(x), we could write (x)σ. We could also multiply\npermutations left to right to agree with the usual way of multiplying elements in a group.\nCertainly all of these methods have been used.\n\nExample 5.4. Permutation multiplication is not usually commutative. Let\n\nσ =\n\n(\n1 2 3 4\n\n4 1 2 3\n\n)\nτ =\n\n(\n1 2 3 4\n\n2 1 4 3\n\n)\n.\n\nThen\nστ =\n\n(\n1 2 3 4\n\n1 4 3 2\n\n)\n,\n\nbut\nτσ =\n\n(\n1 2 3 4\n\n3 2 1 4\n\n)\n.\n\nCycle Notation\nThe notation that we have used to represent permutations up to this point is cumbersome,\nto say the least. To work effectively with permutation groups, we need a more streamlined\nmethod of writing down and manipulating permutations.\n\n\n\n82 CHAPTER 5. PERMUTATION GROUPS\n\nA permutation σ ∈ SX is a cycle of length k if there exist elements a1, a2, . . . , ak ∈ X\nsuch that\n\nσ(a1) = a2\n\nσ(a2) = a3\n...\n\nσ(ak) = a1\n\nand σ(x) = x for all other elements x ∈ X. We will write (a1, a2, . . . , ak) to denote the\ncycle σ. Cycles are the building blocks of all permutations.\n\nExample 5.5. The permutation\n\nσ =\n\n(\n1 2 3 4 5 6 7\n\n6 3 5 1 4 2 7\n\n)\n= (162354)\n\nis a cycle of length 6, whereas\n\nτ =\n\n(\n1 2 3 4 5 6\n\n1 4 2 3 5 6\n\n)\n= (243)\n\nis a cycle of length 3.\nNot every permutation is a cycle. Consider the permutation(\n\n1 2 3 4 5 6\n\n2 4 1 3 6 5\n\n)\n= (1243)(56).\n\nThis permutation actually contains a cycle of length 2 and a cycle of length 4.\n\nExample 5.6. It is very easy to compute products of cycles. Suppose that\n\nσ = (1352) and τ = (256).\n\nIf we think of σ as\n1 7→ 3, 3 7→ 5, 5 7→ 2, 2 7→ 1,\n\nand τ as\n2 7→ 5, 5 7→ 6, 6 7→ 2,\n\nthen for στ remembering that we apply τ first and then σ, it must be the case that\n\n1 7→ 3, 3 7→ 5, 5 7→ 6, 6 7→ 2 7→ 1,\n\nor στ = (1356). If µ = (1634), then σµ = (1652)(34).\n\nTwo cycles in SX , σ = (a1, a2, . . . , ak) and τ = (b1, b2, . . . , bl), are disjoint if ai ̸= bj for\nall i and j.\n\nExample 5.7. The cycles (135) and (27) are disjoint; however, the cycles (135) and (347)\nare not. Calculating their products, we find that\n\n(135)(27) = (135)(27)\n\n(135)(347) = (13475).\n\nThe product of two cycles that are not disjoint may reduce to something less complicated;\nthe product of disjoint cycles cannot be simplified.\n\n\n\n5.1. DEFINITIONS AND NOTATION 83\n\nProposition 5.8. Let σ and τ be two disjoint cycles in SX . Then στ = τσ.\n\nProof. Let σ = (a1, a2, . . . , ak) and τ = (b1, b2, . . . , bl). We must show that στ(x) = τσ(x)\nfor all x ∈ X. If x is neither in {a1, a2, . . . , ak} nor {b1, b2, . . . , bl}, then both σ and τ fix x.\nThat is, σ(x) = x and τ(x) = x. Hence,\n\nστ(x) = σ(τ(x)) = σ(x) = x = τ(x) = τ(σ(x)) = τσ(x).\n\nDo not forget that we are multiplying permutations right to left, which is the opposite of the\norder in which we usually multiply group elements. Now suppose that x ∈ {a1, a2, . . . , ak}.\nThen σ(ai) = a(i mod k)+1; that is,\n\na1 7→ a2\n\na2 7→ a3\n...\n\nak−1 7→ ak\n\nak 7→ a1.\n\nHowever, τ(ai) = ai since σ and τ are disjoint. Therefore,\n\nστ(ai) = σ(τ(ai))\n\n= σ(ai)\n\n= a(i mod k)+1\n\n= τ(a(i mod k)+1)\n\n= τ(σ(ai))\n\n= τσ(ai).\n\nSimilarly, if x ∈ {b1, b2, . . . , bl}, then σ and τ also commute.\n\nTheorem 5.9. Every permutation in Sn can be written as the product of disjoint cycles.\n\nProof. We can assume that X = {1, 2, . . . , n}. If σ ∈ Sn and we define X1 to be\n{σ(1), σ2(1), . . .}, then the set X1 is finite since X is finite. Now let i be the first inte-\nger in X that is not in X1 and define X2 by {σ(i), σ2(i), . . .}. Again, X2 is a finite set.\nContinuing in this manner, we can define finite disjoint sets X3, X4, . . .. Since X is a finite\nset, we are guaranteed that this process will end and there will be only a finite number of\nthese sets, say r. If σi is the cycle defined by\n\nσi(x) =\n\n{\nσ(x) x ∈ Xi\n\nx x /∈ Xi,\n\nthen σ = σ1σ2 · · ·σr. Since the sets X1, X2, . . . , Xr are disjoint, the cycles σ1, σ2, . . . , σr\nmust also be disjoint.\n\nExample 5.10. Let\n\nσ =\n\n(\n1 2 3 4 5 6\n\n6 4 3 1 5 2\n\n)\nτ =\n\n(\n1 2 3 4 5 6\n\n3 2 1 5 6 4\n\n)\n.\n\n\n\n84 CHAPTER 5. PERMUTATION GROUPS\n\nUsing cycle notation, we can write\n\nσ = (1624)\n\nτ = (13)(456)\n\nστ = (136)(245)\n\nτσ = (143)(256).\n\nRemark 5.11. From this point forward we will find it convenient to use cycle notation to\nrepresent permutations. When using cycle notation, we often denote the identity permuta-\ntion by (1).\n\nTranspositions\nThe simplest permutation is a cycle of length 2. Such cycles are called transpositions.\nSince\n\n(a1, a2, . . . , an) = (a1an)(a1an−1) · · · (a1a3)(a1a2),\n\nany cycle can be written as the product of transpositions, leading to the following proposi-\ntion.\n\nProposition 5.12. Any permutation of a finite set containing at least two elements can be\nwritten as the product of transpositions.\n\nExample 5.13. Consider the permutation\n\n(16)(253) = (16)(23)(25) = (16)(45)(23)(45)(25).\n\nAs we can see, there is no unique way to represent permutation as the product of transposi-\ntions. For instance, we can write the identity permutation as (12)(12), as (13)(24)(13)(24),\nand in many other ways. However, as it turns out, no permutation can be written as the\nproduct of both an even number of transpositions and an odd number of transpositions.\nFor instance, we could represent the permutation (16) by\n\n(23)(16)(23)\n\nor by\n(35)(16)(13)(16)(13)(35)(56),\n\nbut (16) will always be the product of an odd number of transpositions.\n\nLemma 5.14. If the identity is written as the product of r transpositions,\n\nid = τ1τ2 · · · τr,\n\nthen r is an even number.\n\nProof. We will employ induction on r. A transposition cannot be the identity; hence,\nr > 1. If r = 2, then we are done. Suppose that r > 2. In this case the product of the last\ntwo transpositions, τr−1τr, must be one of the following cases:\n\n(ab)(ab) = id\n(bc)(ab) = (ac)(bc)\n\n(cd)(ab) = (ab)(cd)\n\n(ac)(ab) = (ab)(bc),\n\n\n\n5.1. DEFINITIONS AND NOTATION 85\n\nwhere a, b, c, and d are distinct.\nThe first equation simply says that a transposition is its own inverse. If this case occurs,\n\ndelete τr−1τr from the product to obtain\n\nid = τ1τ2 · · · τr−3τr−2.\n\nBy induction r − 2 is even; hence, r must be even.\nIn each of the other three cases, we can replace τr−1τr with the right-hand side of the\n\ncorresponding equation to obtain a new product of r transpositions for the identity. In this\nnew product the last occurrence of a will be in the next-to-the-last transposition. We can\ncontinue this process with τr−2τr−1 to obtain either a product of r − 2 transpositions or a\nnew product of r transpositions where the last occurrence of a is in τr−2. If the identity is\nthe product of r − 2 transpositions, then again we are done, by our induction hypothesis;\notherwise, we will repeat the procedure with τr−3τr−2.\n\nAt some point either we will have two adjacent, identical transpositions canceling each\nother out or a will be shuffled so that it will appear only in the first transposition. However,\nthe latter case cannot occur, because the identity would not fix a in this instance. Therefore,\nthe identity permutation must be the product of r − 2 transpositions and, again by our\ninduction hypothesis, we are done.\n\nTheorem 5.15. If a permutation σ can be expressed as the product of an even number\nof transpositions, then any other product of transpositions equaling σ must also contain an\neven number of transpositions. Similarly, if σ can be expressed as the product of an odd\nnumber of transpositions, then any other product of transpositions equaling σ must also\ncontain an odd number of transpositions.\n\nProof. Suppose that\nσ = σ1σ2 · · ·σm = τ1τ2 · · · τn,\n\nwhere m is even. We must show that n is also an even number. The inverse of σ is σm · · ·σ1.\nSince\n\nid = σσm · · ·σ1 = τ1 · · · τnσm · · ·σ1,\nn must be even by Lemma 5.14. The proof for the case in which σ can be expressed as an\nodd number of transpositions is left as an exercise.\n\nIn light of Theorem 5.15, we define a permutation to be even if it can be expressed\nas an even number of transpositions and odd if it can be expressed as an odd number of\ntranspositions.\n\nThe Alternating Groups\nOne of the most important subgroups of Sn is the set of all even permutations, An. The\ngroup An is called the alternating group on n letters.\n\nTheorem 5.16. The set An is a subgroup of Sn.\n\nProof. Since the product of two even permutations must also be an even permutation,\nAn is closed. The identity is an even permutation and therefore is in An. If σ is an even\npermutation, then\n\nσ = σ1σ2 · · ·σr,\nwhere σi is a transposition and r is even. Since the inverse of any transposition is itself,\n\nσ−1 = σrσr−1 · · ·σ1\n\nis also in An.\n\n\n\n86 CHAPTER 5. PERMUTATION GROUPS\n\nProposition 5.17. The number of even permutations in Sn, n ≥ 2, is equal to the number\nof odd permutations; hence, the order of An is n!/2.\n\nProof. Let An be the set of even permutations in Sn and Bn be the set of odd permuta-\ntions. If we can show that there is a bijection between these sets, they must contain the\nsame number of elements. Fix a transposition σ in Sn. Since n ≥ 2, such a σ exists. Define\n\nλσ : An → Bn\n\nby\nλσ(τ) = στ.\n\nSuppose that λσ(τ) = λσ(µ). Then στ = σµ and so\n\nτ = σ−1στ = σ−1σµ = µ.\n\nTherefore, λσ is one-to-one. We will leave the proof that λσ is surjective to the reader.\n\nExample 5.18. The group A4 is the subgroup of S4 consisting of even permutations. There\nare twelve elements in A4:\n\n(1) (12)(34) (13)(24) (14)(23)\n\n(123) (132) (124) (142)\n\n(134) (143) (234) (243).\n\nOne of the end-of-chapter exercises will be to write down all the subgroups of A4. You will\nfind that there is no subgroup of order 6. Does this surprise you?\n\nHistorical Note\n\nLagrange first thought of permutations as functions from a set to itself, but it was\nCauchy who developed the basic theorems and notation for permutations. He was the first\nto use cycle notation. Augustin-Louis Cauchy (1789–1857) was born in Paris at the height\nof the French Revolution. His family soon left Paris for the village of Arcueil to escape\nthe Reign of Terror. One of the family’s neighbors there was Pierre-Simon Laplace (1749–\n1827), who encouraged him to seek a career in mathematics. Cauchy began his career as\na mathematician by solving a problem in geometry given to him by Lagrange. Cauchy\nwrote over 800 papers on such diverse topics as differential equations, finite groups, applied\nmathematics, and complex analysis. He was one of the mathematicians responsible for\nmaking calculus rigorous. Perhaps more theorems and concepts in mathematics have the\nname Cauchy attached to them than that of any other mathematician.\n\n5.2 Dihedral Groups\nAnother special type of permutation group is the dihedral group. Recall the symmetry\ngroup of an equilateral triangle in Chapter 3. Such groups consist of the rigid motions of\na regular n-sided polygon or n-gon. For n = 3, 4, . . ., we define the nth dihedral group\nto be the group of rigid motions of a regular n-gon. We will denote this group by Dn. We\ncan number the vertices of a regular n-gon by 1, 2, . . . , n (Figure 5.19). Notice that there\nare exactly n choices to replace the first vertex. If we replace the first vertex by k, then the\nsecond vertex must be replaced either by vertex k+1 or by vertex k−1; hence, there are 2n\npossible rigid motions of the n-gon. We summarize these results in the following theorem.\n\n\n\n5.2. DIHEDRAL GROUPS 87\n\n1\n\nn− 1 3\n\n2n\n\n4\n\nFigure 5.19: A regular n-gon\n\nTheorem 5.20. The dihedral group, Dn, is a subgroup of Sn of order 2n.\n\n8\n1\n\n2\n\n3\n\n4\n5\n\n6\n\n7\n\n2\n1\n\n8\n\n7\n\n6\n5\n\n4\n\n3\nreflection\n\n3\n2\n\n1\n\n8\n\n7\n6\n\n5\n\n4\n\n2\n1\n\n8\n\n7\n\n6\n5\n\n4\n\n3 rotation\n\nFigure 5.21: Rotations and reflections of a regular n-gon\n\n5\n\n1\n\n2\n\n3 4\n\n2\n\n1\n\n5\n\n4 3\n\n6\n\n1\n\n2\n\n3\n\n4\n\n5\n\n2\n\n1\n\n6\n\n5\n\n4\n\n3\n\nFigure 5.22: Types of reflections of a regular n-gon\n\n\n\n88 CHAPTER 5. PERMUTATION GROUPS\n\nTheorem 5.23. The group Dn, n ≥ 3, consists of all products of the two elements r and\ns, satisfying the relations\n\nrn = 1\n\ns2 = 1\n\nsrs = r−1.\n\nProof. The possible motions of a regular n-gon are either reflections or rotations (Fig-\nure 5.21). There are exactly n possible rotations:\n\nid, 360\n◦\n\nn\n, 2 · 360\n\n◦\n\nn\n, . . . , (n− 1) · 360\n\n◦\n\nn\n.\n\nWe will denote the rotation 360◦/n by r. The rotation r generates all of the other rotations.\nThat is,\n\nrk = k · 360\n◦\n\nn\n.\n\nLabel the n reflections s1, s2, . . . , sn, where sk is the reflection that leaves vertex k fixed.\nThere are two cases of reflection, depending on whether n is even or odd. If there are an\neven number of vertices, then 2 vertices are left fixed by a reflection. If there are an odd\nnumber of vertices, then only a single vertex is left fixed by a reflection (Figure 5.22).\n\nIn either case, the order of sk is two. Let s = s1. Then s2 = id and rn = id. Since\nany rigid motion t of the n-gon replaces the first vertex by the vertex k, the second vertex\nmust be replaced by either k+1 or by k− 1. If the second vertex is replaced by k+1, then\nt = rk−1. If it is replaced by k − 1, then t = rk−1s. Hence, r and s generate Dn; that is,\nDn consists of all finite products of r and s. We will leave the proof that srs = r−1 as an\nexercise.\n\nExample 5.24. The group of rigid motions of a square, D4, consists of eight elements.\nWith the vertices numbered 1, 2, 3, 4 (Figure 5.25), the rotations are\n\nr = (1234)\n\nr2 = (13)(24)\n\nr3 = (1432)\n\nr4 = (1)\n\nand the reflections are\n\ns1 = (24)\n\ns2 = (13).\n\nThe order of D4 is 8. The remaining two elements are\n\nrs1 = (12)(34)\n\nr3s1 = (14)(23).\n\n\n\n5.2. DIHEDRAL GROUPS 89\n\n21\n\n4 3\n\nFigure 5.25: The group D4\n\nThe Motion Group of a Cube\n\nWe can investigate the groups of rigid motions of geometric objects other than a regular\nn-sided polygon to obtain interesting examples of permutation groups. Let us consider the\ngroup of rigid motions of a cube. One of the first questions that we can ask about this group\nis “what is its order?” A cube has 6 sides. If a particular side is facing upward, then there\nare four possible rotations of the cube that will preserve the upward-facing side. Hence, the\norder of the group is 6 · 4 = 24. We have just proved the following proposition.\n\n2\n\n2\n4\n\n4\n1\n\n1\n3\n\n3\n\nFigure 5.26: The motion group of a cube\n\nProposition 5.27. The group of rigid motions of a cube contains 24 elements.\n\nTheorem 5.28. The group of rigid motions of a cube is S4.\n\nProof. From Proposition 5.27, we already know that the motion group of the cube has 24\nelements, the same number of elements as there are in S4. There are exactly four diagonals\nin the cube. If we label these diagonals 1, 2, 3, and 4, we must show that the motion group\nof the cube will give us any permutation of the diagonals (Figure 5.26). If we can obtain\nall of these permutations, then S4 and the group of rigid motions of the cube must be the\nsame. To obtain a transposition we can rotate the cube 180◦ about the axis joining the\nmidpoints of opposite edges (Figure 5.29). There are six such axes, giving all transpositions\nin S4. Since every element in S4 is the product of a finite number of transpositions, the\nmotion group of a cube must be S4.\n\n\n\n90 CHAPTER 5. PERMUTATION GROUPS\n\n2\n\n4 3\n\n1\n\n1 2\n\n43\n1\n\n4 3\n\n2\n\n2 1\n\n43\n\nFigure 5.29: Transpositions in the motion group of a cube\n\n5.3 Exercises\n1. Write the following permutations in cycle notation.\n\n(a) (\n1 2 3 4 5\n\n2 4 1 5 3\n\n)\n(b) (\n\n1 2 3 4 5\n\n4 2 5 1 3\n\n)\n\n(c) (\n1 2 3 4 5\n\n3 5 1 4 2\n\n)\n(d) (\n\n1 2 3 4 5\n\n1 4 3 2 5\n\n)\n\n2. Compute each of the following.\n\n(a) (1345)(234)\n\n(b) (12)(1253)\n\n(c) (143)(23)(24)\n\n(d) (1423)(34)(56)(1324)\n\n(e) (1254)(13)(25)\n\n(f) (1254)(13)(25)2\n\n(g) (1254)−1(123)(45)(1254)\n\n(h) (1254)2(123)(45)\n\n(i) (123)(45)(1254)−2\n\n(j) (1254)100\n\n(k) |(1254)|\n(l) |(1254)2|\n\n(m) (12)−1\n\n(n) (12537)−1\n\n(o) [(12)(34)(12)(47)]−1\n\n(p) [(1235)(467)]−1\n\n3. Express the following permutations as products of transpositions and identify them as\neven or odd.\n\n(a) (14356)\n\n(b) (156)(234)\n\n(c) (1426)(142)\n\n(d) (17254)(1423)(154632)\n\n(e) (142637)\n\n4. Find (a1, a2, . . . , an)\n−1.\n\n5. List all of the subgroups of S4. Find each of the following sets.\n(a) {σ ∈ S4 : σ(1) = 3}\n(b) {σ ∈ S4 : σ(2) = 2}\n\n\n\n5.3. EXERCISES 91\n\n(c) {σ ∈ S4 : σ(1) = 3 and σ(2) = 2}\nAre any of these sets subgroups of S4?\n\n6. Find all of the subgroups in A4. What is the order of each subgroup?\n\n7. Find all possible orders of elements in S7 and A7.\n\n8. Show that A10 contains an element of order 15.\n\n9. Does A8 contain an element of order 26?\n\n10. Find an element of largest order in Sn for n = 3, . . . , 10.\n\n11. What are the possible cycle structures of elements of A5? What about A6?\n\n12. Let σ ∈ Sn have order n. Show that for all integers i and j, σi = σj if and only if i ≡ j\n(mod n).\n\n13. Let σ = σ1 · · ·σm ∈ Sn be the product of disjoint cycles. Prove that the order of σ is\nthe least common multiple of the lengths of the cycles σ1, . . . , σm.\n\n14. Using cycle notation, list the elements in D5. What are r and s? Write every element\nas a product of r and s.\n\n15. If the diagonals of a cube are labeled as Figure 5.26, to which motion of the cube does\nthe permutation (12)(34) correspond? What about the other permutations of the diagonals?\n\n16. Find the group of rigid motions of a tetrahedron. Show that this is the same group as\nA4.\n\n17. Prove that Sn is nonabelian for n ≥ 3.\n\n18. Show that An is nonabelian for n ≥ 4.\n\n19. Prove that Dn is nonabelian for n ≥ 3.\n\n20. Let σ ∈ Sn be a cycle. Prove that σ can be written as the product of at most n − 1\ntranspositions.\n\n21. Let σ ∈ Sn. If σ is not a cycle, prove that σ can be written as the product of at most\nn− 2 transpositions.\n\n22. If σ can be expressed as an odd number of transpositions, show that any other product\nof transpositions equaling σ must also be odd.\n\n23. If σ is a cycle of odd length, prove that σ2 is also a cycle.\n\n24. Show that a 3-cycle is an even permutation.\n\n25. Prove that in An with n ≥ 3, any permutation is a product of cycles of length 3.\n\n26. Prove that any element in Sn can be written as a finite product of the following per-\nmutations.\n(a) (12), (13), . . . , (1n)\n\n(b) (12), (23), . . . , (n− 1, n)\n\n(c) (12), (12 . . . n)\n\n27. Let G be a group and define a map λg : G → G by λg(a) = ga. Prove that λg is a\npermutation of G.\n\n\n\n92 CHAPTER 5. PERMUTATION GROUPS\n\n28. Prove that there exist n! permutations of a set containing n elements.\n\n29. Recall that the center of a group G is\n\nZ(G) = {g ∈ G : for all }.\n\nFind the center of D8. What about the center of D10? What is the center of Dn?\n\n30. Let τ = (a1, a2, . . . , ak) be a cycle of length k.\n\n(a) Prove that if σ is any permutation, then\n\nστσ−1 = (σ(a1), σ(a2), . . . , σ(ak))\n\nis a cycle of length k.\n(b) Let µ be a cycle of length k. Prove that there is a permutation σ such that στσ−1 = µ.\n\n31. For α and β in Sn, define α ∼ β if there exists an σ ∈ Sn such that σασ−1 = β. Show\nthat ∼ is an equivalence relation on Sn.\n\n32. Let σ ∈ SX . If σn(x) = y, we will say that x ∼ y.\n\n(a) Show that ∼ is an equivalence relation on X.\n(b) If σ ∈ An and τ ∈ Sn, show that τ−1στ ∈ An.\n(c) Define the orbit of x ∈ X under σ ∈ SX to be the set\n\nOx,σ = {y : x ∼ y}.\n\nCompute the orbits of each of the following elements in S5:\n\nα = (1254)\n\nβ = (123)(45)\n\nγ = (13)(25).\n\n(d) If Ox,σ ∩Oy,σ ̸= ∅, prove that Ox,σ = Oy,σ. The orbits under a permutation σ are the\nequivalence classes corresponding to the equivalence relation ∼.\n\n(e) A subgroup H of SX is transitive if for every x, y ∈ X, there exists a σ ∈ H such\nthat σ(x) = y. Prove that ⟨σ⟩ is transitive if and only if Ox,σ = X for some x ∈ X.\n\n33. Let α ∈ Sn for n ≥ 3. If αβ = βα for all β ∈ Sn, prove that α must be the identity\npermutation; hence, the center of Sn is the trivial subgroup.\n\n34. If α is even, prove that α−1 is also even. Does a corresponding result hold if α is odd?\n\n35. Show that α−1β−1αβ is even for α, β ∈ Sn.\n\n36. Let r and s be the elements in Dn described in Theorem 5.10.\n\n(a) Show that srs = r−1.\n(b) Show that rks = sr−k in Dn.\n(c) Prove that the order of rk ∈ Dn is n/ gcd(k, n).\n\n\n\n5.4. SAGE 93\n\n5.4 Sage\nA good portion of Sage’s support for group theory is based on routines from gap (Groups,\nAlgorithms, and Programming) at www.gap-system.org, which is included in every copy of\nSage. This is a mature open source package, dating back to 1986. (Forward reference here\nto gap console, etc.)\n\nAs we have seen, groups can be described in many different ways, such as sets of matrices,\nsets of complex numbers, or sets of symbols subject to defining relations. A very concrete\nway to represent groups is via permutations (one-to-one and onto functions of the integers\n1 through n), using function composition as the operation in the group, as described in this\nchapter. Sage has many routines designed to work with groups of this type and they are\nalso a good way for those learning group theory to gain experience with the basic ideas of\ngroup theory. For both these reasons, we will concentrate on these types of groups.\n\nPermutation Groups and Elements\nThe easiest way to work with permutation group elements in Sage is to write them in cycle\nnotation. Since these are products of disjoint cycles (which commute), we do not need to\nconcern ourselves with the actual order of the cycles. If we write (1,3)(2,4) we probably\nunderstand it to be a permutation (the topic of this chapter!) and we know that it could\nbe an element of S4, or perhaps a symmetric group on more symbols than just 4. Sage\ncannot get started that easily and needs a bit of context, so we coerce a string of characters\nwritten with cycle notation into a symmetric group to make group elements. Here are some\nexamples and some sample computations. Remember that Sage and your text differ on how\nto interpret the order of composing two permutations in a product.\n\nG = SymmetricGroup (5)\nsigma = G("(1,3)(2,5,4)")\nsigma*sigma\n\n(2,4,5)\n\nrho = G("(2,4)(1,5)")\nrho^3\n\n(1,5)(2,4)\n\nIf the next three examples seem confusing, or “backwards”, then now would be an\nexcellent time to review the Sage discussion about the order of permutation composition in\nthe subsection “Groups of symmetries”.\n\nsigma*rho\n\n(1,3,5,2)\n\nrho*sigma\n\n(1,4,5,3)\n\nrho^-1* sigma*rho\n\n(1,2,4)(3,5)\n\nThere are alternate ways to create permutation group elements, which can be useful in\nsome situations, but they are not quite as useful in everday use.\n\nhttp://www.gap-system.org/\n\n\n94 CHAPTER 5. PERMUTATION GROUPS\n\nsigma1 = G("(1,3)(2,5,4)")\nsigma1\n\n(1,3)(2,5,4)\n\nsigma2 = G([(1 ,3) ,(2,5,4)])\nsigma2\n\n(1,3)(2,5,4)\n\nsigma3 = G([3,5,1,2,4])\nsigma3\n\n(1,3)(2,5,4)\n\nsigma1 == sigma2\n\nTrue\n\nsigma2 == sigma3\n\nTrue\n\nsigma2.cycle_tuples ()\n\n[(1, 3), (2, 5, 4)]\n\n[sigma3(x) for x in G.domain ()]\n\n[3, 5, 1, 2, 4]\n\nThe second version of σ is a list of “tuples”, which requires a lot of commas and these\nmust be enclosed in a list. (A tuple of length one must be written like (4,) to distinguish it\nfrom using parentheses for grouping, as in 5*(4).) The third version uses the “bottom-row”\nof the more cumbersome two-row notation introduced at the beginning of the chapter — it\nis an ordered list of the output values of the permutation when considered as a function.\n\nSo we then see that despite three different input procedures, all the versions of σ print\nthe same way, and moreso they are actually equal to each other. (This is a subtle difference\n— what an object is in Sage versus how an object displays itself.)\n\nWe can be even more careful about the nature of our elements. Notice that once we get\nSage started, it can promote the product τσ into the larger permutation group. We can\n“promote” elements into larger permutation groups, but it is an error to try to shoe-horn\nan element into a too-small symmetric group.\n\nH = SymmetricGroup (4)\nsigma = H("(1,2,3,4)")\nG = SymmetricGroup (6)\ntau = G("(1,2,3,4,5,6)")\nrho = tau * sigma\nrho\n\n(1,3)(2,4,5,6)\n\n\n\n5.4. SAGE 95\n\nsigma.parent ()\n\nSymmetric group of order 4! as a permutation group\n\ntau.parent ()\n\nSymmetric group of order 6! as a permutation group\n\nrho.parent ()\n\nSymmetric group of order 6! as a permutation group\n\ntau.parent () == rho.parent ()\n\nTrue\n\nsigmaG = G(sigma)\nsigmaG.parent ()\n\nSymmetric group of order 6! as a permutation group\n\nIt is an error to try to coerce a permutation with too many symbols into a permutation\ngroup employing too few symbols.\n\ntauH = H(tau)\n\nTraceback (most recent call last):\n...\nValueError: Invalid permutation vector: (1,2,3,4,5,6)\n\nBetter than working with just elements of the symmetric group, we can create a variety\nof permutation groups in Sage. Here is a sampling for starters:\n\nSage Command Description\nSymmetricGroup(n) Permutations on n symbols, n! elements\nDihedralGroup(n) Symmetries of an n-gon, 2n elements.\nCyclicPermutationGroup(n) Rotations of an n-gon (no flips), n elements\nAlternatingGroup(n) Alternating group on n symbols, n!/2 elements\nKleinFourGroup() A non-cyclic group of order 4\n\nTable 5.30: Some Sage permutation groups\n\nYou can also locate Sage permutation groups with the groups catalog. In the next cell\nplace your cursor right after the final dot and hit the tab-key. You will get a list of methods\nyou can use to create permutation groups. As always, place a question-mark after a method\nand hit the tab-key to get online documentation of a method.\n\ngroups.permutation.\n\n\n\n96 CHAPTER 5. PERMUTATION GROUPS\n\nProperties of Permutation Elements\nSometimes it is easier to grab an element out of a list of elements of a permutation group,\nand then it is already attached to a parent and there is no need for any coercion. In the\nfollowing, rotate and flip are automatically elements of G because of the way we procured\nthem.\n\nD = DihedralGroup (5)\nelements = D.list(); elements\n\n[(), (1,5)(2,4), (1,2,3,4,5), (1,4)(2,3), (1,3,5,2,4), (2,5)(3,4),\n(1,3)(4,5), (1,5,4,3,2), (1,4,2,5,3), (1,2)(3,5)]\n\nrotate = elements [2]\nflip = elements [3]\nflip*rotate == rotate* flip\n\nFalse\n\nSo we see from this final statement that the group of symmetries of a pentagon is not\nabelian. But there is an easier way.\n\nD = DihedralGroup (5)\nD.is_abelian ()\n\nFalse\n\nThere are many more methods you can use for both permutation groups and their\nindividual elements. Use the blank compute cell below to create a permutation group (any\none you like) and an element of a permutation group (any one you like). Then use tab-\ncompletion to see all the methods available for an element, or for a group (name, period,\ntab-key). Some names you may recognize, some we will learn about in the coming chapters,\nsome are highly-specialized research tools you can use when you write your Ph.D. thesis in\ngroup theory. For any of these methods, remember that you can type the name, followed\nby a question mark, to see documentation and examples. Experiment and explore — it is\nreally hard to break anything.\n\nHere are some selected examples of various methods available.\nA4 = AlternatingGroup (4)\nA4.order ()\n\n12\n\nA4.is_finite ()\n\nTrue\n\nA4.is_abelian ()\n\nFalse\n\nA4.is_cyclic ()\n\nFalse\n\n\n\n5.4. SAGE 97\n\nsigma = A4("(1,2,4)")\nsigma^-1\n\n(1,4,2)\n\nsigma.order()\n\n3\n\nA very useful method when studying the alternating group is the permutation group\nelement method .sign(). It will return 1 if a permutation is even and -1 if a permutation\nis odd.\n\nG = SymmetricGroup (3)\nsigma = G("(1,2)")\ntau = G("(1,3)")\nrho = sigma*tau\nsigma.sign()\n\n-1\n\nrho.sign()\n\n1\n\nWe can create subgroups by giving the main group a list of “generators.” These elements\nserve to “generate” a subgroup — imagine multiplying these elements (and their inverses)\ntogether over and over, creating new elements that must also be in the subgroup and also\nbecome involved in new products, until you see no new elements. Now that definition ends\nwith a horribly imprecise statement, but it should suffice for now. A better definition is\nthat the subgroup generated by the elements is the smallest subgroup of the main group\nthat contains all the generators — which is fine if you know what all the subgroups might\nbe.\n\nWith a single generator, the repeated products just become powers of the lone generator.\nThe subgroup generated then is cyclic. With two (or more) generators, especially in a non-\nabelian group, the situation can be much, much more complicated. So let us begin with\njust a single generator. But do not forget to put it in a list anyway.\n\nA4 = AlternatingGroup (4)\nsigma = A4("(1,2,4)")\nsg = A4.subgroup ([sigma])\nsg\n\nSubgroup of (Alternating group of order 4!/2 as a permutation group)\ngenerated by [(1,2,4)]\n\nsg.order ()\n\n3\n\nsg.list()\n\n[(), (1,2,4), (1,4,2)]\n\n\n\n98 CHAPTER 5. PERMUTATION GROUPS\n\nsg.is_abelian ()\n\nTrue\n\nsg.is_cyclic ()\n\nTrue\n\nsg.is_subgroup(A4)\n\nTrue\n\nWe can now redo the example from the very beginning of this chapter. We translate to\nelements to cycle notation, construct the subgroup from two generators (the subgroup is\nnot cyclic), and since the subgroup is abelian, we do not have to view Sage’s Cayley table\nas a diagonal reflection of the table in the example.\n\nG = SymmetricGroup (5)\nsigma = G("(4,5)")\ntau = G("(1,3)")\nH = G.subgroup ([sigma , tau])\nH.list()\n\n[(), (1,3), (4,5), (1,3)(4,5)]\n\ntext_names = [ \' id \' , \' sigma \' , \' tau \' , \' mu \' ]\nH.cayley_table(names=text_names)\n\n* id sigma tau mu\n+------------------------\n\nid| id sigma tau mu\nsigma| sigma id mu tau\n\ntau| tau mu id sigma\nmu| mu tau sigma id\n\nMotion Group of a Cube\nWe could mimic the example in the text and create elements of S4 as permutations of the\ndiagonals. A more obvious, but less insightful, construction is to view the 8 corners of the\ncube as the items being permuted. Then some obvious symmetries of the cube come from\nrunning an axis through the center of a side, through to the center of the opposite side,\nwith quarter-turns or half-turns about these axes forming symmetries. With three such\naxes and four rotations per axis, we get 12 symmetries, except we have counted the identity\npermutation two extra times.\n\nLabel the four corners of the square top with 1 through 4, placing 1 in the left-front\ncorner, and following around clockwise when viewed from above. Use 5 through 8 for the\nbottom square’s corner, so that 5 is directly below 1, 6 below 2, etc. We will use quarter-\nturns, clockwise, around each axis, when viewed from above, the front, and the right.\n\nG = SymmetricGroup (8)\nabove = G("(1,2,3,4)(5,6,7,8)")\nfront = G("(1,4,8,5)(2,3,7,6)")\nright = G("(1,2,6,5)(3,7,8,4)")\ncube = G.subgroup ([above , front , right])\ncube.order()\n\n\n\n5.5. SAGE EXERCISES 99\n\n24\n\ncube.list()\n\n[(), (1,2,3,4)(5,6,7,8), (1,2,6,5)(3,7,8,4),\n(1,4,8,5)(2,3,7,6), (1,6,8)(2,7,4), (2,4,5)(3,8,6),\n(1,3,8)(2,7,5), (1,6)(2,5)(3,8)(4,7), (1,3,6)(4,7,5),\n(1,3)(2,4)(5,7)(6,8), (1,8)(2,7)(3,6)(4,5), (1,7)(2,3)(4,6)(5,8),\n(1,4)(2,8)(3,5)(6,7), (1,5,6,2)(3,4,8,7), (1,5,8,4)(2,6,7,3),\n(1,7)(2,6)(3,5)(4,8), (1,7)(2,8)(3,4)(5,6), (1,4,3,2)(5,8,7,6),\n(1,5)(2,8)(3,7)(4,6), (1,2)(3,5)(4,6)(7,8), (1,8,6)(2,4,7),\n(2,5,4)(3,6,8), (1,6,3)(4,5,7), (1,8,3)(2,5,7)]\n\nSince we know from the discussion in the text that the symmetry group has 24 elements,\nwe see that our three quarter-turns are sufficient to create every symmetry. This prompts\nseveral questions which you can find in Exercise 5.5.4.\n\n5.5 Sage Exercises\nThese exercises are designed to help you become familiar with permutation groups in Sage.\n1. Create the full symmetric group S10 with the command G = SymmetricGroup(10).\n\n2. Create elements of G with the following (varying) syntax. Pay attention to commas,\nquotes, brackets, parentheses. The first two use a string (characters) as input, mimicking\nthe way we write permuations (but with commas). The second two use a list of tuples.\n\n• a = G("(5,7,2,9,3,1,8)")\n\n• b = G("(1,3)(4,5)")\n\n• c = G([(1,2),(3,4)])\n\n• d = G([(1,3),(2,5,8),(4,6,7,9,10)])\n\n(a) Compute a3, bc, ad−1b.\n(b) Compute the orders of each of these four individual elements (a through d) using a\n\nsingle permutation group element method.\n(c) Use the permutation group element method .sign() to determine if a, b, c, d are even\n\nor odd permutations.\n(d) Create two cyclic subgroups of G with the commands:\n\n• H = G.subgroup([a])\n\n• K = G.subgroup([d])\n\nList, and study, the elements of each subgroup. Without using Sage, list the order of\neach subgroup of K. Then use Sage to construct a subgroup of K with order 10.\n\n(e) More complicated subgroups can be formed by using two or more generators. Construct\na subgroup L of G with the command L = G.subgroup([b,c]). Compute the order of\nL and list all of the elements of L.\n\n3. Construct the group of symmetries of the tetrahedron (also the alternating group on\n4 symbols, A4) with the command A=AlternatingGroup(4). Using tools such as orders of\nelements, and generators of subgroups, see if you can find all of the subgroups of A4 (each\none exactly once). Do this without using the .subgroups() method to justify the correctness\nof your answer (though it might be a convenient way to check your work).\n\n\n\n100 CHAPTER 5. PERMUTATION GROUPS\n\nProvide a nice summary as your answer—not just piles of output. So use Sage as a tool,\nas needed, but basically your answer will be a concise paragraph and/or table. This is the\none part of this assignment without clear, precise directions, so spend some time on this\nportion to get it right. Hint: no subgroup of A4 requires more than two generators.\n\n4. The subsection “Motion Group of a Cube” describes the 24 symmetries of a cube as a\nsubgroup of the symmetric group S8 generated by three quarter-turns. Answer the following\nquestions about this symmetry group.\n(a) From the list of elements of the group, can you locate the ten rotations about axes?\n\n(Hint: the identity is easy, the other nine never send any symbol to itself.)\n(b) Can you identify the six symmetries that are a transposition of diagonals? (Hint:\n\n[g for g in cube if g.order()== 2] is a good preliminary filter.)\n(c) Verify that any two of the quarter-turns (above, front, right) are sufficient to generate\n\nthe whole group. How do you know each pair generates the entire group?\n(d) Can you express one of the diagonal transpositions as a product of quarter-turns? This\n\ncan be a notoriously difficult problem, especially for software. It is known as the “word\nproblem.”\n\n(e) Number the six faces of the cube with the numbers 1 through 6 (any way you like).\nNow consider the same three symmetries we used before (quarter-turns about face-\nto-face axes), but now view them as permutations of the six faces. In this way, we\nconstruct each symmetry as an element of S6. Verify that the subgroup generated by\nthese symmetries is the whole symmetry group of the cube. Again, rather than using\nthree generators, try using just two.\n\n5. Save your work, and then see if you can crash your Sage session by building the subgroup\nof S10 generated by the elements b and d of orders 2 and 30 from above. Do not submit the\nlist of elements of N as part of your submitted worksheet.\n\nN = G.subgroup ([b,d])\nN.list()\n\nWhat is the order of N?\n\n\n\n6\n\nCosets and Lagrange’s Theorem\n\nLagrange’s Theorem, one of the most important results in finite group theory, states that the\norder of a subgroup must divide the order of the group. This theorem provides a powerful\ntool for analyzing finite groups; it gives us an idea of exactly what type of subgroups we\nmight expect a finite group to possess. Central to understanding Lagranges’s Theorem is\nthe notion of a coset.\n\n6.1 Cosets\nLet G be a group and H a subgroup of G. Define a left coset of H with representative\ng ∈ G to be the set\n\ngH = {gh : h ∈ H}.\n\nRight cosets can be defined similarly by\n\nHg = {hg : h ∈ H}.\n\nIf left and right cosets coincide or if it is clear from the context to which type of coset that\nwe are referring, we will use the word coset without specifying left or right.\n\nExample 6.1. Let H be the subgroup of Z6 consisting of the elements 0 and 3. The cosets\nare\n\n0 +H = 3 +H = {0, 3}\n1 +H = 4 +H = {1, 4}\n2 +H = 5 +H = {2, 5}.\n\nWe will always write the cosets of subgroups of Z and Zn with the additive notation we have\nused for cosets here. In a commutative group, left and right cosets are always identical.\n\nExample 6.2. Let H be the subgroup of S3 defined by the permutations {(1), (123), (132)}.\nThe left cosets of H are\n\n(1)H = (123)H = (132)H = {(1), (123), (132)}\n(12)H = (13)H = (23)H = {(12), (13), (23)}.\n\nThe right cosets of H are exactly the same as the left cosets:\n\nH(1) = H(123) = H(132) = {(1), (123), (132)}\nH(12) = H(13) = H(23) = {(12), (13), (23)}.\n\n101\n\n\n\n102 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\n\nIt is not always the case that a left coset is the same as a right coset. Let K be the\nsubgroup of S3 defined by the permutations {(1), (12)}. Then the left cosets of K are\n\n(1)K = (12)K = {(1), (12)}\n(13)K = (123)K = {(13), (123)}\n(23)K = (132)K = {(23), (132)};\n\nhowever, the right cosets of K are\n\nK(1) = K(12) = {(1), (12)}\nK(13) = K(132) = {(13), (132)}\nK(23) = K(123) = {(23), (123)}.\n\nThe following lemma is quite useful when dealing with cosets. (We leave its proof as an\nexercise.)\n\nLemma 6.3. Let H be a subgroup of a group G and suppose that g1, g2 ∈ G. The following\nconditions are equivalent.\n\n1. g1H = g2H;\n\n2. Hg−1\n1 = Hg−1\n\n2 ;\n\n3. g1H ⊆ g2H;\n\n4. g2 ∈ g1H;\n\n5. g−1\n1 g2 ∈ H.\n\nIn all of our examples the cosets of a subgroup H partition the larger group G. The\nfollowing theorem proclaims that this will always be the case.\n\nTheorem 6.4. Let H be a subgroup of a group G. Then the left cosets of H in G partition\nG. That is, the group G is the disjoint union of the left cosets of H in G.\n\nProof. Let g1H and g2H be two cosets of H in G. We must show that either g1H∩g2H = ∅\nor g1H = g2H. Suppose that g1H ∩ g2H ̸= ∅ and a ∈ g1H ∩ g2H. Then by the definition\nof a left coset, a = g1h1 = g2h2 for some elements h1 and h2 in H. Hence, g1 = g2h2h\n\n−1\n1 or\n\ng1 ∈ g2H. By Lemma 6.3, g1H = g2H.\n\nRemark 6.5. There is nothing special in this theorem about left cosets. Right cosets also\npartition G; the proof of this fact is exactly the same as the proof for left cosets except that\nall group multiplications are done on the opposite side of H.\n\nLet G be a group and H be a subgroup of G. Define the index of H in G to be the\nnumber of left cosets of H in G. We will denote the index by [G : H].\n\nExample 6.6. Let G = Z6 and H = {0, 3}. Then [G : H] = 3.\n\nExample 6.7. Suppose that G = S3, H = {(1), (123), (132)}, and K = {(1), (12)}. Then\n[G : H] = 2 and [G : K] = 3.\n\nTheorem 6.8. Let H be a subgroup of a group G. The number of left cosets of H in G is\nthe same as the number of right cosets of H in G.\n\n\n\n6.2. LAGRANGE’S THEOREM 103\n\nProof. Let LH and RH denote the set of left and right cosets of H in G, respectively. If\nwe can define a bijective map ϕ : LH → RH , then the theorem will be proved. If gH ∈ LH ,\nlet ϕ(gH) = Hg−1. By Lemma 6.3, the map ϕ is well-defined; that is, if g1H = g2H, then\nHg−1\n\n1 = Hg−1\n2 . To show that ϕ is one-to-one, suppose that\n\nHg−1\n1 = ϕ(g1H) = ϕ(g2H) = Hg−1\n\n2 .\n\nAgain by Lemma 6.3, g1H = g2H. The map ϕ is onto since ϕ(g−1H) = Hg.\n\n6.2 Lagrange’s Theorem\nProposition 6.9. Let H be a subgroup of G with g ∈ G and define a map ϕ : H → gH by\nϕ(h) = gh. The map ϕ is bijective; hence, the number of elements in H is the same as the\nnumber of elements in gH.\n\nProof. We first show that the map ϕ is one-to-one. Suppose that ϕ(h1) = ϕ(h2) for\nelements h1, h2 ∈ H. We must show that h1 = h2, but ϕ(h1) = gh1 and ϕ(h2) = gh2. So\ngh1 = gh2, and by left cancellation h1 = h2. To show that ϕ is onto is easy. By definition\nevery element of gH is of the form gh for some h ∈ H and ϕ(h) = gh.\n\nTheorem 6.10 (Lagrange). Let G be a finite group and let H be a subgroup of G. Then\n|G|/|H| = [G : H] is the number of distinct left cosets of H in G. In particular, the number\nof elements in H must divide the number of elements in G.\n\nProof. The group G is partitioned into [G : H] distinct left cosets. Each left coset has\n|H| elements; therefore, |G| = [G : H]|H|.\n\nCorollary 6.11. Suppose that G is a finite group and g ∈ G. Then the order of g must\ndivide the number of elements in G.\n\nCorollary 6.12. Let |G| = p with p a prime number. Then G is cyclic and any g ∈ G such\nthat g ̸= e is a generator.\n\nProof. Let g be in G such that g ̸= e. Then by Corollary 6.11, the order of g must divide\nthe order of the group. Since |⟨g⟩| > 1, it must be p. Hence, g generates G.\n\nCorollary 6.12 suggests that groups of prime order p must somehow look like Zp.\n\nCorollary 6.13. Let H and K be subgroups of a finite group G such that G ⊃ H ⊃ K.\nThen\n\n[G : K] = [G : H][H : K].\n\nProof. Observe that\n\n[G : K] =\n|G|\n|K|\n\n=\n|G|\n|H|\n\n· |H|\n|K|\n\n= [G : H][H : K].\n\nRemark 6.14 (The converse of Lagrange’s Theorem is false). The group A4 has order 12;\nhowever, it can be shown that it does not possess a subgroup of order 6. According to\nLagrange’s Theorem, subgroups of a group of order 12 can have orders of either 1, 2, 3,\n4, or 6. However, we are not guaranteed that subgroups of every possible order exist. To\nprove that A4 has no subgroup of order 6, we will assume that it does have such a subgroup\nH and show that a contradiction must occur. Since A4 contains eight 3-cycles, we know\nthat H must contain a 3-cycle. We will show that if H contains one 3-cycle, then it must\ncontain more than 6 elements.\n\n\n\n104 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\n\nProposition 6.15. The group A4 has no subgroup of order 6.\n\nProof. Since [A4 : H] = 2, there are only two cosets of H in A4. Inasmuch as one of the\ncosets is H itself, right and left cosets must coincide; therefore, gH = Hg or gHg−1 = H\nfor every g ∈ A4. Since there are eight 3-cycles in A4, at least one 3-cycle must be in H.\nWithout loss of generality, assume that (123) is in H. Then (123)−1 = (132) must also be\nin H. Since ghg−1 ∈ H for all g ∈ A4 and all h ∈ H and\n\n(124)(123)(124)−1 = (124)(123)(142) = (243)\n\n(243)(123)(243)−1 = (243)(123)(234) = (142)\n\nwe can conclude that H must have at least seven elements\n\n(1), (123), (132), (243), (243)−1 = (234), (142), (142)−1 = (124).\n\nTherefore, A4 has no subgroup of order 6.\n\nIn fact, we can say more about when two cycles have the same length.\n\nTheorem 6.16. Two cycles τ and µ in Sn have the same length if and only if there exists\na σ ∈ Sn such that µ = στσ−1.\n\nProof. Suppose that\n\nτ = (a1, a2, . . . , ak)\n\nµ = (b1, b2, . . . , bk).\n\nDefine σ to be the permutation\n\nσ(a1) = b1\n\nσ(a2) = b2\n...\n\nσ(ak) = bk.\n\nThen µ = στσ−1.\nConversely, suppose that τ = (a1, a2, . . . , ak) is a k-cycle and σ ∈ Sn. If σ(ai) = b and\n\nσ(a(i mod k)+1) = b′, then µ(b) = b′. Hence,\n\nµ = (σ(a1), σ(a2), . . . , σ(ak)).\n\nSince σ is one-to-one and onto, µ is a cycle of the same length as τ .\n\n6.3 Fermat’s and Euler’s Theorems\nThe Euler ϕ-function is the map ϕ : N → N defined by ϕ(n) = 1 for n = 1, and, for n > 1,\nϕ(n) is the number of positive integers m with 1 ≤ m < n and gcd(m,n) = 1.\n\nFrom Proposition 3.4, we know that the order of U(n), the group of units in Zn, is ϕ(n).\nFor example, |U(12)| = ϕ(12) = 4 since the numbers that are relatively prime to 12 are 1,\n5, 7, and 11. For any prime p, ϕ(p) = p−1. We state these results in the following theorem.\n\nTheorem 6.17. Let U(n) be the group of units in Zn. Then |U(n)| = ϕ(n).\n\nThe following theorem is an important result in number theory, due to Leonhard Euler.\n\n\n\n6.4. EXERCISES 105\n\nTheorem 6.18 (Euler’s Theorem). Let a and n be integers such that n > 0 and gcd(a, n) =\n1. Then aϕ(n) ≡ 1 (mod n).\n\nProof. By Theorem 6.17 the order of U(n) is ϕ(n). Consequently, aϕ(n) = 1 for all\na ∈ U(n); or aϕ(n) − 1 is divisible by n. Therefore, aϕ(n) ≡ 1 (mod n).\n\nIf we consider the special case of Euler’s Theorem in which n = p is prime and recall\nthat ϕ(p) = p− 1, we obtain the following result, due to Pierre de Fermat.\n\nTheorem 6.19 (Fermat’s Little Theorem). Let p be any prime number and suppose that\np ̸ |a. Then\n\nap−1 ≡ 1 (mod p).\n\nFurthermore, for any integer b, bp ≡ b (mod p).\n\nHistorical Note\n\nJoseph-Louis Lagrange (1736–1813), born in Turin, Italy, was of French and Italian\ndescent. His talent for mathematics became apparent at an early age. Leonhard Euler\nrecognized Lagrange’s abilities when Lagrange, who was only 19, communicated to Euler\nsome work that he had done in the calculus of variations. That year he was also named\na professor at the Royal Artillery School in Turin. At the age of 23 he joined the Berlin\nAcademy. Frederick the Great had written to Lagrange proclaiming that the “greatest king\nin Europe” should have the “greatest mathematician in Europe” at his court. For 20 years\nLagrange held the position vacated by his mentor, Euler. His works include contributions to\nnumber theory, group theory, physics and mechanics, the calculus of variations, the theory\nof equations, and differential equations. Along with Laplace and Lavoisier, Lagrange was\none of the people responsible for designing the metric system. During his life Lagrange\nprofoundly influenced the development of mathematics, leaving much to the next generation\nof mathematicians in the form of examples and new problems to be solved.\n\n6.4 Exercises\n1. Suppose that G is a finite group with an element g of order 5 and an element h of order\n7. Why must |G| ≥ 35?\n\n2. Suppose that G is a finite group with 60 elements. What are the orders of possible\nsubgroups of G?\n\n3. Prove or disprove: Every subgroup of the integers has finite index.\n\n4. Prove or disprove: Every subgroup of the integers has finite order.\n\n5. List the left and right cosets of the subgroups in each of the following.\n\n(a) ⟨8⟩ in Z24\n\n(b) ⟨3⟩ in U(8)\n\n(c) 3Z in Z\n(d) A4 in S4\n\n(e) An in Sn\n\n(f) D4 in S4\n\n(g) T in C∗\n\n(h) H = {(1), (123), (132)} in S4\n\n6. Describe the left cosets of SL2(R) in GL2(R). What is the index of SL2(R) in GL2(R)?\n\n\n\n106 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\n\n7. Verify Euler’s Theorem for n = 15 and a = 4.\n\n8. Use Fermat’s Little Theorem to show that if p = 4n+3 is prime, there is no solution to\nthe equation x2 ≡ −1 (mod p).\n\n9. Show that the integers have infinite index in the additive group of rational numbers.\n\n10. Show that the additive group of real numbers has infinite index in the additive group\nof the complex numbers.\n\n11. Let H be a subgroup of a group G and suppose that g1, g2 ∈ G. Prove that the following\nconditions are equivalent.\n(a) g1H = g2H\n\n(b) Hg−1\n1 = Hg−1\n\n2\n\n(c) g1H ⊆ g2H\n\n(d) g2 ∈ g1H\n\n(e) g−1\n1 g2 ∈ H\n\n12. If ghg−1 ∈ H for all g ∈ G and h ∈ H, show that right cosets are identical to left\ncosets. That is, show that gH = Hg for all g ∈ G.\n\n13. What fails in the proof of Theorem 6.8 if ϕ : LH → RH is defined by ϕ(gH) = Hg?\n\n14. Suppose that gn = e. Show that the order of g divides n.\n\n15. Show that any two permutations α, β ∈ Sn have the same cycle structure if and only\nif there exists a permutation γ such that β = γαγ−1. If β = γαγ−1 for some γ ∈ Sn, then\nα and β are conjugate.\n\n16. If |G| = 2n, prove that the number of elements of order 2 is odd. Use this result to\nshow that G must contain a subgroup of order 2.\n\n17. Suppose that [G : H] = 2. If a and b are not in H, show that ab ∈ H.\n\n18. If [G : H] = 2, prove that gH = Hg.\n\n19. Let H and K be subgroups of a group G. Prove that gH ∩ gK is a coset of H ∩K in\nG.\n\n20. Let H and K be subgroups of a group G. Define a relation ∼ on G by a ∼ b if there\nexists an h ∈ H and a k ∈ K such that hak = b. Show that this relation is an equivalence\nrelation. The corresponding equivalence classes are called double cosets. Compute the\ndouble cosets of H = {(1), (123), (132)} in A4.\n\n21. Let G be a cyclic group of order n. Show that there are exactly ϕ(n) generators for G.\n\n22. Let n = pe11 p\ne2\n2 · · · pekk , where p1, p2, . . . , pk are distinct primes. Prove that\n\nϕ(n) = n\n\n(\n1− 1\n\np1\n\n)(\n1− 1\n\np2\n\n)\n· · ·\n(\n1− 1\n\npk\n\n)\n.\n\n23. Show that\nn =\n\n∑\nd|n\n\nϕ(d)\n\nfor all positive integers n.\n\n\n\n6.5. SAGE 107\n\n6.5 Sage\nSage can create all of the cosets of a subgroup, and all of the subgroups of a group. While\nthese methods can be somewhat slow, they are in many, many ways much better than ex-\nperimenting with pencil and paper, and can greatly assist us in understanding the structure\nof finite groups.\n\nCosets\nSage will create all the right (or left) cosets of a subgroup. Written mathematically, cosets\nare sets, and the order of the elements within the set is irrelevant. With Sage, lists are more\nnatural, and here it is to our advantage.\n\nSage creates the cosets of a subgroup as a list of lists. Each inner list is a single coset.\nThe first coset is always the coset that is the subgroup itself, and the first element of this\ncoset is the identity. Each of the other cosets can be construed to have their first element\nas their representative, and if you use this element as the representative, the elements of\nthe coset are in the same order they would be created by multiplying this representative by\nthe elements of the first coset (the subgroup).\n\nThe keyword side can be \'right\' or \'left\', and if not given, then the default is right\ncosets. The options refer to which side of the product has the representative. Notice\nthat now Sage’s results will be “backwards” compared with the text. Here is Example 6.2\nreprised, but in a slightly different order.\n\nG = SymmetricGroup (3)\na = G("(1,2)")\nH = G.subgroup ([a])\nrc = G.cosets(H, side= \' right \' ); rc\n\n[[(), (1,2)], [(2,3), (1,3,2)], [(1,2,3), (1,3)]]\n\nlc = G.cosets(H, side= \' left \' ); lc\n\n[[(), (1,2)], [(2,3), (1,2,3)], [(1,3,2), (1,3)]]\n\nSo if we work our way through the brackets carefully we can see the difference between\nthe right cosets and the left cosets. Compare these cosets with the ones in the text and see\nthat left and right are reversed. Shouldn’t be a problem — just keep it in mind.\n\nG = SymmetricGroup (3)\nb = G("(1,2,3)")\nH = G.subgroup ([b])\nrc = G.cosets(H, side= \' right \' ); rc\n\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,3), (1,2)]]\n\nlc = G.cosets(H, side= \' left \' ); lc\n\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,2), (1,3)]]\n\nIf we study the bracketing, we can see that the left and right cosets are equal. Let’s see\nwhat Sage thinks:\n\nrc == lc\n\nFalse\n\n\n\n108 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\n\nMathematically, we need sets, but Sage is working with ordered lists, and the order\nmatters. However, if we know our lists do not have duplicates (the .cosets() method will\nnever produce duplicates) then we can sort the lists and a test for equality will perform as\nexpected. The elements of a permutation group have an ordering defined for them — it is\nnot so important what this is, just that some ordering is defined. The sorted() function\nwill take any list and return a sorted version. So for each list of cosets, we will sort the\nindividual cosets and then sort the list of sorted cosets. This is a typical maneuver, though\na bit complicated with the nested lists.\n\nrc_sorted = sorted ([ sorted(coset) for coset in rc])\nrc_sorted\n\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,2), (1,3)]]\n\nlc_sorted = sorted ([ sorted(coset) for coset in lc])\nlc_sorted\n\n[[(), (1,2,3), (1,3,2)], [(2,3), (1,2), (1,3)]]\n\nrc_sorted == lc_sorted\n\nTrue\n\nThe list of all cosets can be quite long (it will include every element of the group) and\ncan take a few seconds to complete, even for small groups. There are more sophisticated,\nand faster, ways to study cosets (such as just using their representatives), but to understand\nthese techniques you also need to understand more theory.\n\nSubgroups\nSage can compute all of the subgroups of a group. This can produce even more output\nthan the coset method and can sometimes take much longer, depending on the structure\nof the group. The list is in order of the size of the subgroups, with smallest first. As a\ndemonstration we will first compute and list all of the subgroups of a small group, and then\nextract just one of these subgroups from the list for some futher study.\n\nG = SymmetricGroup (3)\nsg = G.subgroups (); sg\n\n[Subgroup of (Symmetric group of order 3! as a permutation group)\ngenerated by [()],\n\nSubgroup of (Symmetric group of order 3! as a permutation group)\ngenerated by [(2,3)],\n\nSubgroup of (Symmetric group of order 3! as a permutation group)\ngenerated by [(1,2)],\n\nSubgroup of (Symmetric group of order 3! as a permutation group)\ngenerated by [(1,3)],\n\nSubgroup of (Symmetric group of order 3! as a permutation group)\ngenerated by [(1,2,3)],\n\nSubgroup of (Symmetric group of order 3! as a permutation group)\ngenerated by [(2,3), (1,2,3)]]\n\nH = sg[4]; H\n\n\n\n6.5. SAGE 109\n\nSubgroup of (Symmetric group of order 3! as a permutation group)\ngenerated by [(1,2,3)]\n\nH.order()\n\n3\n\nH.list()\n\n[(), (1,2,3), (1,3,2)]\n\nH.is_cyclic ()\n\nTrue\n\nThe output of the .subgroups() method can be voluminous, so sometimes we are inter-\nested in properties of specific subgroups (as in the previous example) or broader questions\nof the group’s “subgroup structure.” Here we expand on Corollary 6.15. Notice that just\nbecause Sage does not compute a subgroup of order 6 in A4, this is no substitute whatsoever\nfor a proof such as given for the corollary. But the computational result emboldens us to\nsearch for the theoretical result with confidence.\n\nG = AlternatingGroup (4)\nsg = G.subgroups ()\n[H.order () for H in sg]\n\n[1, 2, 2, 2, 3, 3, 3, 3, 4, 12]\n\nSo we see no subgroup of order 6 in the list of subgroups of A4. Notice how Lagrange’s\nTheorem (Theorem 6.10) is in evidence — all the subgroup orders divide 12, the order of\nA4. Be patient, the next subgroup computation may take a while.\n\nG = SymmetricGroup (4)\nsg = G.subgroups ()\n[H.order () for H in sg]\n\n[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,\n6, 6, 6, 6, 8, 8, 8, 12, 24]\n\nAgain, note Lagrange’s Theorem in action. But more interestingly, S4 has a subgroup\nof order 6. Four of them, to be precise. These four subgroups of order 6 are similar to each\nother, can you describe them simply (before digging into the sg list for more information)?\nIf you were curious how many subgroups S4 has, you could simply count the number of\nsubgroups in the sg list. The len() function does this for any list and is often an easy way\nto count things.\n\nlen(sg)\n\n30\n\nSubgroups of Cyclic Groups\nNow that we are more familiar with permutation groups, and know about the .subgroups()\n\nmethod, we can revisit an idea from Chapter 4. The subgroups of a cyclic group are always\ncyclic, but how many are there and what are their orders?\n\n\n\n110 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\n\nG = CyclicPermutationGroup (20)\n[H.order () for H in G.subgroups ()]\n\n[1, 2, 4, 5, 10, 20]\n\nG = CyclicPermutationGroup (19)\n[H.order () for H in G.subgroups ()]\n\n[1, 19]\n\nWe could do this all day, but you have Sage at your disposal, so vary the order of G by\nchanging n and study the output across many runs. Maybe try a cyclic group of order 24\nand compare with the symmetric group S4 (above) which also has order 24. Do you feel a\nconjecture coming on?\n\nn = 8\nG = CyclicPermutationGroup(n)\n[H.order () for H in G.subgroups ()]\n\n[1, 2, 4, 8]\n\nEuler Phi Function\nTo add to our number-theoretic functions from Chapter 2, we note that Sage makes the\nEuler ϕ-function available as the function euler_phi().\n\neuler_phi (345)\n\n176\n\nHere’s an interesting experiment that you can try running several times.\nm = random_prime (10000)\nn = random_prime (10000)\nm, n, euler_phi(m*n) == euler_phi(m)*euler_phi(n)\n\n(5881, 1277, True)\n\nFeel another conjecture coming on? Can you generalize this result?\n\n6.6 Sage Exercises\nThe following exercises are less about cosets and subgroups, and more about using Sage\nas an experimental tool. They are designed to help you become both more efficient, and\nmore expressive, as you write commands in Sage. We will have many opportunities to work\nwith cosets and subgroups in the coming chapters. These exercises do not contain much\nguidance, and get more challenging as they go. They are designed to explore, or confirm,\nresults presented in this chapter or earlier chapters.\n\nImportant: You should answer each of the last three problems with a single (com-\nplicated) line of Sage that concludes by outputting True. A “single line” means you will\nhave several Sage commands packaged up together in complicated ways. It does not mean\nseveral Sage commands seperated by semi-colons and typed in on a single line. Be sure\ninclude some intermediate steps used in building up your solution, but using smaller ranges\n\n\n\n6.6. SAGE EXERCISES 111\n\nof values so as to not overwhelm the reader with lots of output. This will help you, and the\ngrader of your work, have some confidence that the final version is correct.\n\nWhen you check integers below for divisibility, remember that range() produces plain\nintegers, which are quite simple in their functionality. The srange() command produces Sage\nintegers, which have many more capabilities. (See the last exercise for an example.) And\nremember that a list comprehension is a very compact way to examine many possibilities\nat once.\n1. Use .subgroups() to find an example of a group G and an integer m, so that (a) m\ndivides the order of G, and (b) G has no subgroup of order m. (Do not use the group A4\n\nfor G, since this is in the text.) Provide a single line of Sage code that has all the logic to\nproduce the desired m as its output. (You can give your group a simple name on a prior\nline and then just reference the group by name.) Here is a very simple example that might\nhelp you structure your answer.\n\na = 5\nb = 10\nc = 6\nd = 13\na.divides(b)\n\nTrue\n\nnot (b in [c,d])\n\nTrue\n\na.divides(b) and not (b in [c,d])\n\nTrue\n\n2. Verify the truth of Fermat’s Little Theorem (either variant) using the composite number\n391 = 17 · 23 as the choice of the base (either a or b), and for p assuming the value of every\nprime number between 100 and 1000.\nBuild up a solution slowly — make a list of powers (start with just a few primes), then\nmake a list of powers reduced by modular arithmetic, then a list of comparisons with the\npredicted value, then a check on all these logical values resulting from the comparisons.\nThis is a useful strategy for many similar problems. Eventually you will write a single line\nthat performs the verification by eventually printing out True. Here are some more hints\nabout useful functions.\n\na = 20\nb = 6\na.mod(b)\n\n2\n\nprime_range (50, 100)\n\n[53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n\nall([True , True , True , True])\n\nTrue\n\n\n\n112 CHAPTER 6. COSETS AND LAGRANGE’S THEOREM\n\nall([True , True , False , True])\n\nFalse\n\n3. Verify that the group of units mod n has order n−1 when n is prime, again for all primes\nbetween 100 and 1000. As before, your output should be simply True, just once, indicating\nthat the statement about the order is true for all the primes examined. As before, build\nup your solution slowly, and with a smaller range of primes in the beginning. Express your\nanswer as a single line of Sage code.\n\n4. Verify Euler’s Theorem for all values of 0 < n < 100 and for 1 ≤ a ≤ n. This will require\nnested for statements with a conditional. Again, here is a small example that might be\nhelpful for constructing your one line of Sage code. Note the use of srange() in this example.\n\n[a/b for a in srange (9) for b in srange(1,a) if gcd(a,b)==1]\n\n[2, 3, 3/2, 4, 4/3, 5, 5/2, 5/3, 5/4, 6, 6/5,\n7, 7/2, 7/3, 7/4, 7/5, 7/6, 8, 8/3, 8/5, 8/7]\n\n5. The symmetric group on 7 symbols, S7, has 7! = 5040 elements. Consider the following\nquestions without employing Sage, based on what we know about orders of elements of\npermutation groups (Exercise 5.3.13).\n\n• What is the maximum possible order?\n• How many elements are there of order 10?\n• How many elements are there of order 1?\n• How many elements are there of order 2?\n• What is the smallest positive integer for which there is no element with that order?\n\nThese questions will be easier if you are familiar with using binomial coefficients for counting\nin similarly complex situations. But either way, give some serious thought to each question\n(and maybe a few of your own) before firing up Sage.\nNow, compute how many elements there are of each order using the .order() method, and\nthen embed this into a list comprehension which creates a single list of these counts. You\ncan check your work (or check Sage) by wrapping this list in sum() and hopefully getting\n5040.\nComment on the process of studying these questions first without any computational aid,\nand then again with Sage. For which values of n do you think Sage would be too slow and\nyour mind quicker?\n\n\n\n7\n\nIntroduction to Cryptography\n\nCryptography is the study of sending and receiving secret messages. The aim of cryptogra-\nphy is to send messages across a channel so that only the intended recipient of the message\ncan read it. In addition, when a message is received, the recipient usually requires some\nassurance that the message is authentic; that is, that it has not been sent by someone who\nis trying to deceive the recipient. Modern cryptography is heavily dependent on abstract\nalgebra and number theory.\n\nThe message to be sent is called the plaintext message. The disguised message is called\nthe ciphertext. The plaintext and the ciphertext are both written in an alphabet, con-\nsisting of letters or characters. Characters can include not only the familiar alphabetic\ncharacters A, . . ., Z and a, . . ., z but also digits, punctuation marks, and blanks. A cryp-\ntosystem, or cipher, has two parts: encryption, the process of transforming a plaintext\nmessage to a ciphertext message, and decryption, the reverse transformation of changing\na ciphertext message into a plaintext message.\n\nThere are many different families of cryptosystems, each distinguished by a particular\nencryption algorithm. Cryptosystems in a specified cryptographic family are distinguished\nfrom one another by a parameter to the encryption function called a key. A classical\ncryptosystem has a single key, which must be kept secret, known only to the sender and\nthe receiver of the message. If person A wishes to send secret messages to two different\npeople B and C, and does not wish to have B understand C’s messages or vice versa, A\nmust use two separate keys, so one cryptosystem is used for exchanging messages with B,\nand another is used for exchanging messages with C.\n\nSystems that use two separate keys, one for encoding and another for decoding, are\ncalled public key cryptosystems. Since knowledge of the encoding key does not allow\nanyone to guess at the decoding key, the encoding key can be made public. A public key\ncryptosystem allows A and B to send messages to C using the same encoding key. Anyone\nis capable of encoding a message to be sent to C, but only C knows how to decode such a\nmessage.\n\n7.1 Private Key Cryptography\n\nIn single or private key cryptosystems the same key is used for both encrypting and\ndecrypting messages. To encrypt a plaintext message, we apply to the message some func-\ntion which is kept secret, say f . This function will yield an encrypted message. Given\nthe encrypted form of the message, we can recover the original message by applying the\ninverse transformation f−1. The transformation f must be relatively easy to compute, as\nmust f−1; however, f must be extremely difficult to guess from available examples of coded\nmessages.\n\n113\n\n\n\n114 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\n\nExample 7.1. One of the first and most famous private key cryptosystems was the shift\ncode used by Julius Caesar. We first digitize the alphabet by letting A = 00,B = 01, . . . ,Z =\n25. The encoding function will be\n\nf(p) = p+ 3 mod 26;\n\nthat is, A 7→ D,B 7→ E, . . . , Z 7→ C. The decoding function is then\n\nf−1(p) = p− 3 mod 26 = p+ 23 mod 26.\n\nSuppose we receive the encoded message DOJHEUD. To decode this message, we first\ndigitize it:\n\n3, 14, 9, 7, 4, 20, 3.\n\nNext we apply the inverse transformation to get\n\n0, 11, 6, 4, 1, 17, 0,\n\nor ALGEBRA. Notice here that there is nothing special about either of the numbers 3 or\n26. We could have used a larger alphabet or a different shift.\n\nCryptanalysis is concerned with deciphering a received or intercepted message. Meth-\nods from probability and statistics are great aids in deciphering an intercepted message;\nfor example, the frequency analysis of the characters appearing in the intercepted message\noften makes its decryption possible.\n\nExample 7.2. Suppose we receive a message that we know was encrypted by using a shift\ntransformation on single letters of the 26-letter alphabet. To find out exactly what the shift\ntransformation was, we must compute b in the equation f(p) = p + b mod 26. We can do\nthis using frequency analysis. The letter E = 04 is the most commonly occurring letter\nin the English language. Suppose that S = 18 is the most commonly occurring letter in\nthe ciphertext. Then we have good reason to suspect that 18 = 4 + b mod 26, or b = 14.\nTherefore, the most likely encrypting function is\n\nf(p) = p+ 14 mod 26.\n\nThe corresponding decrypting function is\n\nf−1(p) = p+ 12 mod 26.\n\nIt is now easy to determine whether or not our guess is correct.\n\nSimple shift codes are examples of monoalphabetic cryptosystems. In these ciphers a\ncharacter in the enciphered message represents exactly one character in the original message.\nSuch cryptosystems are not very sophisticated and are quite easy to break. In fact, in a\nsimple shift as described in Example 7.1, there are only 26 possible keys. It would be quite\neasy to try them all rather than to use frequency analysis.\n\nLet us investigate a slightly more sophisticated cryptosystem. Suppose that the encoding\nfunction is given by\n\nf(p) = ap+ b mod 26.\n\nWe first need to find out when a decoding function f−1 exists. Such a decoding function\nexists when we can solve the equation\n\nc = ap+ b mod 26\n\n\n\n7.2. PUBLIC KEY CRYPTOGRAPHY 115\n\nfor p. By Proposition 3.4, this is possible exactly when a has an inverse or, equivalently,\nwhen gcd(a, 26) = 1. In this case\n\nf−1(p) = a−1p− a−1b mod 26.\n\nSuch a cryptosystem is called an affine cryptosystem.\n\nExample 7.3. Let us consider the affine cryptosystem f(p) = ap + b mod 26. For this\ncryptosystem to work we must choose an a ∈ Z26 that is invertible. This is only possible if\ngcd(a, 26) = 1. Recognizing this fact, we will let a = 5 since gcd(5, 26) = 1. It is easy to see\nthat a−1 = 21. Therefore, we can take our encryption function to be f(p) = 5p+3 mod 26.\nThus, ALGEBRA is encoded as 3, 6, 7, 23, 8, 10, 3, or DGHXIKD. The decryption function\nwill be\n\nf−1(p) = 21p− 21 · 3 mod 26 = 21p+ 15 mod 26.\n\nA cryptosystem would be more secure if a ciphertext letter could represent more than one\nplaintext letter. To give an example of this type of cryptosystem, called a polyalphabetic\ncryptosystem, we will generalize affine codes by using matrices. The idea works roughly\nthe same as before; however, instead of encrypting one letter at a time we will encrypt pairs\nof letters. We can store a pair of letters p1 and p2 in a vector\n\np =\n\n(\np1\np2\n\n)\n.\n\nLet A be a 2× 2 invertible matrix with entries in Z26. We can define an encoding function\nby\n\nf(p) = Ap + b,\n\nwhere b is a fixed column vector and matrix operations are performed in Z26. The decoding\nfunction must be\n\nf−1(p) = A−1p −A−1b.\n\nExample 7.4. Suppose that we wish to encode the word HELP. The corresponding digit\nstring is 7, 4, 11, 15. If\n\nA =\n\n(\n3 5\n\n1 2\n\n)\n,\n\nthen\nA−1 =\n\n(\n2 21\n\n25 3\n\n)\n.\n\nIf b = (2, 2)t, then our message is encrypted as RRGR. The encrypted letter R represents\nmore than one plaintext letter.\n\nFrequency analysis can still be performed on a polyalphabetic cryptosystem, because we\nhave a good understanding of how pairs of letters appear in the English language. The pair\nth appears quite often; the pair qz never appears. To avoid decryption by a third party, we\nmust use a larger matrix than the one we used in Example 7.4.\n\n7.2 Public Key Cryptography\nIf traditional cryptosystems are used, anyone who knows enough to encode a message will\nalso know enough to decode an intercepted message. In 1976, W. Diffie and M. Hellman\nproposed public key cryptography, which is based on the observation that the encryption and\n\n\n\n116 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\n\ndecryption procedures need not have the same key. This removes the requirement that the\nencoding key be kept secret. The encoding function f must be relatively easy to compute,\nbut f−1 must be extremely difficult to compute without some additional information, so\nthat someone who knows only the encrypting key cannot find the decrypting key without\nprohibitive computation. It is interesting to note that to date, no system has been proposed\nthat has been proven to be “one-way;” that is, for any existing public key cryptosystem,\nit has never been shown to be computationally prohibitive to decode messages with only\nknowledge of the encoding key.\n\nThe RSA Cryptosystem\nThe rsa cryptosystem introduced by R. Rivest, A. Shamir, and L. Adleman in 1978, is\nbased on the difficulty of factoring large numbers. Though it is not a difficult task to find\ntwo large random primes and multiply them together, factoring a 150-digit number that is\nthe product of two large primes would take 100 million computers operating at 10 million\ninstructions per second about 50 million years under the fastest algorithms available in the\nearly 1990s. Although the algorithms have improved, factoring a number that is a product\nof two large primes is still computationally prohibative.\n\nThe rsa cryptosystem works as follows. Suppose that we choose two random 150-\ndigit prime numbers p and q. Next, we compute the product n = pq and also compute\nϕ(n) = m = (p− 1)(q− 1), where ϕ is the Euler ϕ-function. Now we start choosing random\nintegers E until we find one that is relatively prime to m; that is, we choose E such that\ngcd(E,m) = 1. Using the Euclidean algorithm, we can find a number D such that DE ≡ 1\n(mod m). The numbers n and E are now made public.\n\nSuppose now that person B (Bob) wishes to send person A (Alice) a message over a\npublic line. Since E and n are known to everyone, anyone can encode messages. Bob\nfirst digitizes the message according to some scheme, say A = 00,B = 02, . . . ,Z = 25. If\nnecessary, he will break the message into pieces such that each piece is a positive integer\nless than n. Suppose x is one of the pieces. Bob forms the number y = xE mod n and\nsends y to Alice. For Alice to recover x, she need only compute x = yD mod n. Only Alice\nknows D.\nExample 7.5. Before exploring the theory behind the rsa cryptosystem or attempting to\nuse large integers, we will use some small integers just to see that the system does indeed\nwork. Suppose that we wish to send some message, which when digitized is 25. Let p = 23\nand q = 29. Then\n\nn = pq = 667\n\nand\nϕ(n) = m = (p− 1)(q − 1) = 616.\n\nWe can let E = 487, since gcd(616, 487) = 1. The encoded message is computed to be\n\n25487 mod 667 = 169.\n\nThis computation can be reasonably done by using the method of repeated squares as\ndescribed in Chapter 4. Using the Euclidean algorithm, we determine that 191E = 1+151m;\ntherefore, the decrypting key is (n,D) = (667, 191). We can recover the original message\nby calculating\n\n169191 mod 667 = 25.\n\nNow let us examine why the rsa cryptosystem works. We know that DE ≡ 1 (mod m);\nhence, there exists a k such that\n\nDE = km+ 1 = kϕ(n) + 1.\n\n\n\n7.2. PUBLIC KEY CRYPTOGRAPHY 117\n\nThere are two cases to consider. In the first case assume that gcd(x, n) = 1. Then by\nTheorem 6.18,\n\nyD = (xE)D = xDE = xkm+1 = (xϕ(n))kx = (1)kx = x mod n.\n\nSo we see that Alice recovers the original message x when she computes yD mod n.\nFor the other case, assume that gcd(x, n) ̸= 1. Since n = pq and x < n, we know x is\n\na multiple of p or a multiple of q, but not both. We will describe the first possibility only,\nsince the second is entirely similar. There is then an integer r, with r < q and x = rp. Note\nthat we have gcd(x, q) = 1 and that m = ϕ(n) = (p − 1)(q − 1) = ϕ(p)ϕ(q). Then, using\nTheorem 6.18, but now mod q,\n\nxkm = xkϕ(p)ϕ(q) = (xϕ(q))kϕ(p) = (1)kϕ(p) = 1 mod q.\n\nSo there is an integer t such that xkm = 1 + tq. Thus, Alice also recovers the message in\nthis case,\n\nyD = xkm+1 = xkmx = (1 + tq)x = x+ tq(rp) = x+ trn = x mod n.\n\nWe can now ask how one would go about breaking the rsa cryptosystem. To find D\ngiven n and E, we simply need to factor n and solve for D by using the Euclidean algorithm.\nIf we had known that 667 = 23 · 29 in Example 7.5, we could have recovered D.\n\nMessage Verification\nThere is a problem of message verification in public key cryptosystems. Since the encoding\nkey is public knowledge, anyone has the ability to send an encoded message. If Alice\nreceives a message from Bob, she would like to be able to verify that it was Bob who\nactually sent the message. Suppose that Bob’s encrypting key is (n′, E′) and his decrypting\nkey is (n′, D′). Also, suppose that Alice’s encrypting key is (n,E) and her decrypting key is\n(n,D). Since encryption keys are public information, they can exchange coded messages at\ntheir convenience. Bob wishes to assure Alice that the message he is sending is authentic.\nBefore Bob sends the message x to Alice, he decrypts x with his own key:\n\nx′ = xD\n′ mod n′.\n\nAnyone can change x′ back to x just by encryption, but only Bob has the ability to form\nx′. Now Bob encrypts x′ with Alice’s encryption key to form\n\ny′ = x′\nE mod n,\n\na message that only Alice can decode. Alice decodes the message and then encodes the\nresult with Bob’s key to read the original message, a message that could have only been\nsent by Bob.\n\nHistorical Note\n\nEncrypting secret messages goes as far back as ancient Greece and Rome. As we know,\nJulius Caesar used a simple shift code to send and receive messages. However, the formal\nstudy of encoding and decoding messages probably began with the Arabs in the 1400s. In\nthe fifteenth and sixteenth centuries mathematicians such as Alberti and Viete discovered\nthat monoalphabetic cryptosystems offered no real security. In the 1800s, F. W. Kasiski\nestablished methods for breaking ciphers in which a ciphertext letter can represent more\n\n\n\n118 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\n\nthan one plaintext letter, if the same key was used several times. This discovery led to the\nuse of cryptosystems with keys that were used only a single time. Cryptography was placed\non firm mathematical foundations by such people as W. Friedman and L. Hill in the early\npart of the twentieth century.\n\nThe period after World War I saw the development of special-purpose machines for\nencrypting and decrypting messages, and mathematicians were very active in cryptography\nduring World War II. Efforts to penetrate the cryptosystems of the Axis nations were\norganized in England and in the United States by such notable mathematicians as Alan\nTuring and A. A. Albert. The Allies gained a tremendous advantage in World War II by\nbreaking the ciphers produced by the German Enigma machine and the Japanese Purple\nciphers.\n\nBy the 1970s, interest in commercial cryptography had begun to take hold. There was\na growing need to protect banking transactions, computer data, and electronic mail. In\nthe early 1970s, ibm developed and implemented luzifer, the forerunner of the National\nBureau of Standards’ Data Encryption Standard (DES).\n\nThe concept of a public key cryptosystem, due to Diffie and Hellman, is very recent\n(1976). It was further developed by Rivest, Shamir, and Adleman with the rsa cryptosys-\ntem (1978). It is not known how secure any of these systems are. The trapdoor knapsack\ncryptosystem, developed by Merkle and Hellman, has been broken. It is still an open ques-\ntion whether or not the rsa system can be broken. In 1991, rsa Laboratories published a\nlist of semiprimes (numbers with exactly two prime factors) with a cash prize for whoever\nwas able to provide a factorization (http://www.emc.com/emc-plus/rsa-labs/historical/the-\nrsa-challenge-numbers.htm). Although the challenge ended in 2007, many of these numbers\nhave not yet been factored.\n\nThere been a great deal of controversy about research in cryptography and cryptography\nitself. In 1929, when Henry Stimson, Secretary of State under Herbert Hoover, dismissed the\nBlack Chamber (the State Department’s cryptography division) on the ethical grounds that\n“gentlemen do not read each other’s mail.” During the last two decades of the twentieth\ncentury, the National Security Agency wanted to keep information about cryptography\nsecret, whereas the academic community fought for the right to publish basic research.\nCurrently, research in mathematical cryptography and computational number theory is\nvery active, and mathematicians are free to publish their results in these areas.\n\n7.3 Exercises\n1. Encode IXLOVEXMATH using the cryptosystem in Example 1.\n\n2. Decode ZLOOA WKLVA EHARQ WKHA ILQDO, which was encoded using the cryptosystem in\nExample 1.\n\n3. Assuming that monoalphabetic code was used to encode the following secret message,\nwhat was the original message?\n\nAPHUO EGEHP PEXOV FKEUH CKVUE CHKVE APHUO\nEGEHU EXOVL EXDKT VGEFT EHFKE UHCKF TZEXO\nVEZDT TVKUE XOVKV ENOHK ZFTEH TEHKQ LEROF\nPVEHP PEXOV ERYKP GERYT GVKEG XDRTE RGAGA\n\nWhat is the significance of this message in the history of cryptography?\n\n4. What is the total number of possible monoalphabetic cryptosystems? How secure are\nsuch cryptosystems?\n\nhttp://www.emc.com/emc-plus/rsa-labs/historical/the-rsa-challenge-numbers.htm\nhttp://www.emc.com/emc-plus/rsa-labs/historical/the-rsa-challenge-numbers.htm\n\n\n7.4. ADDITIONAL EXERCISES: PRIMALITY AND FACTORING 119\n\n5. Prove that a 2×2 matrix A with entries in Z26 is invertible if and only if gcd(det(A), 26) =\n1.\n\n6. Given the matrix\nA =\n\n(\n3 4\n\n2 3\n\n)\n,\n\nuse the encryption function f(p) = Ap + b to encode the message CRYPTOLOGY, where b =\n(2, 5)t. What is the decoding function?\n\n7. Encrypt each of the following rsa messages x so that x is divided into blocks of integers\nof length 2; that is, if x = 142528, encode 14, 25, and 28 separately.\n(a) n = 3551, E = 629, x = 31\n\n(b) n = 2257, E = 47, x = 23\n\n(c) n = 120979, E = 13251, x = 142371\n\n(d) n = 45629, E = 781, x = 231561\n\n8. Compute the decoding key D for each of the encoding keys in Exercise 7.\n\n9. Decrypt each of the following rsa messages y.\n(a) n = 3551, D = 1997, y = 2791\n\n(b) n = 5893, D = 81, y = 34\n\n(c) n = 120979, D = 27331, y = 112135\n\n(d) n = 79403, D = 671, y = 129381\n\n10. For each of the following encryption keys (n,E) in the rsa cryptosystem, compute D.\n(a) (n,E) = (451, 231)\n\n(b) (n,E) = (3053, 1921)\n\n(c) (n,E) = (37986733, 12371)\n\n(d) (n,E) = (16394854313, 34578451)\n\n11. Encrypted messages are often divided into blocks of n letters. A message such as\nTHE WORLD WONDERS WHY might be encrypted as JIW OCFRJ LPOEVYQ IOC but sent as JIW OCF\n\nRJL POE VYQ IOC. What are the advantages of using blocks of n letters?\n\n12. Find integers n, E, and X such that\n\nXE ≡ X (mod n).\n\nIs this a potential problem in the rsa cryptosystem?\n\n13. Every person in the class should construct an rsa cryptosystem using primes that are\n10 to 15 digits long. Hand in (n,E) and an encoded message. Keep D secret. See if you\ncan break one another’s codes.\n\n7.4 Additional Exercises: Primality and Factoring\nIn the rsa cryptosystem it is important to be able to find large prime numbers easily. Also,\nthis cryptosystem is not secure if we can factor a composite number that is the product\nof two large primes. The solutions to both of these problems are quite easy. To find out\n\n\n\n120 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\n\nif a number n is prime or to factor n, we can use trial division. We simply divide n by\nd = 2, 3, . . . ,\n\n√\nn. Either a factorization will be obtained, or n is prime if no d divides n.\n\nThe problem is that such a computation is prohibitively time-consuming if n is very large.\n1. A better algorithm for factoring odd positive integers is Fermat’s factorization al-\ngorithm.\n(a) Let n = ab be an odd composite number. Prove that n can be written as the difference\n\nof two perfect squares:\nn = x2 − y2 = (x− y)(x+ y).\n\nConsequently, a positive odd integer can be factored exactly when we can find integers\nx and y such that n = x2 − y2.\n\n(b) Write a program to implement the following factorization algorithm based on the\nobservation in part (a). The expression ceiling(sqrt(n)) means the smallest integer\ngreater than or equal to the square root of n. Write another program to do factorization\nusing trial division and compare the speed of the two algorithms. Which algorithm is\nfaster and why?\n\nx := ceiling(sqrt(n))\ny := 1\n\n1 : while x^2 - y^2 > n do\ny := y + 1\n\nif x^2 - y^2 < n then\nx := x + 1\ny := 1\ngoto 1\n\nelse if x^2 - y^2 = 0 then\na := x - y\nb := x + y\nwrite n = a * b\n\n2. (Primality Testing) Recall Fermat’s Little Theorem from Chapter 6. Let p be prime\nwith gcd(a, p) = 1. Then ap−1 ≡ 1 (mod p). We can use Fermat’s Little Theorem as a\nscreening test for primes. For example, 15 cannot be prime since\n\n215−1 ≡ 214 ≡ 4 (mod 15).\n\nHowever, 17 is a potential prime since\n\n217−1 ≡ 216 ≡ 1 (mod 17).\n\nWe say that an odd composite number n is a pseudoprime if\n\n2n−1 ≡ 1 (mod n).\n\nWhich of the following numbers are primes and which are pseudoprimes?\n\n(a) 342\n(b) 811\n\n(c) 601\n(d) 561\n\n(e) 771\n(f) 631\n\n\n\n7.5. REFERENCES AND SUGGESTED READINGS 121\n\n3. Let n be an odd composite number and b be a positive integer such that gcd(b, n) = 1.\nIf bn−1 ≡ 1 (mod n), then n is a pseudoprime base b. Show that 341 is a pseudoprime\nbase 2 but not a pseudoprime base 3.\n\n4. Write a program to determine all primes less than 2000 using trial division. Write a\nsecond program that will determine all numbers less than 2000 that are either primes or\npseudoprimes. Compare the speed of the two programs. How many pseudoprimes are there\nbelow 2000?\nThere exist composite numbers that are pseudoprimes for all bases to which they are rel-\natively prime. These numbers are called Carmichael numbers. The first Carmichael\nnumber is 561 = 3 · 11 · 17. In 1992, Alford, Granville, and Pomerance proved that there\nare an infinite number of Carmichael numbers [4]. However, Carmichael numbers are very\nrare. There are only 2163 Carmichael numbers less than 25× 109. For more sophisticated\nprimality tests, see [1], [6], or [7].\n\n7.5 References and Suggested Readings\n[1] Bressoud, D. M. Factorization and Primality Testing. Springer-Verlag, New York,\n\n1989.\n[2] Diffie, W. and Hellman, M. E. “New Directions in Cryptography,” IEEE Trans. In-\n\nform. Theory 22 (1976), 644–54.\n[3] Gardner, M. “Mathematical games: A new kind of cipher that would take millions of\n\nyears to break,” Scientific American 237 (1977), 120–24.\n[4] Granville, A. “Primality Testing and Carmichael Numbers,” Notices of the American\n\nMathematical Society 39(1992), 696–700.\n[5] Hellman, M. E. “The Mathematics of Public Key Cryptography,” Scientific American\n\n241(1979), 130–39.\n[6] Koblitz, N. A Course in Number Theory and Cryptography. 2nd ed. Springer, New\n\nYork, 1994.\n[7] Pomerance, C., ed. “Cryptology and Computational Number Theory”, Proceedings\n\nof Symposia in Applied Mathematics 42(1990) American Mathematical Society, Prov-\nidence, RI.\n\n[8] Rivest, R. L., Shamir, A., and Adleman, L., “A Method for Obtaining Signatures and\nPublic-key Cryptosystems,” Comm. ACM 21(1978), 120–26.\n\n7.6 Sage\nSince Sage began as software to support research in number theory, we can quickly and easily\ndemonstrate the internal workings of the rsa algorithm. Recognize that, in practice, many\nother details such as encoding between letters and integers, or protecting one’s private key,\nare equally important for the security of communications. So rsa itself is just the theoretical\nfoundation.\n\nConstructing Keys\nWe will suppose that Alice wants to send a secret message to Bob, along with message\nverification (also known as a message with a digital signature). So we begin with the\nconstruction of key pairs (private and public) for both Alice and Bob. We first need two\n\n\n\n122 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\n\nlarge primes for both individuals, and their product. In practice, values of n would have\nhundreds of digits, rather than just 21 as we have done here.\n\np_a = next_prime (10^10)\nq_a = next_prime(p_a)\np_b = next_prime ((3/2) *10^10)\nq_b = next_prime(p_b)\nn_a = p_a * q_a\nn_b = p_b * q_b\nn_a , n_b\n\n(100000000520000000627 , 225000000300000000091)\n\nComputationally, the value of the Euler ϕ-function for a product of primes pq can be\nobtained from (p− 1)(q − 1), but we could use Sage’s built-in function just as well.\n\nm_a = euler_phi(n_a)\nm_b = euler_phi(n_b)\nm_a , m_b\n\n(100000000500000000576 , 225000000270000000072)\n\nNow we can create the encryption and decryption exponents. We choose the encryption\nexponent as a (small) number relatively prime to the value of m. With Sage we can factor m\nquickly to help us choose this value. In practice we would not want to do this computation\nfor large values of m, so we might more easily choose “random” values and check for the first\nvalue which is relatively prime to m. The decryption exponent is the multiplicative inverse,\nmod m, of the encryption exponent. If you construct an improper encryption exponent (not\nrelatively prime to m), the computation of the multiplicative inverse will fail (and Sage will\ntell you so). We do this twice —- for both Alice and Bob.\n\nfactor(m_a)\n\n2^6 * 3 * 11 * 17 * 131 * 521 * 73259 * 557041\n\nE_a = 5*23\nD_a = inverse_mod(E_a , m_a)\nD_a\n\n20869565321739130555\n\nfactor(m_b)\n\n2^3 * 3^4 * 107 * 1298027 * 2500000001\n\nE_b = 7*29\nD_b = inverse_mod(E_b , m_b)\nD_b\n\n24384236482463054195\n\nAt this stage, each individual would publish their values of n and E, while keeping D\nvery private and secure. In practice D should be protected on the user’s hard disk by a\npassword only the owner knows. For even greater security a person might only have two\ncopies of their private key, one on a usb memory stick they always carry with them, and a\nbackup in their sage deposit box. Every time the person uses D they would need to provide\nthe password. The value of m can be discarded. For the record, here are all the keys:\n\n\n\n7.6. SAGE 123\n\nprint "Alice \' s␣public␣key ,␣n:", n_a , "E:", E_a\n\nAlice \' s␣public␣key ,␣n:␣100000000520000000627␣E:␣115\n\nprint "Alice \' s␣private␣key ,␣D:", D_a\n\nAlice \' s␣private␣key ,␣D:␣20869565321739130555\n\nprint "Bob \' s␣public␣key ,␣n:", n_b , "E:", E_b\n\nBob \' s␣public␣key ,␣n:␣225000000300000000091␣E:␣203\n\nprint "Bob \' s␣private␣key ,␣D:", D_b\n\nBob \' s␣private␣key ,␣D:␣24384236482463054195\n\nSigning and Encoding a Message\nAlice is going to construct a message as an English word with four letters. From these four\nletters we will construct a single number to represent the message in a form we can use in\nthe rsa algorithm. The function ord() will convert a single letter to its ascii code value, a\nnumber between 0 and 127. If we use these numbers as “digits” mod 128, we can be sure\nthat Alice’s four-letter word will encode to an integer less than 1284 = 268, 435, 456. The\nparticular maximum value is not important, so long as it is smaller than our value of n since\nall of our subsequent arithmetic is mod n. We choose a popular four-letter word, convert\nto ascii “digits” with a list comprehension, and then construct the integer from the digits\nwith the right base. Notice how we can treat the word as a list and that the first digit in\nthe list is in the “ones” place (we say the list is in “little-endian” order).\n\nword = \' Sage \'\ndigits = [ord(letter) for letter in word]\ndigits\n\n[83, 97, 103, 101]\n\nmessage = ZZ(digits , 128)\nmessage\n\n213512403\n\nFirst, Alice will sign her message to provide message verification. She uses her private\nkey for this, since this is an act that only she should be able to perform.\n\nsigned = power_mod(message , D_a , n_a)\nsigned\n\n47838774644892618423\n\nThen Alice encrypts her message so that only Bob can read it. To do this, she uses\nBob’s public key. Notice how she does not have to even know Bob — for example, she could\nhave obtained Bob’s public key off his web site or maybe Bob announced his public key in\nan advertisement in the New York Times.\n\n\n\n124 CHAPTER 7. INTRODUCTION TO CRYPTOGRAPHY\n\nencrypted = power_mod(signed , E_b , n_b)\nencrypted\n\n111866209291209840488\n\nAlice’s communication is now ready to travel on any communications network, no matter\nhow insecure the network may be, and no matter how many snoops may be monitoring the\nnetwork.\n\nDecoding and Verifying a Message\nNow assume that the value of encrypted has reached Bob. Realize that Bob may not know\nAlice, and realize that Bob does not even necessarily believe what he has received has\ngenuinely originated from Alice. An adversary could be trying to confuse Bob by sending\nmessages that claim to be from Alice. First, Bob must unwrap the encyption Alice has\nprovided. This is an act only Bob, as the intended recipient, should be able to do. And he\ndoes it by using his private key, which only he knows, and which he has kept secure.\n\ndecrypted = power_mod(encrypted , D_b , n_b)\ndecrypted\n\n47838774644892618423\n\nRight now, this means very little to Bob. Anybody could have sent him an encoded\nmessage. However, this was a message Alice signed. Lets unwrap the message signing.\nNotice that this uses Alice’s public key. Bob does not need to know Alice — for example,\nhe could obtain Alice’s key off her web site or maybe Alice announced her public key in an\nadvertisement in the New York Times.\n\nreceived = power_mod(decrypted , E_a , n_a)\nreceived\n\n213512403\n\nBob needs to transform this integer representation back to a word with letters. The\nchr() function converts ascii code values to letters, and we use a list comprehension to do\nthis repeatedly.\n\ndigits = received.digits(base =128)\nletters = [chr(ascii) for ascii in digits]\nletters\n\n[ \' S \' , \' a \' , \' g \' , \' e \' ]\n\nIf we would like a slightly more recognizable result, we can combine the letters into a\nstring.\n\n\' \' .join(letters)\n\n\' Sage \'\n\nBob is pleased to obtain such an informative message from Alice. What would have\nhappened if an imposter had sent a message ostensibly from Alice, or what if an adversary\nhad intercepted Alice’s original message and replaced it with a tampered message? (The\nlatter is known as a “man in the middle” attack.)\n\n\n\n7.7. SAGE EXERCISES 125\n\nIn either case, the rogue party would not be able to duplicate Alice’s first action —\nsigning her message. If an adversary somehow signs the message, or tampers with it, the\nstep where Bob unwraps the signing will lead to total garbage. (Try it!) Because Bob\nreceived a legitimate word, properly capitalized, he has confidence that the message he\nunsigned is the same as the message Alice signed. In practice, if Alice sent several hundred\nwords as her message, the odds that it will unsign as cohrent text are astronomically small.\n\nWhat have we demonstrated?\n\n1. Alice can send messages that only Bob can read.\n\n2. Bob can receive secret messages from anybody.\n\n3. Alice can sign messages, so that then Bob (or anybody else)knows they are genuinely\nfrom Alice.\n\nOf course, without making new keys, you can reverse the roles of Alice and Bob. And if\nCarol makes a key pair, she can communicate with both Alice and Bob in the same fashion.\n\nIf you want to use rsa public-key encryption seriously, investigate the open source\nsoftware GNU Privacy Guard, aka GPG, which is freely available at www.gnupg.org/. Notice\nthat it only makes sense to use encryption programs that allow you to look at the source\ncode.\n\n7.7 Sage Exercises\n1. Construct a keypair for Alice using the first two primes greater than 1012. For your\nchoice of E, use a single prime number and use the smallest possible choice.\nOutput the values of n, E, and D for Alice. Then use Sage commands to verify that Alice’s\nencryption and decryption keys are multiplicative inverses.\n\n2. Construct a keypair for Bob using the first two primes greater than 2 · 1012. For your\nchoice of E, use a single prime number and use the smallest possible choice. Output the\nvalues of n, E, and D for Alice.\nEncode the word Math using ascii values in the same manner as described in this section\n(keep the capitalization as shown). Create a signed message of this word for communication\nfrom Alice to Bob. Output the three integers: the message, the signed message and the\nsigned, encrypted message.\n\n3. Demonstrate how Bob converts the message received from Alice back into the word Math.\nOutput the value of the intermediate computations and the final human-readable message.\n\n4. Create a new signed message from Alice to Bob. Simulate the message being tampered\nwith by adding 1 to the integer Bob receives, before he decrypts it. What result does Bob\nget for the letters of the message when he decrypts and unsigns the tampered message?\n\n5. (Classroom Exercise) Organize a class into several small groups. Have each group con-\nstruct key pairs with some minimum size (digits in n). Each group should keep their private\nkey to themselves, but make their public key available to everybody in the room. It could\nbe written on the board (error-prone) or maybe pasted in a public site like pastebin.com.\nThen each group can send a signed message to another group, where the groups could be\narranged logically in a circular fashion for this purpose. Of course, messages should be\nposted publicly as well. Expect a success rate somewhere between 50% and 100%.\nIf you do not do this in class, grab a study buddy and send each other messages in the same\nmanner. Expect a success rate of 0%, 50% or 100%.\n\nhttps://www.gnupg.org/\nhttp://pastebin.com/\n\n\n8\n\nAlgebraic Coding Theory\n\nCoding theory is an application of algebra that has become increasingly important over the\nlast several decades. When we transmit data, we are concerned about sending a message\nover a channel that could be affected by “noise.” We wish to be able to encode and decode the\ninformation in a manner that will allow the detection, and possibly the correction, of errors\ncaused by noise. This situation arises in many areas of communications, including radio,\ntelephone, television, computer communications, and digital media technology. Probability,\ncombinatorics, group theory, linear algebra, and polynomial rings over finite fields all play\nimportant roles in coding theory.\n\n8.1 Error-Detecting and Correcting Codes\nLet us examine a simple model of a communications system for transmitting and receiving\ncoded messages (Figure 8.1).\n\nm-digit message\n\nEncoder\n\nn-digit code word\n\nTransmitter\n\nNoise\n\nReceiver\n\nn-digit received word\n\nDecoder\n\nm-digit received message or error\n\nFigure 8.1: Encoding and decoding messages\n\nUncoded messages may be composed of letters or characters, but typically they consist\nof binary m-tuples. These messages are encoded into codewords, consisting of binary n-\ntuples, by a device called an encoder. The message is transmitted and then decoded. We\n\n126\n\n\n\n8.1. ERROR-DETECTING AND CORRECTING CODES 127\n\nwill consider the occurrence of errors during transmission. An error occurs if there is a\nchange in one or more bits in the codeword. A decoding scheme is a method that either\nconverts an arbitrarily received n-tuple into a meaningful decoded message or gives an error\nmessage for that n-tuple. If the received message is a codeword (one of the special n-tuples\nallowed to be transmitted), then the decoded message must be the unique message that\nwas encoded into the codeword. For received non-codewords, the decoding scheme will give\nan error indication, or, if we are more clever, will actually try to correct the error and\nreconstruct the original message. Our goal is to transmit error-free messages as cheaply\nand quickly as possible.\n\nExample 8.2. One possible coding scheme would be to send a message several times and\nto compare the received copies with one another. Suppose that the message to be encoded is\na binary n-tuple (x1, x2, . . . , xn). The message is encoded into a binary 3n-tuple by simply\nrepeating the message three times:\n\n(x1, x2, . . . , xn) 7→ (x1, x2, . . . , xn, x1, x2, . . . , xn, x1, x2, . . . , xn).\n\nTo decode the message, we choose as the ith digit the one that appears in the ith place\nin at least two of the three transmissions. For example, if the original message is (0110),\nthen the transmitted message will be (0110 0110 0110). If there is a transmission error in\nthe fifth digit, then the received codeword will be (0110 1110 0110), which will be correctly\ndecoded as (0110).1 This triple-repetition method will automatically detect and correct all\nsingle errors, but it is slow and inefficient: to send a message consisting of n bits, 2n extra\nbits are required, and we can only detect and correct single errors. We will see that it is\npossible to find an encoding scheme that will encode a message of n bits into m bits with\nm much smaller than 3n.\n\nExample 8.3. Even parity, a commonly used coding scheme, is much more efficient\nthan the simple repetition scheme. The ascii (American Standard Code for Information\nInterchange) coding system uses binary 8-tuples, yielding 28 = 256 possible 8-tuples. How-\never, only seven bits are needed since there are only 27 = 128 ascii characters. What\ncan or should be done with the extra bit? Using the full eight bits, we can detect single\ntransmission errors. For example, the ascii codes for A, B, and C are\n\nA = 6510 = 010000012,\n\nB = 6610 = 010000102,\n\nC = 6710 = 010000112.\n\nNotice that the leftmost bit is always set to 0; that is, the 128 ascii characters have codes\n\n000000002 = 010,\n\n...\n011111112 = 12710.\n\nThe bit can be used for error checking on the other seven bits. It is set to either 0 or 1\nso that the total number of 1 bits in the representation of a character is even. Using even\nparity, the codes for A, B, and C now become\n\nA = 010000012,\n\nB = 010000102,\n\nC = 110000112.\n\n1We will adopt the convention that bits are numbered left to right in binary n-tuples.\n\n\n\n128 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nSuppose an A is sent and a transmission error in the sixth bit is caused by noise over the\ncommunication channel so that (0100 0101) is received. We know an error has occurred since\nthe received word has an odd number of 1s, and we can now request that the codeword be\ntransmitted again. When used for error checking, the leftmost bit is called a parity check\nbit.\n\nBy far the most common error-detecting codes used in computers are based on the\naddition of a parity bit. Typically, a computer stores information in m-tuples called words.\nCommon word lengths are 8, 16, and 32 bits. One bit in the word is set aside as the parity\ncheck bit, and is not used to store information. This bit is set to either 0 or 1, depending\non the number of 1s in the word.\n\nAdding a parity check bit allows the detection of all single errors because changing a\nsingle bit either increases or decreases the number of 1s by one, and in either case the parity\nhas been changed from even to odd, so the new word is not a codeword. (We could also\nconstruct an error detection scheme based on odd parity; that is, we could set the parity\ncheck bit so that a codeword always has an odd number of 1s.)\n\nThe even parity system is easy to implement, but has two drawbacks. First, multiple\nerrors are not detectable. Suppose an A is sent and the first and seventh bits are changed\nfrom 0 to 1. The received word is a codeword, but will be decoded into a C instead of an A.\nSecond, we do not have the ability to correct errors. If the 8-tuple (1001 1000) is received,\nwe know that an error has occurred, but we have no idea which bit has been changed.\nWe will now investigate a coding scheme that will not only allow us to detect transmission\nerrors but will actually correct the errors.\n\nTransmitted Received Word\nCodeword 000 001 010 011 100 101 110 111\n\n000 0 1 1 2 1 2 2 3\n111 3 2 2 1 2 1 1 0\n\nTable 8.4: A repetition code\n\nExample 8.5. Suppose that our original message is either a 0 or a 1, and that 0 encodes\nto (000) and 1 encodes to (111). If only a single error occurs during transmission, we can\ndetect and correct the error. For example, if a 101 is received, then the second bit must\nhave been changed from a 1 to a 0. The originally transmitted codeword must have been\n(111). This method will detect and correct all single errors.\n\nIn Table 8.4, we present all possible words that might be received for the transmitted\ncodewords (000) and (111). Table 8.4 also shows the number of bits by which each received\n3-tuple differs from each original codeword.\n\nMaximum-Likelihood Decoding\nThe coding scheme presented in Example 8.5 is not a complete solution to the problem\nbecause it does not account for the possibility of multiple errors. For example, either a\n(000) or a (111) could be sent and a (001) received. We have no means of deciding from the\nreceived word whether there was a single error in the third bit or two errors, one in the first\nbit and one in the second. No matter what coding scheme is used, an incorrect message\ncould be received. We could transmit a (000), have errors in all three bits, and receive\nthe codeword (111). It is important to make explicit assumptions about the likelihood and\ndistribution of transmission errors so that, in a particular application, it will be known\nwhether a given error detection scheme is appropriate. We will assume that transmission\n\n\n\n8.1. ERROR-DETECTING AND CORRECTING CODES 129\n\nerrors are rare, and, that when they do occur, they occur independently in each bit; that is,\nif p is the probability of an error in one bit and q is the probability of an error in a different\nbit, then the probability of errors occurring in both of these bits at the same time is pq.\nWe will also assume that a received n-tuple is decoded into a codeword that is closest to it;\nthat is, we assume that the receiver uses maximum-likelihood decoding.2\n\np1 1\n\np\n0 0\n\nq\n\nq\n\nFigure 8.6: Binary symmetric channel\n\nA binary symmetric channel is a model that consists of a transmitter capable of\nsending a binary signal, either a 0 or a 1, together with a receiver. Let p be the probability\nthat the signal is correctly received. Then q = 1 − p is the probability of an incorrect\nreception. If a 1 is sent, then the probability that a 1 is received is p and the probability\nthat a 0 is received is q (Figure 8.6). The probability that no errors occur during the\ntransmission of a binary codeword of length n is pn. For example, if p = 0.999 and a\nmessage consisting of 10,000 bits is sent, then the probability of a perfect transmission is\n\n(0.999)10,000 ≈ 0.00005.\n\nTheorem 8.7. If a binary n-tuple (x1, . . . , xn) is transmitted across a binary symmetric\nchannel with probability p that no error will occur in each coordinate, then the probability\nthat there are errors in exactly k coordinates is(\n\nn\n\nk\n\n)\nqkpn−k.\n\nProof. Fix k different coordinates. We first compute the probability that an error has\noccurred in this fixed set of coordinates. The probability of an error occurring in a particular\none of these k coordinates is q; the probability that an error will not occur in any of the\nremaining n− k coordinates is p. The probability of each of these n independent events is\nqkpn−k. The number of possible error patterns with exactly k errors occurring is equal to(\n\nn\n\nk\n\n)\n=\n\nn!\n\nk!(n− k)!\n,\n\nthe number of combinations of n things taken k at a time. Each of these error patterns has\nprobability qkpn−k of occurring; hence, the probability of all of these error patterns is(\n\nn\n\nk\n\n)\nqkpn−k.\n\nExample 8.8. Suppose that p = 0.995 and a 500-bit message is sent. The probability that\nthe message was sent error-free is\n\npn = (0.995)500 ≈ 0.082.\n\n2This section requires a knowledge of probability, but can be skipped without loss of continuity.\n\n\n\n130 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nThe probability of exactly one error occurring is(\nn\n\n1\n\n)\nqpn−1 = 500(0.005)(0.995)499 ≈ 0.204.\n\nThe probability of exactly two errors is(\nn\n\n2\n\n)\nq2pn−2 =\n\n500 · 499\n2\n\n(0.005)2(0.995)498 ≈ 0.257.\n\nThe probability of more than two errors is approximately\n\n1− 0.082− 0.204− 0.257 = 0.457.\n\nBlock Codes\nIf we are to develop efficient error-detecting and error-correcting codes, we will need more\nsophisticated mathematical tools. Group theory will allow faster methods of encoding and\ndecoding messages. A code is an (n,m)-block code if the information that is to be coded\ncan be divided into blocks of m binary digits, each of which can be encoded into n binary\ndigits. More specifically, an (n,m)-block code consists of an encoding function\n\nE : Zm\n2 → Zn\n\n2\n\nand a decoding function\nD : Zn\n\n2 → Zm\n2 .\n\nA codeword is any element in the image of E. We also require that E be one-to-one so\nthat two information blocks will not be encoded into the same codeword. If our code is to\nbe error-correcting, then D must be onto.\n\nExample 8.9. The even-parity coding system developed to detect single errors in ascii\ncharacters is an (8, 7)-block code. The encoding function is\n\nE(x7, x6, . . . , x1) = (x8, x7, . . . , x1),\n\nwhere x8 = x7 + x6 + · · ·+ x1 with addition in Z2.\n\nLet x = (x1, . . . , xn) and y = (y1, . . . , yn) be binary n-tuples. The Hamming distance\nor distance, d(x,y), between x and y is the number of bits in which x and y differ. The\ndistance between two codewords is the minimum number of transmission errors required\nto change one codeword into the other. The minimum distance for a code, dmin, is the\nminimum of all distances d(x,y), where x and y are distinct codewords. The weight,\nw(x), of a binary codeword x is the number of 1s in x. Clearly, w(x) = d(x,0), where\n0 = (00 · · · 0).\n\nExample 8.10. Let x = (10101), y = (11010), and z = (00011) be all of the codewords in\nsome code C. Then we have the following Hamming distances:\n\nd(x,y) = 4, d(x, z) = 3, d(y, z) = 3.\n\nThe minimum distance for this code is 3. We also have the following weights:\n\nw(x) = 3, w(y) = 3, w(z) = 2.\n\nThe following proposition lists some basic properties about the weight of a codeword\nand the distance between two codewords. The proof is left as an exercise.\n\n\n\n8.1. ERROR-DETECTING AND CORRECTING CODES 131\n\nProposition 8.11. Let x, y, and z be binary n-tuples. Then\n\n1. w(x) = d(x,0);\n\n2. d(x,y) ≥ 0;\n\n3. d(x,y) = 0 exactly when x = y;\n\n4. d(x,y) = d(y,x);\n\n5. d(x,y) ≤ d(x, z) + d(z,y).\n\nThe weights in a particular code are usually much easier to compute than the Hamming\ndistances between all codewords in the code. If a code is set up carefully, we can use this\nfact to our advantage.\n\nSuppose that x = (1101) and y = (1100) are codewords in some code. If we transmit\n(1101) and an error occurs in the rightmost bit, then (1100) will be received. Since (1100) is\na codeword, the decoder will decode (1100) as the transmitted message. This code is clearly\nnot very appropriate for error detection. The problem is that d(x,y) = 1. If x = (1100) and\ny = (1010) are codewords, then d(x,y) = 2. If x is transmitted and a single error occurs,\nthen y can never be received. Table 8.12 gives the distances between all 4-bit codewords\nin which the first three bits carry information and the fourth is an even parity check bit.\nWe can see that the minimum distance here is 2; hence, the code is suitable as a single\nerror-correcting code.\n\n0000 0011 0101 0110 1001 1010 1100 1111\n0000 0 2 2 2 2 2 2 4\n0011 2 0 2 2 2 2 4 2\n0101 2 2 0 2 2 4 2 2\n0110 2 2 2 0 4 2 2 2\n1001 2 2 2 4 0 2 2 2\n1010 2 2 4 2 2 0 2 2\n1100 2 4 2 2 2 2 0 2\n1111 4 2 2 2 2 2 2 0\n\nTable 8.12: Distances between 4-bit codewords\n\nTo determine exactly what the error-detecting and error-correcting capabilities for a code\nare, we need to analyze the minimum distance for the code. Let x and y be codewords. If\nd(x,y) = 1 and an error occurs where x and y differ, then x is changed to y. The received\ncodeword is y and no error message is given. Now suppose d(x,y) = 2. Then a single error\ncannot change x to y. Therefore, if dmin = 2, we have the ability to detect single errors.\nHowever, suppose that d(x,y) = 2, y is sent, and a noncodeword z is received such that\n\nd(x, z) = d(y, z) = 1.\n\nThen the decoder cannot decide between x and y. Even though we are aware that an error\nhas occurred, we do not know what the error is.\n\nSuppose dmin ≥ 3. Then the maximum-likelihood decoding scheme corrects all single\nerrors. Starting with a codeword x, an error in the transmission of a single bit gives y\nwith d(x,y) = 1, but d(z,y) ≥ 2 for any other codeword z ̸= x. If we do not require the\ncorrection of errors, then we can detect multiple errors when a code has a minimum distance\nthat is greater than or equal to 3.\n\n\n\n132 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nTheorem 8.13. Let C be a code with dmin = 2n + 1. Then C can correct any n or fewer\nerrors. Furthermore, any 2n or fewer errors can be detected in C.\n\nProof. Suppose that a codeword x is sent and the word y is received with at most n\nerrors. Then d(x,y) ≤ n. If z is any codeword other than x, then\n\n2n+ 1 ≤ d(x, z) ≤ d(x,y) + d(y, z) ≤ n+ d(y, z).\n\nHence, d(y, z) ≥ n + 1 and y will be correctly decoded as x. Now suppose that x is\ntransmitted and y is received and that at least one error has occurred, but not more than\n2n errors. Then 1 ≤ d(x,y) ≤ 2n. Since the minimum distance between codewords is 2n+1,\ny cannot be a codeword. Consequently, the code can detect between 1 and 2n errors.\n\nExample 8.14. In Table 8.15, the codewords c1 = (00000), c2 = (00111), c3 = (11100),\nand c4 = (11011) determine a single error-correcting code.\n\n00000 00111 11100 11011\n00000 0 3 3 4\n00111 3 0 4 3\n11100 3 4 0 3\n11011 4 3 3 0\n\nTable 8.15: Hamming distances for an error-correcting code\n\nHistorical Note\n\nModern coding theory began in 1948 with C. Shannon’s paper, “A Mathematical Theory\nof Information” [7]. This paper offered an example of an algebraic code, and Shannon’s\nTheorem proclaimed exactly how good codes could be expected to be. Richard Hamming\nbegan working with linear codes at Bell Labs in the late 1940s and early 1950s after becoming\nfrustrated because the programs that he was running could not recover from simple errors\ngenerated by noise. Coding theory has grown tremendously in the past several decades.\nThe Theory of Error-Correcting Codes, by MacWilliams and Sloane [5], published in 1977,\nalready contained over 1500 references. Linear codes (Reed-Muller (32, 6)-block codes) were\nused on NASA’s Mariner space probes. More recent space probes such as Voyager have used\nwhat are called convolution codes. Currently, very active research is being done with Goppa\ncodes, which are heavily dependent on algebraic geometry.\n\n8.2 Linear Codes\nTo gain more knowledge of a particular code and develop more efficient techniques of en-\ncoding, decoding, and error detection, we need to add additional structure to our codes.\nOne way to accomplish this is to require that the code also be a group. A group code is a\ncode that is also a subgroup of Zn\n\n2 .\nTo check that a code is a group code, we need only verify one thing. If we add any\n\ntwo elements in the code, the result must be an n-tuple that is again in the code. It is not\nnecessary to check that the inverse of the n-tuple is in the code, since every codeword is its\nown inverse, nor is it necessary to check that 0 is a codeword. For instance,\n\n(11000101) + (11000101) = (00000000).\n\n\n\n8.2. LINEAR CODES 133\n\nExample 8.16. Suppose that we have a code that consists of the following 7-tuples:\n\n(0000000) (0001111) (0010101) (0011010)\n\n(0100110) (0101001) (0110011) (0111100)\n\n(1000011) (1001100) (1010110) (1011001)\n\n(1100101) (1101010) (1110000) (1111111).\n\nIt is a straightforward though tedious task to verify that this code is also a subgroup of Z7\n2\n\nand, therefore, a group code. This code is a single error-detecting and single error-correcting\ncode, but it is a long and tedious process to compute all of the distances between pairs of\ncodewords to determine that dmin = 3. It is much easier to see that the minimum weight\nof all the nonzero codewords is 3. As we will soon see, this is no coincidence. However, the\nrelationship between weights and distances in a particular code is heavily dependent on the\nfact that the code is a group.\n\nLemma 8.17. Let x and y be binary n-tuples. Then w(x + y) = d(x,y).\n\nProof. Suppose that x and y are binary n-tuples. Then the distance between x and y is\nexactly the number of places in which x and y differ. But x and y differ in a particular\ncoordinate exactly when the sum in the coordinate is 1, since\n\n1 + 1 = 0\n\n0 + 0 = 0\n\n1 + 0 = 1\n\n0 + 1 = 1.\n\nConsequently, the weight of the sum must be the distance between the two codewords.\n\nTheorem 8.18. Let dmin be the minimum distance for a group code C. Then dmin is the\nminimum of all the nonzero weights of the nonzero codewords in C. That is,\n\ndmin = min{w(x) : x ̸= 0}.\n\nProof. Observe that\n\ndmin = min{d(x,y) : x ̸= y}\n= min{d(x,y) : x + y ̸= 0}\n= min{w(x + y) : x + y ̸= 0}\n= min{w(z) : z ̸= 0}.\n\nLinear Codes\nFrom Example 8.16, it is now easy to check that the minimum nonzero weight is 3; hence,\nthe code does indeed detect and correct all single errors. We have now reduced the problem\nof finding “good” codes to that of generating group codes. One easy way to generate group\ncodes is to employ a bit of matrix theory.\n\nDefine the inner product of two binary n-tuples to be\n\nx · y = x1y1 + · · ·+ xnyn,\n\n\n\n134 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nwhere x = (x1, x2, . . . , xn)\nt and y = (y1, y2, . . . , yn)\n\nt are column vectors.3 For example, if\nx = (011001)t and y = (110101)t, then x · y = 0. We can also look at an inner product as\nthe product of a row matrix with a column matrix; that is,\n\nx · y = xty\n\n=\n(\nx1 x2 · · · xn\n\n)\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\ny1\ny2\n...\nyn\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n= x1y1 + x2y2 + · · ·+ xnyn.\n\nExample 8.19. Suppose that the words to be encoded consist of all binary 3-tuples and\nthat our encoding scheme is even-parity. To encode an arbitrary 3-tuple, we add a fourth bit\nto obtain an even number of 1s. Notice that an arbitrary n-tuple x = (x1, x2, . . . , xn)\n\nt has an\neven number of 1s exactly when x1+x2+ · · ·+xn = 0; hence, a 4-tuple x = (x1, x2, x3, x4)\n\nt\n\nhas an even number of 1s if x1 + x2 + x3 + x4 = 0, or\n\nx · 1 = xt1 =\n(\nx1 x2 x3 x4\n\n)\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n1\n\n1\n\n1\n\n1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 = 0.\n\nThis example leads us to hope that there is a connection between matrices and coding\ntheory.\n\nLet Mm×n(Z2) denote the set of all m × n matrices with entries in Z2. We do matrix\noperations as usual except that all our addition and multiplication operations occur in Z2.\nDefine the null space of a matrix H ∈ Mm×n(Z2) to be the set of all binary n-tuples x\nsuch that Hx = 0. We denote the null space of a matrix H by Null(H).\n\nExample 8.20. Suppose that\n\nH =\n\n\uf8eb\uf8ed0 1 0 1 0\n\n1 1 1 1 0\n\n0 0 1 1 1\n\n\uf8f6\uf8f8 .\n\nFor a 5-tuple x = (x1, x2, x3, x4, x5)\nt to be in the null space of H, Hx = 0. Equivalently,\n\nthe following system of equations must be satisfied:\n\nx2 + x4 = 0\n\nx1 + x2 + x3 + x4 = 0\n\nx3 + x4 + x5 = 0.\n\nThe set of binary 5-tuples satisfying these equations is\n\n(00000) (11110) (10101) (01011).\n\nThis code is easily determined to be a group code.\n\nTheorem 8.21. Let H be in Mm×n(Z2). Then the null space of H is a group code.\n3Since we will be working with matrices, we will write binary n-tuples as column vectors for the remainder\n\nof this chapter.\n\n\n\n8.3. PARITY-CHECK AND GENERATOR MATRICES 135\n\nProof. Since each element of Zn\n2 is its own inverse, the only thing that really needs to be\n\nchecked here is closure. Let x,y ∈ Null(H) for some matrix H in Mm×n(Z2). Then Hx = 0\nand Hy = 0. So\n\nH(x + y) = Hx +Hy = 0 + 0 = 0.\nHence, x + y is in the null space of H and therefore must be a codeword.\n\nA code is a linear code if it is determined by the null space of some matrix H ∈\nMm×n(Z2).\nExample 8.22. Let C be the code given by the matrix\n\nH =\n\n\uf8eb\uf8ed0 0 0 1 1 1\n\n0 1 1 0 1 1\n\n1 0 1 0 0 1\n\n\uf8f6\uf8f8 .\n\nSuppose that the 6-tuple x = (010011)t is received. It is a simple matter of matrix multi-\nplication to determine whether or not x is a codeword. Since\n\nHx =\n\n\uf8eb\uf8ed0\n\n1\n\n1\n\n\uf8f6\uf8f8 ,\n\nthe received word is not a codeword. We must either attempt to correct the word or request\nthat it be transmitted again.\n\n8.3 Parity-Check and Generator Matrices\nWe need to find a systematic way of generating linear codes as well as fast methods of\ndecoding. By examining the properties of a matrix H and by carefully choosing H, it is\npossible to develop very efficient methods of encoding and decoding messages. To this end,\nwe will introduce standard generator and canonical parity-check matrices.\n\nSuppose that H is an m × n matrix with entries in Z2 and n > m. If the last m\ncolumns of the matrix form the m×m identity matrix, Im, then the matrix is a canonical\nparity-check matrix. More specifically, H = (A | Im), where A is the m× (n−m) matrix\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n\na11 a12 · · · a1,n−m\n\na21 a22 · · · a2,n−m\n...\n\n... . . . ...\nam1 am2 · · · am,n−m\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\nand Im is the m×m identity matrix\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 0 · · · 0\n\n0 1 · · · 0\n...\n\n... . . . ...\n0 0 · · · 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\n\nWith each canonical parity-check matrix we can associate an n× (n−m) standard gen-\nerator matrix\n\nG =\n\n(\nIn−m\n\nA\n\n)\n.\n\nOur goal will be to show that an x satisfying Gx = y exists if and only if Hy = 0. Given a\nmessage block x to be encoded, the matrix G will allow us to quickly encode it into a linear\ncodeword y.\n\n\n\n136 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nExample 8.23. Suppose that we have the following eight words to be encoded:\n\n(000), (001), (010), . . . , (111).\n\nFor\n\nA =\n\n\uf8eb\uf8ed0 1 1\n\n1 1 0\n\n1 0 1\n\n\uf8f6\uf8f8 ,\n\nthe associated standard generator and canonical parity-check matrices are\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 0 0\n\n0 1 0\n\n0 0 1\n\n0 1 1\n\n1 1 0\n\n1 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand\n\nH =\n\n\uf8eb\uf8ed0 1 1 1 0 0\n\n1 1 0 0 1 0\n\n1 0 1 0 0 1\n\n\uf8f6\uf8f8 ,\n\nrespectively.\nObserve that the rows in H represent the parity checks on certain bit positions in a\n\n6-tuple. The 1s in the identity matrix serve as parity checks for the 1s in the same row. If\nx = (x1, x2, x3, x4, x5, x6), then\n\n0 = Hx =\n\n\uf8eb\uf8edx2 + x3 + x4\nx1 + x2 + x5\nx1 + x3 + x6\n\n\uf8f6\uf8f8 ,\n\nwhich yields a system of equations:\n\nx2 + x3 + x4 = 0\n\nx1 + x2 + x5 = 0\n\nx1 + x3 + x6 = 0.\n\nHere x4 serves as a check bit for x2 and x3; x5 is a check bit for x1 and x2; and x6 is a\ncheck bit for x1 and x3. The identity matrix keeps x4, x5, and x6 from having to check on\neach other. Hence, x1, x2, and x3 can be arbitrary but x4, x5, and x6 must be chosen to\nensure parity. The null space of H is easily computed to be\n\n(000000) (001101) (010110) (011011)\n\n(100011) (101110) (110101) (111000).\n\nAn even easier way to compute the null space is with the generator matrix G (Table 8.24).\n\n\n\n8.3. PARITY-CHECK AND GENERATOR MATRICES 137\n\nMessage Word x Codeword Gx\n000 000000\n001 001101\n010 010110\n011 011011\n100 100011\n101 101110\n110 110101\n111 111000\n\nTable 8.24: A matrix-generated code\n\nTheorem 8.25. If H ∈ Mm×n(Z2) is a canonical parity-check matrix, then Null(H) consists\nof all x ∈ Zn\n\n2 whose first n −m bits are arbitrary but whose last m bits are determined by\nHx = 0. Each of the last m bits serves as an even parity check bit for some of the first\nn−m bits. Hence, H gives rise to an (n, n−m)-block code.\n\nWe leave the proof of this theorem as an exercise. In light of the theorem, the first\nn−m bits in x are called information bits and the last m bits are called check bits. In\nExample 8.23, the first three bits are the information bits and the last three are the check\nbits.\nTheorem 8.26. Suppose that G is an n × k standard generator matrix. Then C ={\n\ny : Gx = y for x ∈ Zk\n2\n\n}\nis an (n, k)-block code. More specifically, C is a group code.\n\nProof. Let Gx1 = y1 and Gx2 = y2 be two codewords. Then y1 + y2 is in C since\nG(x1 + x2) = Gx1 +Gx2 = y1 + y2.\n\nWe must also show that two message blocks cannot be encoded into the same codeword.\nThat is, we must show that if Gx = Gy, then x = y. Suppose that Gx = Gy. Then\n\nGx −Gy = G(x − y) = 0.\nHowever, the first k coordinates in G(x − y) are exactly x1 − y1, . . . , xk − yk, since they\nare determined by the identity matrix, Ik, part of G. Hence, G(x − y) = 0 exactly when\nx = y.\n\nBefore we can prove the relationship between canonical parity-check matrices and stan-\ndard generating matrices, we need to prove a lemma.\n\nLemma 8.27. Let H = (A | Im) be an m×n canonical parity-check matrix and G =\n(\nIn−m\n\nA\n\n)\nbe the corresponding n× (n−m) standard generator matrix. Then HG = 0.\nProof. Let C = HG. The ijth entry in C is\n\ncij =\n\nn∑\nk=1\n\nhikgkj\n\n=\nn−m∑\nk=1\n\nhikgkj +\nn∑\n\nk=n−m+1\n\nhikgkj\n\n=\nn−m∑\nk=1\n\naikδkj +\nn∑\n\nk=n−m+1\n\nδi−(m−n),kakj\n\n= aij + aij\n\n= 0,\n\n\n\n138 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nwhere\n\nδij =\n\n{\n1, i = j\n\n0, i ̸= j\n\nis the Kronecker delta.\n\nTheorem 8.28. Let H = (A | Im) be an m × n canonical parity-check matrix and let\nG =\n\n(\nIn−m\n\nA\n\n)\nbe the n× (n−m) standard generator matrix associated with H. Let C be the\n\ncode generated by G. Then y is in C if and only if Hy = 0. In particular, C is a linear\ncode with canonical parity-check matrix H.\n\nProof. First suppose that y ∈ C. Then Gx = y for some x ∈ Zm\n2 . By Lemma 8.27,\n\nHy = HGx = 0.\nConversely, suppose that y = (y1, . . . , yn)\n\nt is in the null space of H. We need to find\nan x in Zn−m\n\n2 such that Gxt = y. Since Hy = 0, the following set of equations must be\nsatisfied:\n\na11y1 + a12y2 + · · ·+ a1,n−myn−m + yn−m+1 = 0\n\na21y1 + a22y2 + · · ·+ a2,n−myn−m + yn−m+1 = 0\n\n...\nam1y1 + am2y2 + · · ·+ am,n−myn−m + yn−m+1 = 0.\n\nEquivalently, yn−m+1, . . . , yn are determined by y1, . . . , yn−m:\n\nyn−m+1 = a11y1 + a12y2 + · · ·+ a1,n−myn−m\n\nyn−m+1 = a21y1 + a22y2 + · · ·+ a2,n−myn−m\n\n...\nyn−m+1 = am1y1 + am2y2 + · · ·+ am,n−myn−m.\n\nConsequently, we can let xi = yi for i = 1, . . . , n−m.\n\nIt would be helpful if we could compute the minimum distance of a linear code directly\nfrom its matrix H in order to determine the error-detecting and error-correcting capabilities\nof the code. Suppose that\n\ne1 = (100 · · · 00)t\n\ne2 = (010 · · · 00)t\n\n...\nen = (000 · · · 01)t\n\nare the n-tuples in Zn\n2 of weight 1. For an m × n binary matrix H, Hei is exactly the ith\n\ncolumn of the matrix H.\n\nExample 8.29. Observe that\n\n\uf8eb\uf8ed1 1 1 0 0\n\n1 0 0 1 0\n\n1 1 0 0 1\n\n\uf8f6\uf8f8\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n0\n\n1\n\n0\n\n0\n\n0\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 =\n\n\uf8eb\uf8ed1\n\n0\n\n1\n\n\uf8f6\uf8f8 .\n\n\n\n8.3. PARITY-CHECK AND GENERATOR MATRICES 139\n\nWe state this result in the following proposition and leave the proof as an exercise.\nProposition 8.30. Let ei be the binary n-tuple with a 1 in the ith coordinate and 0’s\nelsewhere and suppose that H ∈ Mm×n(Z2). Then Hei is the ith column of the matrix H.\nTheorem 8.31. Let H be an m × n binary matrix. Then the null space of H is a single\nerror-detecting code if and only if no column of H consists entirely of zeros.\nProof. Suppose that Null(H) is a single error-detecting code. Then the minimum distance\nof the code must be at least 2. Since the null space is a group code, it is sufficient to require\nthat the code contain no codewords of less than weight 2 other than the zero codeword.\nThat is, ei must not be a codeword for i = 1, . . . , n. Since Hei is the ith column of H, the\nonly way in which ei could be in the null space of H would be if the ith column were all\nzeros, which is impossible; hence, the code must have the capability to detect at least single\nerrors.\n\nConversely, suppose that no column of H is the zero column. By Proposition 8.30,\nHei ̸= 0.\n\nExample 8.32. If we consider the matrices\n\nH1 =\n\n\uf8eb\uf8ed1 1 1 0 0\n\n1 0 0 1 0\n\n1 1 0 0 1\n\n\uf8f6\uf8f8\nand\n\nH2 =\n\n\uf8eb\uf8ed1 1 1 0 0\n\n1 0 0 0 0\n\n1 1 0 0 1\n\n\uf8f6\uf8f8 ,\n\nthen the null space of H1 is a single error-detecting code and the null space of H2 is not.\nWe can even do better than Theorem 8.31. This theorem gives us conditions on a matrix\n\nH that tell us when the minimum weight of the code formed by the null space of H is 2.\nWe can also determine when the minimum distance of a linear code is 3 by examining the\ncorresponding matrix.\nExample 8.33. If we let\n\nH =\n\n\uf8eb\uf8ed1 1 1 0\n\n1 0 0 1\n\n1 1 0 0\n\n\uf8f6\uf8f8\nand want to determine whether or not H is the canonical parity-check matrix for an error-\ncorrecting code, it is necessary to make certain that Null(H) does not contain any 4-tuples\nof weight 2. That is, (1100), (1010), (1001), (0110), (0101), and (0011) must not be in\nNull(H). The next theorem states that we can indeed determine that the code generated\nby H is error-correcting by examining the columns of H. Notice in this example that not\nonly does H have no zero columns, but also that no two columns are the same.\nTheorem 8.34. Let H be a binary matrix. The null space of H is a single error-correcting\ncode if and only if H does not contain any zero columns and no two columns of H are\nidentical.\nProof. The n-tuple ei + ej has 1s in the ith and jth entries and 0s elsewhere, and w(ei +\nej) = 2 for i ̸= j. Since\n\n0 = H(ei + ej) = Hei +Hej\n\ncan only occur if the ith and jth columns are identical, the null space of H is a single\nerror-correcting code.\n\n\n\n140 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nSuppose now that we have a canonical parity-check matrix H with three rows. Then\nwe might ask how many more columns we can add to the matrix and still have a null space\nthat is a single error-detecting and single error-correcting code. Since each column has three\nentries, there are 23 = 8 possible distinct columns. We cannot add the columns\uf8eb\uf8ed0\n\n0\n\n0\n\n\uf8f6\uf8f8 ,\n\n\uf8eb\uf8ed1\n\n0\n\n0\n\n\uf8f6\uf8f8 ,\n\n\uf8eb\uf8ed0\n\n1\n\n0\n\n\uf8f6\uf8f8 ,\n\n\uf8eb\uf8ed0\n\n0\n\n1\n\n\uf8f6\uf8f8 .\n\nSo we can add as many as four columns and still maintain a minimum distance of 3.\nIn general, if H is an m×n canonical parity-check matrix, then there are n−m informa-\n\ntion positions in each codeword. Each column has m bits, so there are 2m possible distinct\ncolumns. It is necessary that the columns 0, e1, . . . , em be excluded, leaving 2m − (1 +m)\nremaining columns for information if we are still to maintain the ability not only to detect\nbut also to correct single errors.\n\n8.4 Efficient Decoding\nWe are now at the stage where we are able to generate linear codes that detect and correct\nerrors fairly easily, but it is still a time-consuming process to decode a received n-tuple and\ndetermine which is the closest codeword, because the received n-tuple must be compared to\neach possible codeword to determine the proper decoding. This can be a serious impediment\nif the code is very large.\n\nExample 8.35. Given the binary matrix\n\nH =\n\n\uf8eb\uf8ed1 1 1 0 0\n\n0 1 0 1 0\n\n1 0 0 0 1\n\n\uf8f6\uf8f8\nand the 5-tuples x = (11011)t and y = (01011)t, we can compute\n\nHx =\n\n\uf8eb\uf8ed0\n\n0\n\n0\n\n\uf8f6\uf8f8 and Hy =\n\n\uf8eb\uf8ed1\n\n0\n\n1\n\n\uf8f6\uf8f8 .\n\nHence, x is a codeword and y is not, since x is in the null space and y is not. Notice that\nHy is identical to the first column of H. In fact, this is where the error occurred. If we flip\nthe first bit in y from 0 to 1, then we obtain x.\n\nIf H is an m× n matrix and x ∈ Zn\n2 , then we say that the syndrome of x is Hx. The\n\nfollowing proposition allows the quick detection and correction of errors.\n\nProposition 8.36. Let the m × n binary matrix H determine a linear code and let x be\nthe received n-tuple. Write x as x = c+ e, where c is the transmitted codeword and e is the\ntransmission error. Then the syndrome Hx of the received codeword x is also the syndrome\nof the error e.\n\nProof. The proof follows from the fact that\n\nHx = H(c + e) = Hc +He = 0 +He = He.\n\n\n\n8.4. EFFICIENT DECODING 141\n\nThis proposition tells us that the syndrome of a received word depends solely on the\nerror and not on the transmitted codeword. The proof of the following theorem follows\nimmediately from Proposition 8.36 and from the fact that He is the ith column of the\nmatrix H.\n\nTheorem 8.37. Let H ∈ Mm×n(Z2) and suppose that the linear code corresponding to H\nis single error-correcting. Let r be a received n-tuple that was transmitted with at most one\nerror. If the syndrome of r is 0, then no error has occurred; otherwise, if the syndrome of\nr is equal to some column of H, say the ith column, then the error has occurred in the ith\nbit.\n\nExample 8.38. Consider the matrix\n\nH =\n\n\uf8eb\uf8ed1 0 1 1 0 0\n\n0 1 1 0 1 0\n\n1 1 1 0 0 1\n\n\uf8f6\uf8f8\nand suppose that the 6-tuples x = (111110)t, y = (111111)t, and z = (010111)t have been\nreceived. Then\n\nHx =\n\n\uf8eb\uf8ed1\n\n1\n\n1\n\n\uf8f6\uf8f8 ,Hy =\n\n\uf8eb\uf8ed1\n\n1\n\n0\n\n\uf8f6\uf8f8 ,Hz =\n\n\uf8eb\uf8ed1\n\n0\n\n0\n\n\uf8f6\uf8f8 .\n\nHence, x has an error in the third bit and z has an error in the fourth bit. The transmitted\ncodewords for x and z must have been (110110) and (010011), respectively. The syndrome\nof y does not occur in any of the columns of the matrix H, so multiple errors must have\noccurred to produce y.\n\nCoset Decoding\n\nWe can use group theory to obtain another way of decoding messages. A linear code C is\na subgroup of Zn\n\n2 . Coset or standard decoding uses the cosets of C in Zn\n2 to implement\n\nmaximum-likelihood decoding. Suppose that C is an (n,m)-linear code. A coset of C in\nZn\n2 is written in the form x + C, where x ∈ Zn\n\n2 . By Lagrange’s Theorem (Theorem 6.10),\nthere are 2n−m distinct cosets of C in Zn\n\n2 .\n\nExample 8.39. Let C be the (5, 3)-linear code given by the parity-check matrix\n\nH =\n\n\uf8eb\uf8ed0 1 1 0 0\n\n1 0 0 1 0\n\n1 1 0 0 1\n\n\uf8f6\uf8f8 .\n\nThe code consists of the codewords\n\n(00000) (01101) (10011) (11110).\n\nThere are 25−2 = 23 cosets of C in Z5\n2, each with order 22 = 4. These cosets are listed in\n\nTable 8.40.\n\n\n\n142 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nCoset Coset\nRepresentative\n\nC (00000) (01101) (10011) (11110)\n(10000) + C (10000) (11101) (00011) (01110)\n(01000) + C (01000) (00101) (11011) (10110)\n(00100) + C (00100) (01001) (10111) (11010)\n(00010) + C (00010) (01111) (10001) (11100)\n(00001) + C (00001) (01100) (10010) (11111)\n(10100) + C (00111) (01010) (10100) (11001)\n(00110) + C (00110) (01011) (10101) (11000)\n\nTable 8.40: Cosets of C\n\nOur task is to find out how knowing the cosets might help us to decode a message.\nSuppose that x was the original codeword sent and that r is the n-tuple received. If e is\nthe transmission error, then r = e + x or, equivalently, x = e + r. However, this is exactly\nthe statement that r is an element in the coset e + C. In maximum-likelihood decoding\nwe expect the error e to be as small as possible; that is, e will have the least weight. An\nn-tuple of least weight in a coset is called a coset leader. Once we have determined a coset\nleader for each coset, the decoding process becomes a task of calculating r + e to obtain x.\n\nExample 8.41. In Table 8.40, notice that we have chosen a representative of the least\npossible weight for each coset. These representatives are coset leaders. Now suppose that\nr = (01111) is the received word. To decode r, we find that it is in the coset (00010) + C;\nhence, the originally transmitted codeword must have been (01101) = (01111) + (00010).\n\nA potential problem with this method of decoding is that we might have to examine every\ncoset for the received codeword. The following proposition gives a method of implementing\ncoset decoding. It states that we can associate a syndrome with each coset; hence, we can\nmake a table that designates a coset leader corresponding to each syndrome. Such a list is\ncalled a decoding table.\n\nSyndrome Coset Leader\n(000) (00000)\n(001) (00001)\n(010) (00010)\n(011) (10000)\n(100) (00100)\n(101) (01000)\n(110) (00110)\n(111) (10100)\n\nTable 8.42: Syndromes for each coset\n\nProposition 8.43. Let C be an (n, k)-linear code given by the matrix H and suppose that\nx and y are in Zn\n\n2 . Then x and y are in the same coset of C if and only if Hx = Hy. That\nis, two n-tuples are in the same coset if and only if their syndromes are the same.\n\nProof. Two n-tuples x and y are in the same coset of C exactly when x−y ∈ C; however,\nthis is equivalent to H(x − y) = 0 or Hx = Hy.\n\n\n\n8.5. EXERCISES 143\n\nExample 8.44. Table 8.42 is a decoding table for the code C given in Example 8.39. If\nx = (01111) is received, then its syndrome can be computed to be\n\nHx =\n\n\uf8eb\uf8ed0\n\n1\n\n1\n\n\uf8f6\uf8f8 .\n\nExamining the decoding table, we determine that the coset leader is (00010). It is now easy\nto decode the received codeword.\n\nGiven an (n, k)-block code, the question arises of whether or not coset decoding is a\nmanageable scheme. A decoding table requires a list of cosets and syndromes, one for each\nof the 2n−k cosets of C. Suppose that we have a (32, 24)-block code. We have a huge\nnumber of codewords, 224, yet there are only 232−24 = 28 = 256 cosets.\n\n8.5 Exercises\n1. Why is the following encoding scheme not acceptable?\n\nInformation 0 1 2 3 4 5 6 7 8\nCodeword 000 001 010 011 101 110 111 000 001\n\n2. Without doing any addition, explain why the following set of 4-tuples in Z4\n2 cannot be\n\na group code.\n(0110) (1001) (1010) (1100)\n\n3. Compute the Hamming distances between the following pairs of n-tuples.\n\n(a) (011010), (011100)\n\n(b) (11110101), (01010100)\n\n(c) (00110), (01111)\n\n(d) (1001), (0111)\n\n4. Compute the weights of the following n-tuples.\n\n(a) (011010)\n\n(b) (11110101)\n\n(c) (01111)\n\n(d) (1011)\n\n5. Suppose that a linear code C has a minimum weight of 7. What are the error-detection\nand error-correction capabilities of C?\n\n6. In each of the following codes, what is the minimum distance for the code? What is the\nbest situation we might hope for in connection with error detection and error correction?\n(a) (011010) (011100) (110111) (110000)\n\n(b) (011100) (011011) (111011) (100011) (000000) (010101) (110100) (110011)\n\n(c) (000000) (011100) (110101) (110001)\n\n(d) (0110110) (0111100) (1110000) (1111111) (1001001) (1000011) (0001111) (0000000)\n\n7. Compute the null space of each of the following matrices. What type of (n, k)-block\ncodes are the null spaces? Can you find a matrix (not necessarily a standard generator\nmatrix) that generates each code? Are your generator matrices unique?\n\n\n\n144 CHAPTER 8. ALGEBRAIC CODING THEORY\n\n(a) \uf8eb\uf8ed0 1 0 0 0\n\n1 0 1 0 1\n\n1 0 0 1 0\n\n\uf8f6\uf8f8\n(b) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 0 1 0 0 0\n\n1 1 0 1 0 0\n\n0 1 0 0 1 0\n\n1 1 0 0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n\n(c) (\n1 0 0 1 1\n\n0 1 0 1 1\n\n)\n\n(d) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n0 0 0 1 1 1 1\n\n0 1 1 0 0 1 1\n\n1 0 1 0 1 0 1\n\n0 1 1 0 0 1 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n8. Construct a (5, 2)-block code. Discuss both the error-detection and error-correction\ncapabilities of your code.\n\n9. Let C be the code obtained from the null space of the matrix\n\nH =\n\n\uf8eb\uf8ed0 1 0 0 1\n\n1 0 1 0 1\n\n0 0 1 1 1\n\n\uf8f6\uf8f8 .\n\nDecode the message\n01111 10101 01110 00011\n\nif possible.\n\n10. Suppose that a 1000-bit binary message is transmitted. Assume that the probability\nof a single error is p and that the errors occurring in different bits are independent of one\nanother. If p = 0.01, what is the probability of more than one error occurring? What is the\nprobability of exactly two errors occurring? Repeat this problem for p = 0.0001.\n\n11. Which matrices are canonical parity-check matrices? For those matrices that are canon-\nical parity-check matrices, what are the corresponding standard generator matrices? What\nare the error-detection and error-correction capabilities of the code generated by each of\nthese matrices?\n\n(a) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n1 1 0 0 0\n\n0 0 1 0 0\n\n0 0 0 1 0\n\n1 0 0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n(b) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n\n0 1 1 0 0 0\n\n1 1 0 1 0 0\n\n0 1 0 0 1 0\n\n1 1 0 0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n\n(c) (\n1 1 1 0\n\n1 0 0 1\n\n)\n\n(d) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n0 0 0 1 0 0 0\n\n0 1 1 0 1 0 0\n\n1 0 1 0 0 1 0\n\n0 1 1 0 0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n12. List all possible syndromes for the codes generated by each of the matrices in Exer-\ncise 8.5.11.\n\n13. Let\n\nH =\n\n\uf8eb\uf8ed0 1 1 1 1\n\n0 0 0 1 1\n\n1 0 1 0 1\n\n\uf8f6\uf8f8 .\n\n\n\n8.5. EXERCISES 145\n\nCompute the syndrome caused by each of the following transmission errors.\n(a) An error in the first bit.\n(b) An error in the third bit.\n(c) An error in the last bit.\n(d) Errors in the third and fourth bits.\n\n14. Let C be the group code in Z3\n2 defined by the codewords (000) and (111). Compute the\n\ncosets of H in Z3\n2. Why was there no need to specify right or left cosets? Give the single\n\ntransmission error, if any, to which each coset corresponds.\n\n15. For each of the following matrices, find the cosets of the corresponding code C. Give a\ndecoding table for each code if possible.\n\n(a) \uf8eb\uf8ed0 1 0 0 0\n\n1 0 1 0 1\n\n1 0 0 1 0\n\n\uf8f6\uf8f8\n(b) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n\n0 0 1 0 0\n\n1 1 0 1 0\n\n0 1 0 1 0\n\n1 1 0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n\n(c) (\n1 0 0 1 1\n\n0 1 0 1 1\n\n)\n\n(d) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n1 0 0 1 1 1 1\n\n1 1 1 0 0 1 1\n\n1 0 1 0 1 0 1\n\n1 1 1 0 0 1 0\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\n16. Let x, y, and z be binary n-tuples. Prove each of the following statements.\n(a) w(x) = d(x,0)\n(b) d(x,y) = d(x + z,y + z)\n(c) d(x,y) = w(x − y)\n\n17. A metric on a set X is a map d : X ×X → R satisfying the following conditions.\n(a) d(x,y) ≥ 0 for all x,y ∈ X;\n(b) d(x,y) = 0 exactly when x = y;\n(c) d(x,y) = d(y,x);\n(d) d(x,y) ≤ d(x, z) + d(z,y).\n\nIn other words, a metric is simply a generalization of the notion of distance. Prove that\nHamming distance is a metric on Zn\n\n2 . Decoding a message actually reduces to deciding\nwhich is the closest codeword in terms of distance.\n\n18. Let C be a linear code. Show that either the ith coordinates in the codewords of C are\nall zeros or exactly half of them are zeros.\n\n19. Let C be a linear code. Show that either every codeword has even weight or exactly\nhalf of the codewords have even weight.\n\n20. Show that the codewords of even weight in a linear code C are also a linear code.\n\n21. If we are to use an error-correcting linear code to transmit the 128 ascii characters,\nwhat size matrix must be used? What size matrix must be used to transmit the extended\nascii character set of 256 characters? What if we require only error detection in both cases?\n\n\n\n146 CHAPTER 8. ALGEBRAIC CODING THEORY\n\n22. Find the canonical parity-check matrix that gives the even parity check bit code with\nthree information positions. What is the matrix for seven information positions? What are\nthe corresponding standard generator matrices?\n\n23. How many check positions are needed for a single error-correcting code with 20 infor-\nmation positions? With 32 information positions?\n\n24. Let ei be the binary n-tuple with a 1 in the ith coordinate and 0’s elsewhere and\nsuppose that H ∈ Mm×n(Z2). Show that Hei is the ith column of the matrix H.\n\n25. Let C be an (n, k)-linear code. Define the dual or orthogonal code of C to be\n\nC⊥ = {x ∈ Zn\n2 : x · y = 0 for all y ∈ C}.\n\n(a) Find the dual code of the linear code C where C is given by the matrix\uf8eb\uf8ed1 1 1 0 0\n\n0 0 1 0 1\n\n1 0 0 1 0\n\n\uf8f6\uf8f8 .\n\n(b) Show that C⊥ is an (n, n− k)-linear code.\n(c) Find the standard generator and parity-check matrices of C and C⊥. What happens\n\nin general? Prove your conjecture.\n\n26. Let H be an m × n matrix over Z2, where the ith column is the number i written in\nbinary with m bits. The null space of such a matrix is called a Hamming code.\n(a) Show that the matrix\n\nH =\n\n\uf8eb\uf8ed0 0 0 1 1 1\n\n0 1 1 0 0 1\n\n1 0 1 0 1 0\n\n\uf8f6\uf8f8\ngenerates a Hamming code. What are the error-correcting properties of a Hamming\ncode?\n\n(b) The column corresponding to the syndrome also marks the bit that was in error; that\nis, the ith column of the matrix is i written as a binary number, and the syndrome\nimmediately tells us which bit is in error. If the received word is (101011), compute\nthe syndrome. In which bit did the error occur in this case, and what codeword was\noriginally transmitted?\n\n(c) Give a binary matrix H for the Hamming code with six information positions and four\ncheck positions. What are the check positions and what are the information positions?\nEncode the messages (101101) and (001001). Decode the received words (0010000101)\nand (0000101100). What are the possible syndromes for this code?\n\n(d) What is the number of check bits and the number of information bits in an (m,n)-\nblock Hamming code? Give both an upper and a lower bound on the number of\ninformation bits in terms of the number of check bits. Hamming codes having the\nmaximum possible number of information bits with k check bits are called perfect.\nEvery possible syndrome except 0 occurs as a column. If the number of information\nbits is less than the maximum, then the code is called shortened. In this case, give\nan example showing that some syndromes can represent multiple errors.\n\n\n\n8.6. PROGRAMMING EXERCISES 147\n\n8.6 Programming Exercises\n1. Write a program to implement a (16, 12)-linear code. Your program should be able to\nencode and decode messages using coset decoding. Once your program is written, write\na program to simulate a binary symmetric channel with transmission noise. Compare the\nresults of your simulation with the theoretically predicted error probability.\n\n8.7 References and Suggested Readings\n[1] Blake, I. F. “Codes and Designs,” Mathematics Magazine 52(1979), 81–95.\n[2] Hill, R. A First Course in Coding Theory. Oxford University Press, Oxford, 1990.\n[3] Levinson, N. “Coding Theory: A Counterexample to G. H. Hardy’s Conception of\n\nApplied Mathematics,” American Mathematical Monthly 77(1970), 249–58.\n[4] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\n[5] MacWilliams, F. J. and Sloane, N. J. A. The Theory of Error-Correcting Codes.\n\nNorth-Holland Mathematical Library, 16, Elsevier, Amsterdam, 1983.\n[6] Roman, S. Coding and Information Theory. Springer-Verlag, New York, 1992.\n[7] Shannon, C. E. “A Mathematical Theory of Communication,” Bell System Technical\n\nJournal 27(1948), 379–423, 623–56.\n[8] Thompson, T. M. From Error-Correcting Codes through Sphere Packing to Simple\n\nGroups. Carus Monograph Series, No. 21. Mathematical Association of America,\nWashington, DC, 1983.\n\n[9] van Lint, J. H. Introduction to Coding Theory. Springer, New York, 1999.\n\n8.8 Sage\nSage has a full suite of linear codes and a variety of methods that may be used to investigate\nthem.\n\nConstructing Linear Codes\nThe codes object can be used to get a concise listing of the available implemented codes.\nType codes. and press the Tab key and most interfaces to Sage will give you a list. You\ncan then use a question mark at the end of a method name to learn about the various\nparameters.\n\ncodes.\n\nWe will use the classic binary Hamming (7, 4) code as an illustration. “Binary” means\nwe have vectors with just 0’s and 1’s, the 7 is the length and means the vectors have 7\ncoordinates, and the 4 is the dimension, meaning this code has 24 = 16 vectors comprising\nthe code. The documentation assumes we know a few things from later in the course. We\nuse GF(2) to specify that our code is binary — this will make more sense at the end of the\ncourse. A second parameter is r and we can see from the formulas in the documenation\nthat setting r=3 will give length 7.\n\nH = codes.HammingCode(GF(2), 3); H\n\n[7, 4] Hamming Code over Finite Field of size 2\n\n\n\n148 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nProperties of Linear Codes\n\nWe can examine the Hamming code we just built. First the dimension.\n\nH.dimension ()\n\n4\n\nThe code is small enough that we can list all the codewords.\n\nH.list()\n\n[(0, 0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 1, 1), (0, 1, 0, 0, 1, 0, 1),\n(1, 1, 0, 0, 1, 1, 0), (0, 0, 1, 0, 1, 1, 0), (1, 0, 1, 0, 1, 0, 1),\n(0, 1, 1, 0, 0, 1, 1), (1, 1, 1, 0, 0, 0, 0), (0, 0, 0, 1, 1, 1, 1),\n(1, 0, 0, 1, 1, 0, 0), (0, 1, 0, 1, 0, 1, 0), (1, 1, 0, 1, 0, 0, 1),\n(0, 0, 1, 1, 0, 0, 1), (1, 0, 1, 1, 0, 1, 0), (0, 1, 1, 1, 1, 0, 0),\n(1, 1, 1, 1, 1, 1, 1)]\n\nThe minimum distance is perhaps one of the most important properties. Hamming\ncodes always have minimum distance d = 3, so they are always single error-correcting.\n\nH.minimum_distance ()\n\n3\n\nWe know that the parity-check matrix and the generator matrix are useful for the\nconstruction, description and analysis of linear codes. The Sage method names are just a\nbit cryptic. Sage has extensive routines for analyzing matrices with elements from different\nfields, so we perform much of the subsequent analysis of these matrices within Sage.\n\nC = H.parity_check_matrix (); C\n\n[1 0 1 0 1 0 1]\n[0 1 1 0 0 1 1]\n[0 0 0 1 1 1 1]\n\nThe generator matrix here in the text has columns that are codewords, and linear\ncombinations of the columns (the column space of the matrix) are codewords. In Sage the\ngenerator matrix has rows that are codewords and the row space of the matrix is the code.\nSo here is another place where we need to mentally translate between a choice made in the\ntext and a choice made by the Sage developers.\n\nG = H.generator_matrix (); G\n\n[1 0 0 0 0 1 1]\n[0 1 0 0 1 0 1]\n[0 0 1 0 1 1 0]\n[0 0 0 1 1 1 1]\n\nHere is a partial test that these two matrices are correct, exercising Lemma 8.27. Notice\nthat we need to use the transpose of the generator matrix, for reasons described above.\n\nC*G.transpose () == zero_matrix (3, 4)\n\nTrue\n\n\n\n8.8. SAGE 149\n\nNote that the parity-check may not be canonical and the generator matrix may not be\nstandard. Sage can produce a generator matrix that has a set of columns that forms an\nidentity matrix, though no guarantee is made that these columns are the first columns.\n(Columns, not rows.) Such a matrix is said to be systematic, and the Sage method is\n.generator_matrix_systematic().\n\nH.generator_matrix_systematic ()\n\n[1 0 0 0 0 1 1]\n[0 1 0 0 1 0 1]\n[0 0 1 0 1 1 0]\n[0 0 0 1 1 1 1]\n\nDecoding with a Linear Code\nWe can decode received messages originating from a linear code. Suppose we receive the\nlength 7 binary vector r.\n\nr = vector(GF(2), [1, 1, 1, 1, 0, 0, 1]); r\n\n(1, 1, 1, 1, 0, 0, 1)\n\nWe can recognize that one or more errors has occured, since r is not in the code, as the\nnext computation does not yield the zero vector.\n\nC*r\n\n(1, 1, 0)\n\nA linear code has a .decode method. You may choose from several different algorithms,\nwhile the Hamming codes have their own custom algorithm. The default algorithm is\nsyndrome decoding.\n\nH.decode_to_code(r)\n\n(1, 1, 0, 1, 0, 0, 1)\n\nSo if we are willing to assume that only one error occured (which we might, if the\nprobability of an indivual entry of the vector being in error is very low), then we see that\nan error occured in the third position.\n\nRemember that it could happen that there was more than just one error. For example,\nsuppose the message was the same as before and errors occurred in the third, fifth and sixth\nlocations.\n\nmessage = vector(GF(2), [1, 1, 0, 1, 0, 0, 1])\nerrors = vector(GF(2), [0, 0, 1, 0, 1, 1, 0])\nreceived = message + errors\nreceived\n\n(1, 1, 1, 1, 1, 1, 1)\n\nIt then appears that we have received a codeword, so we assume no errors at all, and\ndecode incorrectly.\n\nH.decode_to_code(received) == message\n\nFalse\n\n\n\n150 CHAPTER 8. ALGEBRAIC CODING THEORY\n\nH.decode_to_code(received) == received\n\nTrue\n\n8.9 Sage Exercises\n1. Create the (binary) Golay code with the codes.BinaryGolayCode() constructor.\n(a) Use Sage methods to compute the length, dimension and minimum distance of the\n\ncode.\n(b) How many errors can this code detect? How many can it correct?\n(c) Find a nonzero codeword and introduce three errors by adding a vector with three 1’s\n\n(your choice) to create a received message. Show that the message is decoded properly.\n(d) Recycle your choices from the previous part, but now add one more error. Does the\n\nnew received message get decoded properly?\n\n2. One technique for improving the characteristics of a code is to add an overall parity-\ncheck bit, much like the lone parity-check bit of the ascii code described in Example 8.3.\nSuch codes are referred to as the extended version of the original.\n(a) Construct the (binary) Golay code and obtain the parity-check matrix. Use Sage com-\n\nmands to enlarge this matrix to create a new parity check matrix that has an additional\noverall parity-check bit. You may find the matrix methods .augment() and .stack()\n\nuseful, as well as the constructors zero_vector() and ones_matrix() (remembering that\nwe specify the binary entries as being from the field GF(2).)\nCreate the extended code by supplying your enlarged parity-check matrix to the\ncodes.LinearCodeFromCheckMatrix() constructor and compute the length, dimension and\nminimum distance of the extended code.\n\n(b) How are the properties of this new code better? At what cost?\n(c) Now create the extended (binary) Golay code with the Sage constructor codes.ExtendedBinaryGolayCode().\n\nWith luck, the sorted lists of your codewords and Sage’s codewords will be equal. If\nnot, the linear code method .is_permutation_equivalent() should return True to indi-\ncate that your code and Sage’s are just rearrangements of each other.\n\n3. Note: This problem is on holiday (as of Sage 6.7), while some buggy Sage code for the\nminimum distance of a Hamming code gets sorted out. The r = 2 case produces an error\nmessage and for r > 5 the computation of the minimum distance has become intolerably\nslow. So it is a bit harder to make a reasonable conjecture from just 3 cases.\nThe dual of an (n, k) block code is formed as all the set of all binary vectors which are\northogonal to every vector of the original code. Exercise 8.5.25 describes this construction\nand asks about some of its properties.\nYou can construct the dual of a code in Sage with the .dual_code() method. Construct\nthe binary Hamming codes, and their duals, with the parameter r ranging from 2 to 5,\ninclusive. Build a table with six columns (perhaps employing the html.table() function)\nthat lists r, the length of the codes, the dimensions of the original and the dual, and the\nminimum distances of the orginal and the dual.\nConjecture formulas for the dimension and minimum distance of the dual of the Hamming\ncode as expressions in the parameter r.\n\n\n\n8.9. SAGE EXERCISES 151\n\n4. A code with minimum distance d is called perfect if every possible vector is within\nHamming distance (d − 1)/2 of some codeword. If we expand our notion of geometry to\naccount for the Hamming distance as the metric, then we can speak of a sphere of radius r\naround a vector (or codeword. For a code of length n, such a sphere will contain\n\n1 +\n\n(\nn\n\n1\n\n)\n+\n\n(\nn\n\n2\n\n)\n+ · · ·+\n\n(\nn\n\nr\n\n)\nvectors within in it. For a perfect code, the spheres of radius d centered at the codewords of\nthe code will exactly partition the entire set of all possible vectors. (This is the connection\nthat means that coding theory meshes with sphere packing problems.)\nA consequence of a code of dimension k being perfect is that\n\n2k\n((\n\nn\n\n0\n\n)\n+\n\n(\nn\n\n1\n\n)\n+\n\n(\nn\n\n2\n\n)\n+ · · ·+\n\n(\nn\n\nd−1\n2\n\n))\n= 2n\n\nConversely, if a code has minimum distance d and the condition above is true, then the\ncode is perfect.\nWrite a Python function, named is_perfect() which accepts a linear code as input and\nreturns True or False. Demonstrate your function by checking that the (binary) Golay code\nis perfect, and then use a loop to verify that the (binary) Hamming codes are perfect for\nall lengths below 32.\n\n\n\n9\n\nIsomorphisms\n\nMany groups may appear to be different at first glance, but can be shown to be the same\nby a simple renaming of the group elements. For example, Z4 and the subgroup of the\ncircle group T generated by i can be shown to be the same by demonstrating a one-to-one\ncorrespondence between the elements of the two groups and between the group operations.\nIn such a case we say that the groups are isomorphic.\n\n9.1 Definition and Examples\nTwo groups (G, ·) and (H, ◦) are isomorphic if there exists a one-to-one and onto map\nϕ : G→ H such that the group operation is preserved; that is,\n\nϕ(a · b) = ϕ(a) ◦ ϕ(b)\n\nfor all a and b in G. If G is isomorphic to H, we write G ∼= H. The map ϕ is called an\nisomorphism.\n\nExample 9.1. To show that Z4\n∼= ⟨i⟩, define a map ϕ : Z4 → ⟨i⟩ by ϕ(n) = in. We must\n\nshow that ϕ is bijective and preserves the group operation. The map ϕ is one-to-one and\nonto because\n\nϕ(0) = 1\n\nϕ(1) = i\n\nϕ(2) = −1\n\nϕ(3) = −i.\n\nSince\nϕ(m+ n) = im+n = imin = ϕ(m)ϕ(n),\n\nthe group operation is preserved.\n\nExample 9.2. We can define an isomorphism ϕ from the additive group of real numbers\n(R,+) to the multiplicative group of positive real numbers (R+, ·) with the exponential\nmap; that is,\n\nϕ(x+ y) = ex+y = exey = ϕ(x)ϕ(y).\n\nOf course, we must still show that ϕ is one-to-one and onto, but this can be determined\nusing calculus.\n\nExample 9.3. The integers are isomorphic to the subgroup of Q∗ consisting of elements of\nthe form 2n. Define a map ϕ : Z → Q∗ by ϕ(n) = 2n. Then\n\nϕ(m+ n) = 2m+n = 2m2n = ϕ(m)ϕ(n).\n\n152\n\n\n\n9.1. DEFINITION AND EXAMPLES 153\n\nBy definition the map ϕ is onto the subset {2n : n ∈ Z} of Q∗. To show that the map is\ninjective, assume that m ̸= n. If we can show that ϕ(m) ̸= ϕ(n), then we are done. Suppose\nthat m > n and assume that ϕ(m) = ϕ(n). Then 2m = 2n or 2m−n = 1, which is impossible\nsince m− n > 0.\n\nExample 9.4. The groups Z8 and Z12 cannot be isomorphic since they have different\norders; however, it is true that U(8) ∼= U(12). We know that\n\nU(8) = {1, 3, 5, 7}\nU(12) = {1, 5, 7, 11}.\n\nAn isomorphism ϕ : U(8) → U(12) is then given by\n\n1 7→ 1\n\n3 7→ 5\n\n5 7→ 7\n\n7 7→ 11.\n\nThe map ϕ is not the only possible isomorphism between these two groups. We could define\nanother isomorphism ψ by ψ(1) = 1, ψ(3) = 11, ψ(5) = 5, ψ(7) = 7. In fact, both of these\ngroups are isomorphic to Z2 × Z2 (see Example 3.28 in Chapter 3).\n\nExample 9.5. Even though S3 and Z6 possess the same number of elements, we would\nsuspect that they are not isomorphic, because Z6 is abelian and S3 is nonabelian. To\ndemonstrate that this is indeed the case, suppose that ϕ : Z6 → S3 is an isomorphism. Let\na, b ∈ S3 be two elements such that ab ̸= ba. Since ϕ is an isomorphism, there exist elements\nm and n in Z6 such that\n\nϕ(m) = a and ϕ(n) = b.\n\nHowever,\nab = ϕ(m)ϕ(n) = ϕ(m+ n) = ϕ(n+m) = ϕ(n)ϕ(m) = ba,\n\nwhich contradicts the fact that a and b do not commute.\n\nTheorem 9.6. Let ϕ : G → H be an isomorphism of two groups. Then the following\nstatements are true.\n\n1. ϕ−1 : H → G is an isomorphism.\n\n2. |G| = |H|.\n\n3. If G is abelian, then H is abelian.\n\n4. If G is cyclic, then H is cyclic.\n\n5. If G has a subgroup of order n, then H has a subgroup of order n.\n\nProof. Assertions (1) and (2) follow from the fact that ϕ is a bijection. We will prove (3)\nhere and leave the remainder of the theorem to be proved in the exercises.\n\n(3) Suppose that h1 and h2 are elements of H. Since ϕ is onto, there exist elements\ng1, g2 ∈ G such that ϕ(g1) = h1 and ϕ(g2) = h2. Therefore,\n\nh1h2 = ϕ(g1)ϕ(g2) = ϕ(g1g2) = ϕ(g2g1) = ϕ(g2)ϕ(g1) = h2h1.\n\n\n\n154 CHAPTER 9. ISOMORPHISMS\n\nWe are now in a position to characterize all cyclic groups.\n\nTheorem 9.7. All cyclic groups of infinite order are isomorphic to Z.\n\nProof. Let G be a cyclic group with infinite order and suppose that a is a generator of G.\nDefine a map ϕ : Z → G by ϕ : n 7→ an. Then\n\nϕ(m+ n) = am+n = aman = ϕ(m)ϕ(n).\n\nTo show that ϕ is injective, suppose that m and n are two elements in Z, where m ̸= n.\nWe can assume that m > n. We must show that am ̸= an. Let us suppose the contrary;\nthat is, am = an. In this case am−n = e, where m− n > 0, which contradicts the fact that\na has infinite order. Our map is onto since any element in G can be written as an for some\ninteger n and ϕ(n) = an.\n\nTheorem 9.8. If G is a cyclic group of order n, then G is isomorphic to Zn.\n\nProof. Let G be a cyclic group of order n generated by a and define a map ϕ : Zn → G\nby ϕ : k 7→ ak, where 0 ≤ k < n. The proof that ϕ is an isomorphism is one of the\nend-of-chapter exercises.\n\nCorollary 9.9. If G is a group of order p, where p is a prime number, then G is isomorphic\nto Zp.\n\nProof. The proof is a direct result of Corollary 6.12.\n\nThe main goal in group theory is to classify all groups; however, it makes sense to\nconsider two groups to be the same if they are isomorphic. We state this result in the\nfollowing theorem, whose proof is left as an exercise.\n\nTheorem 9.10. The isomorphism of groups determines an equivalence relation on the class\nof all groups.\n\nHence, we can modify our goal of classifying all groups to classifying all groups up to\nisomorphism; that is, we will consider two groups to be the same if they are isomorphic.\n\nCayley’s Theorem\n\nCayley proved that if G is a group, it is isomorphic to a group of permutations on some set;\nhence, every group is a permutation group. Cayley’s Theorem is what we call a represen-\ntation theorem. The aim of representation theory is to find an isomorphism of some group\nG that we wish to study into a group that we know a great deal about, such as a group of\npermutations or matrices.\n\nExample 9.11. Consider the group Z3. The Cayley table for Z3 is as follows.\n\n+ 0 1 2\n\n0 0 1 2\n\n1 1 2 0\n\n2 2 0 1\n\n\n\n9.1. DEFINITION AND EXAMPLES 155\n\nThe addition table of Z3 suggests that it is the same as the permutation group G =\n{(0), (012), (021)}. The isomorphism here is\n\n0 7→\n(\n0 1 2\n\n0 1 2\n\n)\n= (0)\n\n1 7→\n(\n0 1 2\n\n1 2 0\n\n)\n= (012)\n\n2 7→\n(\n0 1 2\n\n2 0 1\n\n)\n= (021).\n\nTheorem 9.12 (Cayley). Every group is isomorphic to a group of permutations.\nProof. Let G be a group. We must find a group of permutations G that is isomorphic to\nG. For any g ∈ G, define a function λg : G → G by λg(a) = ga. We claim that λg is a\npermutation of G. To show that λg is one-to-one, suppose that λg(a) = λg(b). Then\n\nga = λg(a) = λg(b) = gb.\n\nHence, a = b. To show that λg is onto, we must prove that for each a ∈ G, there is a b such\nthat λg(b) = a. Let b = g−1a.\n\nNow we are ready to define our group G. Let\nG = {λg : g ∈ G}.\n\nWe must show that G is a group under composition of functions and find an isomorphism\nbetween G and G. We have closure under composition of functions since\n\n(λg ◦ λh)(a) = λg(ha) = gha = λgh(a).\n\nAlso,\nλe(a) = ea = a\n\nand\n(λg−1 ◦ λg)(a) = λg−1(ga) = g−1ga = a = λe(a).\n\nWe can define an isomorphism from G to G by ϕ : g 7→ λg. The group operation is\npreserved since\n\nϕ(gh) = λgh = λgλh = ϕ(g)ϕ(h).\n\nIt is also one-to-one, because if ϕ(g)(a) = ϕ(h)(a), then\nga = λga = λha = ha.\n\nHence, g = h. That ϕ is onto follows from the fact that ϕ(g) = λg for any λg ∈ G.\n\nThe isomorphism g 7→ λg is known as the left regular representation of G.\n\nHistorical Note\n\nArthur Cayley was born in England in 1821, though he spent much of the first part of\nhis life in Russia, where his father was a merchant. Cayley was educated at Cambridge,\nwhere he took the first Smith’s Prize in mathematics. A lawyer for much of his adult life,\nhe wrote several papers in his early twenties before entering the legal profession at the age\nof 25. While practicing law he continued his mathematical research, writing more than 300\npapers during this period of his life. These included some of his best work. In 1863 he left\nlaw to become a professor at Cambridge. Cayley wrote more than 900 papers in fields such\nas group theory, geometry, and linear algebra. His legal knowledge was very valuable to\nCambridge; he participated in the writing of many of the university’s statutes. Cayley was\nalso one of the people responsible for the admission of women to Cambridge.\n\n\n\n156 CHAPTER 9. ISOMORPHISMS\n\n9.2 Direct Products\nGiven two groups G and H, it is possible to construct a new group from the Cartesian\nproduct of G and H, G ×H. Conversely, given a large group, it is sometimes possible to\ndecompose the group; that is, a group is sometimes isomorphic to the direct product of\ntwo smaller groups. Rather than studying a large group G, it is often easier to study the\ncomponent groups of G.\n\nExternal Direct Products\nIf (G, ·) and (H, ◦) are groups, then we can make the Cartesian product of G and H into a\nnew group. As a set, our group is just the ordered pairs (g, h) ∈ G ×H where g ∈ G and\nh ∈ H. We can define a binary operation on G×H by\n\n(g1, h1)(g2, h2) = (g1 · g2, h1 ◦ h2);\n\nthat is, we just multiply elements in the first coordinate as we do in G and elements in the\nsecond coordinate as we do in H. We have specified the particular operations · and ◦ in\neach group here for the sake of clarity; we usually just write (g1, h1)(g2, h2) = (g1g2, h1h2).\n\nProposition 9.13. Let G and H be groups. The set G×H is a group under the operation\n(g1, h1)(g2, h2) = (g1g2, h1h2) where g1, g2 ∈ G and h1, h2 ∈ H.\n\nProof. Clearly the binary operation defined above is closed. If eG and eH are the identities\nof the groups G and H respectively, then (eG, eH) is the identity of G×H. The inverse of\n(g, h) ∈ G×H is (g−1, h−1). The fact that the operation is associative follows directly from\nthe associativity of G and H.\n\nExample 9.14. Let R be the group of real numbers under addition. The Cartesian product\nof R with itself, R× R = R2, is also a group, in which the group operation is just addition\nin each coordinate; that is, (a, b) + (c, d) = (a + c, b + d). The identity is (0, 0) and the\ninverse of (a, b) is (−a,−b).\n\nExample 9.15. Consider\n\nZ2 × Z2 = {(0, 0), (0, 1), (1, 0), (1, 1)}.\n\nAlthough Z2 × Z2 and Z4 both contain four elements, they are not isomorphic. Every\nelement (a, b) in Z2 × Z2 has order 2, since (a, b) + (a, b) = (0, 0); however, Z4 is cyclic.\n\nThe group G×H is called the external direct product of G and H. Notice that there\nis nothing special about the fact that we have used only two groups to build a new group.\nThe direct product\n\nn∏\ni=1\n\nGi = G1 ×G2 × · · · ×Gn\n\nof the groups G1, G2, . . . , Gn is defined in exactly the same manner. If G = G1 = G2 =\n· · · = Gn, we often write Gn instead of G1 ×G2 × · · · ×Gn.\n\nExample 9.16. The group Zn\n2 , considered as a set, is just the set of all binary n-tuples.\n\nThe group operation is the “exclusive or” of two binary n-tuples. For example,\n\n(01011101) + (01001011) = (00010110).\n\nThis group is important in coding theory, in cryptography, and in many areas of computer\nscience.\n\n\n\n9.2. DIRECT PRODUCTS 157\n\nTheorem 9.17. Let (g, h) ∈ G×H. If g and h have finite orders r and s respectively, then\nthe order of (g, h) in G×H is the least common multiple of r and s.\nProof. Suppose that m is the least common multiple of r and s and let n = |(g, h)|. Then\n\n(g, h)m = (gm, hm) = (eG, eH)\n\n(gn, hn) = (g, h)n = (eG, eH).\n\nHence, n must divide m, and n ≤ m. However, by the second equation, both r and s must\ndivide n; therefore, n is a common multiple of r and s. Since m is the least common multiple\nof r and s, m ≤ n. Consequently, m must be equal to n.\n\nCorollary 9.18. Let (g1, . . . , gn) ∈\n∏\nGi. If gi has finite order ri in Gi, then the order of\n\n(g1, . . . , gn) in\n∏\nGi is the least common multiple of r1, . . . , rn.\n\nExample 9.19. Let (8, 56) ∈ Z12 ×Z60. Since gcd(8, 12) = 4, the order of 8 is 12/4 = 3 in\nZ12. Similarly, the order of 56 in Z60 is 15. The least common multiple of 3 and 15 is 15;\nhence, (8, 56) has order 15 in Z12 × Z60.\nExample 9.20. The group Z2 × Z3 consists of the pairs\n\n(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2).\n\nIn this case, unlike that of Z2×Z2 and Z4, it is true that Z2×Z3\n∼= Z6. We need only show\n\nthat Z2 × Z3 is cyclic. It is easy to see that (1, 1) is a generator for Z2 × Z3.\nThe next theorem tells us exactly when the direct product of two cyclic groups is cyclic.\n\nTheorem 9.21. The group Zm × Zn is isomorphic to Zmn if and only if gcd(m,n) = 1.\nProof. We will first show that if Zm × Zn\n\n∼= Zmn, then gcd(m,n) = 1. We will prove the\ncontrapositive; that is, we will show that if gcd(m,n) = d > 1, then Zm×Zn cannot be cyclic.\nNotice that mn/d is divisible by both m and n; hence, for any element (a, b) ∈ Zm × Zn,\n\n(a, b) + (a, b) + · · ·+ (a, b)︸ ︷︷ ︸\nmn/d times\n\n= (0, 0).\n\nTherefore, no (a, b) can generate all of Zm × Zn.\nThe converse follows directly from Theorem 9.17 since lcm(m,n) = mn if and only if\n\ngcd(m,n) = 1.\n\nCorollary 9.22. Let n1, . . . , nk be positive integers. Then\nk∏\n\ni=1\n\nZni\n∼= Zn1···nk\n\nif and only if gcd(ni, nj) = 1 for i ̸= j.\nCorollary 9.23. If\n\nm = pe11 · · · pekk ,\nwhere the pis are distinct primes, then\n\nZm\n∼= Zp\n\ne1\n1\n\n× · · · × Zp\nek\nk\n.\n\nProof. Since the greatest common divisor of peii and p\nej\nj is 1 for i ̸= j, the proof follows\n\nfrom Corollary 9.22.\n\nIn Chapter 13, we will prove that all finite abelian groups are isomorphic to direct\nproducts of the form\n\nZp\ne1\n1\n\n× · · · × Zp\nek\nk\n\nwhere p1, . . . , pk are (not necessarily distinct) primes.\n\n\n\n158 CHAPTER 9. ISOMORPHISMS\n\nInternal Direct Products\nThe external direct product of two groups builds a large group out of two smaller groups.\nWe would like to be able to reverse this process and conveniently break down a group into\nits direct product components; that is, we would like to be able to say when a group is\nisomorphic to the direct product of two of its subgroups.\n\nLet G be a group with subgroups H and K satisfying the following conditions.\n\n• G = HK = {hk : h ∈ H, k ∈ K};\n\n• H ∩K = {e};\n\n• hk = kh for all k ∈ K and h ∈ H.\n\nThen G is the internal direct product of H and K.\n\nExample 9.24. The group U(8) is the internal direct product of\n\nH = {1, 3} and K = {1, 5}.\n\nExample 9.25. The dihedral group D6 is an internal direct product of its two subgroups\n\nH = {id, r3} and K = {id, r2, r4, s, r2s, r4s}.\n\nIt can easily be shown that K ∼= S3; consequently, D6\n∼= Z2 × S3.\n\nExample 9.26. Not every group can be written as the internal direct product of two of its\nproper subgroups. If the group S3 were an internal direct product of its proper subgroups\nH and K, then one of the subgroups, say H, would have to have order 3. In this case H is\nthe subgroup {(1), (123), (132)}. The subgroup K must have order 2, but no matter which\nsubgroup we choose for K, the condition that hk = kh will never be satisfied for h ∈ H and\nk ∈ K.\n\nTheorem 9.27. Let G be the internal direct product of subgroups H and K. Then G is\nisomorphic to H ×K.\n\nProof. Since G is an internal direct product, we can write any element g ∈ G as g = hk\nfor some h ∈ H and some k ∈ K. Define a map ϕ : G→ H ×K by ϕ(g) = (h, k).\n\nThe first problem that we must face is to show that ϕ is a well-defined map; that is, we\nmust show that h and k are uniquely determined by g. Suppose that g = hk = h′k′. Then\nh−1h′ = k(k′)−1 is in both H and K, so it must be the identity. Therefore, h = h′ and\nk = k′, which proves that ϕ is, indeed, well-defined.\n\nTo show that ϕ preserves the group operation, let g1 = h1k1 and g2 = h2k2 and observe\nthat\n\nϕ(g1g2) = ϕ(h1k1h2k2)\n\n= ϕ(h1h2k1k2)\n\n= (h1h2, k1k2)\n\n= (h1, k1)(h2, k2)\n\n= ϕ(g1)ϕ(g2).\n\nWe will leave the proof that ϕ is one-to-one and onto as an exercise.\n\nExample 9.28. The group Z6 is an internal direct product isomorphic to {0, 2, 4}×{0, 3}.\n\n\n\n9.3. EXERCISES 159\n\nWe can extend the definition of an internal direct product of G to a collection of sub-\ngroups H1,H2, . . . , Hn of G, by requiring that\n\n• G = H1H2 · · ·Hn = {h1h2 · · ·hn : hi ∈ Hi};\n\n• Hi ∩ ⟨∪j ̸=iHj⟩ = {e};\n\n• hihj = hjhi for all hi ∈ Hi and hj ∈ Hj .\n\nWe will leave the proof of the following theorem as an exercise.\n\nTheorem 9.29. Let G be the internal direct product of subgroups Hi, where i = 1, 2, . . . , n.\nThen G is isomorphic to\n\n∏\niHi.\n\n9.3 Exercises\n1. Prove that Z ∼= nZ for n ̸= 0.\n\n2. Prove that C∗ is isomorphic to the subgroup of GL2(R) consisting of matrices of the\nform (\n\na b\n\n−b a\n\n)\n.\n\n3. Prove or disprove: U(8) ∼= Z4.\n\n4. Prove that U(8) is isomorphic to the group of matrices(\n1 0\n\n0 1\n\n)\n,\n\n(\n1 0\n\n0 −1\n\n)\n,\n\n(\n−1 0\n\n0 1\n\n)\n,\n\n(\n−1 0\n\n0 −1\n\n)\n.\n\n5. Show that U(5) is isomorphic to U(10), but U(12) is not.\n\n6. Show that the nth roots of unity are isomorphic to Zn.\n\n7. Show that any cyclic group of order n is isomorphic to Zn.\n\n8. Prove that Q is not isomorphic to Z.\n\n9. Let G = R \\ {−1} and define a binary operation on G by\n\na ∗ b = a+ b+ ab.\n\nProve that G is a group under this operation. Show that (G, ∗) is isomorphic to the\nmultiplicative group of nonzero real numbers.\n\n10. Show that the matrices\uf8eb\uf8ed1 0 0\n\n0 1 0\n\n0 0 1\n\n\uf8f6\uf8f8 \uf8eb\uf8ed1 0 0\n\n0 0 1\n\n0 1 0\n\n\uf8f6\uf8f8 \uf8eb\uf8ed0 1 0\n\n1 0 0\n\n0 0 1\n\n\uf8f6\uf8f8\n\uf8eb\uf8ed0 0 1\n\n1 0 0\n\n0 1 0\n\n\uf8f6\uf8f8 \uf8eb\uf8ed0 0 1\n\n0 1 0\n\n1 0 0\n\n\uf8f6\uf8f8 \uf8eb\uf8ed0 1 0\n\n0 0 1\n\n1 0 0\n\n\uf8f6\uf8f8\nform a group. Find an isomorphism of G with a more familiar group of order 6.\n\n\n\n160 CHAPTER 9. ISOMORPHISMS\n\n11. Find five non-isomorphic groups of order 8.\n\n12. Prove S4 is not isomorphic to D12.\n\n13. Let ω = cis(2π/n) be a primitive nth root of unity. Prove that the matrices\n\nA =\n\n(\nω 0\n\n0 ω−1\n\n)\nand B =\n\n(\n0 1\n\n1 0\n\n)\ngenerate a multiplicative group isomorphic to Dn.\n\n14. Show that the set of all matrices of the form(\n±1 k\n\n0 1\n\n)\n,\n\nis a group isomorphic to Dn, where all entries in the matrix are in Zn.\n\n15. List all of the elements of Z4 × Z2.\n\n16. Find the order of each of the following elements.\n(a) (3, 4) in Z4 × Z6\n\n(b) (6, 15, 4) in Z30 × Z45 × Z24\n\n(c) (5, 10, 15) in Z25 × Z25 × Z25\n\n(d) (8, 8, 8) in Z10 × Z24 × Z80\n\n17. Prove that D4 cannot be the internal direct product of two of its proper subgroups.\n\n18. Prove that the subgroup of Q∗ consisting of elements of the form 2m3n for m,n ∈ Z is\nan internal direct product isomorphic to Z× Z.\n\n19. Prove that S3×Z2 is isomorphic to D6. Can you make a conjecture about D2n? Prove\nyour conjecture.\n\n20. Prove or disprove: Every abelian group of order divisible by 3 contains a subgroup of\norder 3.\n\n21. Prove or disprove: Every nonabelian group of order divisible by 6 contains a subgroup\nof order 6.\n\n22. Let G be a group of order 20. If G has subgroups H and K of orders 4 and 5 respectively\nsuch that hk = kh for all h ∈ H and k ∈ K, prove that G is the internal direct product of\nH and K.\n\n23. Prove or disprove the following assertion. Let G, H, and K be groups. If G × K ∼=\nH ×K, then G ∼= H.\n\n24. Prove or disprove: There is a noncyclic abelian group of order 51.\n\n25. Prove or disprove: There is a noncyclic abelian group of order 52.\n\n26. Let ϕ : G → H be a group isomorphism. Show that ϕ(x) = eH if and only if x = eG,\nwhere eG and eH are the identities of G and H, respectively.\n\n27. Let G ∼= H. Show that if G is cyclic, then so is H.\n\n28. Prove that any group G of order p, p prime, must be isomorphic to Zp.\n\n\n\n9.3. EXERCISES 161\n\n29. Show that Sn is isomorphic to a subgroup of An+2.\n\n30. Prove that Dn is isomorphic to a subgroup of Sn.\n\n31. Let ϕ : G1 → G2 and ψ : G2 → G3 be isomorphisms. Show that ϕ−1 and ψ ◦ ϕ are\nboth isomorphisms. Using these results, show that the isomorphism of groups determines\nan equivalence relation on the class of all groups.\n\n32. Prove U(5) ∼= Z4. Can you generalize this result for U(p), where p is prime?\n\n33. Write out the permutations associated with each element of S3 in the proof of Cayley’s\nTheorem.\n\n34. An automorphism of a group G is an isomorphism with itself. Prove that complex\nconjugation is an automorphism of the additive group of complex numbers; that is, show\nthat the map ϕ(a+ bi) = a− bi is an isomorphism from C to C.\n\n35. Prove that a+ ib 7→ a− ib is an automorphism of C∗.\n\n36. Prove that A 7→ B−1AB is an automorphism of SL2(R) for all B in GL2(R).\n\n37. We will denote the set of all automorphisms of G by Aut(G). Prove that Aut(G) is a\nsubgroup of SG, the group of permutations of G.\n\n38. Find Aut(Z6).\n\n39. Find Aut(Z).\n\n40. Find two nonisomorphic groups G and H such that Aut(G) ∼= Aut(H).\n\n41. Let G be a group and g ∈ G. Define a map ig : G→ G by ig(x) = gxg−1. Prove that ig\ndefines an automorphism of G. Such an automorphism is called an inner automorphism.\nThe set of all inner automorphisms is denoted by Inn(G).\n\n42. Prove that Inn(G) is a subgroup of Aut(G).\n\n43. What are the inner automorphisms of the quaternion group Q8? Is Inn(G) = Aut(G)\nin this case?\n\n44. Let G be a group and g ∈ G. Define maps λg : G→ G and ρg : G→ G by λg(x) = gx\nand ρg(x) = xg−1. Show that ig = ρg ◦ λg is an automorphism of G. The isomorphism\ng 7→ ρg is called the right regular representation of G.\n\n45. Let G be the internal direct product of subgroups H and K. Show that the map\nϕ : G→ H ×K defined by ϕ(g) = (h, k) for g = hk, where h ∈ H and k ∈ K, is one-to-one\nand onto.\n\n46. Let G and H be isomorphic groups. If G has a subgroup of order n, prove that H must\nalso have a subgroup of order n.\n\n47. If G ∼= G and H ∼= H, show that G×H ∼= G×H.\n\n48. Prove that G×H is isomorphic to H ×G.\n\n49. Let n1, . . . , nk be positive integers. Show that\nk∏\n\ni=1\n\nZni\n∼= Zn1···nk\n\nif and only if gcd(ni, nj) = 1 for i ̸= j.\n\n\n\n162 CHAPTER 9. ISOMORPHISMS\n\n50. Prove that A×B is abelian if and only if A and B are abelian.\n\n51. If G is the internal direct product of H1,H2, . . . , Hn, prove that G is isomorphic to∏\niHi.\n\n52. Let H1 and H2 be subgroups of G1 and G2, respectively. Prove that H1 × H2 is a\nsubgroup of G1 ×G2.\n\n53. Let m,n ∈ Z. Prove that ⟨m,n⟩ = ⟨d⟩ if and only if d = gcd(m,n).\n\n54. Let m,n ∈ Z. Prove that ⟨m⟩ ∩ ⟨n⟩ = ⟨l⟩ if and only if l = lcm(m,n).\n\n55. (Groups of order 2p) In this series of exercises we will classify all groups of order 2p,\nwhere p is an odd prime.\n(a) Assume G is a group of order 2p, where p is an odd prime. If a ∈ G, show that a must\n\nhave order 1, 2, p, or 2p.\n(b) Suppose that G has an element of order 2p. Prove that G is isomorphic to Z2p. Hence,\n\nG is cyclic.\n(c) Suppose that G does not contain an element of order 2p. Show that G must contain\n\nan element of order p. Hint: Assume that G does not contain an element of order p.\n(d) Suppose that G does not contain an element of order 2p. Show that G must contain\n\nan element of order 2.\n(e) Let P be a subgroup of G with order p and y ∈ G have order 2. Show that yP = Py.\n(f) Suppose that G does not contain an element of order 2p and P = ⟨z⟩ is a subgroup of\n\norder p generated by z. If y is an element of order 2, then yz = zky for some 2 ≤ k < p.\n(g) Suppose that G does not contain an element of order 2p. Prove that G is not abelian.\n(h) Suppose that G does not contain an element of order 2p and P = ⟨z⟩ is a subgroup\n\nof order p generated by z and y is an element of order 2. Show that we can list the\nelements of G as {ziyj | 0 ≤ i < p, 0 ≤ j < 2}.\n\n(i) Suppose that G does not contain an element of order 2p and P = ⟨z⟩ is a subgroup\nof order p generated by z and y is an element of order 2. Prove that the product\n(ziyj)(zrys) can be expressed as a uniquely as zmyn for some non negative integers\nm,n. Thus, conclude that there is only one possibility for a non-abelian group of order\n2p, it must therefore be the one we have seen already, the dihedral group.\n\n9.4 Sage\nSage has limited support for actually creating isomorphisms, though it is possible. However,\nthere is excellent support for determining if two permutation groups are isomorphic. This\nwill allow us to begin a little project to locate all of the groups of order less than 16 in\nSage’s permutation groups.\n\nIsomorphism Testing\nIf G and H are two permutation groups, then the command G.is_isomorphic(H) will return\nTrue or False as the two groups are, or are not, isomorphic. Since “isomorpic to” is an\nequivalence relation by Theorem 9.10, it does not matter which group plays the role of G\n\nand which plays the role of H.\nSo we have a few more examples to work with, let us introduce the Sage command\n\nthat creates an external direct product. If G and H are two permutation groups, then the\n\n\n\n9.4. SAGE 163\n\ncommand direct_product_permgroups([G,H]) will return the external direct product as a new\npermutation group. Notice that this is a function (not a method) and the input is a list.\nRather than just combining two groups in the list, any number of groups can be supplied.\nWe illustrate isomorphism testing and direct products in the context of Theorem 9.21,\nwhich is an equivalence, so tells us exactly when we have isomorphic groups. We use cyclic\npermutation groups as stand-ins for Zn by Theorem 9.8.\n\nFirst, two isomorphic groups.\nm = 12\nn = 7\ngcd(m, n)\n\n1\n\nG = CyclicPermutationGroup(m)\nH = CyclicPermutationGroup(n)\ndp = direct_product_permgroups ([G, H])\nK = CyclicPermutationGroup(m*n)\nK.is_isomorphic(dp)\n\nTrue\n\nNow, two non-isomorphic groups.\nm = 15\nn = 21\ngcd(m, n)\n\n3\n\nG = CyclicPermutationGroup(m)\nH = CyclicPermutationGroup(n)\ndp = direct_product_permgroups ([G, H])\nK = CyclicPermutationGroup(m*n)\nK.is_isomorphic(dp)\n\nFalse\n\nNotice how the simple computation of a greatest common divisor predicts the incredibly\ncomplicated computation of determining if two groups are isomorphic. This is a nice illus-\ntration of the power of mathematics, replacing a difficult problem (group isomorphism) by\na simple one (factoring and divisibility of integers). Let us build one more direct product\nof cyclic groups, but with three groups, each with orders that are pairwise relatively prime.\n\nIf you try the following with larger parameters you may get an error (database_gap).\nm = 6\nn = 5\nr = 7\nG = CyclicPermutationGroup(m)\nH = CyclicPermutationGroup(n)\nL = CyclicPermutationGroup(r)\ndp = direct_product_permgroups ([G, H, L])\nK = CyclicPermutationGroup(m*n*r)\nK.is_isomorphic(dp)\n\nTrue\n\n\n\n164 CHAPTER 9. ISOMORPHISMS\n\nClassifying Finite Groups\nOnce we understand isomorphic groups as being the “same”, or “fundamentally no differ-\nent,” or “structurally identical,” then it is natural to ask how many “really different” finite\ngroups there are. Corollary 9.9 gives a partial answer: for each prime there is just one finite\ngroup, with Zp as a concrete manifestation.\n\nLet us embark on a quest to find all the groups of order less than 16 in Sage as permu-\ntation groups. For prime orders 1, 2, 3, 5, 7, 11 and 13 we know there is really just one group\neach, and we can realize them all:\n\n[CyclicPermutationGroup(p) for p in [1, 2, 3, 5, 7, 11, 13]]\n\n[Cyclic group of order 1 as a permutation group ,\nCyclic group of order 2 as a permutation group ,\nCyclic group of order 3 as a permutation group ,\nCyclic group of order 5 as a permutation group ,\nCyclic group of order 7 as a permutation group ,\nCyclic group of order 11 as a permutation group ,\nCyclic group of order 13 as a permutation group]\n\nSo now our smallest unknown case is order 4. Sage knows at least three such groups,\nand we can use Sage to check if any pair is isomorphic. Notice that since “isomorphic to” is\nan equivalence relation, and hence a transitive relation, the two tests below are sufficient.\n\nG = CyclicPermutationGroup (4)\nH = KleinFourGroup ()\nT1 = CyclicPermutationGroup (2)\nT2 = CyclicPermutationGroup (2)\nK = direct_product_permgroups ([T1, T2])\nG.is_isomorphic(H)\n\nFalse\n\nH.is_isomorphic(K)\n\nTrue\n\nSo we have at least two different groups: Z4 and Z2 × Z2, with the latter also known\nas the Klein 4-group. Sage will not be able to tell us if we have a complete list — this will\nalways require theoretical results like Theorem 9.10. We will shortly have a more general\nresult that handles the case of order 4, but right now, a careful analysis (by hand) of the\npossibilities for the Cayley table of a group of order 4 should lead you to the two possibilities\nabove as the only possibilities. Try to deduce what the Cayley table of an order 4 group\nshould look like, since you know about identity elements, inverses and cancellation.\n\nWe have seen at least two groups of order 6 (next on our list of non-prime orders). One\nis abelian and one is not, so we do not need Sage to tell us they are structurally different.\nBut let us do it anyway.\n\nG = CyclicPermutationGroup (6)\nH = SymmetricGroup (3)\nG.is_isomorphic(H)\n\nFalse\n\nIs that all? There is Z3 × Z2, but that is just Z6 since 2 and 3 are relatively prime.\nThe dihedral group, D3, all symmetries of a triangle, is just S3, the symmetric group on 3\nsymbols.\n\n\n\n9.4. SAGE 165\n\nG = DihedralGroup (3)\nH = SymmetricGroup (3)\nG.is_isomorphic(H)\n\nTrue\n\nExercise 9.3.55 from this section classifies all groups of order 2p, where p is a prime.\nSuch a group is either cyclic or a dihedral group. So the two groups above, Z6 and D3, are\nthe complete list of groups of order 6.\n\nBy this general result, in addition to order 6, we also know the complete lists of groups\nof orders 10 and 14. To Be Continued.\n\nInternal Direct Products\nAn internal direct product is a statement about subgroups of a single group, together with\na theorem that links them to an external direct product. We will work an example here\nthat will illustrate the nature of an internal direct product.\n\nGiven an integer n, the set of positive integers less than n, and relatively prime to n\nforms a group under multiplication mod n. We will work in the set Integers(n) where we\ncan add and multiply, but we want to stay strictly with multiplication only.\n\nFirst we build the subgroup itself. Notice how we must convert x into an integer (an\nelement of ZZ) so that the greatest common divisor computation performs correctly.\n\nZ36 = Integers (36)\nU = [x for x in Z36 if gcd(ZZ(x), 36) == 1]\nU\n\n[1, 5, 7, 11, 13, 17, 19, 23, 25, 29, 31, 35]\n\nSo we have a group of order 12. We are going to try to find a subgroup of order 6 and\na subgroup of order 2 to form the internal direct product, and we will restrict our search\ninitially to cyclic subgroups of order 6. Sage has a method that will give the order of each\nof these elements, relative to multiplication, so let us examine those next.\n\n[x.multiplicative_order () for x in U]\n\n[1, 6, 6, 6, 3, 2, 2, 6, 3, 6, 6, 2]\n\nWe have many choices for generators of a cyclic subgroup of order 6 and for a cyclic\nsubgroup of order 2. Of course, some of the choices for a generator of the subgroup of order\n6 will generate the same subgroup. Can you tell, just by counting, how many subgroups of\norder 6 there are? We are going to pick the first element of order 6, and the last element\nof order 2, for no particular reason. After your work through this once, we encourage you\nto try other choices to understand why some choices lead to an internal direct product and\nsome do not. Notice that we choose the elements from the list U so that they are sure to be\nelements of Z36 and behave properly when multiplied.\n\na = U[1]\nA = [a^i for i in srange (6)]\nA\n\n[1, 5, 25, 17, 13, 29]\n\nb = U[11]\nB = [b^i for i in srange (2)]\nB\n\n\n\n166 CHAPTER 9. ISOMORPHISMS\n\n[1, 35]\n\nSo A and B are two cyclic subgroups. Notice that their intersection is the identity element,\none of our requirements for an internal direct product. So this is a good start.\n\n[x for x in A if x in B]\n\n[1]\n\nZ36 is an abelian group, thus the condition on all products commuting will hold, but we\nillustrate the Sage commands that will check this in a non-abelian situation.\n\nall([x*y == y*x for x in A for y in B])\n\nTrue\n\nFinally, we need to check that by forming products with elements from A and B we create\nthe entire group. Sorting the resulting list will make a check easier for us visually, and is\nrequired if we want Sage to do the check.\n\nT = sorted ([x*y for x in A for y in B])\nT\n\n[1, 5, 7, 11, 13, 17, 19, 23, 25, 29, 31, 35]\n\nT == U\n\nTrue\n\nThat’s it. We now condense all this information into the statement that “U is the internal\ndirect product of A and B.” By Theorem 9.27, we see that U is isomorphic to a product of a\ncyclic group of order 6 and a cyclic group of order 2. So in a very real sense, U is no more or\nless complicated than Z6 × Z2, which is in turn isomorphic to Z3 × Z2 × Z2. So we totally\nunderstand the “structure” of U. For example, we can see that U is not cyclic, since when\nwritten as a product of cyclic groups, the two orders are not relatively prime. The final\nexpression of U suggests you could find three cyclic subgroups of U, with orders 3, 2 and 2,\nso that U is an internal direct product of the three subgroups.\n\n9.5 Sage Exercises\n1. This exercise is about putting Cayley’s Theorem into practice. First, read and study\nthe theorem. Realize that this result by itself is primarily of theoretical interest, but with\nsome more theory we could get into some subtler aspects of this (a subject known as\n“representation theory”).\nYou should create these representations mostly with pencil-and-paper work, using Sage as\na fancy calculator and assistant. You do not need to include all these computations in your\nworksheet. Build the requested group representations and then include enough verifications\nin Sage to prove that that your representation correctly represents the group.\nBegin by building a permutation representation of the quaternions, Q. There are eight\nelements in Q (±1,±I,±J,±K), so you will be constructing a subgroup of S8. For each\ng ∈ Q form the function Tg, defined as Tg(x) = xg. Notice that this definition is the\n“reverse” of that given in the text. This is because Sage composes permutations left-to-\nright, while your text composes right-to-left. To create the permutations Tg, the two-line\nversion of writing permutations could be very useful as an intermediate step. You will\nprobably want to “code” each element of Q with an integer in {1, 2, . . . , 8}.\n\n\n\n9.5. SAGE EXERCISES 167\n\nOne such representation is included in Sage as QuaternionGroup() — your answer should\nlook very similar, but perhaps not identical. Do not submit your answer for a r4presenation\nof the quaternions, but I strongly suggest working this particular group representation until\nyou are sure you have it right — the problems below might be very difficult otherwise.\nYou can use Sage’s .is_isomorphic() method to check if your representations are correct.\nHowever, do not use this as a substitute for the part of each question that asks you to\ninvestigate properties of your representation towards this end.\n(a) Build the permutation representation of Z2 ×Z4 described in Cayley’s Theorem. (Re-\n\nmember that this group is additive, while the theorem uses multiplicative notation.)\nInclude the representation of each of the 8 elements in your submitted work. Then\nconstruct the permutation group as a subgroup of a full symmetric group that is gen-\nerated by exactly two of the eight elements you have already constructed. Hint: which\ntwo elements of Z2 × Z4 might you use to generate all of Z2 × Z4? Use commands\nin Sage to investigate various properties of your permutation group, other than just\n.list(), to provide evidence that your subgroup is correct — include these in your\nsubmitted worksheet.\n\n(b) Build a permutation representation of U(24), the group of units mod 24. Again, list\na representation of each element in your submitted work. Then construct the group\nas a subgroup of a full symmetric group created with three generators. To determine\nthese three generators, you will likely need to understand U(24) as an internal direct\nproduct. Use commands in Sage to investigate various properties of your group, other\nthan just .list(), to provide evidence that your subgroup is correct — include these\nin your submitted worksheet.\n\n2. Consider the symmetries of a 10-gon, D10 in your text, DihedralGroup(10) in Sage. Pre-\nsume that the vertices pf the 10-gon have been labeled 1 through 10 in order. Identify the\npermutation that is a 180 degree rotation and use it to generate a subgroup R of order 2.\nThen identify the permutation that is a 72 degree rotation, and any one of the ten permu-\ntations that are a reflection of the 10-gon about a line. Use these latter two permutations\nto generate a subgroup S of order 10. Use Sage to verify that the full dihedral group is\nthe internal direct product of the subgroups R and S by checking the conditions in the\ndefinition of an internal direct product.\nWe have a theorem which says that if a group is an internal direct product, then it is\nisomorphic to some external direct product. Understand that this does not mean that you\ncan use the converse in this problem. In other words, establishing an isomorphism of G\nwith an external direct product does not prove that G is an internal direct product.\n\n\n\n10\n\nNormal Subgroups and Factor\nGroups\n\nIf H is a subgroup of a group G, then right cosets are not always the same as left cosets;\nthat is, it is not always the case that gH = Hg for all g ∈ G. The subgroups for which this\nproperty holds play a critical role in group theory—they allow for the construction of a new\nclass of groups, called factor or quotient groups. Factor groups may be studied directly or\nby using homomorphisms, a generalization of isomorphisms. We will study homomorphisms\nin Chapter 11.\n\n10.1 Factor Groups and Normal Subgroups\n\nNormal Subgroups\n\nA subgroup H of a group G is normal in G if gH = Hg for all g ∈ G. That is, a normal\nsubgroup of a group G is one in which the right and left cosets are precisely the same.\n\nExample 10.1. Let G be an abelian group. Every subgroup H of G is a normal subgroup.\nSince gh = hg for all g ∈ G and h ∈ H, it will always be the case that gH = Hg.\n\nExample 10.2. Let H be the subgroup of S3 consisting of elements (1) and (12). Since\n\n(123)H = {(123), (13)} and H(123) = {(123), (23)},\n\nH cannot be a normal subgroup of S3. However, the subgroup N , consisting of the permu-\ntations (1), (123), and (132), is normal since the cosets of N are\n\nN = {(1), (123), (132)}\n(12)N = N(12) = {(12), (13), (23)}.\n\nThe following theorem is fundamental to our understanding of normal subgroups.\n\nTheorem 10.3. Let G be a group and N be a subgroup of G. Then the following statements\nare equivalent.\n\n1. The subgroup N is normal in G.\n\n2. For all g ∈ G, gNg−1 ⊂ N .\n\n3. For all g ∈ G, gNg−1 = N .\n\n168\n\n\n\n10.1. FACTOR GROUPS AND NORMAL SUBGROUPS 169\n\nProof. (1) ⇒ (2). Since N is normal in G, gN = Ng for all g ∈ G. Hence, for a given\ng ∈ G and n ∈ N , there exists an n′ in N such that gn = n′g. Therefore, gng−1 = n′ ∈ N\nor gNg−1 ⊂ N .\n\n(2) ⇒ (3). Let g ∈ G. Since gNg−1 ⊂ N , we need only show N ⊂ gNg−1. For n ∈ N ,\ng−1ng = g−1n(g−1)−1 ∈ N . Hence, g−1ng = n′ for some n′ ∈ N . Therefore, n = gn′g−1 is\nin gNg−1.\n\n(3) ⇒ (1). Suppose that gNg−1 = N for all g ∈ G. Then for any n ∈ N there\nexists an n′ ∈ N such that gng−1 = n′. Consequently, gn = n′g or gN ⊂ Ng. Similarly,\nNg ⊂ gN .\n\nFactor Groups\nIf N is a normal subgroup of a group G, then the cosets of N in G form a group G/N under\nthe operation (aN)(bN) = abN . This group is called the factor or quotient group of G\nand N . Our first task is to prove that G/N is indeed a group.\n\nTheorem 10.4. Let N be a normal subgroup of a group G. The cosets of N in G form a\ngroup G/N of order [G : N ].\n\nProof. The group operation on G/N is (aN)(bN) = abN . This operation must be shown\nto be well-defined; that is, group multiplication must be independent of the choice of coset\nrepresentative. Let aN = bN and cN = dN . We must show that\n\n(aN)(cN) = acN = bdN = (bN)(dN).\n\nThen a = bn1 and c = dn2 for some n1 and n2 in N . Hence,\n\nacN = bn1dn2N\n\n= bn1dN\n\n= bn1Nd\n\n= bNd\n\n= bdN.\n\nThe remainder of the theorem is easy: eN = N is the identity and g−1N is the inverse of\ngN . The order of G/N is, of course, the number of cosets of N in G.\n\nIt is very important to remember that the elements in a factor group are sets of elements\nin the original group.\n\nExample 10.5. Consider the normal subgroup of S3, N = {(1), (123), (132)}. The cosets\nof N in S3 are N and (12)N . The factor group S3/N has the following multiplication table.\n\nN (12)N\n\nN N (12)N\n\n(12)N (12)N N\n\nThis group is isomorphic to Z2. At first, multiplying cosets seems both complicated and\nstrange; however, notice that S3/N is a smaller group. The factor group displays a certain\namount of information about S3. Actually, N = A3, the group of even permutations, and\n(12)N = {(12), (13), (23)} is the set of odd permutations. The information captured in\n\n\n\n170 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\n\nG/N is parity; that is, multiplying two even or two odd permutations results in an even\npermutation, whereas multiplying an odd permutation by an even permutation yields an\nodd permutation.\n\nExample 10.6. Consider the normal subgroup 3Z of Z. The cosets of 3Z in Z are\n\n0 + 3Z = {. . . ,−3, 0, 3, 6, . . .}\n1 + 3Z = {. . . ,−2, 1, 4, 7, . . .}\n2 + 3Z = {. . . ,−1, 2, 5, 8, . . .}.\n\nThe group Z/3Z is given by the multiplication table below.\n\n+ 0 + 3Z 1 + 3Z 2 + 3Z\n0 + 3Z 0 + 3Z 1 + 3Z 2 + 3Z\n1 + 3Z 1 + 3Z 2 + 3Z 0 + 3Z\n2 + 3Z 2 + 3Z 0 + 3Z 1 + 3Z\n\nIn general, the subgroup nZ of Z is normal. The cosets of Z/nZ are\n\nnZ\n1 + nZ\n2 + nZ\n\n...\n(n− 1) + nZ.\n\nThe sum of the cosets k + Z and l+ Z is k + l+ Z. Notice that we have written our cosets\nadditively, because the group operation is integer addition.\n\nExample 10.7. Consider the dihedral group Dn, generated by the two elements r and s,\nsatisfying the relations\n\nrn = id\ns2 = id\nsrs = r−1.\n\nThe element r actually generates the cyclic subgroup of rotations, Rn, of Dn. Since srs−1 =\nsrs = r−1 ∈ Rn, the group of rotations is a normal subgroup of Dn; therefore, Dn/Rn is a\ngroup. Since there are exactly two elements in this group, it must be isomorphic to Z2.\n\n10.2 The Simplicity of the Alternating Group\nOf special interest are groups with no nontrivial normal subgroups. Such groups are called\nsimple groups. Of course, we already have a whole class of examples of simple groups, Zp,\nwhere p is prime. These groups are trivially simple since they have no proper subgroups\nother than the subgroup consisting solely of the identity. Other examples of simple groups\nare not so easily found. We can, however, show that the alternating group, An, is simple\nfor n ≥ 5. The proof of this result requires several lemmas.\n\nLemma 10.8. The alternating group An is generated by 3-cycles for n ≥ 3.\n\n\n\n10.2. THE SIMPLICITY OF THE ALTERNATING GROUP 171\n\nProof. To show that the 3-cycles generate An, we need only show that any pair of trans-\npositions can be written as the product of 3-cycles. Since (ab) = (ba), every pair of trans-\npositions must be one of the following:\n\n(ab)(ab) = id\n(ab)(cd) = (acb)(acd)\n\n(ab)(ac) = (acb).\n\nLemma 10.9. Let N be a normal subgroup of An, where n ≥ 3. If N contains a 3-cycle,\nthen N = An.\n\nProof. We will first show that An is generated by 3-cycles of the specific form (ijk), where\ni and j are fixed in {1, 2, . . . , n} and we let k vary. Every 3-cycle is the product of 3-cycles\nof this form, since\n\n(iaj) = (ija)2\n\n(iab) = (ijb)(ija)2\n\n(jab) = (ijb)2(ija)\n\n(abc) = (ija)2(ijc)(ijb)2(ija).\n\nNow suppose that N is a nontrivial normal subgroup of An for n ≥ 3 such that N contains\na 3-cycle of the form (ija). Using the normality of N , we see that\n\n[(ij)(ak)](ija)2[(ij)(ak)]−1 = (ijk)\n\nis in N . Hence, N must contain all of the 3-cycles (ijk) for 1 ≤ k ≤ n. By Lemma 10.8,\nthese 3-cycles generate An; hence, N = An.\n\nLemma 10.10. For n ≥ 5, every nontrivial normal subgroup N of An contains a 3-cycle.\n\nProof. Let σ be an arbitrary element in a normal subgroup N . There are several possible\ncycle structures for σ.\n\n• σ is a 3-cycle.\n\n• σ is the product of disjoint cycles, σ = τ(a1a2 · · · ar) ∈ N , where r > 3.\n\n• σ is the product of disjoint cycles, σ = τ(a1a2a3)(a4a5a6).\n\n• σ = τ(a1a2a3), where τ is the product of disjoint 2-cycles.\n\n• σ = τ(a1a2)(a3a4), where τ is the product of an even number of disjoint 2-cycles.\n\nIf σ is a 3-cycle, then we are done. If N contains a product of disjoint cycles, σ, and at\nleast one of these cycles has length greater than 3, say σ = τ(a1a2 · · · ar), then\n\n(a1a2a3)σ(a1a2a3)\n−1\n\nis in N since N is normal; hence,\n\nσ−1(a1a2a3)σ(a1a2a3)\n−1\n\n\n\n172 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\n\nis also in N . Since\n\nσ−1(a1a2a3)σ(a1a2a3)\n−1 = σ−1(a1a2a3)σ(a1a3a2)\n\n= (a1a2 · · · ar)−1τ−1(a1a2a3)τ(a1a2 · · · ar)(a1a3a2)\n= (a1arar−1 · · · a2)(a1a2a3)(a1a2 · · · ar)(a1a3a2)\n= (a1a3ar),\n\nN must contain a 3-cycle; hence, N = An.\nNow suppose that N contains a disjoint product of the form\n\nσ = τ(a1a2a3)(a4a5a6).\n\nThen\nσ−1(a1a2a4)σ(a1a2a4)\n\n−1 ∈ N\n\nsince\n(a1a2a4)σ(a1a2a4)\n\n−1 ∈ N.\n\nSo\n\nσ−1(a1a2a4)σ(a1a2a4)\n−1 = [τ(a1a2a3)(a4a5a6)]\n\n−1(a1a2a4)τ(a1a2a3)(a4a5a6)(a1a2a4)\n−1\n\n= (a4a6a5)(a1a3a2)τ\n−1(a1a2a4)τ(a1a2a3)(a4a5a6)(a1a4a2)\n\n= (a4a6a5)(a1a3a2)(a1a2a4)(a1a2a3)(a4a5a6)(a1a4a2)\n\n= (a1a4a2a6a3).\n\nSo N contains a disjoint cycle of length greater than 3, and we can apply the previous case.\nSuppose N contains a disjoint product of the form σ = τ(a1a2a3), where τ is the product\n\nof disjoint 2-cycles. Since σ ∈ N , σ2 ∈ N , and\n\nσ2 = τ(a1a2a3)τ(a1a2a3)\n\n= (a1a3a2).\n\nSo N contains a 3-cycle.\nThe only remaining possible case is a disjoint product of the form\n\nσ = τ(a1a2)(a3a4),\n\nwhere τ is the product of an even number of disjoint 2-cycles. But\n\nσ−1(a1a2a3)σ(a1a2a3)\n−1\n\nis in N since (a1a2a3)σ(a1a2a3)\n−1 is in N ; and so\n\nσ−1(a1a2a3)σ(a1a2a3)\n−1 = τ−1(a1a2)(a3a4)(a1a2a3)τ(a1a2)(a3a4)(a1a2a3)\n\n−1\n\n= (a1a3)(a2a4).\n\nSince n ≥ 5, we can find b ∈ {1, 2, . . . , n} such that b ̸= a1, a2, a3, a4. Let µ = (a1a3b). Then\n\nµ−1(a1a3)(a2a4)µ(a1a3)(a2a4) ∈ N\n\nand\n\nµ−1(a1a3)(a2a4)µ(a1a3)(a2a4) = (a1ba3)(a1a3)(a2a4)(a1a3b)(a1a3)(a2a4)\n\n= (a1a3b).\n\nTherefore, N contains a 3-cycle. This completes the proof of the lemma.\n\n\n\n10.3. EXERCISES 173\n\nTheorem 10.11. The alternating group, An, is simple for n ≥ 5.\n\nProof. Let N be a normal subgroup of An. By Lemma 10.10, N contains a 3-cycle. By\nLemma 10.9, N = An; therefore, An contains no proper nontrivial normal subgroups for\nn ≥ 5.\n\nHistorical Note\n\nOne of the foremost problems of group theory has been to classify all simple finite\ngroups. This problem is over a century old and has been solved only in the last few decades\nof the twentieth century. In a sense, finite simple groups are the building blocks of all\nfinite groups. The first nonabelian simple groups to be discovered were the alternating\ngroups. Galois was the first to prove that A5 was simple. Later, mathematicians such as C.\nJordan and L. E. Dickson found several infinite families of matrix groups that were simple.\nOther families of simple groups were discovered in the 1950s. At the turn of the century,\nWilliam Burnside conjectured that all nonabelian simple groups must have even order. In\n1963, W. Feit and J. Thompson proved Burnside’s conjecture and published their results\nin the paper “Solvability of Groups of Odd Order,” which appeared in the Pacific Journal\nof Mathematics. Their proof, running over 250 pages, gave impetus to a program in the\n1960s and 1970s to classify all finite simple groups. Daniel Gorenstein was the organizer of\nthis remarkable effort. One of the last simple groups was the “Monster,” discovered by R.\nGreiss. The Monster, a 196,833×196,833 matrix group, is one of the 26 sporadic, or special,\nsimple groups. These sporadic simple groups are groups that fit into no infinite family of\nsimple groups. Some of the sporadic groups play an important role in physics.\n\n10.3 Exercises\n1. For each of the following groups G, determine whether H is a normal subgroup of G. If\nH is a normal subgroup, write out a Cayley table for the factor group G/H.\n(a) G = S4 and H = A4\n\n(b) G = A5 and H = {(1), (123), (132)}\n(c) G = S4 and H = D4\n\n(d) G = Q8 and H = {1,−1, I,−I}\n(e) G = Z and H = 5Z\n\n2. Find all the subgroups of D4. Which subgroups are normal? What are all the factor\ngroups of D4 up to isomorphism?\n\n3. Find all the subgroups of the quaternion group, Q8. Which subgroups are normal? What\nare all the factor groups of Q8 up to isomorphism?\n\n4. Let T be the group of nonsingular upper triangular 2 × 2 matrices with entries in R;\nthat is, matrices of the form (\n\na b\n\n0 c\n\n)\n,\n\nwhere a, b, c ∈ R and ac ̸= 0. Let U consist of matrices of the form(\n1 x\n\n0 1\n\n)\n,\n\nwhere x ∈ R.\n\n\n\n174 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\n\n(a) Show that U is a subgroup of T .\n(b) Prove that U is abelian.\n(c) Prove that U is normal in T .\n(d) Show that T/U is abelian.\n(e) Is T normal in GL2(R)?\n\n5. Show that the intersection of two normal subgroups is a normal subgroup.\n\n6. If G is abelian, prove that G/H must also be abelian.\n\n7. Prove or disprove: If H is a normal subgroup of G such that H and G/H are abelian,\nthen G is abelian.\n\n8. If G is cyclic, prove that G/H must also be cyclic.\n\n9. Prove or disprove: If H and G/H are cyclic, then G is cyclic.\n\n10. Let H be a subgroup of index 2 of a group G. Prove that H must be a normal subgroup\nof G. Conclude that Sn is not simple for n ≥ 3.\n\n11. If a group G has exactly one subgroup H of order k, prove that H is normal in G.\n\n12. Define the centralizer of an element g in a group G to be the set\n\nC(g) = {x ∈ G : xg = gx}.\n\nShow that C(g) is a subgroup of G. If g generates a normal subgroup of G, prove that C(g)\nis normal in G.\n\n13. Recall that the center of a group G is the set\n\nZ(G) = {x ∈ G : xg = gx for all }.\n\n(a) Calculate the center of S3.\n(b) Calculate the center of GL2(R).\n(c) Show that the center of any group G is a normal subgroup of G.\n(d) If G/Z(G) is cyclic, show that G is abelian.\n\n14. Let G be a group and let G′ = ⟨aba−1b−1⟩; that is, G′ is the subgroup of all finite\nproducts of elements in G of the form aba−1b−1. The subgroup G′ is called the commutator\nsubgroup of G.\n\n(a) Show that G′ is a normal subgroup of G.\n(b) Let N be a normal subgroup of G. Prove that G/N is abelian if and only if N contains\n\nthe commutator subgroup of G.\n\n10.4 Sage\nSage has several convenient functions that will allow us to investigate quickly if a subgroup\nis normal, and if so, the nature of the resulting quotient group. But for an initial under-\nstanding, we can also work with the raw cosets. Let us get our hands dirty first, then learn\nabout the easy way.\n\n\n\n10.4. SAGE 175\n\nMultiplying Cosets\nThe definiton of a factor group requires a normal subgroup, and then we define a way to\n“multiply” two cosets of the subgroup to produce another coset. It is important to realize\nthat we can interpret the definition of a normal subgroup to be exactly the condition we\nneed for our new multiplication to be workable. We will do two examples — first with a\nnormal subgroup, then with a subgroup that is not normal.\n\nConsider the dihedral group D8 that is the symmetry group of an 8-gon. If we take the\nelement that creates a quarter-turn, we can use it generate a cyclic subgroup of order 4.\nThis will be a normal subgroup (trust us for the moment on this). First, build the (right)\ncosets (notice there is no output):\n\nG = DihedralGroup (8)\nquarter_turn = G( \' (1,3,5,7)(2,4,6,8) \' )\nS = G.subgroup ([ quarter_turn ])\nC = G.cosets(S)\n\nSo C is a list of lists, with every element of the group G occuring exactly once somewhere.\nYou could ask Sage to print out C for you if you like, but we will try to avoid that here. We\nwant to multiply two cosets (lists) together. How do we do this? Take any element out of\nthe first list, and any element out of the second list and multiply them together (which we\nknow how to do since they are elements of G). Now we have an element of G. What do we\ndo with this element, since we really want a coset as the result of the product of two cosets?\nSimple — we see which coset the product is in. Let us give it a try. We will multiply coset 1\nwith coset 3 (there are 4 cosets by Lagrange’s Theorem). Study the following code carefully\nto see if you can understand what it is doing, and then read the explanation that follows.\n\np = C[1][0]*C[3][0]\n[i for i in srange(len(C)) if p in C[i]]\n\n[2]\n\nWhat have we accomplished? In the first line we create p as the product of two group\nelements, one from coset 1 and one from coset 3 (C[1], C[3]). Since we can choose any\nelement from each coset, we choose the first element of each (C[ ][0]). Then we count our\nway through all the cosets, selecting only cosets that contain p. Since p will only be in one\ncoset, we expect a list with just one element. Here, our one-element list contains only 2. So\nwe say the product of coset 1 and coset 3 is coset 2.\n\nThe point here is that this result (coset 1 times coset 3 is coset 2) should always be\nthe same, no matter which elements we pick from the two cosets to form p. So let us do it\nagain, but this time we will not simply choose the first element from each of coset 1 and\ncoset 3, instead we will choose the third element of coset 1 and the second element of coset\n3 (remember, we are counting from zero!).\n\np = C[1][2]*C[3][1]\n[i for i in srange(len(C)) if p in C[i]]\n\n[2]\n\nGood. We have the same result. If you are still trusting us on S being a normal subgroup\nof G, then this is the result that the theory predicts. Make a copy of the above compute cell\nand try other choices for the representatives of each coset. Then try the product of other\ncosets, with varying representatives.\n\nNow is a good time to introduce a way to extend Sage and add new functions. We\nwill design a coset-multiplication function. Read the following carefully and then see the\nsubsequent explanation.\n\n\n\n176 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\n\ndef coset_product(i, j, C):\np = C[i][0]*C[j][0]\nc = [k for k in srange(len(C)) if p in C[k]]\nreturn c[0]\n\nThe first line creates a new Sage function named coset_product. This is accomplished\nwith the word def, and note the colon ending the line. The inputs to the function are the\nnumbers of the cosets we want to multiply and the complete list of the cosets. The middle\ntwo lines should look familiar from above. We know c is a one-element list, so c[0] will\nextract this one coset number, and return is what determines that this is the output of\nthe function. Notice that the indentation above must be exactly as shown. We could have\nwritten all this computation on a single line without making a new function, but that begins\nto get unwieldly. You need to execute the code block above to actually define the function,\nand there will be no output if successful. Now we can use our new function to repeat our\nwork above:\n\ncoset_product (1, 3, C)\n\n2\n\nNow you know the basics of how to add onto Sage and do much more than it was\ndesigned for. And with some practice, you could suggest and contribute new functions to\nSage, since it is an open source project. Nice.\n\nNow let us examine a situation where the subgroup is not normal. So we will see\nthat our definition of coset multiplication is insufficient in this case. And realize that our\nnew coset_product function is also useless since it assumes the cosets come from a normal\nsubgroup.\n\nConsider the alternating group A4 which we can interpet as the symmetry group of a\ntetrahedron. For a subgroup, take an element that fixes one vertex and rotates the opposite\nface — this will generate a cyclic subgroup of order 3, and by Lagrange’s Theorem we will\nget four cosets. We compute them here. (Again, no output is requested.)\n\nG = AlternatingGroup (4)\nface_turn = G("(1,2,3)")\nS = G.subgroup ([ face_turn ])\nC = G.cosets(S)\n\nAgain, let’s consider the product of coset 1 and coset 3:\np = C[1][0]*C[3][0]\n[i for i in srange(len(C)) if p in C[i]]\n\n[0]\n\nAgain, but now for coset 3, choose the second element of the coset to produce the\nproduct p:\n\np = C[1][0]*C[3][1]\n[i for i in srange(len(C)) if p in C[i]]\n\n[2]\n\nSo, is the product of coset 1 and coset 3 equal to coset 0 or coset 2? We cannot say!\nSo there is no way to construct a quotient group for this subgroup. You can experiment\nsome more with this subgroup, but in some sense, we are done with this example — there\nis nothing left to say.\n\n\n\n10.4. SAGE 177\n\nSage Methods for Normal Subgroups\nYou can easily ask Sage if a subgroup is normal or not. This is viewed as a property of the\nsubgroup, but you must tell Sage what the “supergroup” is, since the answer can change\ndepending on this value. (For example H.is_normal(H) will always be True.) Here are our\ntwo examples from above.\n\nG = DihedralGroup (8)\nquarter_turn = G( \' (1,3,5,7)(2,4,6,8) \' )\nS = G.subgroup ([ quarter_turn ])\nS.is_normal(G)\n\nTrue\n\nG = AlternatingGroup (4)\nface_turn = G("(1,2,3)")\nS = G.subgroup ([ face_turn ])\nS.is_normal(G)\n\nFalse\n\nThe text proves in Section 10.2 that A5 is simple, i.e. A5 has no normal subgroups.\nWe could build every subgroup of A5 and ask if it is normal in A5 using the .is_normal()\n\nmethod. But Sage has this covered for us already.\nG = AlternatingGroup (5)\nG.is_simple ()\n\nTrue\n\nWe can also build a quotient group when we have a normal subgroup.\nG = DihedralGroup (8)\nquarter_turn = G( \' (1,3,5,7)(2,4,6,8) \' )\nS = G.subgroup ([ quarter_turn ])\nQ = G.quotient(S)\nQ\n\nPermutation Group with generators [(1,2)(3,4), (1,3)(2,4)]\n\nThis is useful, but also a bit unsettling. We have the quotient group, but any notion of\ncosets has been lost, since Q is returned as a new permutation group on a different set of\nsymbols. We cannot presume that the numbers used for the new permutation group Q bear\nany resemblance to the cosets we get from the .cosets() method. But we can see that the\nquotient group is described as a group generated by two elements of order two. We could\nask for the order of the group, or by Lagrange’s Theorem we know the quotient has order\n4. We can say now that there are only two groups of order four, the cyclic group of order\n4 and a non-cyclic group of order 4, known to us as the Klein 4-group or Z2 × Z2. This\nquotient group looks like the non-cyclic one since the cyclic group of order 4 has just one\nelement of order 2. Let us see what Sage says.\n\nQ.is_isomorphic(KleinFourGroup ())\n\nTrue\n\nYes, that’s it.\nFinally, Sage can build us a list of all of the normal subgroups of a group. The list of\n\ngroups themselves, as we have seen before, is sometimes an overwhelming amount of infor-\nmation. We will demonstrate by just listing the orders of the normal subgroups produced.\n\n\n\n178 CHAPTER 10. NORMAL SUBGROUPS AND FACTOR GROUPS\n\nG = DihedralGroup (8)\nN = G.normal_subgroups ()\n[H.order () for H in N]\n\n[1, 2, 4, 8, 8, 8, 16]\n\nSo, in particular, we see that our “quarter-turn” subgroup is the only normal subgroup\nof order 4 in this group.\n\n10.5 Sage Exercises\n1. Build every subgroup of the alternating group on 5 symbols, A5, and check that each\nis not a normal subgroup (except for the two trivial cases). This command might take a\ncouple seconds to run. Compare this with the time needed to run the .is_simple() method\nand realize that there is a significant amount of theory and cleverness brought to bear in\nspeeding up commands like this. (It is possible that your Sage installation lacks gap’s\n“Table of Marks” library and you will be unable to compute the list of subgroups.)\n\n2. Consider the quotient group of the group of symmetries of an 8-gon, formed with the\ncyclic subgroup of order 4 generated by a quarter-turn. Use the coset_product function\nto determine the Cayley table for this quotient group. Use the number of each coset, as\nproduced by the .cosets() method as names for the elements of the quotient group. You\nwill need to build the table “by hand” as there is no easy way to have Sage’s Cayley table\ncommand do this one for you. You can build a table in the Sage Notebook pop-up editor\n(shift-click on a blue line) or you might read the documentation of the html.table() method.\n\n3. Consider the cyclic subgroup of order 4 in the symmetries of an 8-gon. Verify that\nthe subgroup is normal by first building the raw left and right cosets (without using the\n.cosets() method) and then checking their equality in Sage, all with a single command that\nemploys sorting with the sorted() command.\n\n4. Again, use the same cyclic subgroup of order 4 in the group of symmetries of an 8-gon.\nCheck that the subgroup is normal by using part (2) of Theorem 10.3. Construct a one-line\ncommand that does the complete check and returns True. Maybe sort the elements of the\nsubgroup S first, then slowly build up the necessary lists, commands, and conditions in\nsteps. Notice that this check does not require ever building the cosets.\n\n5. Repeat the demonstration from the previous subsection that for the symmetries of a\ntetrahedron, a cyclic subgroup of order 3 results in an undefined coset multiplication. Above,\nthe default setting for the .cosets() method builds right cosets — but in this problem, work\ninstead with left cosets. You need to choose two cosets to multiply, and then demonstrate\ntwo choices for representatives that lead to different results for the product of the cosets.\n\n6. Construct some dihedral groups of order 2n (i.e. symmetries of an n-gon, Dn in the text,\nDihedralGroup(n) in Sage). Maybe all of them for 3 ≤ n ≤ 100. For each dihedral group,\nconstruct a list of the orders of each of the normal subgroups (so use .normal_subgroups()).\nYou may need to wait ten or twenty seconds for this to finish - be patient. Observe enough\nexamples to hypothesize a pattern to your observations, check your hypothesis against each\nof your examples and then state your hypothesis clearly.\nCan you predict how many normal subgroups there are in the dihedral groupD470448 without\nusing Sage to build all the normal subgroups? Can you describe all of the normal subgroups\nof a dihedral group in a way that would let us predict all of the normal subgroups of D470448\n\nwithout using Sage?\n\n\n\n11\n\nHomomorphisms\n\nOne of the basic ideas of algebra is the concept of a homomorphism, a natural generalization\nof an isomorphism. If we relax the requirement that an isomorphism of groups be bijective,\nwe have a homomorphism.\n\n11.1 Group Homomorphisms\nA homomorphism between groups (G, ·) and (H, ◦) is a map ϕ : G→ H such that\n\nϕ(g1 · g2) = ϕ(g1) ◦ ϕ(g2)\n\nfor g1, g2 ∈ G. The range of ϕ in H is called the homomorphic image of ϕ.\nTwo groups are related in the strongest possible way if they are isomorphic; however, a\n\nweaker relationship may exist between two groups. For example, the symmetric group Sn\nand the group Z2 are related by the fact that Sn can be divided into even and odd permu-\ntations that exhibit a group structure like that Z2, as shown in the following multiplication\ntable.\n\neven odd\neven even odd\nodd odd even\n\nWe use homomorphisms to study relationships such as the one we have just described.\n\nExample 11.1. Let G be a group and g ∈ G. Define a map ϕ : Z → G by ϕ(n) = gn.\nThen ϕ is a group homomorphism, since\n\nϕ(m+ n) = gm+n = gmgn = ϕ(m)ϕ(n).\n\nThis homomorphism maps Z onto the cyclic subgroup of G generated by g.\n\nExample 11.2. Let G = GL2(R). If\n\nA =\n\n(\na b\n\nc d\n\n)\nis in G, then the determinant is nonzero; that is, det(A) = ad − bc ̸= 0. Also, for any two\nelements A and B in G, det(AB) = det(A) det(B). Using the determinant, we can define a\nhomomorphism ϕ : GL2(R) → R∗ by A 7→ det(A).\n\n179\n\n\n\n180 CHAPTER 11. HOMOMORPHISMS\n\nExample 11.3. Recall that the circle group T consists of all complex numbers z such that\n|z| = 1. We can define a homomorphism ϕ from the additive group of real numbers R to T\nby ϕ : θ 7→ cos θ + i sin θ. Indeed,\n\nϕ(α+ β) = cos(α+ β) + i sin(α+ β)\n\n= (cosα cosβ − sinα sinβ) + i(sinα cosβ + cosα sinβ)\n= (cosα+ i sinα)(cosβ + i sinβ)\n= ϕ(α)ϕ(β).\n\nGeometrically, we are simply wrapping the real line around the circle in a group-theoretic\nfashion.\n\nThe following proposition lists some basic properties of group homomorphisms.\n\nProposition 11.4. Let ϕ : G1 → G2 be a homomorphism of groups. Then\n\n1. If e is the identity of G1, then ϕ(e) is the identity of G2;\n\n2. For any element g ∈ G1, ϕ(g−1) = [ϕ(g)]−1;\n\n3. If H1 is a subgroup of G1, then ϕ(H1) is a subgroup of G2;\n\n4. If H2 is a subgroup of G2, then ϕ−1(H2) = {g ∈ G1 : ϕ(g) ∈ H2} is a subgroup of G1.\nFurthermore, if H2 is normal in G2, then ϕ−1(H2) is normal in G1.\n\nProof. (1) Suppose that e and e′ are the identities of G1 and G2, respectively; then\n\ne′ϕ(e) = ϕ(e) = ϕ(ee) = ϕ(e)ϕ(e).\n\nBy cancellation, ϕ(e) = e′.\n(2) This statement follows from the fact that\n\nϕ(g−1)ϕ(g) = ϕ(g−1g) = ϕ(e) = e′.\n\n(3) The set ϕ(H1) is nonempty since the identity of G2 is in ϕ(H1). Suppose that H1\n\nis a subgroup of G1 and let x and y be in ϕ(H1). There exist elements a, b ∈ H1 such that\nϕ(a) = x and ϕ(b) = y. Since\n\nxy−1 = ϕ(a)[ϕ(b)]−1 = ϕ(ab−1) ∈ ϕ(H1),\n\nϕ(H1) is a subgroup of G2 by Proposition 3.31.\n(4) Let H2 be a subgroup of G2 and define H1 to be ϕ−1(H2); that is, H1 is the set of\n\nall g ∈ G1 such that ϕ(g) ∈ H2. The identity is in H1 since ϕ(e) = e′. If a and b are in H1,\nthen ϕ(ab−1) = ϕ(a)[ϕ(b)]−1 is in H2 since H2 is a subgroup of G2. Therefore, ab−1 ∈ H1\n\nand H1 is a subgroup of G1. If H2 is normal in G2, we must show that g−1hg ∈ H1 for\nh ∈ H1 andvg ∈ G1. But\n\nϕ(g−1hg) = [ϕ(g)]−1ϕ(h)ϕ(g) ∈ H2,\n\nsince H2 is a normal subgroup of G2. Therefore, g−1hg ∈ H1.\n\nLet ϕ : G→ H be a group homomorphism and suppose that e is the identity of H. By\nProposition 11.4, ϕ−1({e}) is a subgroup of G. This subgroup is called the kernel of ϕ and\nwill be denoted by kerϕ. In fact, this subgroup is a normal subgroup of G since the trivial\nsubgroup is normal in H. We state this result in the following theorem, which says that\nwith every homomorphism of groups we can naturally associate a normal subgroup.\n\n\n\n11.2. THE ISOMORPHISM THEOREMS 181\n\nTheorem 11.5. Let ϕ : G → H be a group homomorphism. Then the kernel of ϕ is a\nnormal subgroup of G.\n\nExample 11.6. Let us examine the homomorphism ϕ : GL2(R) → R∗ defined by A 7→\ndet(A). Since 1 is the identity of R∗, the kernel of this homomorphism is all 2× 2 matrices\nhaving determinant one. That is, kerϕ = SL2(R).\n\nExample 11.7. The kernel of the group homomorphism ϕ : R → C∗ defined by ϕ(θ) =\ncos θ + i sin θ is {2πn : n ∈ Z}. Notice that kerϕ ∼= Z.\n\nExample 11.8. Suppose that we wish to determine all possible homomorphisms ϕ from\nZ7 to Z12. Since the kernel of ϕ must be a subgroup of Z7, there are only two possible\nkernels, {0} and all of Z7. The image of a subgroup of Z7 must be a subgroup of Z12.\nHence, there is no injective homomorphism; otherwise, Z12 would have a subgroup of order\n7, which is impossible. Consequently, the only possible homomorphism from Z7 to Z12 is\nthe one mapping all elements to zero.\n\nExample 11.9. Let G be a group. Suppose that g ∈ G and ϕ is the homomorphism from Z\nto G given by ϕ(n) = gn. If the order of g is infinite, then the kernel of this homomorphism\nis {0} since ϕ maps Z onto the cyclic subgroup of G generated by g. However, if the order\nof g is finite, say n, then the kernel of ϕ is nZ.\n\n11.2 The Isomorphism Theorems\nAlthough it is not evident at first, factor groups correspond exactly to homomorphic images,\nand we can use factor groups to study homomorphisms. We already know that with every\ngroup homomorphism ϕ : G→ H we can associate a normal subgroup of G, kerϕ. The con-\nverse is also true; that is, every normal subgroup of a group G gives rise to homomorphism\nof groups.\n\nLet H be a normal subgroup of G. Define the natural or canonical homomorphism\n\nϕ : G→ G/H\n\nby\nϕ(g) = gH.\n\nThis is indeed a homomorphism, since\n\nϕ(g1g2) = g1g2H = g1Hg2H = ϕ(g1)ϕ(g2).\n\nThe kernel of this homomorphism is H. The following theorems describe the relationships\nbetween group homomorphisms, normal subgroups, and factor groups.\n\nTheorem 11.10 (First Isomorphism Theorem). If ψ : G → H is a group homomorphism\nwith K = kerψ, then K is normal in G. Let ϕ : G→ G/K be the canonical homomorphism.\nThen there exists a unique isomorphism η : G/K → ψ(G) such that ψ = ηϕ.\n\nProof. We already know thatK is normal inG. Define η : G/K → ψ(G) by η(gK) = ψ(g).\nWe first show that η is a well-defined map. If g1K = g2K, then for some k ∈ K, g1k = g2;\nconsequently,\n\nη(g1K) = ψ(g1) = ψ(g1)ψ(k) = ψ(g1k) = ψ(g2) = η(g2K).\n\n\n\n182 CHAPTER 11. HOMOMORPHISMS\n\nThus, η does not depend on the choice of coset representatives and the map η : G/K → ψ(G)\nis uniquely defined since ψ = ηϕ. We must also show that η is a homomorphism, but\n\nη(g1Kg2K) = η(g1g2K)\n\n= ψ(g1g2)\n\n= ψ(g1)ψ(g2)\n\n= η(g1K)η(g2K).\n\nClearly, η is onto ψ(G). To show that η is one-to-one, suppose that η(g1K) = η(g2K).\nThen ψ(g1) = ψ(g2). This implies that ψ(g−1\n\n1 g2) = e, or g−1\n1 g2 is in the kernel of ψ; hence,\n\ng−1\n1 g2K = K; that is, g1K = g2K.\n\nMathematicians often use diagrams called commutative diagrams to describe such\ntheorems. The following diagram “commutes” since ψ = ηϕ.\n\nψ\n\nϕ η\n\nG H\n\nG/K\n\nExample 11.11. Let G be a cyclic group with generator g. Define a map ϕ : Z → G by\nn 7→ gn. This map is a surjective homomorphism since\n\nϕ(m+ n) = gm+n = gmgn = ϕ(m)ϕ(n).\n\nClearly ϕ is onto. If |g| = m, then gm = e. Hence, kerϕ = mZ and Z/ kerϕ = Z/mZ ∼= G.\nOn the other hand, if the order of g is infinite, then kerϕ = 0 and ϕ is an isomorphism of\nG and Z. Hence, two cyclic groups are isomorphic exactly when they have the same order.\nUp to isomorphism, the only cyclic groups are Z and Zn.\n\nTheorem 11.12 (Second Isomorphism Theorem). Let H be a subgroup of a group G (not\nnecessarily normal in G) and N a normal subgroup of G. Then HN is a subgroup of G,\nH ∩N is a normal subgroup of H, and\n\nH/H ∩N ∼= HN/N.\n\nProof. We will first show that HN = {hn : h ∈ H,n ∈ N} is a subgroup of G. Suppose\nthat h1n1, h2n2 ∈ HN . Since N is normal, (h2)−1n1h2 ∈ N . So\n\n(h1n1)(h2n2) = h1h2((h2)\n−1n1h2)n2\n\nis in HN . The inverse of hn ∈ HN is in HN since\n\n(hn)−1 = n−1h−1 = h−1(hn−1h−1).\n\nNext, we prove that H ∩ N is normal in H. Let h ∈ H and n ∈ H ∩ N . Then\nh−1nh ∈ H since each element is in H. Also, h−1nh ∈ N since N is normal in G; therefore,\nh−1nh ∈ H ∩N .\n\nNow define a map ϕ from H to HN/N by h 7→ hN . The map ϕ is onto, since any coset\nhnN = hN is the image of h in H. We also know that ϕ is a homomorphism because\n\nϕ(hh′) = hh′N = hNh′N = ϕ(h)ϕ(h′).\n\n\n\n11.2. THE ISOMORPHISM THEOREMS 183\n\nBy the First Isomorphism Theorem, the image of ϕ is isomorphic to H/ kerϕ; that is,\n\nHN/N = ϕ(H) ∼= H/ kerϕ.\n\nSince\nkerϕ = {h ∈ H : h ∈ N} = H ∩N,\n\nHN/N = ϕ(H) ∼= H/H ∩N .\n\nTheorem 11.13 (Correspondence Theorem). Let N be a normal subgroup of a group G.\nThen H 7→ H/N is a one-to-one correspondence between the set of subgroups H containing\nN and the set of subgroups of G/N . Furthermore, the normal subgroups of G containing N\ncorrespond to normal subgroups of G/N .\n\nProof. Let H be a subgroup of G containing N . Since N is normal in H, H/N makes\nsense. Let aN and bN be elements of H/N . Then (aN)(b−1N) = ab−1N ∈ H/N ; hence,\nH/N is a subgroup ofG/N .\n\nLet S be a subgroup of G/N . This subgroup is a set of cosets of N . If H = {g ∈ G :\ngN ∈ S}, then for h1, h2 ∈ H, we have that (h1N)(h2N) = h1h2N ∈ S and h−1\n\n1 N ∈ S.\nTherefore, H must be a subgroup of G. Clearly, H contains N . Therefore, S = H/N .\nConsequently, the map H 7→ H/N is onto.\n\nSuppose that H1 and H2 are subgroups of G containing N such that H1/N = H2/N .\nIf h1 ∈ H1, then h1N ∈ H1/N . Hence, h1N = h2N ⊂ H2 for some h2 in H2. However,\nsince N is contained in H2, we know that h1 ∈ H2 or H1 ⊂ H2. Similarly, H2 ⊂ H1. Since\nH1 = H2, the map H 7→ H/N is one-to-one.\n\nSuppose that H is normal in G and N is a subgroup of H. Then it is easy to verify\nthat the map G/N → G/H defined by gN 7→ gH is a homomorphism. The kernel of this\nhomomorphism is H/N , which proves that H/N is normal in G/N .\n\nConversely, suppose that H/N is normal in G/N . The homomorphism given by\n\nG→ G/N → G/N\n\nH/N\n\nhas kernel H. Hence, H must be normal in G.\n\nNotice that in the course of the proof of Theorem 11.13, we have also proved the following\ntheorem.\n\nTheorem 11.14 (Third Isomorphism Theorem). Let G be a group and N and H be normal\nsubgroups of G with N ⊂ H. Then\n\nG/H ∼=\nG/N\n\nH/N\n.\n\nExample 11.15. By the Third Isomorphism Theorem,\n\nZ/mZ ∼= (Z/mnZ)/(mZ/mnZ).\n\nSince |Z/mnZ| = mn and |Z/mZ| = m, we have |mZ/mnZ| = n.\n\n\n\n184 CHAPTER 11. HOMOMORPHISMS\n\n11.3 Exercises\n1. Prove that det(AB) = det(A)det(B) for A,B ∈ GL2(R). This shows that the determi-\nnant is a homomorphism from GL2(R) to R∗.\n\n2. Which of the following maps are homomorphisms? If the map is a homomorphism, what\nis the kernel?\n(a) ϕ : R∗ → GL2(R) defined by\n\nϕ(a) =\n\n(\n1 0\n\n0 a\n\n)\n(b) ϕ : R → GL2(R) defined by\n\nϕ(a) =\n\n(\n1 0\n\na 1\n\n)\n(c) ϕ : GL2(R) → R defined by\n\nϕ\n\n((\na b\n\nc d\n\n))\n= a+ d\n\n(d) ϕ : GL2(R) → R∗ defined by\n\nϕ\n\n((\na b\n\nc d\n\n))\n= ad− bc\n\n(e) ϕ : M2(R) → R defined by\n\nϕ\n\n((\na b\n\nc d\n\n))\n= b,\n\nwhere M2(R) is the additive group of 2× 2 matrices with entries in R.\n\n3. Let A be an m× n matrix. Show that matrix multiplication, x 7→ Ax, defines a homo-\nmorphism ϕ : Rn → Rm.\n\n4. Let ϕ : Z → Z be given by ϕ(n) = 7n. Prove that ϕ is a group homomorphism. Find\nthe kernel and the image of ϕ.\n\n5. Describe all of the homomorphisms from Z24 to Z18.\n\n6. Describe all of the homomorphisms from Z to Z12.\n\n7. In the group Z24, let H = ⟨4⟩ and N = ⟨6⟩.\n(a) List the elements in HN (we usually write H+N for these additive groups) and H∩N .\n(b) List the cosets in HN/N , showing the elements in each coset.\n(c) List the cosets in H/(H ∩N), showing the elements in each coset.\n(d) Give the correspondence between HN/N and H/(H ∩ N) described in the proof of\n\nthe Second Isomorphism Theorem.\n\n8. If G is an abelian group and n ∈ N, show that ϕ : G→ G defined by g 7→ gn is a group\nhomomorphism.\n\n9. If ϕ : G→ H is a group homomorphism and G is abelian, prove that ϕ(G) is also abelian.\n\n10. If ϕ : G→ H is a group homomorphism and G is cyclic, prove that ϕ(G) is also cyclic.\n\n\n\n11.4. ADDITIONAL EXERCISES: AUTOMORPHISMS 185\n\n11. Show that a homomorphism defined on a cyclic group is completely determined by its\naction on the generator of the group.\n\n12. If a group G has exactly one subgroup H of order k, prove that H is normal in G.\n\n13. Prove or disprove: Q/Z ∼= Q.\n\n14. Let G be a finite group and N a normal subgroup of G. If H is a subgroup of G/N ,\nprove that ϕ−1(H) is a subgroup in G of order |H|·|N |, where ϕ : G→ G/N is the canonical\nhomomorphism.\n\n15. Let G1 and G2 be groups, and let H1 and H2 be normal subgroups of G1 and G2\n\nrespectively. Let ϕ : G1 → G2 be a homomorphism. Show that ϕ induces a natural\nhomomorphism ϕ : (G1/H1) → (G2/H2) if ϕ(H1) ⊆ H2.\n\n16. If H and K are normal subgroups of G and H ∩K = {e}, prove that G is isomorphic\nto a subgroup of G/H ×G/K.\n\n17. Let ϕ : G1 → G2 be a surjective group homomorphism. Let H1 be a normal subgroup\nof G1 and suppose that ϕ(H1) = H2. Prove or disprove that G1/H1\n\n∼= G2/H2.\n\n18. Let ϕ : G → H be a group homomorphism. Show that ϕ is one-to-one if and only if\nϕ−1(e) = {e}.\n\n19. Given a homomorphism ϕ : G→ H define a relation ∼ on G by a ∼ b if ϕ(a) = ϕ(b) for\na, b ∈ G. Show this relation is an equivalence relation and describe the equivalence classes.\n\n11.4 Additional Exercises: Automorphisms\n1. Let Aut(G) be the set of all automorphisms of G; that is, isomorphisms from G to itself.\nProve this set forms a group and is a subgroup of the group of permutations of G; that is,\nAut(G) ≤ SG.\n\n2. An inner automorphism of G,\n\nig : G→ G,\n\nis defined by the map\nig(x) = gxg−1,\n\nfor g ∈ G. Show that ig ∈ Aut(G).\n\n3. The set of all inner automorphisms is denoted by Inn(G). Show that Inn(G) is a subgroup\nof Aut(G).\n\n4. Find an automorphism of a group G that is not an inner automorphism.\n\n5. Let G be a group and ig be an inner automorphism of G, and define a map\n\nG→ Aut(G)\n\nby\ng 7→ ig.\n\nProve that this map is a homomorphism with image Inn(G) and kernel Z(G). Use this\nresult to conclude that\n\nG/Z(G) ∼= Inn(G).\n\n\n\n186 CHAPTER 11. HOMOMORPHISMS\n\n6. Compute Aut(S3) and Inn(S3). Do the same thing for D4.\n\n7. Find all of the homomorphisms ϕ : Z → Z. What is Aut(Z)?\n\n8. Find all of the automorphisms of Z8. Prove that Aut(Z8) ∼= U(8).\n\n9. For k ∈ Zn, define a map ϕk : Zn → Zn by a 7→ ka. Prove that ϕk is a homomorphism.\n\n10. Prove that ϕk is an isomorphism if and only if k is a generator of Zn.\n\n11. Show that every automorphism of Zn is of the form ϕk, where k is a generator of Zn.\n\n12. Prove that ψ : U(n) → Aut(Zn) is an isomorphism, where ψ : k 7→ ϕk.\n\n11.5 Sage\nSage is able to create homomorphisms (and by extension, isomorphisms and automorphisms)\nbetween finite permutation groups. There is a limited supply of commands then available\nto manipulate these functions, but we can still illustrate many of the ideas in this chapter.\n\nHomomorphisms\nThe principal device for creating a homomorphism is to specify the specific images of the\nset of generators for the domain. Consider cyclic groups of order 12 and 20:\n\nG = {ai|a12 = e} H = {xi|x20 = e}\n\nand define a homomorphism by just defining the image of the generator of G, and define\nthe rest of the mapping by extending the mapping via the operation-preserving property of\na homomorphism.\n\nϕ : G→ H, ϕ(a) = x5\n\n⇒ ϕ(ai) = ϕ(a)i = (x5)i = x5i\n\nThe constructor PermutationGroupMorphism requires the two groups, then a list of images for\neach generator (in order!), and then will create the homomorphism. Note that we can then\nuse the result as a function. In the example below, we first verify that C12 has a single\ngenerator (no surprise there), which we then send to a particular element of order 4 in\nthe codomain. Sage then constructs the unique homomorphism that is consistent with this\nrequirement.\n\nC12 = CyclicPermutationGroup (12)\nC20 = CyclicPermutationGroup (20)\ndomain_gens = C12.gens()\n[g.order () for g in domain_gens]\n\n[12]\n\nx = C20.gen (0)\ny = x^5\ny.order()\n\n4\n\nphi = PermutationGroupMorphism(C12 , C20 , [y])\nphi\n\n\n\n11.5. SAGE 187\n\nPermutation group morphism:\nFrom: Cyclic group of order 12 as a permutation group\nTo: Cyclic group of order 20 as a permutation group\nDefn: [(1,2,3,4,5,6,7,8,9,10,11,12)] ->\n\n[(1 ,6,11,16)(2,7,12,17)(3,8,13,18)(4,9,14,19)(5,10,15,20)]\n\na = C12("(1,6,11,4,9,2,7,12,5,10,3,8)")\nphi(a)\n\n(1,6,11,16)(2,7,12,17)(3,8,13,18)(4,9,14,19)(5,10,15,20)\n\nb = C12("(1,3,5,7,9,11)(2,4,6,8,10,12)")\nphi(b)\n\n(1,11)(2 ,12)(3 ,13) (4,14)(5,15)(6,16)(7,17)(8,18)(9,19) (10 ,20)\n\nc = C12("(1,9,5)(2,10,6)(3,11,7)(4,12,8)")\nphi(c)\n\n()\n\nNote that the element c must therefore be in the kernel of phi.\nWe can then compute the subgroup of the domain that is the kernel, in this case a cyclic\n\ngroup of order 3 inside the cyclic group of order 12. We can compute the image of any\nsubgroup, but here we will build the whole homomorphic image by supplying the whole\ndomain to the .image() method. Here the image is a cyclic subgroup of order 4 inside the\ncyclic group of order 20. Then we can verify the First Isomorphism Theorem.\n\nK = phi.kernel (); K\n\nSubgroup of (Cyclic group of order 12 as a permutation group)\ngenerated by [(1,5,9)(2,6,10)(3,7,11)(4,8,12)]\n\nIm = phi.image(C12); Im\n\nSubgroup of (Cyclic group of order 20 as a permutation group)\ngenerated by\n\n[(1 ,6,11 ,16)(2,7,12,17)(3,8,13,18)(4,9,14,19)(5,10,15,20)]\n\nIm.is_isomorphic(C12.quotient(K))\n\nTrue\n\nHere is a slightly more complicated example. The dihedral group D20 is the symmetry\ngroup of a 20-gon. Inside this group is a subgroup that is isomorphic to the symmetry\ngroup of a 5-gon (pentagon). Is this a surprise, or is this obvious? Here is a way to make\nprecise the statement “D20 contains a copy of D5.”\n\nWe build the domain and find its generators, so we know how many images to supply in\nthe definition of the homomorphism. Then we construct the codomain, from which we will\nconstruct images. Our choice here is to send a reflection to a reflection, and a rotation to a\nrotation. But the rotations will both have order 5, and both are a rotation by 72 degrees.\n\nG = DihedralGroup (5)\nH = DihedralGroup (20)\nG.gens()\n\n\n\n188 CHAPTER 11. HOMOMORPHISMS\n\n[(1,2,3,4,5), (1,5)(2,4)]\n\nH.gens()\n\n[(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20),\n(1,20) (2,19) (3,18)(4,17)(5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11)]\n\nx = H.gen (0)^4\ny = H.gen (1)\nrho = PermutationGroupMorphism(G, H, [x, y])\nrho.kernel ()\n\nSubgroup of (Dihedral group of order 10 as a permutation group)\ngenerated by [()]\n\nSince the kernel is trivial, rho is a one-to-one function (see Exercise 11.3.18). But more\nimportantly, by the First Isomorphishm Theorem, G is isomorphic to the image of the\nhomomorphism. We compute the image and check the claim.\n\nIm = rho.image(G); Im\n\nSubgroup of (Dihedral group of order 40 as a permutation group)\ngenerated by\n[(1,5,9,13,17)(2,6,10,14,18)(3,7,11,15,19)(4,8,12,16,20),\n(1,20)(2 ,19)(3 ,18) (4,17)(5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11)]\n\nIm.is_subgroup(H)\n\nTrue\n\nIm.is_isomorphic(G)\n\nTrue\n\nJust providing a list of images for the generators of the domain is no guarantee that\nthe function will extend to a homomorphism. For starters, the order of each image must\ndivide the order of the corresponding preimage. (Can you prove this?) And similarly,\nif the domain is abelian, then the image must also be abelian, so in this case the list of\nimages should not generate a non-abelian subgroup. Here is an example. There are no\nhomomorphisms from a cyclic group of order 7 to a cyclic group of order 4 (other than the\ntrivial function that takes every element to the identity). To see this, consider the possible\norders of the kernel, and of the two possibilities, see that one is impossible and the other\narises with the trivial homomorphism. Unfortunately, Sage acts as if nothing is wrong in\ncreating a homomorphism between these groups, but what Sage builds is useless and raises\nerrors when you try to use it.\n\nG = CyclicPermutationGroup (7)\nH = CyclicPermutationGroup (4)\ntau = PermutationGroupMorphism_im_gens(G, H, H.gens())\ntau\n\nPermutation group morphism:\nFrom: Cyclic group of order 7 as a permutation group\nTo: Cyclic group of order 4 as a permutation group\nDefn: [(1,2,3,4,5,6,7)] -> [(1,2,3,4)]\n\n\n\n11.5. SAGE 189\n\ntau.kernel ()\n\nTraceback (most recent call last):\n...\nRuntimeError: Gap produced error output\n...\n\nRather than creating homomorphisms ourselves, in certain situations Sage knows of the\nexistence of natural homomorphisms and will create them for you. One such case is a direct\nproduct construction. Given a group G, the method .direct_product(H) will create the direct\nproduct G×H. (This is not the same command as the function direct_product_permgroups()\n\nfrom before.) Not only does this command create the direct product, but it also builds four\nhomomorphisms, one with domain G, one with domain H and two with domain G×H. So\nthe output consists of five objects, the first being the actual group, and the remainder are\nhomomorphisms. We will demonstrate the call here, and leave a more thorough investigation\nfor the exercises.\n\nG = CyclicPermutationGroup (3)\nH = DihedralGroup (4)\nresults = G.direct_product(H)\nresults [0]\n\nPermutation Group with generators [(4,5,6,7), (4,7)(5,6), (1,2,3)]\n\nresults [1]\n\nPermutation group morphism:\nFrom: Cyclic group of order 3 as a permutation group\nTo: Permutation Group with generators\n\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\nDefn: Embedding( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 1 )\n\nresults [2]\n\nPermutation group morphism:\nFrom: Dihedral group of order 8 as a permutation group\nTo: Permutation Group with generators\n\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\nDefn: Embedding( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 2 )\n\nresults [3]\n\nPermutation group morphism:\nFrom: Permutation Group with generators\n\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\nTo: Cyclic group of order 3 as a permutation group\nDefn: Projection( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 1 )\n\nresults [4]\n\nPermutation group morphism:\nFrom: Permutation Group with generators\n\n[(4,5,6,7), (4,7)(5,6), (1,2,3)]\nTo: Dihedral group of order 8 as a permutation group\nDefn: Projection( Group( [ (1,2,3), (4,5,6,7), (4,7)(5,6) ] ), 2 )\n\n\n\n190 CHAPTER 11. HOMOMORPHISMS\n\n11.6 Sage Exercises\n1. An automorphism is an isomorphism between a group and itself. The identity function\n(x 7→ x) is always an isomorphism, which we consider trivial. Use Sage to construct a\nnontrivial automorphism of the cyclic group of order 12. Check that the mapping is both\nonto and one-to-one by computing the image and kernel and performing the proper tests\non these subgroups. Now construct all of the possible automorphisms of the cyclic group of\norder 12 without any duplicates.\n\n2. The four homomorphisms created by the direct product construction are each an example\nof a more general construction of homomorphisms involving groups G, H and G × H.\nBy using the same groups as in the example in the previous subsection, see if you can\ndiscover and describe these constructions with exact definitions of the four homomorphisms\nin general.\nYour tools for investigating a Sage group homomorphism are limited, you might take each\ngenerator of the domain and see what its image is. Here is an example of the type of\ncomputation you might do repeatedly. We’ll investigate the second homomorphism. The\ndomain is the dihedral group, and we will compute the image of the first generator.\n\nG = CyclicPermutationGroup (3)\nH = DihedralGroup (4)\nresults = G.direct_product(H)\nphi = results [2]\nH.gens()\n\n[(1,2,3,4), (1,4)(2,3)]\n\na = H.gen (0); a\n\n(1,2,3,4)\n\nphi(a)\n\n(4,5,6,7)\n\n3. Consider two permutation groups. The first is the subgroup of S7 generated by (1, 2, 3)\nand (4, 5, 6, 7). The second is a subgroup of S12 generated by (1, 2, 3)(4, 5, 6)(7, 8, 9)(10, 11, 12)\nand (1, 10, 7, 4)(2, 11, 8, 5)(3, 12, 9, 6). Build these two groups and use the proper Sage com-\nmand to see that they are isomorphic. Then construct a homomorphism between these two\ngroups that is an isomorphism and include enough details to verify that the mapping is\nreally an isomorphism.\n\n4. The second paragraph of this chapter informally describes a homomorphism from Sn to\nZ2, where the even permutations all map to one of the elements and the odd permutations\nall map to the other element. Replace Sn by S6 and replace Z2 by the permutation version\nof the cyclic subgroup of order 2, and construct a nontrivial homomorphism between these\ntwo groups. Evaluate your homomorphism with enough even and odd permutations to be\nconvinced that it is correct. Then construct the kernel and verify that it is the group you\nexpect.\nHints: First, decide which elements of the group of order 2 will be associated with even\npermutations and which will be associated with odd permutations. Then examine the\ngenerators of S6 to help decide just how to build the homomorphism.\n\n\n\n11.6. SAGE EXERCISES 191\n\n5. The dihedral group D20 has several normal subgroups, as seen below. Each of these is\nthe kernel of a homomorphism with D20 as the domain. For each normal subgroup of D20\n\nconstruct a homomorphism from D20 to D20 that has the normal subgroup as the kernel.\nInclude in your work verifications that you are creating the desired kernels. There is a\npattern to many of these, but the three of order 20 will be a challenge.\n\nG = DihedralGroup (20)\n[H.order () for H in G.normal_subgroups ()]\n\n[1, 2, 4, 5, 10, 20, 20, 20, 40]\n\n\n\n12\n\nMatrix Groups and Symmetry\n\nWhen Felix Klein (1849–1925) accepted a chair at the University of Erlangen, he outlined in\nhis inaugural address a program to classify different geometries. Central to Klein’s program\nwas the theory of groups: he considered geometry to be the study of properties that are\nleft invariant under transformation groups. Groups, especially matrix groups, have now\nbecome important in the study of symmetry and have found applications in such disciplines\nas chemistry and physics. In the first part of this chapter, we will examine some of the\nclassical matrix groups, such as the general linear group, the special linear group, and the\northogonal group. We will then use these matrix groups to investigate some of the ideas\nbehind geometric symmetry.\n\n12.1 Matrix Groups\nSome Facts from Linear Algebra\nBefore we study matrix groups, we must recall some basic facts from linear algebra. One of\nthe most fundamental ideas of linear algebra is that of a linear transformation. A linear\ntransformation or linear map T : Rn → Rm is a map that preserves vector addition and\nscalar multiplication; that is, for vectors x and y in Rn and a scalar α ∈ R,\n\nT (x + y) = T (x) + T (y)\nT (αy) = αT (y).\n\nAn m × n matrix with entries in R represents a linear transformation from Rn to Rm. If\nwe write vectors x = (x1, . . . , xn)\n\nt and y = (y1, . . . , yn)\nt in Rn as column matrices, then an\n\nm× n matrix\n\nA =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\na11 a12 · · · a1n\na21 a22 · · · a2n\n...\n\n... . . . ...\nam1 am2 · · · amn\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\nmaps the vectors to Rm linearly by matrix multiplication. Observe that if α is a real number,\n\nA(x + y) = Ax +Ay and αAx = A(αx),\n\nwhere\n\nx =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\nx1\nx2\n...\nxn\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n192\n\n\n\n12.1. MATRIX GROUPS 193\n\nWe will often abbreviate the matrix A by writing (aij).\nConversely, if T : Rn → Rm is a linear map, we can associate a matrix A with T by\n\nconsidering what T does to the vectors\n\ne1 = (1, 0, . . . , 0)t\n\ne2 = (0, 1, . . . , 0)t\n\n...\nen = (0, 0, . . . , 1)t.\n\nWe can write any vector x = (x1, . . . , xn)\nt as\n\nx1e1 + x2e2 + · · ·+ xnen.\n\nConsequently, if\n\nT (e1) = (a11, a21, . . . , am1)\nt,\n\nT (e2) = (a12, a22, . . . , am2)\nt,\n\n...\nT (en) = (a1n, a2n, . . . , amn)\n\nt,\n\nthen\n\nT (x) = T (x1e1 + x2e2 + · · ·+ xnen)\n\n= x1T (e1) + x2T (e2) + · · ·+ xnT (en)\n\n=\n\n(\nn∑\n\nk=1\n\na1kxk, . . . ,\nn∑\n\nk=1\n\namkxk\n\n)t\n\n= Ax.\n\nExample 12.1. If we let T : R2 → R2 be the map given by\n\nT (x1, x2) = (2x1 + 5x2,−4x1 + 3x2),\n\nthe axioms that T must satisfy to be a linear transformation are easily verified. The column\nvectors Te1 = (2,−4)t and Te2 = (5, 3)t tell us that T is given by the matrix\n\nA =\n\n(\n2 5\n\n−4 3\n\n)\n.\n\nSince we are interested in groups of matrices, we need to know which matrices have\nmultiplicative inverses. Recall that an n × n matrix A is invertible exactly when there\nexists another matrix A−1 such that AA−1 = A−1A = I, where\n\nI =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n1 0 · · · 0\n\n0 1 · · · 0\n...\n\n... . . . ...\n0 0 · · · 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\nis the n×n identity matrix. From linear algebra we know that A is invertible if and only if\nthe determinant of A is nonzero. Sometimes an invertible matrix is said to be nonsingular.\n\n\n\n194 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\n\nExample 12.2. If A is the matrix (\n2 1\n\n5 3\n\n)\n,\n\nthen the inverse of A is\n\nA−1 =\n\n(\n3 −1\n\n−5 2\n\n)\n.\n\nWe are guaranteed that A−1 exists, since det(A) = 2 · 3− 5 · 1 = 1 is nonzero.\n\nSome other facts about determinants will also prove useful in the course of this chapter.\nLet A and B be n × n matrices. From linear algebra we have the following properties of\ndeterminants.\n\n• The determinant is a homomorphism into the multiplicative group of real numbers;\nthat is, det(AB) = (detA)(detB).\n\n• If A is an invertible matrix, then det(A−1) = 1/detA.\n\n• If we define the transpose of a matrix A = (aij) to be At = (aji), then det(At) = detA.\n\n• Let T be the linear transformation associated with an n × n matrix A. Then T\nmultiplies volumes by a factor of |detA|. In the case of R2, this means that T\nmultiplies areas by |detA|.\n\nLinear maps, matrices, and determinants are covered in any elementary linear algebra\ntext; however, if you have not had a course in linear algebra, it is a straightforward process\nto verify these properties directly for 2 × 2 matrices, the case with which we are most\nconcerned.\n\nThe General and Special Linear Groups\n\nThe set of all n × n invertible matrices forms a group called the general linear group.\nWe will denote this group by GLn(R). The general linear group has several important\nsubgroups. The multiplicative properties of the determinant imply that the set of matrices\nwith determinant one is a subgroup of the general linear group. Stated another way, suppose\nthat det(A) = 1 and det(B) = 1. Then det(AB) = det(A) det(B) = 1 and det(A−1) =\n1/ detA = 1. This subgroup is called the special linear group and is denoted by SLn(R).\n\nExample 12.3. Given a 2× 2 matrix\n\nA =\n\n(\na b\n\nc d\n\n)\n,\n\nthe determinant of A is ad− bc. The group GL2(R) consists of those matrices in which\nad− bc ̸= 0. The inverse of A is\n\nA−1 =\n1\n\nad− bc\n\n(\nd −b\n−c a\n\n)\n.\n\nIf A is in SL2(R), then\n\nA−1 =\n\n(\nd −b\n−c a\n\n)\n.\n\n\n\n12.1. MATRIX GROUPS 195\n\nGeometrically, SL2(R) is the group that preserves the areas of parallelograms. Let\n\nA =\n\n(\n1 1\n\n0 1\n\n)\nbe in SL2(R). In Figure 12.4, the unit square corresponding to the vectors x = (1, 0)t\n\nand y = (0, 1)t is taken by A to the parallelogram with sides (1, 0)t and (1, 1)t; that is,\nAx = (1, 0)t and Ay = (1, 1)t. Notice that these two parallelograms have the same area.\n\ny\n\nx\n\n(0, 1)\n\n(1, 0)\n\ny\n\nx\n\n(1, 1)\n\n(1, 0)\n\nFigure 12.4: SL2(R) acting on the unit square\n\nThe Orthogonal Group O(n)\n\nAnother subgroup of GLn(R) is the orthogonal group. A matrix A is orthogonal if A−1 =\nAt. The orthogonal group consists of the set of all orthogonal matrices. We write O(n)\nfor the n× n orthogonal group. We leave as an exercise the proof that O(n) is a subgroup\nof GLn(R).\n\nExample 12.5. The following matrices are orthogonal:\n\n(\n3/5 −4/5\n\n4/5 3/5\n\n)\n,\n\n(\n1/2 −\n\n√\n3/2√\n\n3/2 1/2\n\n)\n,\n\n\uf8eb\uf8ed−1/\n√\n2 0 1/\n\n√\n2\n\n1/\n√\n6 −2/\n\n√\n6 1/\n\n√\n6\n\n1/\n√\n3 1/\n\n√\n3 1/\n\n√\n3\n\n\uf8f6\uf8f8 .\n\nThere is a more geometric way of viewing the group O(n). The orthogonal matrices\nare exactly those matrices that preserve the length of vectors. We can define the length\nof a vector using the Euclidean inner product, or dot product, of two vectors. The\nEuclidean inner product of two vectors x = (x1, . . . , xn)\n\nt and y = (y1, . . . , yn)\nt is\n\n⟨x,y⟩ = xty = (x1, x2, . . . , xn)\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\ny1\ny2\n...\nyn\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 = x1y1 + · · ·+ xnyn.\n\nWe define the length of a vector x = (x1, . . . , xn)\nt to be\n\n∥x∥ =\n√\n\n⟨x,x⟩ =\n√\nx21 + · · ·+ x2n.\n\nAssociated with the notion of the length of a vector is the idea of the distance between two\nvectors. We define the distance between two vectors x and y to be ∥x − y∥. We leave as\nan exercise the proof of the following proposition about the properties of Euclidean inner\nproducts.\n\n\n\n196 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\n\nProposition 12.6. Let x, y, and w be vectors in Rn and α ∈ R. Then\n\n1. ⟨x,y⟩ = ⟨y,x⟩.\n\n2. ⟨x,y + w⟩ = ⟨x,y⟩+ ⟨x,w⟩.\n\n3. ⟨αx,y⟩ = ⟨x, αy⟩ = α⟨x,y⟩.\n\n4. ⟨x,x⟩ ≥ 0 with equality exactly when x = 0.\n\n5. If ⟨x,y⟩ = 0 for all x in Rn, then y = 0.\n\nExample 12.7. The vector x = (3, 4)t has length\n√\n32 + 42 = 5. We can also see that the\n\northogonal matrix\n\nA =\n\n(\n3/5 −4/5\n\n4/5 3/5\n\n)\npreserves the length of this vector. The vector Ax = (−7/5, 24/5)t also has length 5.\n\nSince det(AAt) = det(I) = 1 and det(A) = det(At), the determinant of any orthogonal\nmatrix is either 1 or −1. Consider the column vectors\n\naj =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\na1j\na2j\n...\nanj\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\nof the orthogonal matrix A = (aij). Since AAt = I, ⟨ar,as⟩ = δrs, where\n\nδrs =\n\n{\n1 r = s\n\n0 r ̸= s\n\nis the Kronecker delta. Accordingly, column vectors of an orthogonal matrix all have length\n1; and the Euclidean inner product of distinct column vectors is zero. Any set of vectors\nsatisfying these properties is called an orthonormal set. Conversely, given an n×n matrix\nA whose columns form an orthonormal set, it follows that A−1 = At.\n\nWe say that a matrixA is distance-preserving, length-preserving, or inner product-\npreserving when ∥Tx − Ty∥ = ∥x − y∥∥Tx∥ = ∥x∥, or ⟨Tx, Ty⟩ = ⟨x,y⟩, respectively.\nThe following theorem, which characterizes the orthogonal group, says that these notions\nare the same.\n\nTheorem 12.8. Let A be an n× n matrix. The following statements are equivalent.\n\n1. The columns of the matrix A form an orthonormal set.\n\n2. A−1 = At.\n\n3. For vectors x and y, ⟨Ax, Ay⟩ = ⟨x,y⟩.\n\n4. For vectors x and y, ∥Ax −Ay∥ = ∥x − y∥.\n\n5. For any vector x, ∥Ax∥ = ∥x∥.\n\n\n\n12.1. MATRIX GROUPS 197\n\nProof. We have already shown (1) and (2) to be equivalent.\n(2) ⇒ (3).\n\n⟨Ax, Ay⟩ = (Ax)tAy\n= xtAtAy\n= xty\n= ⟨x,y⟩.\n\n(3) ⇒ (2). Since\n⟨x,x⟩ = ⟨Ax, Ax⟩\n\n= xtAtAx\n= ⟨x, AtAx⟩,\n\nwe know that ⟨x, (AtA− I)x⟩ = 0 for all x. Therefore, AtA− I = 0 or A−1 = At.\n(3) ⇒ (4). If A is inner product-preserving, then A is distance-preserving, since\n\n∥Ax −Ay∥2 = ∥A(x − y)∥2\n\n= ⟨A(x − y), A(x − y)⟩\n= ⟨x − y,x − y⟩\n= ∥x − y∥2.\n\n(4) ⇒ (5). If A is distance-preserving, then A is length-preserving. Letting y = 0, we\nhave\n\n∥Ax∥ = ∥Ax −Ay∥ = ∥x − y∥ = ∥x∥.\n(5) ⇒ (3). We use the following identity to show that length-preserving implies inner\n\nproduct-preserving:\n⟨x,y⟩ = 1\n\n2\n\n[\n∥x + y∥2 − ∥x∥2 − ∥y∥2\n\n]\n.\n\nObserve that\n\n⟨Ax, Ay⟩ = 1\n\n2\n\n[\n∥Ax +Ay∥2 − ∥Ax∥2 − ∥Ay∥2\n\n]\n=\n\n1\n\n2\n\n[\n∥A(x + y)∥2 − ∥Ax∥2 − ∥Ay∥2\n\n]\n=\n\n1\n\n2\n\n[\n∥x + y∥2 − ∥x∥2 − ∥y∥2\n\n]\n= ⟨x,y⟩.\n\ny\n\nx\n\n(a, b)\n\n(a,−b)\n\ny\n\nx\n\n(cos θ, sin θ)\n\n(sin θ,− cos θ)\n\nθ\n\nFigure 12.9: O(2) acting on R2\n\n\n\n198 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\n\nExample 12.10. Let us examine the orthogonal group on R2 a bit more closely. An element\nT ∈ O(2) is determined by its action on e1 = (1, 0)t and e2 = (0, 1)t. If T (e1) = (a, b)t,\nthen a2 + b2 = 1 and T (e2) = (−b, a)t. Hence, T can be represented by\n\nA =\n\n(\na −b\nb a\n\n)\n=\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\n,\n\nwhere 0 ≤ θ < 2π. A matrix T in O(2) either reflects or rotates a vector in R2 (Figure 12.9).\nA reflection about the horizontal axis is given by the matrix(\n\n1 0\n\n0 −1\n\n)\n,\n\nwhereas a rotation by an angle θ in a counterclockwise direction must come from a matrix\nof the form (\n\ncos θ sin θ\nsin θ − cos θ\n\n)\n.\n\nA reflection about a line ℓ is simply a reflection about the horizontal axis followed by a\nrotation. If detA = −1, then A gives a reflection.\n\nTwo of the other matrix or matrix-related groups that we will consider are the special\northogonal group and the group of Euclidean motions. The special orthogonal group,\nSO(n), is just the intersection of O(n) and SLn(R); that is, those elements in O(n) with\ndeterminant one. The Euclidean group, E(n), can be written as ordered pairs (A,x),\nwhere A is in O(n) and x is in Rn. We define multiplication by\n\n(A,x)(B,y) = (AB,Ay + x).\nThe identity of the group is (I,0); the inverse of (A,x) is (A−1,−A−1x). In Exercise 12.3.6,\nyou are asked to check that E(n) is indeed a group under this operation.\n\ny\n\nx\n\nx\n\ny\n\nx\n\nx + y\n\nFigure 12.11: Translations in R2\n\n12.2 Symmetry\nAn isometry or rigid motion in Rn is a distance-preserving function f from Rn to Rn.\nThis means that f must satisfy\n\n∥f(x)− f(y)∥ = ∥x − y∥\n\nfor all x,y ∈ Rn. It is not difficult to show that f must be a one-to-one map. By Theo-\nrem 12.8, any element in O(n) is an isometry on Rn; however, O(n) does not include all\n\n\n\n12.2. SYMMETRY 199\n\npossible isometries on Rn. Translation by a vector x, Ty(x) = x + y is also an isometry\n(Figure 12.11); however, T cannot be in O(n) since it is not a linear map.\n\nWe are mostly interested in isometries in R2. In fact, the only isometries in R2 are\nrotations and reflections about the origin, translations, and combinations of the two. For\nexample, a glide reflection is a translation followed by a reflection (Figure 12.12). In Rn\n\nall isometries are given in the same manner. The proof is very easy to generalize.\n\ny\n\nx\n\nx\n\ny\n\nx\n\nT (x)\n\nFigure 12.12: Glide reflections\n\nLemma 12.13. An isometry f that fixes the origin in R2 is a linear transformation. In\nparticular, f is given by an element in O(2).\n\nProof. Let f be an isometry in R2 fixing the origin. We will first show that f preserves\ninner products. Since f(0) = 0, ∥f(x)∥ = ∥x∥; therefore,\n\n∥x∥2 − 2⟨f(x), f(y)⟩+ ∥y∥2 = ∥f(x)∥2 − 2⟨f(x), f(y)⟩+ ∥f(y)∥2\n\n= ⟨f(x)− f(y), f(x)− f(y)⟩\n= ∥f(x)− f(y)∥2\n\n= ∥x − y∥2\n\n= ⟨x − y,x − y⟩\n= ∥x∥2 − 2⟨x,y⟩+ ∥y∥2.\n\nConsequently,\n⟨f(x), f(y)⟩ = ⟨x,y⟩.\n\nNow let e1 and e2 be (1, 0)t and (0, 1)t, respectively. If\n\nx = (x1, x2) = x1e1 + x2e2,\n\nthen\nf(x) = ⟨f(x), f(e1)⟩f(e1) + ⟨f(x), f(e2)⟩f(e2) = x1f(e1) + x2f(e2).\n\nThe linearity of f easily follows.\n\nFor any arbitrary isometry, f , Txf will fix the origin for some vector x in R2; hence,\nTxf(y) = Ay for some matrix A ∈ O(2). Consequently, f(y) = Ay+x. Given the isometries\n\nf(y) = Ay + x1\n\ng(y) = By + x2,\n\n\n\n200 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\n\ntheir composition is\n\nf(g(y)) = f(By + x2) = ABy +Ax2 + x1.\n\nThis last computation allows us to identify the group of isometries on R2 with E(2).\n\nTheorem 12.14. The group of isometries on R2 is the Euclidean group, E(2).\n\nA symmetry group in Rn is a subgroup of the group of isometries on Rn that fixes a\nset of points X ⊂ R2. It is important to realize that the symmetry group of X depends\nboth on Rn and on X. For example, the symmetry group of the origin in R1 is Z2, but the\nsymmetry group of the origin in R2 is O(2).\n\nTheorem 12.15. The only finite symmetry groups in R2 are Zn and Dn.\n\nProof. Let G = {f1, f2, . . . , fn} be a finite symmetry group that fixes a set of points in\nX ⊂ R2. Choose a point x ∈ X. This point may not be a fixed point—it could be moved\nby G to another point in X. Define a set S = {y1,y2, . . .yn}, where yi = fi(x). Now, let\n\nz =\n1\n\nn\n\nn∑\ni=1\n\nxi.\n\nWhile the point z is not necessarily in the set X, it is fixed by every element in the symetry\ngroup. Without loss of generality, we may now assume that z is the origin.\n\nAny finite symmetry group G in R2 that fixes the origin must be a finite subgroup\nof O(2), since translations and glide reflections have infinite order. By Example 12.10,\nelements in O(2) are either rotations of the form\n\nRθ =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\nor reflections of the form\n\nTϕ =\n\n(\ncosϕ − sinϕ\nsinϕ cosϕ\n\n)(\n1 0\n\n0 −1\n\n)\n=\n\n(\ncosϕ sinϕ\nsinϕ − cosϕ\n\n)\n.\n\nNotice that det(Rθ) = 1, det(Tϕ) = −1, and T 2\nϕ = I. We can divide the proof up into two\n\ncases. In the first case, all of the elements in G have determinant one. In the second case,\nthere exists at least one element in G with determinant −1.\n\nCase 1. The determinant of every element in G is one. In this case every element in\nG must be a rotation. Since G is finite, there is a smallest angle, say θ0, such that the\ncorresponding element Rθ0 is the smallest rotation in the positive direction. We claim that\nRθ0 generates G. If not, then for some positive integer n there is an angle θ1 between nθ0\nand (n + 1)θ0. If so, then (n + 1)θ0 − θ1 corresponds to a rotation smaller than θ0, which\ncontradicts the minimality of θ0.\n\nCase 2. The group G contains a reflection T . The kernel of the homomorphism ϕ :\nG→ {−1, 1} given by A 7→ det(A) consists of elements whose determinant is 1. Therefore,\n|G/ kerϕ| = 2. We know that the kernel is cyclic by the first case and is a subgroup of G\nof, say, order n. Hence, |G| = 2n. The elements of G are\n\nRθ, . . . , R\nn−1\nθ , TRθ, . . . , TR\n\nn−1\nθ .\n\nThese elements satisfy the relation\n\nTRθT = R−1\nθ .\n\nConsequently, G must be isomorphic to Dn in this case.\n\n\n\n12.2. SYMMETRY 201\n\nThe Wallpaper Groups\n\nSuppose that we wish to study wallpaper patterns in the plane or crystals in three dimen-\nsions. Wallpaper patterns are simply repeating patterns in the plane (Figure 12.16). The\nanalogs of wallpaper patterns in R3 are crystals, which we can think of as repeating pat-\nterns of molecules in three dimensions (Figure 12.17). The mathematical equivalent of a\nwallpaper or crystal pattern is called a lattice.\n\nFigure 12.16: A wallpaper pattern in R2\n\nFigure 12.17: A crystal structure in R3\n\nLet us examine wallpaper patterns in the plane a little more closely. Suppose that x\nand y are linearly independent vectors in R2; that is, one vector cannot be a scalar multiple\nof the other. A lattice of x and y is the set of all linear combinations mx + ny, where m\nand n are integers. The vectors x and y are said to be a basis for the lattice.\n\nNotice that a lattice can have several bases. For example, the vectors (1, 1)t and (2, 0)t\n\nhave the same lattice as the vectors (−1, 1)t and (−1,−1)t (Figure 12.18). However, any\nlattice is completely determined by a basis. Given two bases for the same lattice, say\n{x1,x2} and {y1,y2}, we can write\n\ny1 = α1x1 + α2x2\n\ny2 = β1x1 + β2x2,\n\nwhere α1, α2, β1, and β2 are integers. The matrix corresponding to this transformation is\n\nU =\n\n(\nα1 α2\n\nβ1 β2\n\n)\n.\n\n\n\n202 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\n\nIf we wish to give x1 and x2 in terms of y1 and y2, we need only calculate U−1; that is,\n\nU−1\n\n(\ny1\n\ny2\n\n)\n=\n\n(\nx1\n\nx2\n\n)\n.\n\nSince U has integer entries, U−1 must also have integer entries; hence the determinants of\nboth U and U−1 must be integers. Because UU−1 = I,\n\ndet(UU−1) = det(U)det(U−1) = 1;\n\nconsequently, det(U) = ±1. A matrix with determinant ±1 and integer entries is called\nunimodular. For example, the matrix (\n\n3 1\n\n5 2\n\n)\nis unimodular. It should be clear that there is a minimum length for vectors in a lattice.\n\n(2, 0)\n\n(1, 1)(−1, 1)\n\n(−1,−1)\n\nFigure 12.18: A lattice in R2\n\nWe can classify lattices by studying their symmetry groups. The symmetry group of a\nlattice is the subgroup of E(2) that maps the lattice to itself. We consider two lattices in R2\n\nto be equivalent if they have the same symmetry group. Similarly, classification of crystals\nin R3 is accomplished by associating a symmetry group, called a space group, with each\ntype of crystal. Two lattices are considered different if their space groups are not the same.\nThe natural question that now arises is how many space groups exist.\n\nA space group is composed of two parts: a translation subgroup and a point. The\ntranslation subgroup is an infinite abelian subgroup of the space group made up of the\ntranslational symmetries of the crystal; the point group is a finite group consisting of ro-\ntations and reflections of the crystal about a point. More specifically, a space group is a\nsubgroup of G ⊂ E(2) whose translations are a set of the form {(I, t) : t ∈ L}, where L is a\nlattice. Space groups are, of course, infinite. Using geometric arguments, we can prove the\nfollowing theorem (see [5] or [6]).\n\nTheorem 12.19. Every translation group in R2 is isomorphic to Z× Z.\n\nThe point group of G is G0 = {A : (A, b) ∈ G for some b}. In particular, G0 must be a\nsubgroup of O(2). Suppose that x is a vector in a lattice L with space group G, translation\ngroup H, and point group G0. For any element (A,y) in G,\n\n(A,y)(I,x)(A,y)−1 = (A,Ax + y)(A−1,−A−1y)\n= (AA−1,−AA−1y +Ax + y)\n= (I,Ax);\n\n\n\n12.2. SYMMETRY 203\n\nhence, (I, Ax) is in the translation group of G. More specifically, Ax must be in the lattice\nL. It is important to note that G0 is not usually a subgroup of the space group G; however,\nif T is the translation subgroup of G, then G/T ∼= G0. The proof of the following theorem\ncan be found in [2], [5], or [6].\n\nTheorem 12.20. The point group in the wallpaper groups is isomorphic to Zn or Dn,\nwhere n = 1, 2, 3, 4, 6.\n\nTo answer the question of how the point groups and the translation groups can be\ncombined, we must look at the different types of lattices. Lattices can be classified by the\nstructure of a single lattice cell. The possible cell shapes are parallelogram, rectangular,\nsquare, rhombic, and hexagonal (Figure 12.21). The wallpaper groups can now be classified\naccording to the types of reflections that occur in each group: these are ordinarily reflections,\nglide reflections, both, or none.\n\nRectangular\nSquare Rhombic\n\nParallelogram\nHexagonal\n\nFigure 12.21: Types of lattices in R2\n\n\n\n204 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\n\nNotation and Reflections or\nSpace Groups Point Group Lattice Type Glide Reflections?\n\np1 Z1 parallelogram none\np2 Z2 parallelogram none\np3 Z3 hexagonal none\np4 Z4 square none\np6 Z6 hexagonal none\npm D1 rectangular reflections\npg D1 rectangular glide reflections\ncm D1 rhombic both\n\npmm D2 rectangular reflections\npmg D2 rectangular glide reflections\npgg D2 rectangular both\n\nc2mm D2 rhombic both\np3m1, p31m D3 hexagonal both\n\np4m, p4g D4 square both\np6m D6 hexagonal both\n\nTable 12.22: The 17 wallpaper groups\n\nTheorem 12.23. There are exactly 17 wallpaper groups.\n\np4m p4g\n\nFigure 12.24: The wallpaper groups p4m and p4g\n\nThe 17 wallpaper groups are listed in Table 12.22. The groups p3m1 and p31m can\nbe distinguished by whether or not all of their threefold centers lie on the reflection axes:\nthose of p3m1 must, whereas those of p31m may not. Similarly, the fourfold centers of p4m\nmust lie on the reflection axes whereas those of p4g need not (Figure 12.24). The complete\nproof of this theorem can be found in several of the references at the end of this chapter,\nincluding [5], [6], [10], and [11].\n\nHistorical Note\n\nSymmetry groups have intrigued mathematicians for a long time. Leonardo da Vinci\nwas probably the first person to know all of the point groups. At the International Congress\nof Mathematicians in 1900, David Hilbert gave a now-famous address outlining 23 problems\nto guide mathematics in the twentieth century. Hilbert’s eighteenth problem asked whether\nor not crystallographic groups in n dimensions were always finite. In 1910, L. Bieberbach\n\n\n\n12.3. EXERCISES 205\n\nproved that crystallographic groups are finite in every dimension. Finding out how many\nof these groups there are in each dimension is another matter. In R3 there are 230 different\nspace groups; in R4 there are 4783. No one has been able to compute the number of\nspace groups for R5 and beyond. It is interesting to note that the crystallographic groups\nwere found mathematically for R3 before the 230 different types of crystals were actually\ndiscovered in nature.\n\n12.3 Exercises\n1. Prove the identity\n\n⟨x,y⟩ = 1\n\n2\n\n[\n∥x + y∥2 − ∥x∥2 − ∥y∥2\n\n]\n.\n\n2. Show that O(n) is a group.\n\n3. Prove that the following matrices are orthogonal. Are any of these matrices in SO(n)?\n\n(a) (\n1/\n\n√\n2 −1/\n\n√\n2\n\n1/\n√\n2 1/\n\n√\n2\n\n)\n\n(b) (\n1/\n\n√\n5 2/\n\n√\n5\n\n−2/\n√\n5 1/\n\n√\n5\n\n)\n\n(c) \uf8eb\uf8ed 4/\n√\n5 0 3/\n\n√\n5\n\n−3/\n√\n5 0 4/\n\n√\n5\n\n0 −1 0\n\n\uf8f6\uf8f8\n(d) \uf8eb\uf8ed 1/3 2/3 −2/3\n\n−2/3 2/3 1/3\n\n−2/3 1/3 2/3\n\n\uf8f6\uf8f8\n4. Determine the symmetry group of each of the figures in Figure 12.25.\n\n(b)\n\n(a)\n(c)\n\nFigure 12.25\n\n5. Let x, y, and w be vectors in Rn and α ∈ R. Prove each of the following properties of\ninner products.\n(a) ⟨x,y⟩ = ⟨y,x⟩.\n(b) ⟨x,y + w⟩ = ⟨x,y⟩+ ⟨x,w⟩.\n(c) ⟨αx,y⟩ = ⟨x, αy⟩ = α⟨x,y⟩.\n(d) ⟨x,x⟩ ≥ 0 with equality exactly when x = 0.\n(e) If ⟨x,y⟩ = 0 for all x in Rn, then y = 0.\n\n\n\n206 CHAPTER 12. MATRIX GROUPS AND SYMMETRY\n\n6. Verify that\nE(n) = {(A,x) : A ∈ O(n) and x ∈ Rn}\n\nis a group.\n\n7. Prove that {(2, 1), (1, 1)} and {(12, 5), (7, 3)} are bases for the same lattice.\n\n8. Let G be a subgroup of E(2) and suppose that T is the translation subgroup of G. Prove\nthat the point group of G is isomorphic to G/T .\n\n9. Let A ∈ SL2(R) and suppose that the vectors x and y form two sides of a parallelogram\nin R2. Prove that the area of this parallelogram is the same as the area of the parallelogram\nwith sides Ax and Ay.\n\n10. Prove that SO(n) is a normal subgroup of O(n).\n\n11. Show that any isometry f in Rn is a one-to-one map.\n\n12. Prove or disprove: an element in E(2) of the form (A,x), where x ̸= 0, has infinite\norder.\n\n13. Prove or disprove: There exists an infinite abelian subgroup of O(n).\n\n14. Let x = (x1, x2) be a point on the unit circle in R2; that is, x21 + x22 = 1. If A ∈ O(2),\nshow that Ax is also a point on the unit circle.\n\n15. Let G be a group with a subgroup H (not necessarily normal) and a normal subgroup\nN . Then G is a semidirect product of N by H if\n\n• H ∩N = {id};\n• HN = G.\n\nShow that each of the following is true.\n(a) S3 is the semidirect product of A3 by H = {(1), (12)}.\n(b) The quaternion group, Q8, cannot be written as a semidirect product.\n(c) E(2) is the semidirect product of O(2) by H, where H consists of all translations in\n\nR2.\n\n16. Determine which of the 17 wallpaper groups preserves the symmetry of the pattern in\nFigure 12.16.\n\n17. Determine which of the 17 wallpaper groups preserves the symmetry of the pattern in\nFigure 12.26.\n\nFigure 12.26\n\n\n\n12.4. REFERENCES AND SUGGESTED READINGS 207\n\n18. Find the rotation group of a dodecahedron.\n\n19. For each of the 17 wallpaper groups, draw a wallpaper pattern having that group as a\nsymmetry group.\n\n12.4 References and Suggested Readings\n[1] Coxeter, H. M. and Moser, W. O. J. Generators and Relations for Discrete Groups,\n\n3rd ed. Springer-Verlag, New York, 1972.\n[2] Grove, L. C. and Benson, C. T. Finite Reflection Groups. 2nd ed. Springer-Verlag,\n\nNew York, 1985.\n[3] Hiller, H. “Crystallography and Cohomology of Groups,” American Mathematical\n\nMonthly 93 (1986), 765–79.\n[4] Lockwood, E. H. and Macmillan, R. H. Geometric Symmetry. Cambridge University\n\nPress, Cambridge, 1978.\n[5] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\n[6] Martin, G. Transformation Groups: An Introduction to Symmetry. Springer-Verlag,\n\nNew York, 1982.\n[7] Milnor, J. “Hilbert’s Problem 18: On Crystallographic Groups, Fundamental Do-\n\nmains, and Sphere Packing,” t Proceedings of Symposia in Pure Mathematics 18,\nAmerican Mathematical Society, 1976.\n\n[8] Phillips, F. C. An Introduction to Crystallography. 4th ed. Wiley, New York, 1971.\n[9] Rose, B. I. and Stafford, R. D. “An Elementary Course in Mathematical Symmetry,”\n\nAmerican Mathematical Monthly 88 (1980), 54–64.\n[10] Schattschneider, D. “The Plane Symmetry Groups: Their Recognition and Their\n\nNotation,” American Mathematical Monthly 85(1978), 439–50.\n[11] Schwarzenberger, R. L. “The 17 Plane Symmetry Groups,” Mathematical Gazette\n\n58(1974), 123–31.\n[12] Weyl, H. Symmetry. Princeton University Press, Princeton, NJ, 1952.\n\n12.5 Sage\nThere is no Sage material for this chapter.\n\n12.6 Sage Exercises\nThere are no Sage exercises for this chapter.\n\n\n\n13\n\nThe Structure of Groups\n\nThe ultimate goal of group theory is to classify all groups up to isomorphism; that is, given a\nparticular group, we should be able to match it up with a known group via an isomorphism.\nFor example, we have already proved that any finite cyclic group of order n is isomorphic\nto Zn; hence, we “know” all finite cyclic groups. It is probably not reasonable to expect\nthat we will ever know all groups; however, we can often classify certain types of groups or\ndistinguish between groups in special cases.\n\nIn this chapter we will characterize all finite abelian groups. We shall also investigate\ngroups with sequences of subgroups. If a group has a sequence of subgroups, say\n\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e},\n\nwhere each subgroup Hi is normal in Hi+1 and each of the factor groups Hi+1/Hi is abelian,\nthen G is a solvable group. In addition to allowing us to distinguish between certain classes\nof groups, solvable groups turn out to be central to the study of solutions to polynomial\nequations.\n\n13.1 Finite Abelian Groups\nIn our investigation of cyclic groups we found that every group of prime order was isomorphic\nto Zp, where p was a prime number. We also determined that Zmn\n\n∼= Zm × Zn when\ngcd(m,n) = 1. In fact, much more is true. Every finite abelian group is isomorphic to a\ndirect product of cyclic groups of prime power order; that is, every finite abelian group is\nisomorphic to a group of the type\n\nZp\nα1\n1\n\n× · · · × Zpαn\nn\n,\n\nwhere each pk is prime (not necessarily distinct).\nFirst, let us examine a slight generalization of finite abelian groups. Suppose that G\n\nis a group and let {gi} be a set of elements in G, where i is in some index set I (not\nnecessarily finite). The smallest subgroup of G containing all of the gi’s is the subgroup of\nG generated by the gi’s. If this subgroup of G is in fact all of G, then G is generated by\nthe set {gi : i ∈ I}. In this case the gi’s are said to be the generators of G. If there is a\nfinite set {gi : i ∈ I} that generates G, then G is finitely generated.\n\nExample 13.1. Obviously, all finite groups are finitely generated. For example, the group\nS3 is generated by the permutations (12) and (123). The group Z×Zn is an infinite group\nbut is finitely generated by {(1, 0), (0, 1)}.\n\nExample 13.2. Not all groups are finitely generated. Consider the rational numbers Q\nunder the operation of addition. Suppose that Q is finitely generated with generators\n\n208\n\n\n\n13.1. FINITE ABELIAN GROUPS 209\n\np1/q1, . . . , pn/qn, where each pi/qi is a fraction expressed in its lowest terms. Let p be some\nprime that does not divide any of the denominators q1, . . . , qn. We claim that 1/p cannot\nbe in the subgroup of Q that is generated by p1/q1, . . . , pn/qn, since p does not divide the\ndenominator of any element in this subgroup. This fact is easy to see since the sum of any\ntwo generators is\n\npi/qi + pj/qj = (piqj + pjqi)/(qiqj).\n\nProposition 13.3. Let H be the subgroup of a group G that is generated by {gi ∈ G : i ∈ I}.\nThen h ∈ H exactly when it is a product of the form\n\nh = gα1\ni1\n\n· · · gαn\nin\n,\n\nwhere the giks are not necessarily distinct.\n\nProof. Let K be the set of all products of the form gα1\ni1\n\n· · · gαn\nin\n\n, where the giks are not\nnecessarily distinct. Certainly K is a subset of H. We need only show that K is a subgroup\nof G. If this is the case, then K = H, since H is the smallest subgroup containing all the\ngis.\n\nClearly, the set K is closed under the group operation. Since g0i = 1, the identity is in\nK. It remains to show that the inverse of an element g = gk1i1 · · · gknin in K must also be in\nK. However,\n\ng−1 = (gk1i1 · · · gknin )\n−1 = (g−kn\n\nin\n· · · g−k1\n\ni1\n).\n\nThe reason that powers of a fixed gi may occur several times in the product is that we\nmay have a nonabelian group. However, if the group is abelian, then the gis need occur\nonly once. For example, a product such as a−3b5a7 in an abelian group could always be\nsimplified (in this case, to a4b5).\n\nNow let us restrict our attention to finite abelian groups. We can express any finite\nabelian group as a finite direct product of cyclic groups. More specifically, letting p be\nprime, we define a group G to be a p-group if every element in G has as its order a power\nof p. For example, both Z2 × Z2 and Z4 are 2-groups, whereas Z27 is a 3-group. We shall\nprove the Fundamental Theorem of Finite Abelian Groups which tells us that every finite\nabelian group is isomorphic to a direct product of cyclic p-groups.\n\nTheorem 13.4 (Fundamental Theorem of Finite Abelian Groups). Every finite abelian\ngroup G is isomorphic to a direct product of cyclic groups of the form\n\nZp\nα1\n1\n\n× Zp\nα2\n2\n\n× · · · × Zpαn\nn\n\nhere the pi’s are primes (not necessarily distinct).\n\nExample 13.5. Suppose that we wish to classify all abelian groups of order 540 = 22 ·33 ·5.\nThe Fundamental Theorem of Finite Abelian Groups tells us that we have the following six\npossibilities.\n\n• Z2 × Z2 × Z3 × Z3 × Z3 × Z5;\n\n• Z2 × Z2 × Z3 × Z9 × Z5;\n\n• Z2 × Z2 × Z27 × Z5;\n\n• Z4 × Z3 × Z3 × Z3 × Z5;\n\n• Z4 × Z3 × Z9 × Z5;\n\n\n\n210 CHAPTER 13. THE STRUCTURE OF GROUPS\n\n• Z4 × Z27 × Z5.\n\nThe proof of the Fundamental Theorem of Finite Abelian Groups depends on several\nlemmas.\n\nLemma 13.6. Let G be a finite abelian group of order n. If p is a prime that divides n,\nthen G contains an element of order p.\n\nProof. We will prove this lemma by induction. If n = 1, then there is nothing to show.\nNow suppose that the order of G is n the lemma is true for all groups of order k, where\nk < n. Furthermore, let p be a prime that divides n.\n\nIf G has no proper nontrivial subgroups, then G = ⟨a⟩, where a is any element other\nthan the identity. By Exercise 4.4.39, the order of G must be prime. Since p divides n, we\nknow that p = n, and G contains p− 1 elements of order p.\n\nNow suppose that G contains a nontrivial proper subgroup H. Then 1 < |H| < n. If\np | |H|, then H contains an element of order p by induction and the lemma is true. Suppose\nthat p does not divide the order of H. Since G is abelian, it must be the case that H is a\nnormal subgroup of G, and |G| = |H| · |G/H|. Consequently, p must divide |G/H|. Since\n|G/H| < |G| = n, we know that G/H contains an element aH of order p by the induction\nhypothesis. Thus,\n\nH = (aH)p = apH,\n\nand ap ∈ H but a /∈ H. If |H| = r, then p and r are relatively prime, and there exist\nintegers s and t such that sp + tr = 1. Furthermore, the order of ap must divide r, and\n(ap)r = (ar)p = 1.\n\nWe claim that ar has order p. We must show that ar ̸= 1. Suppose ar = 1. Then\n\na = asp+tr\n\n= aspatr\n\n= (ap)s(ar)t\n\n= (ap)s1\n\n= (ap)s.\n\nSince ap ∈ H, it must be the case that a = (ap)s ∈ H, which is a contradiction. Therefore,\nar ̸= 1 is an element of order p in G.\n\nLemma 13.6 is a special case of Cauchy’s Theorem (Theorem 15.1, which states that if\nG be a finite group and p a prime such that p divides the order of G, then G contains a\nsubgroup of order p. We will prove Cauchy’s Theorem in Chapter 15.\n\nLemma 13.7. A finite abelian group is a p-group if and only if its order is a power of p.\n\nProof. If |G| = pn then by Lagrange’s theorem, then the order of any g ∈ G must divide\npn, and therefore must be a power of p. Conversely, if |G| is not a power of p, then it has\nsome other prime divisor q, so by Lemma 13.6, G has an element of order q and thus is not\na p-group.\n\nLemma 13.8. Let G be a finite abelian group of order n = pα1\n1 · · · pαk\n\nk , where where p1, . . . , pk\nare distinct primes and α1, α2, . . . , αk are positive integers. Then G is the internal direct\nproduct of subgroups G1, G2, . . . , Gk, where Gi is the subgroup of G consisting of all elements\nof order pki for some integer k.\n\n\n\n13.1. FINITE ABELIAN GROUPS 211\n\nProof. Since G is an abelian group, we are guaranteed that Gi is a subgroup of G for\ni = 1, . . . , n. Since the identity has order p0i = 1, we know that 1 ∈ Gi. If g ∈ Gi has order\npri , then g−1 must also have order pri . Finally, if h ∈ Gi has order psi , then\n\n(gh)p\nt\ni = gp\n\nt\nihp\n\nt\ni = 1 · 1 = 1,\n\nwhere t is the maximum of r and s.\nWe must show that\n\nG = G1G2 · · ·Gn\n\nand Gi ∩ Gj = {1} for i ̸= j. Suppose that g1 ∈ G1 is in the subgroup generated by\nG2, G3, . . . , Gk. Then g1 = g2g3 · · · gk for gi ∈ Gi. Since gi has order pαi , we know that\ngp\n\nαi\n\ni = 1 for i = 2, 3, . . . , k, and g\np\nα2\n2 ···pαk\n\nk\n1 = 1. Since the order of g1 is a power of p1\n\nand gcd(p1, pα2\n2 · · · pαk\n\nk ) = 1, it must be the case that g1 = 1 and the intersection of G1\n\nwith any of the subgroups G2, G3, . . . , Gk is the identity. A similar argument shows that\nGi ∩ Gj = {1} for i ̸= j. Hence, G1G2 · · ·Gn is an internal direct product of subgroups.\nSince\n\n|G1G2 · · ·Gk| = pα1\n1 · · · pαk\n\nk = |G|,\n\nit follows that G = G1G2 · · ·Gk.\n\nIf remains for us to determine the possible structure of each pi-group Gi in Lemma 13.8.\n\nLemma 13.9. Let G be a finite abelian p-group and suppose that g ∈ G has maximal order.\nThen G is isomorphic to ⟨g⟩ ×H for some subgroup H of G.\n\nProof. By Lemma 13.7, we may assume that the order of G is pn. We shall induct on n.\nIf n = 1, then G is cyclic of order p and must be generated by g. Suppose now that the\nstatement of the lemma holds for all integers k with 1 ≤ k < n and let g be of maximal\norder in G, say |g| = pm. Then apm = e for all a ∈ G. Now choose h in G such that h /∈ ⟨g⟩,\nwhere h has the smallest possible order. Certainly such an h exists; otherwise, G = ⟨g⟩ and\nwe are done. Let H = ⟨h⟩.\n\nWe claim that ⟨g⟩ ∩H = {e}. It suffices to show that |H| = p. Since |hp| = |h|/p, the\norder of hp is smaller than the order of h and must be in ⟨g⟩ by the minimality of h; that\nis, hp = gr for some number r. Hence,\n\n(gr)p\nm−1\n\n= (hp)p\nm−1\n\n= hp\nm\n= e,\n\nand the order of gr must be less than or equal to pm−1. Therefore, gr cannot generate ⟨g⟩.\nNotice that p must occur as a factor of r, say r = ps, and hp = gr = gps. Define a to be\ng−sh. Then a cannot be in ⟨g⟩; otherwise, h would also have to be in ⟨g⟩. Also,\n\nap = g−sphp = g−rhp = h−php = e.\n\nWe have now formed an element a with order p such that a /∈ ⟨g⟩. Since h was chosen to\nhave the smallest order of all of the elements that are not in ⟨g⟩, |H| = p.\n\nNow we will show that the order of gH in the factor group G/H must be the same as\nthe order of g in G. If |gH| < |g| = pm, then\n\nH = (gH)p\nm−1\n\n= gp\nm−1\n\nH;\n\nhence, gpm−1 must be in ⟨g⟩ ∩ H = {e}, which contradicts the fact that the order of g is\npm. Therefore, gH must have maximal order in G/H. By the Correspondence Theorem\nand our induction hypothesis,\n\nG/H ∼= ⟨gH⟩ ×K/H\n\n\n\n212 CHAPTER 13. THE STRUCTURE OF GROUPS\n\nfor some subgroup K of G containing H. We claim that ⟨g⟩∩K = {e}. If b ∈ ⟨g⟩∩K, then\nbH ∈ ⟨gH⟩ ∩K/H = {H} and b ∈ ⟨g⟩ ∩H = {e}. It follows that G = ⟨g⟩K implies that\nG ∼= ⟨g⟩ ×K.\n\nThe proof of the Fundamental Theorem of Finite Abelian Groups follows very quickly\nfrom Lemma 13.9. Suppose that G is a finite abelian group and let g be an element of\nmaximal order in G. If ⟨g⟩ = G, then we are done; otherwise, G ∼= Z|g| × H for some\nsubgroup H contained in G by the lemma. Since |H| < |G|, we can apply mathematical\ninduction.\n\nWe now state the more general theorem for all finitely generated abelian groups. The\nproof of this theorem can be found in any of the references at the end of this chapter.\n\nTheorem 13.10 (The Fundamental Theorem of Finitely Generated Abelian Groups). Ev-\nery finitely generated abelian group G is isomorphic to a direct product of cyclic groups of\nthe form\n\nZp\nα1\n1\n\n× Zp\nα2\n2\n\n× · · · × Zpαn\nn\n\n× Z× · · · × Z,\n\nwhere the pi’s are primes (not necessarily distinct).\n\n13.2 Solvable Groups\nA subnormal series of a group G is a finite sequence of subgroups\n\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e},\n\nwhere Hi is a normal subgroup of Hi+1. If each subgroup Hi is normal in G, then the series\nis called a normal series. The length of a subnormal or normal series is the number of\nproper inclusions.\n\nExample 13.11. Any series of subgroups of an abelian group is a normal series. Consider\nthe following series of groups:\n\nZ ⊃ 9Z ⊃ 45Z ⊃ 180Z ⊃ {0},\nZ24 ⊃ ⟨2⟩ ⊃ ⟨6⟩ ⊃ ⟨12⟩ ⊃ {0}.\n\nExample 13.12. A subnormal series need not be a normal series. Consider the following\nsubnormal series of the group D4:\n\nD4 ⊃ {(1), (12)(34), (13)(24), (14)(23)} ⊃ {(1), (12)(34)} ⊃ {(1)}.\n\nThe subgroup {(1), (12)(34)} is not normal in D4; consequently, this series is not a normal\nseries.\n\nA subnormal (normal) series {Kj} is a refinement of a subnormal (normal) series\n{Hi} if {Hi} ⊂ {Kj}. That is, each Hi is one of the Kj .\n\nExample 13.13. The series\n\nZ ⊃ 3Z ⊃ 9Z ⊃ 45Z ⊃ 90Z ⊃ 180Z ⊃ {0}\n\nis a refinement of the series\n\nZ ⊃ 9Z ⊃ 45Z ⊃ 180Z ⊃ {0}.\n\n\n\n13.2. SOLVABLE GROUPS 213\n\nThe best way to study a subnormal or normal series of subgroups, {Hi} of G, is actually\nto study the factor groups Hi+1/Hi. We say that two subnormal (normal) series {Hi} and\n{Kj} of a group G are isomorphic if there is a one-to-one correspondence between the\ncollections of factor groups {Hi+1/Hi} and {Kj+1/Kj}.\n\nExample 13.14. The two normal series\n\nZ60 ⊃ ⟨3⟩ ⊃ ⟨15⟩ ⊃ {0}\nZ60 ⊃ ⟨4⟩ ⊃ ⟨20⟩ ⊃ {0}\n\nof the group Z60 are isomorphic since\n\nZ60/⟨3⟩ ∼= ⟨20⟩/{0} ∼= Z3\n\n⟨3⟩/⟨15⟩ ∼= ⟨4⟩/⟨20⟩ ∼= Z5\n\n⟨15⟩/{0} ∼= Z60/⟨4⟩ ∼= Z4.\n\nA subnormal series {Hi} of a group G is a composition series if all the factor groups\nare simple; that is, if none of the factor groups of the series contains a normal subgroup. A\nnormal series {Hi} of G is a principal series if all the factor groups are simple.\n\nExample 13.15. The group Z60 has a composition series\n\nZ60 ⊃ ⟨3⟩ ⊃ ⟨15⟩ ⊃ ⟨30⟩ ⊃ {0}\n\nwith factor groups\n\nZ60/⟨3⟩ ∼= Z3\n\n⟨3⟩/⟨15⟩ ∼= Z5\n\n⟨15⟩/⟨30⟩ ∼= Z2\n\n⟨30⟩/{0} ∼= Z2.\n\nSince Z60 is an abelian group, this series is automatically a principal series. Notice that a\ncomposition series need not be unique. The series\n\nZ60 ⊃ ⟨2⟩ ⊃ ⟨4⟩ ⊃ ⟨20⟩ ⊃ {0}\n\nis also a composition series.\n\nExample 13.16. For n ≥ 5, the series\n\nSn ⊃ An ⊃ {(1)}\n\nis a composition series for Sn since Sn/An\n∼= Z2 and An is simple.\n\nExample 13.17. Not every group has a composition series or a principal series. Suppose\nthat\n\n{0} = H0 ⊂ H1 ⊂ · · · ⊂ Hn−1 ⊂ Hn = Z\n\nis a subnormal series for the integers under addition. Then H1 must be of the form kZ\nfor some k ∈ N. In this case H1/H0\n\n∼= kZ is an infinite cyclic group with many nontrivial\nproper normal subgroups.\n\nAlthough composition series need not be unique as in the case of Z60, it turns out that\nany two composition series are related. The factor groups of the two composition series\nfor Z60 are Z2, Z2, Z3, and Z5; that is, the two composition series are isomorphic. The\nJordan-Hölder Theorem says that this is always the case.\n\n\n\n214 CHAPTER 13. THE STRUCTURE OF GROUPS\n\nTheorem 13.18 (Jordan-Hölder). Any two composition series of G are isomorphic.\n\nProof. We shall employ mathematical induction on the length of the composition series.\nIf the length of a composition series is 1, then G must be a simple group. In this case any\ntwo composition series are isomorphic.\n\nSuppose now that the theorem is true for all groups having a composition series of length\nk, where 1 ≤ k < n. Let\n\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}\nG = Km ⊃ Km−1 ⊃ · · · ⊃ K1 ⊃ K0 = {e}\n\nbe two composition series for G. We can form two new subnormal series for G since\nHi ∩Km−1 is normal in Hi+1 ∩Km−1 and Kj ∩Hn−1 is normal in Kj+1 ∩Hn−1:\n\nG = Hn ⊃ Hn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e}\nG = Km ⊃ Km−1 ⊃ Km−1 ∩Hn−1 ⊃ · · · ⊃ K0 ∩Hn−1 = {e}.\n\nSince Hi ∩ Km−1 is normal in Hi+1 ∩ Km−1, the Second Isomorphism Theorem (Theo-\nrem 11.12) implies that\n\n(Hi+1 ∩Km−1)/(Hi ∩Km−1) = (Hi+1 ∩Km−1)/(Hi ∩ (Hi+1 ∩Km−1))\n∼= Hi(Hi+1 ∩Km−1)/Hi,\n\nwhere Hi is normal in Hi(Hi+1 ∩ Km−1). Since {Hi} is a composition series, Hi+1/Hi\n\nmust be simple; consequently, Hi(Hi+1 ∩Km−1)/Hi is either Hi+1/Hi or Hi/Hi. That is,\nHi(Hi+1 ∩Km−1) must be either Hi or Hi+1. Removing any nonproper inclusions from the\nseries\n\nHn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e},\n\nwe have a composition series for Hn−1. Our induction hypothesis says that this series must\nbe equivalent to the composition series\n\nHn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}.\n\nHence, the composition series\n\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}\n\nand\nG = Hn ⊃ Hn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e}\n\nare equivalent. If Hn−1 = Km−1, then the composition series {Hi} and {Kj} are equivalent\nand we are done; otherwise, Hn−1Km−1 is a normal subgroup of G properly containing\nHn−1. In this case Hn−1Km−1 = G and we can apply the Second Isomorphism Theorem\nonce again; that is,\n\nKm−1/(Km−1 ∩Hn−1) ∼= (Hn−1Km−1)/Hn−1 = G/Hn−1.\n\nTherefore,\nG = Hn ⊃ Hn−1 ⊃ Hn−1 ∩Km−1 ⊃ · · · ⊃ H0 ∩Km−1 = {e}\n\nand\nG = Km ⊃ Km−1 ⊃ Km−1 ∩Hn−1 ⊃ · · · ⊃ K0 ∩Hn−1 = {e}\n\nare equivalent and the proof of the theorem is complete.\n\n\n\n13.3. EXERCISES 215\n\nA group G is solvable if it has a subnormal series {Hi} such that all of the factor groups\nHi+1/Hi are abelian. Solvable groups will play a fundamental role when we study Galois\ntheory and the solution of polynomial equations.\n\nExample 13.19. The group S4 is solvable since\n\nS4 ⊃ A4 ⊃ {(1), (12)(34), (13)(24), (14)(23)} ⊃ {(1)}\n\nhas abelian factor groups; however, for n ≥ 5 the series\n\nSn ⊃ An ⊃ {(1)}\n\nis a composition series for Sn with a nonabelian factor group. Therefore, Sn is not a solvable\ngroup for n ≥ 5.\n\n13.3 Exercises\n1. Find all of the abelian groups of order less than or equal to 40 up to isomorphism.\n\n2. Find all of the abelian groups of order 200 up to isomorphism.\n\n3. Find all of the abelian groups of order 720 up to isomorphism.\n\n4. Find all of the composition series for each of the following groups.\n\n(a) Z12\n\n(b) Z48\n\n(c) The quaternions, Q8\n\n(d) D4\n\n(e) S3 × Z4\n\n(f) S4\n\n(g) Sn, n ≥ 5\n\n(h) Q\n\n5. Show that the infinite direct product G = Z2 × Z2 × · · · is not finitely generated.\n\n6. Let G be an abelian group of order m. If n divides m, prove that G has a subgroup of\norder n.\n\n7. A group G is a torsion group if every element of G has finite order. Prove that a\nfinitely generated abelian torsion group must be finite.\n\n8. Let G, H, and K be finitely generated abelian groups. Show that if G ×H ∼= G ×K,\nthen H ∼= K. Give a counterexample to show that this cannot be true in general.\n\n9. Let G and H be solvable groups. Show that G×H is also solvable.\n\n10. If G has a composition (principal) series and if N is a proper normal subgroup of G,\nshow there exists a composition (principal) series containing N .\n\n11. Prove or disprove: Let N be a normal subgroup of G. If N and G/N have composition\nseries, then G must also have a composition series.\n\n12. Let N be a normal subgroup of G. If N and G/N are solvable groups, show that G is\nalso a solvable group.\n\n13. Prove that G is a solvable group if and only if G has a series of subgroups\n\nG = Pn ⊃ Pn−1 ⊃ · · · ⊃ P1 ⊃ P0 = {e}\n\nwhere Pi is normal in Pi+1 and the order of Pi+1/Pi is prime.\n\n\n\n216 CHAPTER 13. THE STRUCTURE OF GROUPS\n\n14. Let G be a solvable group. Prove that any subgroup of G is also solvable.\n\n15. Let G be a solvable group and N a normal subgroup of G. Prove that G/N is solvable.\n\n16. Prove that Dn is solvable for all integers n.\n\n17. Suppose that G has a composition series. If N is a normal subgroup of G, show that\nN and G/N also have composition series.\n\n18. Let G be a cyclic p-group with subgroups H and K. Prove that either H is contained\nin K or K is contained in H.\n\n19. Suppose that G is a solvable group with order n ≥ 2. Show that G contains a normal\nnontrivial abelian subgroup.\n\n20. Recall that the commutator subgroup G′ of a group G is defined as the subgroup\nof G generated by elements of the form a−1b−1ab for a, b ∈ G. We can define a series of\nsubgroups of G by G(0) = G, G(1) = G′, and G(i+1) = (G(i))′.\n(a) Prove that G(i+1) is normal in (G(i))′. The series of subgroups\n\nG(0) = G ⊃ G(1) ⊃ G(2) ⊃ · · ·\n\nis called the derived series of G.\n(b) Show that G is solvable if and only if G(n) = {e} for some integer n.\n\n21. Suppose that G is a solvable group with order n ≥ 2. Show that G contains a normal\nnontrivial abelian factor group.\n\n22. (Zassenhaus Lemma) Let H and K be subgroups of a group G. Suppose also that H∗\n\nand K∗ are normal subgroups of H and K respectively. Then\n(a) H∗(H ∩K∗) is a normal subgroup of H∗(H ∩K).\n(b) K∗(H∗ ∩K) is a normal subgroup of K∗(H ∩K).\n(c) H∗(H ∩K)/H∗(H ∩K∗) ∼= K∗(H ∩K)/K∗(H∗ ∩K) ∼= (H ∩K)/(H∗ ∩K)(H ∩K∗).\n\n23. (Schreier’s Theorem) Use the Zassenhaus Lemma to prove that two subnormal (nor-\nmal) series of a group G have isomorphic refinements.\n\n24. Use Schreier’s Theorem to prove the Jordan-Hölder Theorem.\n\n13.4 Programming Exercises\n1. Write a program that will compute all possible abelian groups of order n. What is the\nlargest n for which your program will work?\n\n13.5 References and Suggested Readings\n[1] Hungerford, T. W. Algebra. Springer, New York, 1974.\n[2] Lang, S. Algebra. 3rd ed. Springer, New York, 2002.\n[3] Rotman, J. J. An Introduction to the Theory of Groups. 4th ed. Springer, New York,\n\n1995.\n\n\n\n13.6. SAGE 217\n\n13.6 Sage\nCyclic groups, and direct products of cyclic groups, are implemented in Sage as permutation\ngroups. However, these groups quickly become very unwieldly representations and it should\nbe easier to work with finite abelian groups in Sage. So we will postpone any specifics for\nthis chapter until that happens. However, now that we understand the notion of isomorphic\ngroups and the structure of finite abelian groups, we can return to our quest to classify all\nof the groups with order less than 16.\n\nClassification of Finite Groups\nIt does not take any sophisticated tools to understand groups of order 2p, where p is an\nodd prime. There are two possibilities — a cyclic group of order 2p and the dihedral group\nof order 2p that is the set of symmetries of a regular p-gon. The proof requires some close,\ntight reasoning, but the required theorems are generally just concern orders of elements,\nLagrange’s Theorem and cosets. See Exercise 9.3.55. This takes care of orders n = 6, 10, 14.\n\nFor n = 9, the upcoming Corollary 14.16 will tell us that any group of order p2 (where\np is a prime) is abelian. So we know from this section that the only two possibilities are Z9\n\nand Z3 × Z3. Similarly, the upcoming Theorem 15.10 will tell us that every group of order\nn = 15 is abelian. Now this leaves just one possibility for this order: Z3 × Z5\n\n∼= Z15.\nWe have just two orders left to analyze: n = 8 and n = 12. The possibilities are\n\ngroups we already know, with one exception. However, the analysis that these are the only\npossibilities is more complicated, and will not be pursued now, nor in the next few chapters.\nNotice that n = 16 is more complicated still, with 14 different possibilities (which explains\nwhy we stopped here).\n\nFor n = 8 there are 3 abelian groups, and the two non-abelian groups are the dihedral\ngroup (symmetries of a square) and the quaternions.\n\nFor n = 12 there are 2 abelian groups, and 3 non-abelian groups. We know two of the\nnon-abelian groups as a dihedral group, and the alternating group on 4 symbols (which\nis also the symmetries of a tetrahedron). The third non-abelian group is an example of a\n“dicyclic” group, which is an infinite family of groups, each with order divisible by 4. The\norder 12 dicyclic group can also be constructed as a “semi-direct product” of two cyclic\ngroups — this is a construction worth knowing as you pursue further study of group theory.\nThe order 8 dicyclic group is also the quaternions and more generally, the dicyclic groups\nof order 2k, k > 2 are known as “generalized quaternion groups.”\n\nThe following examples will show you how to construct some of these groups, while also\nexercising a few of the commands and allowing us to be more certain the following table is\naccurate.\n\nS = SymmetricGroup (3)\nD = DihedralGroup (3)\nS.is_isomorphic(D)\n\nTrue\n\nC3 = CyclicPermutationGroup (3)\nC5 = CyclicPermutationGroup (5)\nDP = direct_product_permgroups ([C3, C5])\nC = CyclicPermutationGroup (15)\nDP.is_isomorphic(C)\n\nTrue\n\n\n\n218 CHAPTER 13. THE STRUCTURE OF GROUPS\n\nQ = QuaternionGroup ()\nDI = DiCyclicGroup (2)\nQ.is_isomorphic(DI)\n\nTrue\n\nGroups of Small Order as Permutation Groups\n\nWe list here constructions, as permutation groups in Sage, for all of the groups of order less\nthan 16.\n\n\n\n13.7. SAGE EXERCISES 219\n\nOrder Construction Notes, Alternatives\n1 CyclicPermutationGroup(1) Trivial\n2 CyclicPermutationGroup(2) SymmetricGroup(2)\n\n3 CyclicPermutationGroup(3) Prime order\n4 CyclicPermutationGroup(4) Cyclic\n4 KleinFourGroup() Abelian, non-cyclic\n5 CyclicPermutationGroup(5) Prime order\n6 CyclicPermutationGroup(6) Cyclic\n6 SymmetricGroup(3) Non-abelian\n\nDihedralGroup(3)\n\n7 CyclicPermutationGroup(7) Prime order\n8 CyclicPermutationGroup(8) Cyclic\n8 C2=CyclicPermutationGroup(2)\n\nC4=CyclicPermutationGroup(4)\n\nG=direct_product_permgroups([C2,C4]) Abelian, non-cyclic\n8 C2=CyclicPermutationGroup(2)\n\nG=direct_product_permgroups([C2,C2,C2]) Abelian, non-cyclic\n8 DihedralGroup(4) Non-abelian\n8 QuaternionGroup() Quaternions\n\nDiCyclicGroup(2)\n\n9 CyclicPermutationGroup(9) Cyclic\n9 C3=CyclicPermutationGroup(3)\n\nG=direct_product_permgroups([C3,C3]) Abelian, non-cyclic\n10 CyclicPermutationGroup(10) Cyclic\n10 DihedralGroup(5) Non-abelian\n11 CyclicPermutationGroup(11) Prime order\n12 CyclicPermutationGroup(12) Cyclic\n12 C2=CyclicPermutationGroup(2)\n\nC6=CyclicPermutationGroup(6)\n\nG=direct_product_permgroups([C2,C6]) Abelian, non-cyclic\n12 DihedralGroup(6) Non-abelian\n12 AlternatingGroup(4) Non-abelian\n\nSymmetries of tetrahedron\n12 DiCyclicGroup(3) Non-abelian\n\nSemi-direct product Z3 ⋊ Z4\n\n13 CyclicPermutationGroup(13) Prime order\n14 CyclicPermutationGroup(14) Cyclic\n14 DihedralGroup(7) Non-abelian\n15 CyclicPermutationGroup(15) Cyclic\n\nTable 13.20: The Groups of Order 16 or Less in Sage\n\n13.7 Sage Exercises\nThere are no Sage exercises for this chapter.\n\n\n\n14\n\nGroup Actions\n\nGroup actions generalize group multiplication. If G is a group and X is an arbitrary set, a\ngroup action of an element g ∈ G and x ∈ X is a product, gx, living in X. Many problems\nin algebra are best be attacked via group actions. For example, the proofs of the Sylow\ntheorems and of Burnside’s Counting Theorem are most easily understood when they are\nformulated in terms of group actions.\n\n14.1 Groups Acting on Sets\nLet X be a set and G be a group. A (left) action of G on X is a map G×X → X given\nby (g, x) 7→ gx, where\n\n1. ex = x for all x ∈ X;\n\n2. (g1g2)x = g1(g2x) for all x ∈ X and all g1, g2 ∈ G.\n\nUnder these considerations X is called a G-set. Notice that we are not requiring X to\nbe related to G in any way. It is true that every group G acts on every set X by the trivial\naction (g, x) 7→ x; however, group actions are more interesting if the set X is somehow\nrelated to the group G.\n\nExample 14.1. Let G = GL2(R) and X = R2. Then G acts on X by left multiplication.\nIf v ∈ R2 and I is the identity matrix, then Iv = v. If A and B are 2×2 invertible matrices,\nthen (AB)v = A(Bv) since matrix multiplication is associative.\n\nExample 14.2. Let G = D4 be the symmetry group of a square. If X = {1, 2, 3, 4}\nis the set of vertices of the square, then we can consider D4 to consist of the following\npermutations:\n\n{(1), (13), (24), (1432), (1234), (12)(34), (14)(23), (13)(24)}.\n\nThe elements of D4 act on X as functions. The permutation (13)(24) acts on vertex 1 by\nsending it to vertex 3, on vertex 2 by sending it to vertex 4, and so on. It is easy to see\nthat the axioms of a group action are satisfied.\n\nIn general, if X is any set and G is a subgroup of SX , the group of all permutations\nacting on X, then X is a G-set under the group action\n\n(σ, x) 7→ σ(x)\n\nfor σ ∈ G and x ∈ X.\n\n220\n\n\n\n14.1. GROUPS ACTING ON SETS 221\n\nExample 14.3. If we let X = G, then every group G acts on itself by the left regular\nrepresentation; that is, (g, x) 7→ λg(x) = gx, where λg is left multiplication:\n\ne · x = λex = ex = x\n\n(gh) · x = λghx = λgλhx = λg(hx) = g · (h · x).\n\nIf H is a subgroup of G, then G is an H-set under left multiplication by elements of H.\n\nExample 14.4. Let G be a group and suppose that X = G. If H is a subgroup of G, then\nG is an H-set under conjugation; that is, we can define an action of H on G,\n\nH ×G→ G,\n\nvia\n(h, g) 7→ hgh−1\n\nfor h ∈ H and g ∈ G. Clearly, the first axiom for a group action holds. Observing that\n\n(h1h2, g) = h1h2g(h1h2)\n−1\n\n= h1(h2gh\n−1\n2 )h−1\n\n1\n\n= (h1, (h2, g)),\n\nwe see that the second condition is also satisfied.\n\nExample 14.5. Let H be a subgroup of G and LH the set of left cosets of H. The set LH\n\nis a G-set under the action\n(g, xH) 7→ gxH.\n\nAgain, it is easy to see that the first axiom is true. Since (gg′)xH = g(g′xH), the second\naxiom is also true.\n\nIf G acts on a set X and x, y ∈ X, then x is said to be G-equivalent to y if there exists\na g ∈ G such that gx = y. We write x ∼G y or x ∼ y if two elements are G-equivalent.\n\nProposition 14.6. Let X be a G-set. Then G-equivalence is an equivalence relation on X.\n\nProof. The relation ∼ is reflexive since ex = x. Suppose that x ∼ y for x, y ∈ X. Then\nthere exists a g such that gx = y. In this case g−1y = x; hence, y ∼ x. To show that the\nrelation is transitive, suppose that x ∼ y and y ∼ z. Then there must exist group elements\ng and h such that gx = y and hy = z. So z = hy = (hg)x, and x is equivalent to z.\n\nIf X is a G-set, then each partition of X associated with G-equivalence is called an\norbit of X under G. We will denote the orbit that contains an element x of X by Ox.\n\nExample 14.7. Let G be the permutation group defined by\n\nG = {(1), (123), (132), (45), (123)(45), (132)(45)}\n\nand X = {1, 2, 3, 4, 5}. Then X is a G-set. The orbits are O1 = O2 = O3 = {1, 2, 3} and\nO4 = O5 = {4, 5}.\n\nNow suppose that G is a group acting on a set X and let g be an element of G. The\nfixed point set of g in X, denoted by Xg, is the set of all x ∈ X such that gx = x. We can\nalso study the group elements g that fix a given x ∈ X. This set is more than a subset of G,\nit is a subgroup. This subgroup is called the stabilizer subgroup or isotropy subgroup\nof x. We will denote the stabilizer subgroup of x by Gx.\n\n\n\n222 CHAPTER 14. GROUP ACTIONS\n\nRemark 14.8. It is important to remember that Xg ⊂ X and Gx ⊂ G.\n\nExample 14.9. Let X = {1, 2, 3, 4, 5, 6} and suppose that G is the permutation group\ngiven by the permutations\n\n{(1), (12)(3456), (35)(46), (12)(3654)}.\n\nThen the fixed point sets of X under the action of G are\n\nX(1) = X,\n\nX(35)(46) = {1, 2},\nX(12)(3456) = X(12)(3654) = ∅,\n\nand the stabilizer subgroups are\n\nG1 = G2 = {(1), (35)(46)},\nG3 = G4 = G5 = G6 = {(1)}.\n\nIt is easily seen that Gx is a subgroup of G for each x ∈ X.\n\nProposition 14.10. Let G be a group acting on a set X and x ∈ X. The stabilizer group\nof x, Gx, is a subgroup of G.\n\nProof. Clearly, e ∈ Gx since the identity fixes every element in the set X. Let g, h ∈ Gx.\nThen gx = x and hx = x. So (gh)x = g(hx) = gx = x; hence, the product of two elements\nin Gx is also in Gx. Finally, if g ∈ Gx, then x = ex = (g−1g)x = (g−1)gx = g−1x. So g−1\n\nis in Gx.\n\nWe will denote the number of elements in the fixed point set of an element g ∈ G by\n|Xg| and denote the number of elements in the orbit of x ∈ X by |Ox|. The next theorem\ndemonstrates the relationship between orbits of an element x ∈ X and the left cosets of Gx\n\nin G.\n\nTheorem 14.11. Let G be a finite group and X a finite G-set. If x ∈ X, then |Ox| = [G :\nGx].\n\nProof. We know that |G|/|Gx| is the number of left cosets of Gx in G by Lagrange’s\nTheorem (Theorem 6.10). We will define a bijective map ϕ between the orbit Ox of X and\nthe set of left cosets LGx of Gx in G. Let y ∈ Ox. Then there exists a g in G such that\ngx = y. Define ϕ by ϕ(y) = gGx. To show that ϕ is one-to-one, assume that ϕ(y1) = ϕ(y2).\nThen\n\nϕ(y1) = g1Gx = g2Gx = ϕ(y2),\n\nwhere g1x = y1 and g2x = y2. Since g1Gx = g2Gx, there exists a g ∈ Gx such that g2 = g1g,\n\ny2 = g2x = g1gx = g1x = y1;\n\nconsequently, the map ϕ is one-to-one. Finally, we must show that the map ϕ is onto. Let\ngGx be a left coset. If gx = y, then ϕ(y) = gGx.\n\n\n\n14.2. THE CLASS EQUATION 223\n\n14.2 The Class Equation\nLet X be a finite G-set and XG be the set of fixed points in X; that is,\n\nXG = {x ∈ X : gx = x for all g ∈ G}.\n\nSince the orbits of the action partition X,\n\n|X| = |XG|+\nn∑\n\ni=k\n\n|Oxi |,\n\nwhere xk, . . . , xn are representatives from the distinct nontrivial orbits of X.\nNow consider the special case in which G acts on itself by conjugation, (g, x) 7→ gxg−1.\n\nThe center of G,\nZ(G) = {x : xg = gx for all g ∈ G},\n\nis the set of points that are fixed by conjugation. The nontrivial orbits of the action are\ncalled the conjugacy classes of G. If x1, . . . , xk are representatives from each of the\nnontrivial conjugacy classes of G and |Ox1 | = n1, . . . , |Oxk\n\n| = nk, then\n\n|G| = |Z(G)|+ n1 + · · ·+ nk.\n\nThe stabilizer subgroups of each of the xi’s, C(xi) = {g ∈ G : gxi = xig}, are called the\ncentralizer subgroups of the xi’s. From Theorem 14.11, we obtain the class equation:\n\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)].\n\nOne of the consequences of the class equation is that the order of each conjugacy class must\ndivide the order of G.\n\nExample 14.12. It is easy to check that the conjugacy classes in S3 are the following:\n\n{(1)}, {(123), (132)}, {(12), (13), (23)}.\n\nThe class equation is 6 = 1 + 2 + 3.\n\nExample 14.13. The center of D4 is {(1), (13)(24)}, and the conjugacy classes are\n\n{(13), (24)}, {(1432), (1234)}, {(12)(34), (14)(23)}.\n\nThus, the class equation for D4 is 8 = 2 + 2 + 2 + 2.\n\nExample 14.14. For Sn it takes a bit of work to find the conjugacy classes. We begin\nwith cycles. Suppose that σ = (a1, . . . , ak) is a cycle and let τ ∈ Sn. By Theorem 6.16,\n\nτστ−1 = (τ(a1), . . . , τ(ak)).\n\nConsequently, any two cycles of the same length are conjugate. Now let σ = σ1σ2 · · ·σr be\na cycle decomposition, where the length of each cycle σi is ri. Then σ is conjugate to every\nother τ ∈ Sn whose cycle decomposition has the same lengths.\n\nThe number of conjugate classes in Sn is the number of ways in which n can be parti-\ntioned into sums of positive integers. In the case of S3 for example, we can partition the\ninteger 3 into the following three sums:\n\n3 = 1 + 1 + 1\n\n3 = 1 + 2\n\n3 = 3;\n\n\n\n224 CHAPTER 14. GROUP ACTIONS\n\ntherefore, there are three conjugacy classes. The problem of finding the number of such\npartitions for any positive integer n is what computer scientists call NP-complete. This\neffectively means that the problem cannot be solved for a large n because the computations\nwould be too time-consuming for even the largest computer.\n\nTheorem 14.15. Let G be a group of order pn where p is prime. Then G has a nontrivial\ncenter.\n\nProof. We apply the class equation\n\n|G| = |Z(G)|+ n1 + · · ·+ nk.\n\nSince each ni > 1 and ni | |G|, it follows that p must divide each ni. Also, p | |G|; hence, p\nmust divide |Z(G)|. Since the identity is always in the center of G, |Z(G)| ≥ 1. Therefore,\n|Z(G)| ≥ p, and there exists some g ∈ Z(G) such that g ̸= 1.\n\nCorollary 14.16. Let G be a group of order p2 where p is prime. Then G is abelian.\n\nProof. By Theorem 14.15, |Z(G)| = p or p2. If |Z(G)| = p2, then we are done. Suppose\nthat |Z(G)| = p. Then Z(G) and G/Z(G) both have order p and must both be cyclic\ngroups. Choosing a generator aZ(G) for G/Z(G), we can write any element gZ(G) in the\nquotient group as amZ(G) for some integer m; hence, g = amx for some x in the center of\nG. Similarly, if hZ(G) ∈ G/Z(G), there exists a y in Z(G) such that h = any for some\ninteger n. Since x and y are in the center of G, they commute with all other elements of G;\ntherefore,\n\ngh = amxany = am+nxy = anyamx = hg,\n\nand G must be abelian.\n\n14.3 Burnside’s Counting Theorem\nSuppose that we wish to color the vertices of a square with two different colors, say black and\nwhite. We might suspect that there would be 24 = 16 different colorings. However, some of\nthese colorings are equivalent. If we color the first vertex black and the remaining vertices\nwhite, it is the same as coloring the second vertex black and the remaining ones white since\nwe could obtain the second coloring simply by rotating the square 90◦ (Figure 14.17).\n\nB W\n\nW W\n\nW B\n\nW W\n\nW W\n\nB W\n\nW W\n\nW B\n\nFigure 14.17: Equivalent colorings of square\n\nBurnside’s Counting Theorem offers a method of computing the number of distinguish-\nable ways in which something can be done. In addition to its geometric applications, the\n\n\n\n14.3. BURNSIDE’S COUNTING THEOREM 225\n\ntheorem has interesting applications to areas in switching theory and chemistry. The proof\nof Burnside’s Counting Theorem depends on the following lemma.\n\nLemma 14.18. Let X be a G-set and suppose that x ∼ y. Then Gx is isomorphic to Gy.\nIn particular, |Gx| = |Gy|.\n\nProof. Let G act on X by (g, x) 7→ g · x. Since x ∼ y, there exists a g ∈ G such that\ng · x = y. Let a ∈ Gx. Since\n\ngag−1 · y = ga · g−1y = ga · x = g · x = y,\n\nwe can define a map ϕ : Gx → Gy by ϕ(a) = gag−1. The map ϕ is a homomorphism since\n\nϕ(ab) = gabg−1 = gag−1gbg−1 = ϕ(a)ϕ(b).\n\nSuppose that ϕ(a) = ϕ(b). Then gag−1 = gbg−1 or a = b; hence, the map is injective. To\nshow that ϕ is onto, let b be in Gy; then g−1bg is in Gx since\n\ng−1bg · x = g−1b · gx = g−1b · y = g−1 · y = x;\n\nand ϕ(g−1bg) = b.\n\nTheorem 14.19 (Burnside). Let G be a finite group acting on a set X and let k denote\nthe number of orbits of X. Then\n\nk =\n1\n\n|G|\n∑\ng∈G\n\n|Xg|.\n\nProof. We look at all the fixed points x of all the elements in g ∈ G; that is, we look at\nall g’s and all x’s such that gx = x. If viewed in terms of fixed point sets, the number of\nall g’s fixing x’s is ∑\n\ng∈G\n|Xg|.\n\nHowever, if viewed in terms of the stabilizer subgroups, this number is∑\nx∈X\n\n|Gx|;\n\nhence,\n∑\n\ng∈G |Xg| =\n∑\n\nx∈X |Gx|. By Lemma 14.18,∑\ny∈Ox\n\n|Gy| = |Ox| · |Gx|.\n\nBy Theorem 14.11 and Lagrange’s Theorem, this expression is equal to |G|. Summing over\nall of the k distinct orbits, we conclude that∑\n\ng∈G\n|Xg| =\n\n∑\nx∈X\n\n|Gx| = k · |G|.\n\n\n\n226 CHAPTER 14. GROUP ACTIONS\n\nExample 14.20. Let X = {1, 2, 3, 4, 5} and suppose that G is the permutation group\nG = {(1), (13), (13)(25), (25)}. The orbits of X are {1, 3}, {2, 5}, and {4}. The fixed point\nsets are\n\nX(1) = X\n\nX(13) = {2, 4, 5}\nX(13)(25) = {4}\n\nX(25) = {1, 3, 4}.\n\nBurnside’s Theorem says that\n\nk =\n1\n\n|G|\n∑\ng∈G\n\n|Xg| =\n1\n\n4\n(5 + 3 + 1 + 3) = 3.\n\nA Geometric Example\nBefore we apply Burnside’s Theorem to switching-theory problems, let us examine the\nnumber of ways in which the vertices of a square can be colored black or white. Notice\nthat we can sometimes obtain equivalent colorings by simply applying a rigid motion to the\nsquare. For instance, as we have pointed out, if we color one of the vertices black and the\nremaining three white, it does not matter which vertex was colored black since a rotation\nwill give an equivalent coloring.\n\nThe symmetry group of a square, D4, is given by the following permutations:\n\n(1) (13) (24) (1432)\n\n(1234) (12)(34) (14)(23) (13)(24)\n\nThe group G acts on the set of vertices {1, 2, 3, 4} in the usual manner. We can describe\nthe different colorings by mappings from X into Y = {B,W} where B and W represent\nthe colors black and white, respectively. Each map f : X → Y describes a way to color\nthe corners of the square. Every σ ∈ D4 induces a permutation σ̃ of the possible colorings\ngiven by σ̃(f) = f ◦ σ for f : X → Y . For example, suppose that f is defined by\n\nf(1) = B\n\nf(2) =W\n\nf(3) =W\n\nf(4) =W\n\nand σ = (12)(34). Then σ̃(f) = f ◦ σ sends vertex 2 to B and the remaining vertices to\nW . The set of all such σ̃ is a permutation group G̃ on the set of possible colorings. Let X̃\ndenote the set of all possible colorings; that is, X̃ is the set of all possible maps from X to\nY . Now we must compute the number of G̃-equivalence classes.\n\n1. X̃(1) = X̃ since the identity fixes every possible coloring. |X̃| = 24 = 16.\n\n2. X̃(1234) consists of all f ∈ X̃ such that f is unchanged by the permutation (1234). In\nthis case f(1) = f(2) = f(3) = f(4), so that all values of f must be the same; that is,\neither f(x) = B or f(x) =W for every vertex x of the square. So |X̃(1234)| = 2.\n\n3. |X̃(1432)| = 2.\n\n4. For X̃(13)(24), f(1) = f(3) and f(2) = f(4). Thus, |X̃(13)(24)| = 22 = 4.\n\n\n\n14.3. BURNSIDE’S COUNTING THEOREM 227\n\n5. |X̃(12)(34)| = 4.\n\n6. |X̃(14)(23)| = 4.\n\n7. For X̃(13), f(1) = f(3) and the other corners can be of any color; hence, |X̃(13)| =\n23 = 8.\n\n8. |X̃(24)| = 8.\n\nBy Burnside’s Theorem, we can conclude that there are exactly\n\n1\n\n8\n(24 + 21 + 22 + 21 + 22 + 22 + 23 + 23) = 6\n\nways to color the vertices of the square.\n\nProposition 14.21. Let G be a permutation group of X and X̃ the set of functions from\nX to Y . Then there exists a permutation group G̃ acting on X̃, where σ̃ ∈ G̃ is defined by\nσ̃(f) = f ◦ σ for σ ∈ G and f ∈ X̃. Furthermore, if n is the number of cycles in the cycle\ndecomposition of σ, then |X̃σ| = |Y |n.\n\nProof. Let σ ∈ G and f ∈ X̃. Clearly, f ◦ σ is also in X̃. Suppose that g is another\nfunction from X to Y such that σ̃(f) = σ̃(g). Then for each x ∈ X,\n\nf(σ(x)) = σ̃(f)(x) = σ̃(g)(x) = g(σ(x)).\n\nSince σ is a permutation of X, every element x′ in X is the image of some x in X under σ;\nhence, f and g agree on all elements of X. Therefore, f = g and σ̃ is injective. The map\nσ 7→ σ̃ is onto, since the two sets are the same size.\n\nSuppose that σ is a permutation of X with cycle decomposition σ = σ1σ2 · · ·σn. Any\nf in X̃σ must have the same value on each cycle of σ. Since there are n cycles and |Y |\npossible values for each cycle, |X̃σ| = |Y |n.\n\nExample 14.22. Let X = {1, 2, . . . , 7} and suppose that Y = {A,B,C}. If g is the\npermutation of X given by (13)(245) = (13)(245)(6)(7), then n = 4. Any f ∈ X̃g must\nhave the same value on each cycle in g. There are |Y | = 3 such choices for any value, so\n|X̃g| = 34 = 81.\n\nExample 14.23. Suppose that we wish to color the vertices of a square using four different\ncolors. By Proposition 14.21, we can immediately decide that there are\n\n1\n\n8\n(44 + 41 + 42 + 41 + 42 + 42 + 43 + 43) = 55\n\npossible ways.\n\nSwitching Functions\nIn switching theory we are concerned with the design of electronic circuits with binary\ninputs and outputs. The simplest of these circuits is a switching function that has n inputs\nand a single output (Figure 14.24). Large electronic circuits can often be constructed by\ncombining smaller modules of this kind. The inherent problem here is that even for a simple\ncircuit a large number of different switching functions can be constructed. With only four\ninputs and a single output, we can construct 65,536 different switching functions. However,\nwe can often replace one switching function with another merely by permuting the input\nleads to the circuit (Figure 14.25).\n\n\n\n228 CHAPTER 14. GROUP ACTIONS\n\nf f(x1, x2, . . . , xn)\n\nxn\n\nx2\n\nx1\n\n...\n\nFigure 14.24: A switching function of n variables\n\nWe define a switching or Boolean function of n variables to be a function from Zn\n2\n\nto Z2. Since any switching function can have two possible values for each binary n-tuple\nand there are 2n binary n-tuples, 22n switching functions are possible for n variables. In\ngeneral, allowing permutations of the inputs greatly reduces the number of different kinds\nof modules that are needed to build a large circuit.\n\nf f(a, b)\na\n\nb\nf f(b, a) = g(a, b)\n\na\n\nb\n\nFigure 14.25: A switching function of two variables\n\nThe possible switching functions with two input variables a and b are listed in Ta-\nble 14.26. Two switching functions f and g are equivalent if g can be obtained from f\nby a permutation of the input variables. For example, g(a, b, c) = f(b, c, a). In this case\ng ∼ f via the permutation (acb). In the case of switching functions of two variables, the\npermutation (ab) reduces 16 possible switching functions to 12 equivalent functions since\n\nf2 ∼ f4\n\nf3 ∼ f5\n\nf10 ∼ f12\n\nf11 ∼ f13.\n\nInputs Outputs\nf0 f1 f2 f3 f4 f5 f6 f7\n\n0 0 0 0 0 0 0 0 0 0\n0 1 0 0 0 0 1 1 1 1\n1 0 0 0 1 1 0 0 1 1\n1 1 0 1 0 1 0 1 0 1\nInputs Outputs\n\nf8 f9 f10 f11 f12 f13 f14 f15\n0 0 1 1 1 1 1 1 1 1\n0 1 0 0 0 0 1 1 1 1\n1 0 0 0 1 1 0 0 1 1\n1 1 0 1 0 1 0 1 0 1\n\nTable 14.26: Switching functions in two variables\n\nFor three input variables there are 22\n3\n= 256 possible switching functions; in the case\n\nof four variables there are 22\n4\n= 65,536. The number of equivalence classes is too large to\n\nreasonably calculate directly. It is necessary to employ Burnside’s Theorem.\n\n\n\n14.3. BURNSIDE’S COUNTING THEOREM 229\n\nConsider a switching function with three possible inputs, a, b, and c. As we have\nmentioned, two switching functions f and g are equivalent if a permutation of the input\nvariables of f gives g. It is important to notice that a permutation of the switching functions\nis not simply a permutation of the input values {a, b, c}. A switching function is a set of\noutput values for the inputs a, b, and c, so when we consider equivalent switching functions,\nwe are permuting 23 possible outputs, not just three input values. For example, each binary\ntriple (a, b, c) has a specific output associated with it. The permutation (acb) changes\noutputs as follows:\n\n(0, 0, 0) 7→ (0, 0, 0)\n\n(0, 0, 1) 7→ (0, 1, 0)\n\n(0, 1, 0) 7→ (1, 0, 0)\n\n...\n(1, 1, 0) 7→ (1, 0, 1)\n\n(1, 1, 1) 7→ (1, 1, 1).\n\nLet X be the set of output values for a switching function in n variables. Then |X| = 2n.\nWe can enumerate these values as follows:\n\n(0, . . . , 0, 1) 7→ 0\n\n(0, . . . , 1, 0) 7→ 1\n\n(0, . . . , 1, 1) 7→ 2\n\n...\n(1, . . . , 1, 1) 7→ 2n − 1.\n\nNow let us consider a circuit with four input variables and a single output. Suppose that\nwe can permute the leads of any circuit according to the following permutation group:\n\n(a) (ac) (bd) (adcb)\n\n(abcd) (ab)(cd) (ad)(bc) (ac)(bd).\n\nThe permutations of the four possible input variables induce the permutations of the output\nvalues in Table 14.27.\n\nHence, there are\n\n1\n\n8\n(216 + 2 · 212 + 2 · 26 + 3 · 210) = 9616\n\npossible switching functions of four variables under this group of permutations. This number\nwill be even smaller if we consider the full symmetric group on four letters.\n\n\n\n230 CHAPTER 14. GROUP ACTIONS\n\nGroup Number\nPermutation Switching Function Permutation of Cycles\n(a) (0) 16\n(ac) (2, 8)(3, 9)(6, 12)(7, 13) 12\n(bd) (1, 4)(3, 6)(9, 12)(11, 14) 12\n(adcb) (1, 2, 4, 8)(3, 6.12, 9)(5, 10)(7, 14, 13, 11) 6\n(abcd) (1, 8, 4, 2)(3, 9, 12, 6)(5, 10)(7, 11, 13, 14) 6\n(ab)(cd) (1, 2)(4, 8)(5, 10)(6, 9)(7, 11)(13, 14) 10\n(ad)(bc) (1, 8)(2, 4)(3, 12)(5, 10)(7, 14)(11, 13) 10\n(ac)(bd) (1, 4)(2, 8)(3, 12)(6, 9)(7, 13)(11, 14) 10\n\nTable 14.27: Permutations of switching functions in four variables\n\nHistorical Note\n\nWilliam Burnside was born in London in 1852. He attended Cambridge University from\n1871 to 1875 and won the Smith’s Prize in his last year. After his graduation he lectured\nat Cambridge. He was made a member of the Royal Society in 1893. Burnside wrote\napproximately 150 papers on topics in applied mathematics, differential geometry, and\nprobability, but his most famous contributions were in group theory. Several of Burnside’s\nconjectures have stimulated research to this day. One such conjecture was that every group\nof odd order is solvable; that is, for a group G of odd order, there exists a sequence of\nsubgroups\n\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e}\n\nsuch that Hi is normal in Hi+1 and Hi+1/Hi is abelian. This conjecture was finally proven\nby W. Feit and J. Thompson in 1963. Burnside’s The Theory of Groups of Finite Order,\npublished in 1897, was one of the first books to treat groups in a modern context as opposed\nto permutation groups. The second edition, published in 1911, is still a classic.\n\n14.4 Exercises\n1. Examples 14.1–14.5 in the first section each describe an action of a group G on a set X,\nwhich will give rise to the equivalence relation defined by G-equivalence. For each example,\ncompute the equivalence classes of the equivalence relation, the G-equivalence classes.\n\n2. Compute all Xg and all Gx for each of the following permutation groups.\n(a) X = {1, 2, 3},\n\nG = S3 = {(1), (12), (13), (23), (123), (132)}\n(b) X = {1, 2, 3, 4, 5, 6}, G = {(1), (12), (345), (354), (12)(345), (12)(354)}\n\n3. Compute the G-equivalence classes of X for each of the G-sets in Exercise 14.4.2. For\neach x ∈ X verify that |G| = |Ox| · |Gx|.\n\n4. Let G be the additive group of real numbers. Let the action of θ ∈ G on the real plane\nR2 be given by rotating the plane counterclockwise about the origin through θ radians. Let\nP be a point on the plane other than the origin.\n(a) Show that R2 is a G-set.\n(b) Describe geometrically the orbit containing P .\n\n\n\n14.4. EXERCISES 231\n\n(c) Find the group GP .\n\n5. Let G = A4 and suppose that G acts on itself by conjugation; that is, (g, h) 7→ ghg−1.\n\n(a) Determine the conjugacy classes (orbits) of each element of G.\n\n(b) Determine all of the isotropy subgroups for each element of G.\n\n6. Find the conjugacy classes and the class equation for each of the following groups.\n\n(a) S4 (b) D5 (c) Z9 (d) Q8\n\n7. Write the class equation for S5 and for A5.\n\n8. If a square remains fixed in the plane, how many different ways can the corners of the\nsquare be colored if three colors are used?\n\n9. How many ways can the vertices of an equilateral triangle be colored using three different\ncolors?\n\n10. Find the number of ways a six-sided die can be constructed if each side is marked\ndifferently with 1, . . . , 6 dots.\n\n11. Up to a rotation, how many ways can the faces of a cube be colored with three different\ncolors?\n\n12. Consider 12 straight wires of equal lengths with their ends soldered together to form\nthe edges of a cube. Either silver or copper wire can be used for each edge. How many\ndifferent ways can the cube be constructed?\n\n13. Suppose that we color each of the eight corners of a cube. Using three different colors,\nhow many ways can the corners be colored up to a rotation of the cube?\n\n14. Each of the faces of a regular tetrahedron can be painted either red or white. Up to a\nrotation, how many different ways can the tetrahedron be painted?\n\n15. Suppose that the vertices of a regular hexagon are to be colored either red or white.\nHow many ways can this be done up to a symmetry of the hexagon?\n\n16. A molecule of benzene is made up of six carbon atoms and six hydrogen atoms, linked\ntogether in a hexagonal shape as in Figure 14.28.\n\n(a) How many different compounds can be formed by replacing one or more of the hydrogen\natoms with a chlorine atom?\n\n(b) Find the number of different chemical compounds that can be formed by replacing\nthree of the six hydrogen atoms in a benzene ring with a CH3 radical.\n\n\n\n232 CHAPTER 14. GROUP ACTIONS\n\nH\n\nH\n\nHH\n\nH H\n\nFigure 14.28: A benzene ring\n\n17. How many equivalence classes of switching functions are there if the input variables x1,\nx2, and x3 can be permuted by any permutation in S3? What if the input variables x1, x2,\nx3, and x4 can be permuted by any permutation in S4?\n\n18. How many equivalence classes of switching functions are there if the input variables\nx1, x2, x3, and x4 can be permuted by any permutation in the subgroup of S4 generated\nby the permutation (x1x2x3x4)?\n\n19. A striped necktie has 12 bands of color. Each band can be colored by one of four\npossible colors. How many possible different-colored neckties are there?\n\n20. A group acts faithfully on a G-set X if the identity is the only element of G that\nleaves every element of X fixed. Show that G acts faithfully on X if and only if no two\ndistinct elements of G have the same action on each element of X.\n\n21. Let p be prime. Show that the number of different abelian groups of order pn (up to\nisomorphism) is the same as the number of conjugacy classes in Sn.\n\n22. Let a ∈ G. Show that for any g ∈ G, gC(a)g−1 = C(gag−1).\n\n23. Let |G| = pn and suppose that |Z(G)| = pn−1 for p prime. Prove that G is abelian.\n\n24. Let G be a group with order pn where p is prime and X a finite G-set. If XG = {x ∈\nX : gx = x for all g ∈ G} is the set of elements in X fixed by the group action, then prove\nthat |X| ≡ |XG| (mod p).\n\n25. If G is a group of order pn, where p is prime and n ≥ 2, show that G must have a\nproper subgroup of order p. If n ≥ 3, is it true that G will have a proper subgroup of order\np2?\n\n14.5 Programming Exercise\n1. Write a program to compute the number of conjugacy classes in Sn. What is the largest\nn for which your program will work?\n\n14.6 References and Suggested Reading\n[1] De Bruijin, N. G. “Pólya’s Theory of Counting,” in Applied Combinatorial Mathemat-\n\nics, Beckenbach, E. F., ed. Wiley, New York, 1964.\n\n\n\n14.7. SAGE 233\n\n[2] Eidswick, J. A. “Cubelike Puzzles—What Are They and How Do You Solve Them?”\nAmerican Mathematical Monthly 93(1986), 157–76.\n\n[3] Harary, F., Palmer, E. M., and Robinson, R. W. “Pólya’s Contributions to Chem-\nical Enumeration,” in Chemical Applications of Graph Theory, Balaban, A. T., ed.\nAcademic Press, London, 1976.\n\n[4] Gårding, L. and Tambour, T. Algebra for Computer Science. Springer-Verlag, New\nYork, 1988.\n\n[5] Laufer, H. B. Discrete Mathematics and Applied Modern Algebra. PWS-Kent, Boston,\n1984.\n\n[6] Pólya, G. and Read, R. C. Combinatorial Enumeration of Groups, Graphs, and Chem-\nical Compounds. Springer-Verlag, New York, 1985.\n\n[7] Shapiro, L. W. “Finite Groups Acting on Sets with Applications,” Mathematics Mag-\nazine, May–June 1973, 136–47.\n\n14.7 Sage\nGroups can be realized in many ways, such as as sets of permutations, as sets of matrices,\nor as sets of abstract symbols related by certain rules (“presentations”) and in myriad other\nways. We have concentrated on permutation groups because of their concrete feel, with\nelements written as functions, and because of their thorough implementation in Sage. Group\nactions are of great interest when the set they act on is the group itself, and group actions\nwill figure prominently in the proofs of the main results of the next chapter. However, any\ntime we have a group action on a set, we can view that group as a permutation group on\nthe elements of the set. So permutation groups are an area of group theory of independent\ninterest, with its own definitions and theorems.\n\nWe will describe Sage’s commands applicable when a group action arises naturally via\nconjugation, and then move into the more general situation in a more general application.\n\nConjugation as a Group Action\nWe might think we need to be careful how Sage defines conjugation (gxg−1 versus g−1xg)\nand the difference between Sage and the text on the order of products. However, if you look\nat the definition of the center and centralizer subgroups you can see that any difference in\nordering is irrelevant. Here are the group action commands for the particular action that\nis conjugation of the elements of the group.\n\nSage has a permutation group method .center() which returns the subgroup of fixed\npoints. The permutation group method, .centralizer(g), returns a subgroup that is the\nstabilizer of the group element g. Finally, the orbits are given by conjugacy classes, but\nSage will not flood you with the full conjugacy classes and instead gives back a list of\none element per conjugacy class, the representatives, via the permutation group method\n.conjugacy_classes_representatives(). You can manually reconstruct a conjugacy class\nfrom a representative, as we do in the example below.\n\nHere is an example of the above commands in action. Notice that an abelian group\nwould be a bad choice for this example.\n\nD = DihedralGroup (8)\nC = D.center (); C\n\nSubgroup of (Dihedral group of order 16 as a permutation group)\ngenerated by [(1,5)(2,6)(3,7)(4,8)]\n\n\n\n234 CHAPTER 14. GROUP ACTIONS\n\nC.list()\n\n[(), (1,5)(2,6)(3,7)(4,8)]\n\na = D("(1,2)(3,8)(4,7)(5,6)")\nC1 = D.centralizer(a); C1.list()\n\n[(), (1,2)(3,8)(4,7)(5,6), (1,5)(2,6)(3,7)(4,8),\n(1,6)(2,5)(3,4)(7,8)]\n\nb = D("(1,2,3,4,5,6,7,8)")\nC2 = D.centralizer(b); C2.order()\n\n8\n\nCCR = D.conjugacy_classes_representatives (); CCR\n\n[(), (2,8)(3,7)(4,6), (1,2)(3,8)(4,7)(5,6), (1,2,3,4,5,6,7,8),\n(1,3,5,7)(2,4,6,8), (1,4,7,2,5,8,3,6), (1,5)(2,6)(3,7)(4,8)]\n\nr = CCR [2]; r\n\n(1,2)(3,8)(4,7)(5,6)\n\nconj = []\nx = [conj.append(g^-1*r*g) for g in D if not g^-1*r*g in conj]\nconj\n\n[(1 ,2)(3,8)(4,7)(5,6), (1,4)(2,3)(5,8)(6,7),\n(1,6)(2,5)(3,4)(7,8), (1,8)(2,7)(3,6)(4,5)]\n\nNotice that in the one conjugacy class constructed all the elements have the same cycle\nstructure, which is no accident. Notice too that rep and a are the same element, and the\nproduct of the order of the centralizer (4) and the size of the conjugacy class (4) equals the\norder of the group (16), which is a variant of the conclusion of Theorem 14.11.\n\nVerify that the following is a demonstration of the class equation in the special case\nwhen the action is conjugation, but would be valid for any group, rather than just D.\n\nsizes = [D.order()/D.centralizer(g).order()\nfor g in D.conjugacy_classes_representatives ()]\n\nsizes\n\n[1, 4, 4, 2, 2, 2, 1]\n\nD.order() == sum(sizes)\n\nTrue\n\n\n\n14.7. SAGE 235\n\nGraph Automorphisms\nAs mentioned, group actions can be even more interesting when the set they act on is\ndifferent from the group itself. One class of examples is the group of symmetries of a\ngeometric solid, where the objects in the set are the vertices of the object, or perhaps\nsome other aspect such as edges, faces or diagonals. In this case, the group is all those\npermutations that move the solid but leave it filling the same space before the motion\n(“rigid motions”).\n\nIn this section we will examine something very similar. A graph is a mathematical\nobject, consisting of vertices and edges, but the only structure is whether or not any given\npair of vertices are joined by an edge or not. The group consists of permutations of vertices\nthat preserve the structure, that is, permutations of vertices that take edges to edges and\nnon-edges to non-edges. It is very similar to a symmetry group, but there is no notion of\nany geometric relationships being preserved.\n\nHere is an example. You will need to run the first compute cell to define the graph and\nget a nice graphic representation.\n\nQ = graphs.CubeGraph (3)\nQ.plot(layout= \' spring \' )\n\nA = Q.automorphism_group ()\nA.order()\n\n48\n\nYour plot should look like the vertices and edges of a cube, but may not quite look\nregular, which is fine, since the geometry is not relevant. Vertices are labeled with strings\nof three binary digits, 0 or 1, and any two vertices are connected by an edge if their strings\ndiffer in exactly one location. We might expect the group of symmetries to have order 24,\nrather than order 48, given its resemblance to a cube (in appearance and in name). However,\nwhen not restricted to rigid motions, we have new permutations that preserve edges. One\nin particular is to interchange two “opposite faces.” Locate two 4-cycles opposite of each\nother, listed in the same order: 000, 010, 110, 100 and 001, 011, 111, 101. Notice that each\ncycle looks very similar, but all the vertices of the first end in a zero and the second cycle\nhas vertices ending in a one.\n\nWe can create explicitly the permutation that interchanges these two opposite faces,\nusing a text version of the permutation in cycle notation.\n\na = A(" ( \'000 \' , \'001 \') ( \'010 \' , \'011 \') ( \'110 \' , \'111 \') ( \'100 \' , \'101 \') ")\na in A\n\nTrue\n\nWe can use this group to illustrate the relevant Sage commands for group actions.\nA.orbits ()\n\n[[ \' 000 \' , \' 001 \' , \' 010 \' , \' 100 \' , \' 011 \' , \' 101 \' , \' 110 \' , \' 111 \' ]]\n\nSo this action has only one (big) orbit. This implies that every vertex is “like” any\nother. When a permutation group behaves this way, we say the group is transitive.\n\nA.is_transitive ()\n\nTrue\n\n\n\n236 CHAPTER 14. GROUP ACTIONS\n\nIf every vertex is “the same” we can compute the stabilizer of any vertex, since they\nwill all be isomorphic. Because vertex 000 is the simplest in some sense, we compute its\nstabilizer.\n\nS = A.stabilizer( \' 000 \' )\nS.list()\n\n[(),\n( \' 001 \' , \' 100 \' , \' 010 \' )( \' 011 \' , \' 101 \' , \' 110 \' ),\n( \' 010 \' , \' 100 \' )( \' 011 \' , \' 101 \' ),\n( \' 001 \' , \' 010 \' , \' 100 \' )( \' 011 \' , \' 110 \' , \' 101 \' ),\n( \' 001 \' , \' 100 \' )( \' 011 \' , \' 110 \' ),\n( \' 001 \' , \' 010 \' )( \' 101 \' , \' 110 \' )]\n\nThat S has 6 elements is no surprise, since the group has order 48 and the size of the\nlone orbit is 8. But we can go one step further. The three vertices of the graph attached\ndirectly to 000 are 100, 010, 001. Any automorphism of the graph that fixes 000 must then\npermute the three adjacent vertices. There are 3! = 6 possible ways to do this, and you can\ncheck that each appears in one of the six elements of the stabilizer. So we can understand a\ntransitive group by considering the smaller stabilizer, and in this case we can see that each\nelement of the stabilizer is determined by how it permutes the neighbors of the stabilized\nvertex.\n\nTransitive groups are both unusual and important. To contrast, here is a graph auto-\nmorphism group that is far from transitive (without being trivial). A path is a graph that\nhas all of its vertices in a line. Run the first compute cell to see a path on 11 vertices.\n\nP = graphs.PathGraph (11)\nP.plot()\n\nA = P.automorphism_group ()\nA.list()\n\n[(), (0,10)(1,9)(2,8)(3,7)(4,6)]\n\nThe automorphism group is the trivial identity automorphism (always) and an order 2\npermutation that “flips” the path end-to-end. The group is far from transitive and there\nare many orbits.\n\nA.is_transitive ()\n\nFalse\n\nA.orbits ()\n\n[[0, 10], [1, 9], [2, 8], [3, 7], [4, 6], [5]]\n\nMost of the stabilizers are trivial, with one exception. As subgroups of a group of order\n2, there really are not too many options.\n\nA.stabilizer (2).list()\n\n[()]\n\nA.stabilizer (5).list()\n\n[(), (0,10)(1,9)(2,8)(3,7)(4,6)]\n\n\n\n14.8. SAGE EXERCISES 237\n\nHow would this final example have been different if we had used a path on 10 vertices?\nNOTE: There was once a small bug with stabilizers being created as subgroups of\n\nsymmetric groups on fewer symbols than the correct number. This is fixed in Sage 4.8 and\nnewer. Note the correct output below, and you can check your installation by running these\ncommands. If you do not see the singleton [4] in your output, you should definitely update\nyour copy of Sage.\n\nG = SymmetricGroup (4)\nS = G.stabilizer (4)\nS.orbits ()\n\n[[1, 2, 3], [4]]\n\n14.8 Sage Exercises\n1. Construct the Higman-Sims graph with the command graphs.HigmanSimsGraph(). Then\nconstruct the automorphism group and determine the order of the one interesting normal\nsubgroup of this group. You can try plotting the graph, but the graphic is unlikely to be\nvery informative.\n\n2. This exercise asks you to verify the class equation outside of the usual situation where\nthe group action is conjugation. Consider the example of the automorphism group of the\npath on 11 vertices. First construct the list of orbits. From each orbit, grab the first element\nof the orbit as a representative. Compute the size of the orbit as the index of the stabilizer\nof the representative in the group via Theorem 14.11. (Yes, you could just compute the size\nof the full orbit, but the idea of the exercise is to use more group-theoretic results.) Then\nsum these orbit-sizes, which should equal the size of the whole vertex set since the orbits\nform a partition.\n\n3. Construct a simple graph (no lopps or multiple edges), with at least two vertices and\nat least one edge, whose automorphism group is trivial. You might start experimenting by\ndrawing pictures on paper before constructing the graph. A command like the following\nwill let you construct a graph from edges. The graph below looks like a triangle or 3-cycle.\n\nG = Graph ([(1 ,2), (2,3), (3,1)])\nG.plot()\n\n4. For the following two pairs of groups, compute the list of conjugacy class representatives\nfor each group in the pair. For each part, compare and contrast the results for the two\ngroups in the pair, with thoughtful and insightful comments.\n(a) The full symmetric group on 5 symbols, S5, and the alternating group on 5 symbols,\n\nA5.\n(b) The dihedral groups that are symmetries of a 7-gon and an 8-gon, D7 and D8.\n\n5. Use the command graphs.CubeGraph(4) to build the four-dimensional cube graph, Q4.\nUsing a plain .plot() command (without a spring layout) should create a nice plot. Con-\nstruct the automorphism group of the graph, which will provide a group action on the\nvertex set.\n(a) Construct the orbits of this action, and comment.\n(b) Construct a stabilizer of a single vertex (which is a subgroup of the full automorphism\n\ngroup) and then consider the action of this group on the vertex set. Construct the orbits\n\n\n\n238 CHAPTER 14. GROUP ACTIONS\n\nof this new action, and comment carefully and fully on your observations, especially\nin terms of the vertices of the graph.\n\n6. Build the graph given by the commands below. The result should be a symmetric-looking\ngraph with an automorphism group of order 16.\n\nG = graphs.CycleGraph (8)\nG.add_edges ([(0 ,2) ,(1,3) ,(4,6) ,(5,7)])\nG.plot()\n\nRepeat the two parts of the previous exercise, but realize that in the second part there\nare now two different stabilizers to create, so build both and compare the differences in\nthe stabilizers and their orbits. Creating a second plot with G.plot(layout=\'planar\') might\nprovide extra insight.\n\n\n\n15\n\nThe Sylow Theorems\n\nWe already know that the converse of Lagrange’s Theorem is false. If G is a group of\norder m and n divides m, then G does not necessarily possess a subgroup of order n. For\nexample, A4 has order 12 but does not possess a subgroup of order 6. However, the Sylow\nTheorems do provide a partial converse for Lagrange’s Theorem—in certain cases they\nguarantee us subgroups of specific orders. These theorems yield a powerful set of tools for\nthe classification of all finite nonabelian groups.\n\n15.1 The Sylow Theorems\nWe will use what we have learned about group actions to prove the Sylow Theorems. Recall\nfor a moment what it means for G to act on itself by conjugation and how conjugacy classes\nare distributed in the group according to the class equation, discussed in Chapter 14. A\ngroup G acts on itself by conjugation via the map (g, x) 7→ gxg−1. Let x1, . . . , xk be\nrepresentatives from each of the distinct conjugacy classes of G that consist of more than\none element. Then the class equation can be written as\n\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)],\n\nwhere Z(G) = {g ∈ G : gx = xg for all x ∈ G} is the center of G and C(xi) = {g ∈ G :\ngxi = xig} is the centralizer subgroup of xi.\n\nWe begin our investigation of the Sylow Theorems by examining subgroups of order p,\nwhere p is prime. A group G is a p-group if every element in G has as its order a power of\np, where p is a prime number. A subgroup of a group G is a p-subgroup if it is a p-group.\n\nTheorem 15.1 (Cauchy). Let G be a finite group and p a prime such that p divides the\norder of G. Then G contains a subgroup of order p.\n\nProof. We will use induction on the order of G. If |G| = p, then clearly order k, where\np ≤ k < n and p divides k, has an element of order p. Assume that |G| = n and p | n and\nconsider the class equation of G:\n\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)].\n\nWe have two cases.\nCase 1. The order of one of the centralizer subgroups, C(xi), is divisible by p for some i,\n\ni = 1, . . . , k. In this case, by our induction hypothesis, we are done. Since C(xi) is a proper\nsubgroup of G and p divides |C(xi)|, C(xi) must contain an element of order p. Hence, G\nmust contain an element of order p.\n\nCase 2. The order of no centralizer subgroup is divisible by p. Then p divides [G : C(xi)],\nthe order of each conjugacy class in the class equation; hence, p must divide the center of\n\n239\n\n\n\n240 CHAPTER 15. THE SYLOW THEOREMS\n\nG, Z(G). Since Z(G) is abelian, it must have a subgroup of order p by the Fundamental\nTheorem of Finite Abelian Groups. Therefore, the center of G contains an element of order\np.\n\nCorollary 15.2. Let G be a finite group. Then G is a p-group if and only if |G| = pn.\n\nExample 15.3. Let us consider the group A5. We know that |A5| = 60 = 22 · 3 · 5. By\nCauchy’s Theorem, we are guaranteed that A5 has subgroups of orders 2, 3 and 5. The\nSylow Theorems will give us even more information about the possible subgroups of A5.\n\nWe are now ready to state and prove the first of the Sylow Theorems. The proof is very\nsimilar to the proof of Cauchy’s Theorem.\n\nTheorem 15.4 (First Sylow Theorem). Let G be a finite group and p a prime such that pr\ndivides |G|. Then G contains a subgroup of order pr.\n\nProof. We induct on the order of G once again. If |G| = p, then we are done. Now\nsuppose that the order of G is n with n > p and that the theorem is true for all groups of\norder less than n, where p divides n. We shall apply the class equation once again:\n\n|G| = |Z(G)|+ [G : C(x1)] + · · ·+ [G : C(xk)].\n\nFirst suppose that p does not divide [G : C(xi)] for some i. Then pr | |C(xi)|, since pr\ndivides |G| = |C(xi)| · [G : C(xi)]. Now we can apply the induction hypothesis to C(xi).\n\nHence, we may assume that p divides [G : C(xi)] for all i. Since p divides |G|, the class\nequation says that p must divide |Z(G)|; hence, by Cauchy’s Theorem, Z(G) has an element\nof order p, say g. Let N be the group generated by g. Clearly, N is a normal subgroup\nof Z(G) since Z(G) is abelian; therefore, N is normal in G since every element in Z(G)\ncommutes with every element in G. Now consider the factor group G/N of order |G|/p. By\nthe induction hypothesis, G/N contains a subgroup H of order pr−1. The inverse image of\nH under the canonical homomorphism ϕ : G→ G/N is a subgroup of order pr in G.\n\nA Sylow p-subgroup P of a group G is a maximal p-subgroup of G. To prove the other\ntwo Sylow Theorems, we need to consider conjugate subgroups as opposed to conjugate\nelements in a group. For a group G, let S be the collection of all subgroups of G. For any\nsubgroup H, S is a H-set, where H acts on S by conjugation. That is, we have an action\n\nH × S → S\n\ndefined by\nh ·K 7→ hKh−1\n\nfor K in S.\nThe set\n\nN(H) = {g ∈ G : gHg−1 = H}\n\nis a subgroup of G called the the normalizer of H in G. Notice that H is a normal\nsubgroup of N(H). In fact, N(H) is the largest subgroup of G in which H is normal.\n\nLemma 15.5. Let P be a Sylow p-subgroup of a finite group G and let x have as its order\na power of p. If x−1Px = P , then x ∈ P .\n\n\n\n15.1. THE SYLOW THEOREMS 241\n\nProof. Certainly x ∈ N(P ), and the cyclic subgroup, ⟨xP ⟩ ⊂ N(P )/P , has as its order a\npower of p. By the Correspondence Theorem there exists a subgroup H of N(P ) containing\nP such that H/P = ⟨xP ⟩. Since |H| = |P | · |⟨xP ⟩|, the order of H must be a power of\np. However, P is a Sylow p-subgroup contained in H. Since the order of P is the largest\npower of p dividing |G|, H = P . Therefore, H/P is the trivial subgroup and xP = P , or\nx ∈ P .\n\nLemma 15.6. Let H and K be subgroups of G. The number of distinct H-conjugates of\nK is [H : N(K) ∩H].\nProof. We define a bijection between the conjugacy classes of K and the right cosets of\nN(K)∩H by h−1Kh 7→ (N(K)∩H)h. To show that this map is a bijection, let h1, h2 ∈ H\nand suppose that (N(K) ∩ H)h1 = (N(K) ∩ H)h2. Then h2h\n\n−1\n1 ∈ N(K). Therefore,\n\nK = h2h\n−1\n1 Kh1h\n\n−1\n2 or h−1\n\n1 Kh1 = h−1\n2 Kh2, and the map is an injection. It is easy to\n\nsee that this map is surjective; hence, we have a one-to-one and onto map between the\nH-conjugates of K and the right cosets of N(K) ∩H in H.\n\nTheorem 15.7 (Second Sylow Theorem). Let G be a finite group and p a prime dividing\n|G|. Then all Sylow p-subgroups of G are conjugate. That is, if P1 and P2 are two Sylow\np-subgroups, there exists a g ∈ G such that gP1g\n\n−1 = P2.\nProof. Let P be a Sylow p-subgroup of G and suppose that |G| = prm with |P | = pr. Let\n\nS = {P = P1, P2, . . . , Pk}\n\nconsist of the distinct conjugates of P in G. By Lemma 15.6, k = [G : N(P )]. Notice that\n\n|G| = prm = |N(P )| · [G : N(P )] = |N(P )| · k.\n\nSince pr divides |N(P )|, p cannot divide k.\nGiven any other Sylow p-subgroup Q, we must show that Q ∈ S. Consider the Q-\n\nconjugacy classes of each Pi. Clearly, these conjugacy classes partition S. The size of the\npartition containing Pi is [Q : N(Pi)∩Q] by Lemma 15.6, and Lagrange’s Theorem tells us\nthat |Q| = [Q : N(Pi)∩Q]|N(Pi)∩Q|. Thus, [Q : N(Pi)∩Q] must be a divisor of |Q| = pr.\nHence, the number of conjugates in every equivalence class of the partition is a power of\np. However, since p does not divide k, one of these equivalence classes must contain only a\nsingle Sylow p-subgroup, say Pj . In this case, x−1Pjx = Pj for all x ∈ Q. By Lemma 15.5,\nPj = Q.\n\nTheorem 15.8 (Third Sylow Theorem). Let G be a finite group and let p be a prime\ndividing the order of G. Then the number of Sylow p-subgroups is congruent to 1 (mod p)\nand divides |G|.\nProof. Let P be a Sylow p-subgroup acting on the set of Sylow p-subgroups,\n\nS = {P = P1, P2, . . . , Pk},\n\nby conjugation. From the proof of the Second Sylow Theorem, the only P -conjugate of P\nis itself and the order of the other P -conjugacy classes is a power of p. Each P -conjugacy\nclass contributes a positive power of p toward |S| except the equivalence class {P}. Since\n|S| is the sum of positive powers of p and 1, |S| ≡ 1 (mod p).\n\nNow suppose that G acts on S by conjugation. Since all Sylow p-subgroups are conju-\ngate, there can be only one orbit under this action. For P ∈ S,\n\n|S| = |orbit of P | = [G : N(P )]\n\nby Lemma 15.6. But [G : N(P )] is a divisor of |G|; consequently, the number of Sylow\np-subgroups of a finite group must divide the order of the group.\n\n\n\n242 CHAPTER 15. THE SYLOW THEOREMS\n\nHistorical Note\n\nPeter Ludvig Mejdell Sylow was born in 1832 in Christiania, Norway (now Oslo). After\nattending Christiania University, Sylow taught high school. In 1862 he obtained a temporary\nappointment at Christiania University. Even though his appointment was relatively brief,\nhe influenced students such as Sophus Lie (1842–1899). Sylow had a chance at a permanent\nchair in 1869, but failed to obtain the appointment. In 1872, he published a 10-page paper\npresenting the theorems that now bear his name. Later Lie and Sylow collaborated on a\nnew edition of Abel’s works. In 1898, a chair at Christiania University was finally created\nfor Sylow through the efforts of his student and colleague Lie. Sylow died in 1918.\n\n15.2 Examples and Applications\nExample 15.9. Using the Sylow Theorems, we can determine that A5 has subgroups of\norders 2, 3, 4, and 5. The Sylow p-subgroups of A5 have orders 3, 4, and 5. The Third Sylow\nTheorem tells us exactly how many Sylow p-subgroups A5 has. Since the number of Sylow\n5-subgroups must divide 60 and also be congruent to 1 (mod 5), there are either one or six\nSylow 5-subgroups in A5. All Sylow 5-subgroups are conjugate. If there were only a single\nSylow 5-subgroup, it would be conjugate to itself; that is, it would be a normal subgroup of\nA5. Since A5 has no normal subgroups, this is impossible; hence, we have determined that\nthere are exactly six distinct Sylow 5-subgroups of A5.\n\nThe Sylow Theorems allow us to prove many useful results about finite groups. By\nusing them, we can often conclude a great deal about groups of a particular order if certain\nhypotheses are satisfied.\n\nTheorem 15.10. If p and q are distinct primes with p < q, then every group G of order\npq has a single subgroup of order q and this subgroup is normal in G. Hence, G cannot be\nsimple. Furthermore, if q ̸≡ 1 (mod p), then G is cyclic.\n\nProof. We know that G contains a subgroup H of order q. The number of conjugates of\nH divides pq and is equal to 1 + kq for k = 0, 1, . . .. However, 1 + q is already too large to\ndivide the order of the group; hence, H can only be conjugate to itself. That is, H must be\nnormal in G.\n\nThe group G also has a Sylow p-subgroup, say K. The number of conjugates of K must\ndivide q and be equal to 1 + kp for k = 0, 1, . . .. Since q is prime, either 1 + kp = q or\n1 + kp = 1. If 1 + kp = 1, then K is normal in G. In this case, we can easily show that G\nsatisfies the criteria, given in Chapter 9, for the internal direct product of H and K. Since\nH is isomorphic to Zq and K is isomorphic to Zp, G ∼= Zp×Zq\n\n∼= Zpq by Theorem 9.21.\n\nExample 15.11. Every group of order 15 is cyclic. This is true because 15 = 5 · 3 and\n5 ̸≡ 1 (mod 3).\n\nExample 15.12. Let us classify all of the groups of order 99 = 32 · 11 up to isomorphism.\nFirst we will show that every group G of order 99 is abelian. By the Third Sylow Theorem,\nthere are 1+3k Sylow 3-subgroups, each of order 9, for some k = 0, 1, 2, . . .. Also, 1+3k must\ndivide 11; hence, there can only be a single normal Sylow 3-subgroup H in G. Similarly,\nthere are 1+11k Sylow 11-subgroups and 1+11k must divide 9. Consequently, there is only\none Sylow 11-subgroup K in G. By Corollary 14.16, any group of order p2 is abelian for p\nprime; hence, H is isomorphic either to Z3 × Z3 or to Z9. Since K has order 11, it must\nbe isomorphic to Z11. Therefore, the only possible groups of order 99 are Z3 × Z3 × Z11 or\nZ9 × Z11 up to isomorphism.\n\n\n\n15.2. EXAMPLES AND APPLICATIONS 243\n\nTo determine all of the groups of order 5 · 7 · 47 = 1645, we need the following theorem.\n\nTheorem 15.13. Let G′ = ⟨aba−1b−1 : a, b ∈ G⟩ be the subgroup consisting of all finite\nproducts of elements of the form aba−1b−1 in a group G. Then G′ is a normal subgroup of\nG and G/G′ is abelian.\n\nThe subgroup G′ of G is called the commutator subgroup of G. We leave the proof\nof this theorem as an exercise (Exercise 10.3.14 in Chapter 10).\n\nExample 15.14. We will now show that every group of order 5 · 7 · 47 = 1645 is abelian,\nand cyclic by Corollary 9.21. By the Third Sylow Theorem, G has only one subgroup H1\n\nof order 47. So G/H1 has order 35 and must be abelian by Theorem 15.10. Hence, the\ncommutator subgroup of G is contained in H which tells us that |G′| is either 1 or 47. If\n|G′| = 1, we are done. Suppose that |G′| = 47. The Third Sylow Theorem tells us that\nG has only one subgroup of order 5 and one subgroup of order 7. So there exist normal\nsubgroups H2 and H3 in G, where |H2| = 5 and |H3| = 7. In either case the quotient group\nis abelian; hence, G′ must be a subgroup of Hi, i = 1, 2. Therefore, the order of G′ is 1,\n5, or 7. However, we already have determined that |G′| = 1 or 47. So the commutator\nsubgroup of G is trivial, and consequently G is abelian.\n\nFinite Simple Groups\n\nGiven a finite group, one can ask whether or not that group has any normal subgroups.\nRecall that a simple group is one with no proper nontrivial normal subgroups. As in the\ncase of A5, proving a group to be simple can be a very difficult task; however, the Sylow\nTheorems are useful tools for proving that a group is not simple. Usually, some sort of\ncounting argument is involved.\n\nExample 15.15. Let us show that no group G of order 20 can be simple. By the Third\nSylow Theorem, G contains one or more Sylow 5-subgroups. The number of such subgroups\nis congruent to 1 (mod 5) and must also divide 20. The only possible such number is 1.\nSince there is only a single Sylow 5-subgroup and all Sylow 5-subgroups are conjugate, this\nsubgroup must be normal.\n\nExample 15.16. Let G be a finite group of order pn, n > 1 and p prime. By Theorem 14.15,\nG has a nontrivial center. Since the center of any group G is a normal subgroup, G cannot\nbe a simple group. Therefore, groups of orders 4, 8, 9, 16, 25, 27, 32, 49, 64, and 81 are not\nsimple. In fact, the groups of order 4, 9, 25, and 49 are abelian by Corollary 14.16.\n\nExample 15.17. No group of order 56 = 23 ·7 is simple. We have seen that if we can show\nthat there is only one Sylow p-subgroup for some prime p dividing 56, then this must be a\nnormal subgroup and we are done. By the Third Sylow Theorem, there are either one or\neight Sylow 7-subgroups. If there is only a single Sylow 7-subgroup, then it must be normal.\n\nOn the other hand, suppose that there are eight Sylow 7-subgroups. Then each of these\nsubgroups must be cyclic; hence, the intersection of any two of these subgroups contains\nonly the identity of the group. This leaves 8 · 6 = 48 distinct elements in the group, each\nof order 7. Now let us count Sylow 2-subgroups. There are either one or seven Sylow\n2-subgroups. Any element of a Sylow 2-subgroup other than the identity must have as its\norder a power of 2; and therefore cannot be one of the 48 elements of order 7 in the Sylow\n7-subgroups. Since a Sylow 2-subgroup has order 8, there is only enough room for a single\nSylow 2-subgroup in a group of order 56. If there is only one Sylow 2-subgroup, it must be\nnormal.\n\n\n\n244 CHAPTER 15. THE SYLOW THEOREMS\n\nFor other groups G, it is more difficult to prove that G is not simple. Suppose G has\norder 48. In this case the technique that we employed in the last example will not work.\nWe need the following lemma to prove that no group of order 48 is simple.\n\nLemma 15.18. Let H and K be finite subgroups of a group G. Then\n\n|HK| = |H| · |K|\n|H ∩K|\n\n.\n\nProof. Recall that\nHK = {hk : h ∈ H, k ∈ K}.\n\nCertainly, |HK| ≤ |H| · |K| since some element in HK could be written as the product of\ndifferent elements in H and K. It is quite possible that h1k1 = h2k2 for h1, h2 ∈ H and\nk1, k2 ∈ K. If this is the case, let\n\na = (h1)\n−1h2 = k1(k2)\n\n−1.\n\nNotice that a ∈ H ∩K, since (h1)\n−1h2 is in H and k2(k1)\n\n−1 is in K; consequently,\n\nh2 = h1a\n−1\n\nk2 = ak1.\n\nConversely, let h = h1b\n−1 and k = bk1 for b ∈ H ∩K. Then hk = h1k1, where h ∈ H\n\nand k ∈ K. Hence, any element hk ∈ HK can be written in the form hiki for hi ∈ H and\nki ∈ K, as many times as there are elements in H ∩K; that is, |H ∩K| times. Therefore,\n|HK| = (|H| · |K|)/|H ∩K|.\n\nExample 15.19. To demonstrate that a group G of order 48 is not simple, we will show\nthat G contains either a normal subgroup of order 8 or a normal subgroup of order 16. By\nthe Third Sylow Theorem, G has either one or three Sylow 2-subgroups of order 16. If there\nis only one subgroup, then it must be a normal subgroup.\n\nSuppose that the other case is true, and two of the three Sylow 2-subgroups are H and\nK. We claim that |H ∩K| = 8. If |H ∩K| ≤ 4, then by Lemma 15.18,\n\n|HK| = 16 · 16\n4\n\n= 64,\n\nwhich is impossible. Notice that H ∩K has index two in both of H and K, so is normal in\nboth, and thus H and K are each in the normalizer of H ∩K. Because H is a subgroup of\nN(H ∩K) and because N(H ∩K) has strictly more than 16 elements, |N(H ∩K)| must\nbe a multiple of 16 greater than 1, as well as dividing 48. The only possibility is that\n|N(H ∩K)| = 48. Hence, N(H ∩K) = G.\n\nThe following famous conjecture of Burnside was proved in a long and difficult paper\nby Feit and Thompson [2].\n\nTheorem 15.20 (Odd Order Theorem). Every finite simple group of nonprime order must\nbe of even order.\n\nThe proof of this theorem laid the groundwork for a program in the 1960s and 1970s\nthat classified all finite simple groups. The success of this program is one of the outstanding\nachievements of modern mathematics.\n\n\n\n15.3. EXERCISES 245\n\n15.3 Exercises\n1. What are the orders of all Sylow p-subgroups where G has order 18, 24, 54, 72, and 80?\n\n2. Find all the Sylow 3-subgroups of S4 and show that they are all conjugate.\n\n3. Show that every group of order 45 has a normal subgroup of order 9.\n\n4. Let H be a Sylow p-subgroup of G. Prove that H is the only Sylow p-subgroup of G\ncontained in N(H).\n\n5. Prove that no group of order 96 is simple.\n\n6. Prove that no group of order 160 is simple.\n\n7. If H is a normal subgroup of a finite group G and |H| = pk for some prime p, show that\nH is contained in every Sylow p-subgroup of G.\n\n8. Let G be a group of order p2q2, where p and q are distinct primes such that q ∤ p2 − 1\nand p ∤ q2 − 1. Prove that G must be abelian. Find a pair of primes for which this is true.\n\n9. Show that a group of order 33 has only one Sylow 3-subgroup.\n\n10. Let H be a subgroup of a group G. Prove or disprove that the normalizer of H is\nnormal in G.\n\n11. Let G be a finite group divisible by a prime p. Prove that if there is only one Sylow\np-subgroup in G, it must be a normal subgroup of G.\n\n12. Let G be a group of order pr, p prime. Prove that G contains a normal subgroup of\norder pr−1.\n\n13. Suppose that G is a finite group of order pnk, where k < p. Show that G must contain\na normal subgroup.\n\n14. Let H be a subgroup of a finite group G. Prove that gN(H)g−1 = N(gHg−1) for any\ng ∈ G.\n\n15. Prove that a group of order 108 must have a normal subgroup.\n\n16. Classify all the groups of order 175 up to isomorphism.\n\n17. Show that every group of order 255 is cyclic.\n\n18. Let G have order pe11 · · · penn and suppose that G has n Sylow p-subgroups P1, . . . , Pn\n\nwhere |Pi| = peii . Prove that G is isomorphic to P1 × · · · × Pn.\n\n19. Let P be a normal Sylow p-subgroup of G. Prove that every inner automorphism of G\nfixes P .\n\n20. What is the smallest possible order of a group G such that G is nonabelian and |G| is\nodd? Can you find such a group?\n\n21. (The Frattini Lemma) If H is a normal subgroup of a finite group G and P is a Sylow\np-subgroup of H, for each g ∈ G show that there is an h in H such that gPg−1 = hPh−1.\nAlso, show that if N is the normalizer of P , then G = HN .\n\n22. Show that if the order of G is pnq, where p and q are primes and p > q, then G contains\na normal subgroup.\n\n\n\n246 CHAPTER 15. THE SYLOW THEOREMS\n\n23. Prove that the number of distinct conjugates of a subgroup H of a finite group G is\n[G : N(H)].\n\n24. Prove that a Sylow 2-subgroup of S5 is isomorphic to D4.\n\n25. (Another Proof of the Sylow Theorems)\n\n(a) Suppose p is prime and p does not divide m. Show that\n\np ∤\n(\npkm\n\npk\n\n)\n.\n\n(b) Let S denote the set of all pk element subsets of G. Show that p does not divide |S|.\n\n(c) Define an action of G on S by left multiplication, aT = {at : t ∈ T} for a ∈ G and\nT ∈ S. Prove that this is a group action.\n\n(d) Prove p ∤ |OT | for some T ∈ S.\n\n(e) Let {T1, . . . , Tu} be an orbit such that p ∤ u and H = {g ∈ G : gT1 = T1}. Prove that\nH is a subgroup of G and show that |G| = u|H|.\n\n(f) Show that pk divides |H| and pk ≤ |H|.\n\n(g) Show that |H| = |OT | ≤ pk; conclude that therefore pk = |H|.\n\n26. Let G be a group. Prove that G′ = ⟨aba−1b−1 : a, b ∈ G⟩ is a normal subgroup of G\nand G/G′ is abelian. Find an example to show that {aba−1b−1 : a, b ∈ G} is not necessarily\na group.\n\n15.4 A Project\n\nThe main objective of finite group theory is to classify all possible finite groups up to\nisomorphism. This problem is very difficult even if we try to classify the groups of order\nless than or equal to 60. However, we can break the problem down into several intermediate\nproblems. This is a challenging project that requires a working knowledge of the group\ntheory you have learned up to this point. Even if you do not complete it, it will teach you\na great deal about finite groups. You can use Table 15.21 as a guide.\n\n\n\n15.5. REFERENCES AND SUGGESTED READINGS 247\n\nOrder Number Order Number Order Number Order Number\n1 ? 16 14 31 1 46 2\n2 ? 17 1 32 51 47 1\n3 ? 18 ? 33 1 48 52\n4 ? 19 ? 34 ? 49 ?\n5 ? 20 5 35 1 50 5\n6 ? 21 ? 36 14 51 ?\n7 ? 22 2 37 1 52 ?\n8 ? 23 1 38 ? 53 ?\n9 ? 24 ? 39 2 54 15\n10 ? 25 2 40 14 55 2\n11 ? 26 2 41 1 56 ?\n12 5 27 5 42 ? 57 2\n13 ? 28 ? 43 1 58 ?\n14 ? 29 1 44 4 59 1\n15 1 30 4 45 ? 60 13\n\nTable 15.21: Numbers of distinct groups G, |G| ≤ 60\n\n1. Find all simple groups G ( |G| ≤ 60). Do not use the Odd Order Theorem unless you\nare prepared to prove it.\n\n2. Find the number of distinct groups G, where the order of G is n for n = 1, . . . , 60.\n\n3. Find the actual groups (up to isomorphism) for each n.\n\n15.5 References and Suggested Readings\n[1] Edwards, H. “A Short History of the Fields Medal,” Mathematical Intelligencer 1(1978),\n\n127–29.\n[2] Feit, W. and Thompson, J. G. “Solvability of Groups of Odd Order,” Pacific Journal\n\nof Mathematics 13(1963), 775–1029.\n[3] Gallian, J. A. “The Search for Finite Simple Groups,” Mathematics Magazine 49(1976),\n\n163–79.\n[4] Gorenstein, D. “Classifying the Finite Simple Groups,” Bulletin of the American\n\nMathematical Society 14(1986), 1–98.\n[5] Gorenstein, D. Finite Groups. AMS Chelsea Publishing, Providence RI, 1968.\n[6] Gorenstein, D., Lyons, R., and Solomon, R. The Classification of Finite Simple\n\nGroups. American Mathematical Society, Providence RI, 1994.\n\n15.6 Sage\nSylow Subgroups\nThe Sage permutation group method .sylow_subgroup(p) will return a single Sylow p-\nsubgroup. If the prime is not a proper divisor of the group order it returns a subgroup\nof order p0, in other words, a trivial subgroup. So be careful about how you construct your\nprimes. Sometimes, you may only want one such Sylow subgroup, since any two Sylow\n\n\n\n248 CHAPTER 15. THE SYLOW THEOREMS\n\np-subgroups are conjugate, and hence isomorphic (Theorem 15.7). This also means we can\ncreate other Sylow p-subgroups by conjugating the one we have. The permutation group\nmethod .conjugate(g) will conjugate the group by g.\n\nWith repeated conjugations of a single Sylow p-subgroup, we will always create duplicate\nsubgroups. So we need to use a slightly complicated construction to form a list of just the\nunique subgroups as the list of conjugates. This routine that computes all Sylow p-subgroups\ncan be helpful throughout this section. It could be made much more efficient by conjugating\nby just one element per coset of the normalizer, but it will be sufficient for our purposes\nhere. Be sure to execute the next cell if you are online, so the function is defined for use\nlater.\n\ndef all_sylow(G, p):\n\' \' \' Form the set of all distinct Sylow p-subgroups of G \' \' \'\nscriptP = []\nP = G.sylow_subgroup(p)\nfor x in G:\n\nH = P.conjugate(x)\nif not(H in scriptP):\n\nscriptP.append(H)\nreturn scriptP\n\nLets investigate the Sylow subgroups of the dihedral group D18. As a group of order\n36 = 22 ·32, we know by the First Sylow Theorem that there is a Sylow 2-subgroup of order\n4 and a Sylow 3-subgroup of order 9. First for p = 2, we obtain one Sylow 2-subgroup,\nform all the conjugates, and form a list of non-duplicate subgroups. (These commands take\na while to execute, so be patient.)\n\nG = DihedralGroup (18)\nS2 = G.sylow_subgroup (2); S2\n\nSubgroup of (Dihedral group of order 36 as a permutation group)\ngenerated by\n[(2 ,18) (3,17)(4,16) (5,15)(6,14)(7,13)(8,12)(9,11),\n(1,10)(2 ,11)(3 ,12) (4,13)(5,14)(6,15)(7,16)(8,17)(9,18)]\n\nuniqS2 = all_sylow(G, 2)\nuniqS2\n\n[Permutation Group with generators\n[(2 ,18) (3,17) (4,16)(5,15)(6,14)(7,13)(8,12)(9,11),\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\nPermutation Group with generators\n[(1,3) (4,18) (5,17)(6,16)(7,15)(8,14)(9,13) (10 ,12),\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\nPermutation Group with generators\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\n(1,17) (2,16) (3,15)(4,14)(5,13)(6,12)(7,11)(8,10)],\nPermutation Group with generators\n[(1,5)(2,4)(6 ,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13),\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\nPermutation Group with generators\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\n(1,15) (2,14) (3,13)(4,12)(5,11)(6,10)(7,9)(16 ,18)],\nPermutation Group with generators\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\n(1,13) (2,12) (3,11)(4,10)(5,9)(6,8)(14 ,18) (15 ,17)],\n\n\n\n15.6. SAGE 249\n\nPermutation Group with generators\n[(1,7)(2,6)(3,5)(8,18)(9,17) (10 ,16) (11 ,15) (12 ,14),\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)],\nPermutation Group with generators\n[(1 ,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18),\n(1,11) (2,10)(3,9)(4,8)(5,7)(12 ,18) (13 ,17) (14 ,16)],\nPermutation Group with generators\n[(1,9)(2,8)(3,7)(4,6)(10 ,18) (11 ,17) (12 ,16) (13 ,15),\n(1,10) (2,11) (3,12)(4,13)(5,14)(6,15)(7,16)(8,17)(9,18)]]\n\nlen(uniqS2)\n\n9\n\nThe Third Sylow Theorem tells us that for p = 2 we would expect 1, 3 or 9 Sylow 2-\nsubgroups, so our computational result of 9 subgroups is consistent with what the theory\npredicts. Can you visualize each of these subgroups as symmetries of an 18-gon? Notice\nthat we also have many subgroups of order 2 inside of these subgroups of order 4.\n\nNow for the case of p = 3.\nG = DihedralGroup (18)\nS3 = G.sylow_subgroup (3); S3\n\nSubgroup of (Dihedral group of order 36 as a permutation group)\ngenerated by\n[(1 ,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18),\n(1,15,11,7,3,17,13,9,5)(2,16,12,8,4,18,14,10,6)]\n\nuniqS3 = all_sylow(G, 3)\nuniqS3\n\n[Permutation Group with generators\n[(1 ,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18),\n(1,15,11,7,3,17,13,9,5)(2,16,12,8,4,18,14,10,6)]]\n\nlen(uniqS3)\n\n1\n\nWhat does the Third Sylow Theorem predict? Just 1 or 4 Sylow 3-subgroups. Having\nfound just one subgroup computationally, we know that all of the conjugates of the lone\nSylow 3-subgroup are equal. In other words, the Sylow 3-subgroup is normal in D18. Let\nus check anyway.\n\nS3.is_normal(G)\n\nTrue\n\nAt least one of the subgroups of order 3 contained in this Sylow 3-subgroup should be\nobvious by looking at the orders of the generators, and then you may even notice that the\ngenerators given could be reduced, and one is a power of the other.\n\nS3.is_cyclic ()\n\nTrue\n\nRemember that there are many other subgroups, of other orders. For example, can you\nconstruct a subgroup of order 6 = 2 · 3 in D18?\n\n\n\n250 CHAPTER 15. THE SYLOW THEOREMS\n\nNormalizers\nA new command that is relevant to this section is the construction of a normalizer. The Sage\ncommand G.normalizer(H) will return the subgroup of G containing elements that normalize\nthe subgroup H. We illustrate its use with the Sylow subgroups from above.\n\nG = DihedralGroup (18)\nS2 = G.sylow_subgroup (2)\nS3 = G.sylow_subgroup (3)\nN2 = G.normalizer(S2); N2\n\nSubgroup of (Dihedral group of order 36 as a permutation group)\ngenerated by\n[(2 ,18) (3,17)(4,16) (5,15)(6,14)(7,13)(8,12)(9,11),\n(1,10)(2 ,11)(3 ,12) (4,13)(5,14)(6,15)(7,16)(8,17)(9,18)]\n\nN2 == S2\n\nTrue\n\nN3 = G.normalizer(S3); N3\n\nSubgroup of (Dihedral group of order 36 as a permutation group)\ngenerated by\n[(2 ,18) (3,17)(4,16) (5,15)(6,14)(7,13)(8,12)(9,11),\n(1,2)(3,18) (4,17)(5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11),\n(1,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18),\n(1,15,11,7,3,17,13,9,5)(2,16,12,8,4,18,14,10,6)]\n\nN3 == G\n\nTrue\n\nThe normalizer of a subgroup always contains the whole subgroup, so the normalizer\nof S2 is as small as possible. We already knew S3 is normal in G, so it is no surprise that\nits normalizer is as big as possible — every element of G normalizes S3. Let us compute a\nnormalizer in D18 that is more “interesting.”\n\nG = DihedralGroup (18)\na = G("(1,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18)")\nb = G("(1,5)(2,4)(6,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13)")\nH = G.subgroup ([a, b])\nH.order()\n\n6\n\nN = G.normalizer(H)\nN\n\nSubgroup of (Dihedral group of order 36 as a permutation group)\ngenerated by\n[(1 ,2)(3 ,18)(4 ,17) (5,16)(6,15)(7,14)(8,13)(9,12) (10 ,11),\n(1,5)(2,4)(6 ,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13),\n(1,13,7)(2,14,8)(3,15,9)(4,16,10)(5,17,11)(6,18,12)]\n\nN.order()\n\n\n\n15.6. SAGE 251\n\n12\n\nSo for this subgroup of order 6, the normalizer is strictly bigger than the subgroup, but\nstill strictly smaller than the whole group (and hence not normal in the dihedral group).\nTrivially, a subgroup is normal in its normalizer:\n\nH.is_normal(G)\n\nFalse\n\nH.is_normal(N)\n\nTrue\n\nFinite Simple Groups\nWe saw earlier Sage’s permutation group method .is_simple(). Example 15.16 tells us that\na group of order 64 is never simple. The dicyclic group DiCyclicGroup(16) is a non-abelian\ngroup of 64, so we can test this method on this group. It turns out this group has many\nnormal subgroups — the list will always contain the trivial subgroup and the group itself,\nso any number exceeding 2 indicates a non-trivial normal subgroup.\n\nDC=DiCyclicGroup (16)\nDC.order ()\n\n64\n\nDC.is_simple ()\n\nFalse\n\nns = DC.normal_subgroups ()\nlen(ns)\n\n9\n\nHere is a rather interesting group, one of the 26 sporadic simple groups, known as the\nHigman-Sims group, HS. The generators used below come from the representation on\n100 points in gap format, available off of web.mat.bham.ac.uk/atlas/v2.0/spor/HS/. Two\ngenerators of just order 2 and order 5 (as you can esily see), generating exactly 44 352 000\nelements, but no normal subgroups. Amazing.\n\nG = SymmetricGroup (100)\na = G([(1 ,60), (2,72), (3,81), (4,43), (5,11), (6,87),\n\n(7 ,34), (9,63), (12 ,46), (13 ,28), (14 ,71), (15 ,42),\n(16 ,97), (18 ,57), (19 ,52), (21 ,32), (23 ,47), (24 ,54),\n(25 ,83), (26 ,78), (29 ,89), (30 ,39), (33 ,61), (35 ,56),\n(37 ,67), (44 ,76), (45 ,88), (48 ,59), (49 ,86), (50 ,74),\n(51 ,66), (53 ,99), (55 ,75), (62 ,73), (65 ,79), (68 ,82),\n(77 ,92), (84 ,90), (85 ,98), (94 ,100)])\n\nb = G([(1 ,86 ,13 ,10 ,47), (2,53,30,8,38),\n(3 ,40,48,25,17), (4,29,92,88,43), (5,98,66,54, 65),\n(6 ,27,51,73,24), (7,83,16,20,28), (9,23,89,95,61),\n(11 ,42 ,46 ,91 ,32), (12,14, 81,55,68), (15 ,90 ,31 ,56 ,37),\n(18 ,69 ,45 ,84 ,76), (19 ,59 ,79 ,35 ,93), (21 ,22 ,64 ,39 ,100),\n(26 ,58 ,96 ,85 ,77), (33 ,52 ,94 ,75 ,44), (34 ,62 ,87 ,78 ,50),\n(36 ,82 ,60 ,74 ,72), (41 ,80 ,70 ,49 ,67), (57 ,63 ,71 ,99 ,97)])\n\na.order(), b.order()\n\nhttp://web.mat.bham.ac.uk/atlas/v2.0/spor/HS/\n\n\n252 CHAPTER 15. THE SYLOW THEOREMS\n\n(2, 5)\n\nHS = G.subgroup ([a, b])\nHS.order ()\n\n44352000\n\nHS.is_simple ()\n\nTrue\n\nWe saw this group earlier in the exercises for Chapter 14 on group actions, where it\nwas the single non-trivial normal subgroup of the automorphism group of the Higman-Sims\ngraph, hence its name.\n\nGAP Console and Interface\nThis concludes our exclusive study of group theory, though we will be using groups some\nin the subsequent sections. As we have remarked, much of Sage’s computation with groups\nis performed by the open source program, “Groups, Algorithms, and Programming,” which\nis better know as simply gap. If after this course you outgrow Sage’s support for groups,\nthen learning gap would be your next step as a group theorist. Every copy of Sage includes\na copy of gap and is easy to see which version of gap is included:\n\ngap.version ()\n\n\' 4.8.3 \'\n\nYou can interact with gap in Sage in several ways. The most direct is by creating a\npermutation group via Sage’s gap() command.\n\nG = gap( \' Group(␣(1,2,3,4,5,6),␣(1,3,5)␣) \' )\nG\n\nGroup( [ (1,2,3,4,5,6), (1,3,5) ] )\n\nNow we can use most any gap command with G, via the convention that most gap\ncommands expect a group as the first argument, and we instead provide the group by using\nthe object-orientedG. syntax. If you consult the gap documentation you will see that Center\n\nis a gap command that expects a group as its lone argument, and Centralizer is a gap\ncommand that expects two arguments — a group and then a group element.\n\nG.Center ()\n\nGroup( [ ( 1, 3, 5)( 2, 4, 6) ] )\n\nG.Centralizer( \' (1,␣3,␣5) \' )\n\nGroup( [ (1,3,5), (2,4,6), (1,3,5)(2,4,6) ] )\n\nIf you use the Sage Notebook interface you can set the first line of a compute cell to %gap\n\nand the entire cell will be interpreted as if you were interacting directly with gap. This\nmeans you would now use gap’s syntax, which you can see above is slightly different than\nSage’s universal syntax. You can also use the drop-down box at the top of a worksheet, and\nselect gap as the system (rather than sage) and your whole worksheet will be interpreted as\ngap commands. Here is one simple example, which you should be able to evaluate in your\ncurrent worksheet. This particular example will not run properly in a Sage Cell in a web\npage version of this section.\n\n\n\n15.7. SAGE EXERCISES 253\n\n%gap\nG := Group( (1,2,3,4,5,6), (1,3,5) );\nCentralizer(G, (1,3,5));\n\nNotice that\n\n• We do not need to wrap the individual permutations in as many quotation marks as\nwe do in Sage.\n\n• Assignment is := not =. If you forget the colon, you will get an error message such as\nVariable: \'G\' must have a value\n\n• A line must end with a semi-colon. If you forget, several lines will be merged together.\n\nYou can get help about gap commands with a command such as the following, though\nyou will soon see that gap assumes you know a lot more algebra than Sage assumes you\nknow.\n\nprint gap.help( \' SymmetricGroup \' , pager=False)\n\nIn the command-line version of Sage, you can also use the gap “console.” Again, you\nneed to use gap syntax, and you do not have many of the conveniences of the Sage notebook.\nIt is also good to know in advance that quit; is how you can leave the gap console and\nget back to Sage. If you run Sage at the command-line, use the command gap_console() to\nstart gap running.\n\nIt is a comfort to know that with Sage you get a complete copy of gap, installed and all\nready to run. However, this is not a tutorial on gap, so consult the documentation available\nat the main gap website: www.gap-system.org to learn how to get the most out of gap.\n\n15.7 Sage Exercises\n\n1. This exercise verifies Theorem 15.13. The commutator subgroup is computed with\nthe permutation group method .commutator(). For the dihedral group of order 40, D20\n\n(DihedralGroup(20) in Sage), compute the commutator subgroup and form the quotient\nwith the dihedral group. Then verify that this quotient is abelian. Can you identify the\nquotient group exactly (in other words, up to isomorphism)?\n\n2. For each possible prime, find all of the distinct Sylow p-subgroups of the alternating\ngroup A5. Confirm that your results are consistent with the Third Sylow Theorem for each\nprime. We know that A5 is a simple group. Explain how this would explain or predict some\naspects of your answers.\nCount the number of distinct elements contained in the union of all the Sylow subgroups\nyou just found. What is interesting about this count?\n\n3. For the dihedral group D36 (symmetries of a 36-gon) and each possible prime, determine\nthe possibilities for the number of distinct Sylow p-subgroups as predicted by the Third\nSylow Theorem (15.8). Now compute the actual number of distinct Sylow p-subgroups for\neach prime and comment on the result.\nIt can be proved that any group with order 72 is not a simple group, using techniques such\nas those used in the later examples in this chapter. Discuss this result in teh context of\nyour computations with Sage.\n\nhttp://www.gap-system.org\n\n\n254 CHAPTER 15. THE SYLOW THEOREMS\n\n4. This exercise verifies Lemma 15.6. Let G be the dihedral group of order 36, D18. Let\nH be the one Sylow 3-subgroup. Let K be the subgroup of order 6 generated by the two\npermutations a and b given below. First, form a list of the distinct conjugates of K by the\nelements of H, and determine the number of subgroups in this list. Compare this with the\nindex given in the statement of the lemma, employing a single (long) statement making use\nof the .order(), .normalizer() and .intersection() methods with G, H and K, only.\n\nG = DihedralGroup (18)\na = G("(1,7,13)(2,8,14)(3,9,15)(4,10,16)(5,11,17)(6,12,18)")\nb = G("(1,5)(2,4)(6,18)(7,17)(8,16)(9,15) (10 ,14) (11 ,13)")\n\n5. Example 15.19 shows that every group of order 48 has a normal subgroup. The dicyclic\ngroups are an infinite family of non-abelian groups with order 4n, which includes the quater-\nnions (the case of n = 2). So the permutation group DiCyclicGroup(12) has order 48. Use\nSage to follow the logic of the proof in Example 15.19 and construct a normal subgroup in\nthis group. (In other words, do not just ask for a list of the normal subgroups from Sage,\nbut instead trace through the implications in the example to arrive at a normal subgroup,\nand then check your answer.)\n\n6. The proofs of the Second and Third Sylow Theorems (15.7, 15.8) employ a group action\non sets of Sylow p-subgroups, For the Second Theorem, the list is proposed as incomplete\nand is proved to be all of the Sylow p-subgroups. In this exercise we will see how these\nactions behave, and how they are different when we use different groups acting on the same\nset.\nConstruct the six Sylow 5-subgroups of the alternating group A5. This will be the set of\nobjects for both of our actions. Conjugating one of these Sylow 5-subgroups by an element\nof A5 will produce another Sylow 5-subgroup, and so can be used to create a group action.\nFor such an action, from each group element form a Sage permutation of the subgroups by\nnumbering the six subgroups and using these integers as markers for the subgroups. You\nwill find the Python list method .index() very helpful. Now use all of these permutations to\ngenerate a permutation group (a subgroup of S6). Finally, use permutation group methods\nfor orbits and stabilisers, etc. to explore the actions.\nFor the first action, use all of A5 as the group. Show that the resulting action is transitive.\nIn other words, there is exactly one single orbit.\nFor the second action, use just one of the Sylow 5-subgroups as the group. Write the class\nequation for this action in a format that suggests the “congruent to 1 mod p” part of the\nconclusion of the Third Theorem.\n\n\n\n16\n\nRings\n\nUp to this point we have studied sets with a single binary operation satisfying certain ax-\nioms, but we are often more interested in working with sets that have two binary operations.\nFor example, one of the most natural algebraic structures to study is the integers with the\noperations of addition and multiplication. These operations are related to one another by\nthe distributive property. If we consider a set with two such related binary operations sat-\nisfying certain axioms, we have an algebraic structure called a ring. In a ring we add and\nmultiply elements such as real numbers, complex numbers, matrices, and functions.\n\n16.1 Rings\nA nonempty set R is a ring if it has two closed binary operations, addition and multipli-\ncation, satisfying the following conditions.\n\n1. a+ b = b+ a for a, b ∈ R.\n\n2. (a+ b) + c = a+ (b+ c) for a, b, c ∈ R.\n\n3. There is an element 0 in R such that a+ 0 = a for all a ∈ R.\n\n4. For every element a ∈ R, there exists an element −a in R such that a+ (−a) = 0.\n\n5. (ab)c = a(bc) for a, b, c ∈ R.\n\n6. For a, b, c ∈ R,\n\na(b+ c) = ab+ ac\n\n(a+ b)c = ac+ bc.\n\nThis last condition, the distributive axiom, relates the binary operations of addition and\nmultiplication. Notice that the first four axioms simply require that a ring be an abelian\ngroup under addition, so we could also have defined a ring to be an abelian group (R,+)\ntogether with a second binary operation satisfying the fifth and sixth conditions given above.\n\nIf there is an element 1 ∈ R such that 1 ̸= 0 and 1a = a1 = a for each element a ∈ R,\nwe say that R is a ring with unity or identity. A ring R for which ab = ba for all a, b in R\nis called a commutative ring. A commutative ring R with identity is called an integral\ndomain if, for every a, b ∈ R such that ab = 0, either a = 0 or b = 0. A division ring\nis a ring R, with an identity, in which every nonzero element in R is a unit; that is, for\neach a ∈ R with a ̸= 0, there exists a unique element a−1 such that a−1a = aa−1 = 1. A\ncommutative division ring is called a field. The relationship among rings, integral domains,\ndivision rings, and fields is shown in Figure 16.1.\n\n255\n\n\n\n256 CHAPTER 16. RINGS\n\nRings with\nIdentity\n\nDivision\nRings\n\nCommutative\nRings\n\nIntegral\nDomains\n\nRings\n\nFields\n\nFigure 16.1: Types of rings\n\nExample 16.2. As we have mentioned previously, the integers form a ring. In fact, Z is\nan integral domain. Certainly if ab = 0 for two integers a and b, either a = 0 or b = 0.\nHowever, Z is not a field. There is no integer that is the multiplicative inverse of 2, since\n1/2 is not an integer. The only integers with multiplicative inverses are 1 and −1.\nExample 16.3. Under the ordinary operations of addition and multiplication, all of the\nfamiliar number systems are rings: the rationals, Q; the real numbers, R; and the complex\nnumbers, C. Each of these rings is a field.\nExample 16.4. We can define the product of two elements a and b in Zn by ab (mod n).\nFor instance, in Z12, 5 · 7 ≡ 11 (mod 12). This product makes the abelian group Zn into\na ring. Certainly Zn is a commutative ring; however, it may fail to be an integral domain.\nIf we consider 3 · 4 ≡ 0 (mod 12) in Z12, it is easy to see that a product of two nonzero\nelements in the ring can be equal to zero.\n\nA nonzero element a in a ring R is called a zero divisor if there is a nonzero element\nb in R such that ab = 0. In the previous example, 3 and 4 are zero divisors in Z12.\nExample 16.5. In calculus the continuous real-valued functions on an interval [a, b] form\na commutative ring. We add or multiply two functions by adding or multiplying the values\nof the functions. If f(x) = x2 and g(x) = cosx, then (f + g)(x) = f(x) + g(x) = x2 + cosx\nand (fg)(x) = f(x)g(x) = x2 cosx.\nExample 16.6. The 2×2 matrices with entries in R form a ring under the usual operations\nof matrix addition and multiplication. This ring is noncommutative, since it is usually the\ncase that AB ̸= BA. Also, notice that we can have AB = 0 when neither A nor B is zero.\nExample 16.7. For an example of a noncommutative division ring, let\n\n1 =\n\n(\n1 0\n\n0 1\n\n)\n, i =\n\n(\n0 1\n\n−1 0\n\n)\n, j =\n\n(\n0 i\n\ni 0\n\n)\n, k =\n\n(\ni 0\n\n0 −i\n\n)\n,\n\nwhere i2 = −1. These elements satisfy the following relations:\ni2 = j2 = k2 = −1\n\nij = k\njk = i\nki = j\nji = −k\n\nkj = −i\nik = −j.\n\n\n\n16.1. RINGS 257\n\nLet H consist of elements of the form a + bi + cj + dk, where a, b, c, d are real numbers.\nEquivalently, H can be considered to be the set of all 2× 2 matrices of the form(\n\nα β\n\n−β α\n\n)\n,\n\nwhere α = a + di and β = b + ci are complex numbers. We can define addition and\nmultiplication on H either by the usual matrix operations or in terms of the generators 1,\ni, j, and k:\n\n(a1 + b1i + c1j + d1k) + (a2 + b2i + c2j + d2k)\n= (a1 + a2) + (b1 + b2)i + (c1 + c2)j + (d1 + d2)k\n\nand\n(a1 + b1i + c1j + d1k)(a2 + b2i + c2j + d2k) = α+ βi + γj + δk,\n\nwhere\n\nα = a1a2 − b1b2 − c1c2 − d1d2\n\nβ = a1b2 + a2b1 + c1d2 − d1c2\n\nγ = a1c2 − b1d2 + c1a2 − d1b2\n\nδ = a1d2 + b1c2 − c1b2 − d1a2.\n\nThough multiplication looks complicated, it is actually a straightforward computation if\nwe remember that we just add and multiply elements in H like polynomials and keep in\nmind the relationships between the generators i, j, and k. The ring H is called the ring of\nquaternions.\n\nTo show that the quaternions are a division ring, we must be able to find an inverse for\neach nonzero element. Notice that\n\n(a+ bi + cj + dk)(a− bi − cj − dk) = a2 + b2 + c2 + d2.\n\nThis element can be zero only if a, b, c, and d are all zero. So if a+ bi + cj + dk ̸= 0,\n\n(a+ bi + cj + dk)\n(\na− bi − cj − dk\na2 + b2 + c2 + d2\n\n)\n= 1.\n\nProposition 16.8. Let R be a ring with a, b ∈ R. Then\n\n1. a0 = 0a = 0;\n\n2. a(−b) = (−a)b = −ab;\n\n3. (−a)(−b) = ab.\n\nProof. To prove (1), observe that\n\na0 = a(0 + 0) = a0 + a0;\n\nhence, a0 = 0. Similarly, 0a = 0. For (2), we have ab + a(−b) = a(b − b) = a0 = 0;\nconsequently, −ab = a(−b). Similarly, −ab = (−a)b. Part (3) follows directly from (2)\nsince (−a)(−b) = −(a(−b)) = −(−ab) = ab.\n\nJust as we have subgroups of groups, we have an analogous class of substructures for\nrings. A subring S of a ring R is a subset S of R such that S is also a ring under the\ninherited operations from R.\n\n\n\n258 CHAPTER 16. RINGS\n\nExample 16.9. The ring nZ is a subring of Z. Notice that even though the original ring\nmay have an identity, we do not require that its subring have an identity. We have the\nfollowing chain of subrings:\n\nZ ⊂ Q ⊂ R ⊂ C.\n\nThe following proposition gives us some easy criteria for determining whether or not\na subset of a ring is indeed a subring. (We will leave the proof of this proposition as an\nexercise.)\n\nProposition 16.10. Let R be a ring and S a subset of R. Then S is a subring of R if and\nonly if the following conditions are satisfied.\n\n1. S ̸= ∅.\n\n2. rs ∈ S for all r, s ∈ S.\n\n3. r − s ∈ S for all r, s ∈ S.\n\nExample 16.11. Let R = M2(R) be the ring of 2 × 2 matrices with entries in R. If T is\nthe set of upper triangular matrices in R; i.e.,\n\nT =\n\n{(\na b\n\n0 c\n\n)\n: a, b, c ∈ R\n\n}\n,\n\nthen T is a subring of R. If\n\nA =\n\n(\na b\n\n0 c\n\n)\nand B =\n\n(\na′ b′\n\n0 c′\n\n)\nare in T , then clearly A−B is also in T . Also,\n\nAB =\n\n(\naa′ ab′ + bc′\n\n0 cc′\n\n)\nis in T .\n\n16.2 Integral Domains and Fields\nLet us briefly recall some definitions. If R is a ring and r is a nonzero element in R, then\nr is said to be a zero divisor if there is some nonzero element s ∈ R such that rs = 0. A\ncommutative ring with identity is said to be an integral domain if it has no zero divisors.\nIf an element a in a ring R with identity has a multiplicative inverse, we say that a is a\nunit. If every nonzero element in a ring R is a unit, then R is called a division ring. A\ncommutative division ring is called a field.\n\nExample 16.12. If i2 = −1, then the set Z[i] = {m + ni : m,n ∈ Z} forms a ring known\nas the Gaussian integers. It is easily seen that the Gaussian integers are a subring of the\ncomplex numbers since they are closed under addition and multiplication. Let α = a + bi\nbe a unit in Z[i]. Then α = a− bi is also a unit since if αβ = 1, then αβ = 1. If β = c+ di,\nthen\n\n1 = αβαβ = (a2 + b2)(c2 + d2).\n\nTherefore, a2 + b2 must either be 1 or −1; or, equivalently, a + bi = ±1 or a + bi = ±i.\nTherefore, units of this ring are ±1 and ±i; hence, the Gaussian integers are not a field.\nWe will leave it as an exercise to prove that the Gaussian integers are an integral domain.\n\n\n\n16.2. INTEGRAL DOMAINS AND FIELDS 259\n\nExample 16.13. The set of matrices\n\nF =\n\n{(\n1 0\n\n0 1\n\n)\n,\n\n(\n1 1\n\n1 0\n\n)\n,\n\n(\n0 1\n\n1 1\n\n)\n,\n\n(\n0 0\n\n0 0\n\n)}\nwith entries in Z2 forms a field.\n\nExample 16.14. The set Q(\n√\n2 ) = {a + b\n\n√\n2 : a, b ∈ Q} is a field. The inverse of an\n\nelement a+ b\n√\n2 in Q(\n\n√\n2 ) is\n\na\n\na2 − 2b2\n+\n\n−b\na2 − 2b2\n\n√\n2.\n\nWe have the following alternative characterization of integral domains.\n\nProposition 16.15 (Cancellation Law). Let D be a commutative ring with identity. Then\nD is an integral domain if and only if for all nonzero elements a ∈ D with ab = ac, we have\nb = c.\n\nProof. Let D be an integral domain. Then D has no zero divisors. Let ab = ac with\na ̸= 0. Then a(b− c) = 0. Hence, b− c = 0 and b = c.\n\nConversely, let us suppose that cancellation is possible in D. That is, suppose that\nab = ac implies b = c. Let ab = 0. If a ̸= 0, then ab = a0 or b = 0. Therefore, a cannot be\na zero divisor.\n\nThe following surprising theorem is due to Wedderburn.\n\nTheorem 16.16. Every finite integral domain is a field.\n\nProof. Let D be a finite integral domain and D∗ be the set of nonzero elements of D. We\nmust show that every element in D∗ has an inverse. For each a ∈ D∗ we can define a map\nλa : D∗ → D∗ by λa(d) = ad. This map makes sense, because if a ̸= 0 and d ̸= 0, then\nad ̸= 0. The map λa is one-to-one, since for d1, d2 ∈ D∗,\n\nad1 = λa(d1) = λa(d2) = ad2\n\nimplies d1 = d2 by left cancellation. Since D∗ is a finite set, the map λa must also be\nonto; hence, for some d ∈ D∗, λa(d) = ad = 1. Therefore, a has a left inverse. Since D is\ncommutative, d must also be a right inverse for a. Consequently, D is a field.\n\nFor any nonnegative integer n and any element r in a ring R we write r + · · · + r (n\ntimes) as nr. We define the characteristic of a ring R to be the least positive integer n\nsuch that nr = 0 for all r ∈ R. If no such integer exists, then the characteristic of R is\ndefined to be 0. We will denote the characteristic of R by charR.\n\nExample 16.17. For every prime p, Zp is a field of characteristic p. By Proposition 3.4,\nevery nonzero element in Zp has an inverse; hence, Zp is a field. If a is any nonzero element\nin the field, then pa = 0, since the order of any nonzero element in the abelian group Zp is\np.\n\nLemma 16.18. Let R be a ring with identity. If 1 has order n, then the characteristic of\nR is n.\n\nProof. If 1 has order n, then n is the least positive integer such that n1 = 0. Thus, for\nall r ∈ R,\n\nnr = n(1r) = (n1)r = 0r = 0.\n\nOn the other hand, if no positive n exists such that n1 = 0, then the characteristic of R is\nzero.\n\n\n\n260 CHAPTER 16. RINGS\n\nTheorem 16.19. The characteristic of an integral domain is either prime or zero.\n\nProof. Let D be an integral domain and suppose that the characteristic of D is n with\nn ̸= 0. If n is not prime, then n = ab, where 1 < a < n and 1 < b < n. By Lemma 16.18,\nwe need only consider the case n1 = 0. Since 0 = n1 = (ab)1 = (a1)(b1) and there are no\nzero divisors in D, either a1 = 0 or b1 = 0. Hence, the characteristic of D must be less than\nn, which is a contradiction. Therefore, n must be prime.\n\n16.3 Ring Homomorphisms and Ideals\nIn the study of groups, a homomorphism is a map that preserves the operation of the group.\nSimilarly, a homomorphism between rings preserves the operations of addition and multi-\nplication in the ring. More specifically, if R and S are rings, then a ring homomorphism\nis a map ϕ : R→ S satisfying\n\nϕ(a+ b) = ϕ(a) + ϕ(b)\n\nϕ(ab) = ϕ(a)ϕ(b)\n\nfor all a, b ∈ R. If ϕ : R → S is a one-to-one and onto homomorphism, then ϕ is called an\nisomorphism of rings.\n\nThe set of elements that a ring homomorphism maps to 0 plays a fundamental role in\nthe theory of rings. For any ring homomorphism ϕ : R→ S, we define the kernel of a ring\nhomomorphism to be the set\n\nkerϕ = {r ∈ R : ϕ(r) = 0}.\n\nExample 16.20. For any integer n we can define a ring homomorphism ϕ : Z → Zn by\na 7→ a (mod n). This is indeed a ring homomorphism, since\n\nϕ(a+ b) = (a+ b) (mod n)\n\n= a (mod n) + b (mod n)\n\n= ϕ(a) + ϕ(b)\n\nand\n\nϕ(ab) = ab (mod n)\n\n= a (mod n) · b (mod n)\n\n= ϕ(a)ϕ(b).\n\nThe kernel of the homomorphism ϕ is nZ.\n\nExample 16.21. Let C[a, b] be the ring of continuous real-valued functions on an interval\n[a, b] as in Example 16.5. For a fixed α ∈ [a, b], we can define a ring homomorphism\nϕα : C[a, b] → R by ϕα(f) = f(α). This is a ring homomorphism since\n\nϕα(f + g) = (f + g)(α) = f(α) + g(α) = ϕα(f) + ϕα(g)\n\nϕα(fg) = (fg)(α) = f(α)g(α) = ϕα(f)ϕα(g).\n\nRing homomorphisms of the type ϕα are called evaluation homomorphisms.\n\nIn the next proposition we will examine some fundamental properties of ring homomor-\nphisms. The proof of the proposition is left as an exercise.\n\n\n\n16.3. RING HOMOMORPHISMS AND IDEALS 261\n\nProposition 16.22. Let ϕ : R→ S be a ring homomorphism.\n\n1. If R is a commutative ring, then ϕ(R) is a commutative ring.\n\n2. ϕ(0) = 0.\n\n3. Let 1R and 1S be the identities for R and S, respectively. If ϕ is onto, then ϕ(1R) = 1S.\n\n4. If R is a field and ϕ(R) ̸= {0}, then ϕ(R) is a field.\n\nIn group theory we found that normal subgroups play a special role. These subgroups\nhave nice characteristics that make them more interesting to study than arbitrary subgroups.\nIn ring theory the objects corresponding to normal subgroups are a special class of subrings\ncalled ideals. An ideal in a ring R is a subring I of R such that if a is in I and r is in R,\nthen both ar and ra are in I; that is, rI ⊂ I and Ir ⊂ I for all r ∈ R.\n\nExample 16.23. Every ring R has at least two ideals, {0} and R. These ideals are called\nthe trivial ideals.\n\nLet R be a ring with identity and suppose that I is an ideal in R such that 1 is in I.\nSince for any r ∈ R, r1 = r ∈ I by the definition of an ideal, I = R.\n\nExample 16.24. If a is any element in a commutative ring R with identity, then the set\n\n⟨a⟩ = {ar : r ∈ R}\n\nis an ideal in R. Certainly, ⟨a⟩ is nonempty since both 0 = a0 and a = a1 are in ⟨a⟩. The\nsum of two elements in ⟨a⟩ is again in ⟨a⟩ since ar + ar′ = a(r + r′). The inverse of ar is\n−ar = a(−r) ∈ ⟨a⟩. Finally, if we multiply an element ar ∈ ⟨a⟩ by an arbitrary element\ns ∈ R, we have s(ar) = a(sr). Therefore, ⟨a⟩ satisfies the definition of an ideal.\n\nIf R is a commutative ring with identity, then an ideal of the form ⟨a⟩ = {ar : r ∈ R}\nis called a principal ideal.\n\nTheorem 16.25. Every ideal in the ring of integers Z is a principal ideal.\n\nProof. The zero ideal {0} is a principal ideal since ⟨0⟩ = {0}. If I is any nonzero ideal\nin Z, then I must contain some positive integer m. There exists a least positive integer n\nin I by the Principle of Well-Ordering. Now let a be any element in I. Using the division\nalgorithm, we know that there exist integers q and r such that\n\na = nq + r\n\nwhere 0 ≤ r < n. This equation tells us that r = a− nq ∈ I, but r must be 0 since n is the\nleast positive element in I. Therefore, a = nq and I = ⟨n⟩.\n\nExample 16.26. The set nZ is ideal in the ring of integers. If na is in nZ and b is in Z,\nthen nab is in nZ as required. In fact, by Theorem 16.25, these are the only ideals of Z.\n\nProposition 16.27. The kernel of any ring homomorphism ϕ : R→ S is an ideal in R.\n\nProof. We know from group theory that kerϕ is an additive subgroup of R. Suppose that\nr ∈ R and a ∈ kerϕ. Then we must show that ar and ra are in kerϕ. However,\n\nϕ(ar) = ϕ(a)ϕ(r) = 0ϕ(r) = 0\n\nand\nϕ(ra) = ϕ(r)ϕ(a) = ϕ(r)0 = 0.\n\n\n\n262 CHAPTER 16. RINGS\n\nRemark 16.28. In our definition of an ideal we have required that rI ⊂ I and Ir ⊂ I for\nall r ∈ R. Such ideals are sometimes referred to as two-sided ideals. We can also consider\none-sided ideals; that is, we may require only that either rI ⊂ I or Ir ⊂ I for r ∈ R\nhold but not both. Such ideals are called left ideals and right ideals, respectively. Of\ncourse, in a commutative ring any ideal must be two-sided. In this text we will concentrate\non two-sided ideals.\nTheorem 16.29. Let I be an ideal of R. The factor group R/I is a ring with multiplication\ndefined by\n\n(r + I)(s+ I) = rs+ I.\n\nProof. We already know that R/I is an abelian group under addition. Let r+ I and s+ I\nbe in R/I. We must show that the product (r + I)(s + I) = rs + I is independent of the\nchoice of coset; that is, if r′ ∈ r + I and s′ ∈ s + I, then r′s′ must be in rs + I. Since\nr′ ∈ r+ I, there exists an element a in I such that r′ = r+ a. Similarly, there exists a b ∈ I\nsuch that s′ = s+ b. Notice that\n\nr′s′ = (r + a)(s+ b) = rs+ as+ rb+ ab\n\nand as + rb + ab ∈ I since I is an ideal; consequently, r′s′ ∈ rs + I. We will leave as\nan exercise the verification of the associative law for multiplication and the distributive\nlaws.\n\nThe ring R/I in Theorem 16.29 is called the factor or quotient ring. Just as with\ngroup homomorphisms and normal subgroups, there is a relationship between ring homo-\nmorphisms and ideals.\nTheorem 16.30. Let I be an ideal of R. The map ϕ : R→ R/I defined by ϕ(r) = r+ I is\na ring homomorphism of R onto R/I with kernel I.\nProof. Certainly ϕ : R → R/I is a surjective abelian group homomorphism. It remains\nto show that ϕ works correctly under ring multiplication. Let r and s be in R. Then\n\nϕ(r)ϕ(s) = (r + I)(s+ I) = rs+ I = ϕ(rs),\n\nwhich completes the proof of the theorem.\n\nThe map ϕ : R → R/I is often called the natural or canonical homomorphism. In\nring theory we have isomorphism theorems relating ideals and ring homomorphisms similar\nto the isomorphism theorems for groups that relate normal subgroups and homomorphisms\nin Chapter 11. We will prove only the First Isomorphism Theorem for rings in this chapter\nand leave the proofs of the other two theorems as exercises. All of the proofs are similar to\nthe proofs of the isomorphism theorems for groups.\nTheorem 16.31 (First Isomorphism Theorem). Let ψ : R → S be a ring homomorphism.\nThen kerψ is an ideal of R. If ϕ : R → R/ kerψ is the canonical homomorphism, then\nthere exists a unique isomorphism η : R/ kerψ → ψ(R) such that ψ = ηϕ.\nProof. Let K = kerψ. By the First Isomorphism Theorem for groups, there exists a\nwell-defined group homomorphism η : R/K → ψ(R) defined by η(r + K) = ψ(r) for the\nadditive abelian groups R and R/K. To show that this is a ring homomorphism, we need\nonly show that η((r +K)(s+K)) = η(r +K)η(s+K); but\n\nη((r +K)(s+K)) = η(rs+K)\n\n= ψ(rs)\n\n= ψ(r)ψ(s)\n\n= η(r +K)η(s+K).\n\n\n\n16.4. MAXIMAL AND PRIME IDEALS 263\n\nTheorem 16.32 (Second Isomorphism Theorem). Let I be a subring of a ring R and J an\nideal of R. Then I ∩ J is an ideal of I and\n\nI/I ∩ J ∼= (I + J)/J.\n\nTheorem 16.33 (Third Isomorphism Theorem). Let R be a ring and I and J be ideals of\nR where J ⊂ I. Then\n\nR/I ∼=\nR/J\n\nI/J\n.\n\nTheorem 16.34 (Correspondence Theorem). Let I be an ideal of a ring R. Then S 7→ S/I\nis a one-to-one correspondence between the set of subrings S containing I and the set of\nsubrings of R/I. Furthermore, the ideals of R containing I correspond to ideals of R/I.\n\n16.4 Maximal and Prime Ideals\nIn this particular section we are especially interested in certain ideals of commutative rings.\nThese ideals give us special types of factor rings. More specifically, we would like to char-\nacterize those ideals I of a commutative ring R such that R/I is an integral domain or a\nfield.\n\nA proper ideal M of a ring R is a maximal ideal of R if the ideal M is not a proper\nsubset of any ideal of R except R itself. That is, M is a maximal ideal if for any ideal I\nproperly containing M , I = R. The following theorem completely characterizes maximal\nideals for commutative rings with identity in terms of their corresponding factor rings.\n\nTheorem 16.35. Let R be a commutative ring with identity and M an ideal in R. Then\nM is a maximal ideal of R if and only if R/M is a field.\n\nProof. Let M be a maximal ideal in R. If R is a commutative ring, then R/M must also\nbe a commutative ring. Clearly, 1 +M acts as an identity for R/M . We must also show\nthat every nonzero element in R/M has an inverse. If a+M is a nonzero element in R/M ,\nthen a /∈ M . Define I to be the set {ra +m : r ∈ R and m ∈ M}. We will show that I is\nan ideal in R. The set I is nonempty since 0a+0 = 0 is in I. If r1a+m1 and r2a+m2 are\ntwo elements in I, then\n\n(r1a+m1)− (r2a+m2) = (r1 − r2)a+ (m1 −m2)\n\nis in I. Also, for any r ∈ R it is true that rI ⊂ I; hence, I is closed under multiplication\nand satisfies the necessary conditions to be an ideal. Therefore, by Proposition 16.10 and\nthe definition of an ideal, I is an ideal properly containing M . Since M is a maximal ideal,\nI = R; consequently, by the definition of I there must be an m in M and an element b in\nR such that 1 = ab+m. Therefore,\n\n1 +M = ab+M = ba+M = (a+M)(b+M).\n\nConversely, suppose that M is an ideal and R/M is a field. Since R/M is a field, it\nmust contain at least two elements: 0 +M = M and 1 +M . Hence, M is a proper ideal\nof R. Let I be any ideal properly containing M . We need to show that I = R. Choose\na in I but not in M . Since a +M is a nonzero element in a field, there exists an element\nb+M in R/M such that (a+M)(b+M) = ab+M = 1 +M . Consequently, there exists\nan element m ∈M such that ab+m = 1 and 1 is in I. Therefore, r1 = r ∈ I for all r ∈ R.\nConsequently, I = R.\n\n\n\n264 CHAPTER 16. RINGS\n\nExample 16.36. Let pZ be an ideal in Z, where p is prime. Then pZ is a maximal ideal\nsince Z/pZ ∼= Zp is a field.\n\nA proper ideal P in a commutative ring R is called a prime ideal if whenever ab ∈ P ,\nthen either a ∈ P or b ∈ P .1\n\nExample 16.37. It is easy to check that the set P = {0, 2, 4, 6, 8, 10} is an ideal in Z12.\nThis ideal is prime. In fact, it is a maximal ideal.\n\nProposition 16.38. Let R be a commutative ring with identity 1, where 1 ̸= 0. Then P is\na prime ideal in R if and only if R/P is an integral domain.\n\nProof. First let us assume that P is an ideal in R and R/P is an integral domain. Suppose\nthat ab ∈ P . If a+P and b+P are two elements of R/P such that (a+P )(b+P ) = 0+P = P ,\nthen either a+ P = P or b+ P = P . This means that either a is in P or b is in P , which\nshows that P must be prime.\n\nConversely, suppose that P is prime and\n\n(a+ P )(b+ P ) = ab+ P = 0 + P = P.\n\nThen ab ∈ P . If a /∈ P , then b must be in P by the definition of a prime ideal; hence,\nb+ P = 0 + P and R/P is an integral domain.\n\nExample 16.39. Every ideal in Z is of the form nZ. The factor ring Z/nZ ∼= Zn is an\nintegral domain only when n is prime. It is actually a field. Hence, the nonzero prime ideals\nin Z are the ideals pZ, where p is prime. This example really justifies the use of the word\n“prime” in our definition of prime ideals.\n\nSince every field is an integral domain, we have the following corollary.\n\nCorollary 16.40. Every maximal ideal in a commutative ring with identity is also a prime\nideal.\n\nHistorical Note\n\nAmalie Emmy Noether, one of the outstanding mathematicians of the twentieth century,\nwas born in Erlangen, Germany in 1882. She was the daughter of Max Noether (1844–\n1921), a distinguished mathematician at the University of Erlangen. Together with Paul\nGordon (1837–1912), Emmy Noether’s father strongly influenced her early education. She\nentered the University of Erlangen at the age of 18. Although women had been admitted\nto universities in England, France, and Italy for decades, there was great resistance to\ntheir presence at universities in Germany. Noether was one of only two women among\nthe university’s 986 students. After completing her doctorate under Gordon in 1907, she\ncontinued to do research at Erlangen, occasionally lecturing when her father was ill.\n\nNoether went to Göttingen to study in 1916. David Hilbert and Felix Klein tried un-\nsuccessfully to secure her an appointment at Göttingen. Some of the faculty objected to\nwomen lecturers, saying, “What will our soldiers think when they return to the university\nand are expected to learn at the feet of a woman?” Hilbert, annoyed at the question, re-\nsponded, “Meine Herren, I do not see that the sex of a candidate is an argument against\nher admission as a Privatdozent. After all, the Senate is not a bathhouse.” At the end of\nWorld War I, attitudes changed and conditions greatly improved for women. After Noether\n\n1It is possible to define prime ideals in a noncommutative ring. See [1] or [3].\n\n\n\n16.5. AN APPLICATION TO SOFTWARE DESIGN 265\n\npassed her habilitation examination in 1919, she was given a title and was paid a small sum\nfor her lectures.\n\nIn 1922, Noether became a Privatdozent at Göttingen. Over the next 11 years she used\naxiomatic methods to develop an abstract theory of rings and ideals. Though she was not\ngood at lecturing, Noether was an inspiring teacher. One of her many students was B. L.\nvan der Waerden, author of the first text treating abstract algebra from a modern point of\nview. Some of the other mathematicians Noether influenced or closely worked with were\nAlexandroff, Artin, Brauer, Courant, Hasse, Hopf, Pontryagin, von Neumann, and Weyl.\nOne of the high points of her career was an invitation to address the International Congress\nof Mathematicians in Zurich in 1932. In spite of all the recognition she received from her\ncolleagues, Noether’s abilities were never recognized as they should have been during her\nlifetime. She was never promoted to full professor by the Prussian academic bureaucracy.\n\nIn 1933, Noether, a Jew, was banned from participation in all academic activities in\nGermany. She emigrated to the United States, took a position at Bryn Mawr College, and\nbecame a member of the Institute for Advanced Study at Princeton. Noether died suddenly\non April 14, 1935. After her death she was eulogized by such notable scientists as Albert\nEinstein.\n\n16.5 An Application to Software Design\nThe Chinese Remainder Theorem is a result from elementary number theory about the\nsolution of systems of simultaneous congruences. The Chinese mathematician Sun-tsï wrote\nabout the theorem in the first century A.D. This theorem has some interesting consequences\nin the design of software for parallel processors.\n\nLemma 16.41. Let m and n be positive integers such that gcd(m,n) = 1. Then for a, b ∈ Z\nthe system\n\nx ≡ a (mod m)\n\nx ≡ b (mod n)\n\nhas a solution. If x1 and x2 are two solutions of the system, then x1 ≡ x2 (mod mn).\n\nProof. The equation x ≡ a (mod m) has a solution since a+km satisfies the equation for\nall k ∈ Z. We must show that there exists an integer k1 such that\n\na+ k1m ≡ b (mod n).\n\nThis is equivalent to showing that\n\nk1m ≡ (b− a) (mod n)\n\nhas a solution for k1. Since m and n are relatively prime, there exist integers s and t such\nthat ms+ nt = 1. Consequently,\n\n(b− a)ms = (b− a)− (b− a)nt,\n\nor\n[(b− a)s]m ≡ (b− a) (mod n).\n\nNow let k1 = (b− a)s.\n\n\n\n266 CHAPTER 16. RINGS\n\nTo show that any two solutions are congruent modulo mn, let c1 and c2 be two solutions\nof the system. That is,\n\nci ≡ a (mod m)\n\nci ≡ b (mod n)\n\nfor i = 1, 2. Then\n\nc2 ≡ c1 (mod m)\n\nc2 ≡ c1 (mod n).\n\nTherefore, both m and n divide c1 − c2. Consequently, c2 ≡ c1 (mod mn).\n\nExample 16.42. Let us solve the system\n\nx ≡ 3 (mod 4)\n\nx ≡ 4 (mod 5).\n\nUsing the Euclidean algorithm, we can find integers s and t such that 4s + 5t = 1. Two\nsuch integers are s = 4 and t = −3. Consequently,\n\nx = a+ k1m = 3 + 4k1 = 3 + 4[(5− 4)4] = 19.\n\nTheorem 16.43 (Chinese Remainder Theorem). Let n1, n2, . . . , nk be positive integers such\nthat gcd(ni, nj) = 1 for i ̸= j. Then for any integers a1, . . . , ak, the system\n\nx ≡ a1 (mod n1)\n\nx ≡ a2 (mod n2)\n\n...\nx ≡ ak (mod nk)\n\nhas a solution. Furthermore, any two solutions of the system are congruent modulo\nn1n2 · · ·nk.\n\nProof. We will use mathematical induction on the number of equations in the system. If\nthere are k = 2 equations, then the theorem is true by Lemma 16.41. Now suppose that\nthe result is true for a system of k equations or less and that we wish to find a solution of\n\nx ≡ a1 (mod n1)\n\nx ≡ a2 (mod n2)\n\n...\nx ≡ ak+1 (mod nk+1).\n\nConsidering the first k equations, there exists a solution that is unique modulo n1 · · ·nk,\nsay a. Since n1 · · ·nk and nk+1 are relatively prime, the system\n\nx ≡ a (mod n1 · · ·nk)\nx ≡ ak+1 (mod nk+1)\n\nhas a solution that is unique modulo n1 . . . nk+1 by the lemma.\n\n\n\n16.5. AN APPLICATION TO SOFTWARE DESIGN 267\n\nExample 16.44. Let us solve the system\n\nx ≡ 3 (mod 4)\n\nx ≡ 4 (mod 5)\n\nx ≡ 1 (mod 9)\n\nx ≡ 5 (mod 7).\n\nFrom Example 16.42 we know that 19 is a solution of the first two congruences and any\nother solution of the system is congruent to 19 (mod 20). Hence, we can reduce the system\nto a system of three congruences:\n\nx ≡ 19 (mod 20)\n\nx ≡ 1 (mod 9)\n\nx ≡ 5 (mod 7).\n\nSolving the next two equations, we can reduce the system to\n\nx ≡ 19 (mod 180)\n\nx ≡ 5 (mod 7).\n\nSolving this last system, we find that 19 is a solution for the system that is unique up to\nmodulo 1260.\n\nOne interesting application of the Chinese Remainder Theorem in the design of computer\nsoftware is that the theorem allows us to break up a calculation involving large integers into\nseveral less formidable calculations. A computer will handle integer calculations only up to\na certain size due to the size of its processor chip, which is usually a 32 or 64-bit processor\nchip. For example, the largest integer available on a computer with a 64-bit processor chip\nis\n\n263 − 1 = 9,223,372,036,854,775,807.\n\nLarger processors such as 128 or 256-bit have been proposed or are under development.\nThere is even talk of a 512-bit processor chip. The largest integer that such a chip could\nstore with be 2511−1, which would be a 154 digit number. However, we would need to deal\nwith much larger numbers to break sophisticated encryption schemes.\n\nSpecial software is required for calculations involving larger integers which cannot be\nadded directly by the machine. By using the Chinese Remainder Theorem we can break\ndown large integer additions and multiplications into calculations that the computer can\nhandle directly. This is especially useful on parallel processing computers which have the\nability to run several programs concurrently.\n\nMost computers have a single central processing unit (CPU) containing one processor\nchip and can only add two numbers at a time. To add a list of ten numbers, the CPU must\ndo nine additions in sequence. However, a parallel processing computer has more than one\nCPU. A computer with 10 CPUs, for example, can perform 10 different additions at the\nsame time. If we can take a large integer and break it down into parts, sending each part\nto a different CPU, then by performing several additions or multiplications simultaneously\non those parts, we can work with an integer that the computer would not be able to handle\nas a whole.\n\nExample 16.45. Suppose that we wish to multiply 2134 by 1531. We will use the integers\n95, 97, 98, and 99 because they are relatively prime. We can break down each integer into\n\n\n\n268 CHAPTER 16. RINGS\n\nfour parts:\n\n2134 ≡ 44 (mod 95)\n\n2134 ≡ 0 (mod 97)\n\n2134 ≡ 76 (mod 98)\n\n2134 ≡ 55 (mod 99)\n\nand\n\n1531 ≡ 11 (mod 95)\n\n1531 ≡ 76 (mod 97)\n\n1531 ≡ 61 (mod 98)\n\n1531 ≡ 46 (mod 99).\n\nMultiplying the corresponding equations, we obtain\n\n2134 · 1531 ≡ 44 · 11 ≡ 9 (mod 95)\n\n2134 · 1531 ≡ 0 · 76 ≡ 0 (mod 97)\n\n2134 · 1531 ≡ 76 · 61 ≡ 30 (mod 98)\n\n2134 · 1531 ≡ 55 · 46 ≡ 55 (mod 99).\n\nEach of these four computations can be sent to a different processor if our computer has\nseveral CPUs. By the above calculation, we know that 2134 ·1531 is a solution of the system\n\nx ≡ 9 (mod 95)\n\nx ≡ 0 (mod 97)\n\nx ≡ 30 (mod 98)\n\nx ≡ 55 (mod 99).\n\nThe Chinese Remainder Theorem tells us that solutions are unique up to modulo 95 · 97 ·\n98 · 99 = 89,403,930. Solving this system of congruences for x tells us that 2134 · 1531 =\n3,267,154.\n\nThe conversion of the computation into the four subcomputations will take some com-\nputing time. In addition, solving the system of congruences can also take considerable time.\nHowever, if we have many computations to be performed on a particular set of numbers, it\nmakes sense to transform the problem as we have done above and to perform the necessary\ncalculations simultaneously.\n\n16.6 Exercises\n1. Which of the following sets are rings with respect to the usual operations of addition\nand multiplication? If the set is a ring, is it also a field?\n(a) 7Z\n(b) Z18\n\n(c) Q(\n√\n2 ) = {a+ b\n\n√\n2 : a, b ∈ Q}\n\n(d) Q(\n√\n2,\n√\n3 ) = {a+ b\n\n√\n2 + c\n\n√\n3 + d\n\n√\n6 : a, b, c, d ∈ Q}\n\n(e) Z[\n√\n3 ] = {a+ b\n\n√\n3 : a, b ∈ Z}\n\n(f) R = {a+ b 3\n√\n3 : a, b ∈ Q}\n\n\n\n16.6. EXERCISES 269\n\n(g) Z[i] = {a+ bi : a, b ∈ Z and i2 = −1}\n(h) Q( 3\n\n√\n3 ) = {a+ b 3\n\n√\n3 + c 3\n\n√\n9 : a, b, c ∈ Q}\n\n2. Let R be the ring of 2× 2 matrices of the form(\na b\n\n0 0\n\n)\n,\n\nwhere a, b ∈ R. Show that although R is a ring that has no identity, we can find a subring\nS of R with an identity.\n\n3. List or characterize all of the units in each of the following rings.\n(a) Z10\n\n(b) Z12\n\n(c) Z7\n\n(d) M2(Z), the 2× 2 matrices with entries in Z\n(e) M2(Z2), the 2× 2 matrices with entries in Z2\n\n4. Find all of the ideals in each of the following rings. Which of these ideals are maximal\nand which are prime?\n(a) Z18\n\n(b) Z25\n\n(c) M2(R), the 2× 2 matrices with entries in R\n(d) M2(Z), the 2× 2 matrices with entries in Z\n(e) Q\n\n5. For each of the following rings R with ideal I, give an addition table and a multiplication\ntable for R/I.\n(a) R = Z and I = 6Z\n(b) R = Z12 and I = {0, 3, 6, 9}\n\n6. Find all homomorphisms ϕ : Z/6Z → Z/15Z.\n\n7. Prove that R is not isomorphic to C.\n\n8. Prove or disprove: The ring Q(\n√\n2 ) = {a + b\n\n√\n2 : a, b ∈ Q} is isomorphic to the ring\n\nQ(\n√\n3 ) = {a+ b\n\n√\n3 : a, b ∈ Q}.\n\n9. What is the characteristic of the field formed by the set of matrices\n\nF =\n\n{(\n1 0\n\n0 1\n\n)\n,\n\n(\n1 1\n\n1 0\n\n)\n,\n\n(\n0 1\n\n1 1\n\n)\n,\n\n(\n0 0\n\n0 0\n\n)}\nwith entries in Z2?\n\n10. Define a map ϕ : C → M2(R) by\n\nϕ(a+ bi) =\n\n(\na b\n\n−b a\n\n)\n.\n\nShow that ϕ is an isomorphism of C with its image in M2(R).\n\n\n\n270 CHAPTER 16. RINGS\n\n11. Prove that the Gaussian integers, Z[i], are an integral domain.\n\n12. Prove that Z[\n√\n3 i] = {a+ b\n\n√\n3 i : a, b ∈ Z} is an integral domain.\n\n13. Solve each of the following systems of congruences.\n\n\n\n16.6. EXERCISES 271\n\n(a)\n\nx ≡ 2 (mod 5)\n\nx ≡ 6 (mod 11)\n\n(b)\n\nx ≡ 3 (mod 7)\n\nx ≡ 0 (mod 8)\n\nx ≡ 5 (mod 15)\n\n(c)\n\nx ≡ 2 (mod 4)\n\nx ≡ 4 (mod 7)\n\nx ≡ 7 (mod 9)\n\nx ≡ 5 (mod 11)\n\n(d)\n\nx ≡ 3 (mod 5)\n\nx ≡ 0 (mod 8)\n\nx ≡ 1 (mod 11)\n\nx ≡ 5 (mod 13)\n\n14. Use the method of parallel computation outlined in the text to calculate 2234 + 4121\nby dividing the calculation into four separate additions modulo 95, 97, 98, and 99.\n\n15. Explain why the method of parallel computation outlined in the text fails for 2134·1531\nif we attempt to break the calculation down into two smaller calculations modulo 98 and\n99.\n\n16. If R is a field, show that the only two ideals of R are {0} and R itself.\n\n17. Let a be any element in a ring R with identity. Show that (−1)a = −a.\n\n18. Let ϕ : R→ S be a ring homomorphism. Prove each of the following statements.\n(a) If R is a commutative ring, then ϕ(R) is a commutative ring.\n(b) ϕ(0) = 0.\n(c) Let 1R and 1S be the identities for R and S, respectively. If ϕ is onto, then ϕ(1R) = 1S .\n(d) If R is a field and ϕ(R) ̸= 0, then ϕ(R) is a field.\n\n19. Prove that the associative law for multiplication and the distributive laws hold in R/I.\n\n20. Prove the Second Isomorphism Theorem for rings: Let I be a subring of a ring R and\nJ an ideal in R. Then I ∩ J is an ideal in I and\n\nI/I ∩ J ∼= I + J/J.\n\n21. Prove the Third Isomorphism Theorem for rings: Let R be a ring and I and J be ideals\nof R, where J ⊂ I. Then\n\nR/I ∼=\nR/J\n\nI/J\n.\n\n22. Prove the Correspondence Theorem: Let I be an ideal of a ring R. Then S → S/I\nis a one-to-one correspondence between the set of subrings S containing I and the set of\nsubrings of R/I. Furthermore, the ideals of R correspond to ideals of R/I.\n\n23. Let R be a ring and S a subset of R. Show that S is a subring of R if and only if each\nof the following conditions is satisfied.\n(a) S ̸= ∅.\n\n\n\n272 CHAPTER 16. RINGS\n\n(b) rs ∈ S for all r, s ∈ S.\n(c) r − s ∈ S for all r, s ∈ S.\n\n24. Let R be a ring with a collection of subrings {Rα}. Prove that\n∩\nRα is a subring of R.\n\nGive an example to show that the union of two subrings cannot be a subring.\n\n25. Let {Iα}α∈A be a collection of ideals in a ring R. Prove that\n∩\n\nα∈A Iα is also an ideal\nin R. Give an example to show that if I1 and I2 are ideals in R, then I1 ∪ I2 may not be\nan ideal.\n\n26. Let R be an integral domain. Show that if the only ideals in R are {0} and R itself, R\nmust be a field.\n\n27. Let R be a commutative ring. An element a in R is nilpotent if an = 0 for some\npositive integer n. Show that the set of all nilpotent elements forms an ideal in R.\n\n28. A ring R is a Boolean ring if for every a ∈ R, a2 = a. Show that every Boolean ring\nis a commutative ring.\n\n29. Let R be a ring, where a3 = a for all a ∈ R. Prove that R must be a commutative ring.\n\n30. Let R be a ring with identity 1R and S a subring of R with identity 1S . Prove or\ndisprove that 1R = 1S .\n\n31. If we do not require the identity of a ring to be distinct from 0, we will not have a very\ninteresting mathematical structure. Let R be a ring such that 1 = 0. Prove that R = {0}.\n\n32. Let S be a nonempty subset of a ring R. Prove that there is a subring R′ of R that\ncontains S.\n\n33. Let R be a ring. Define the center of R to be\n\nZ(R) = {a ∈ R : ar = ra for all r ∈ R}.\n\nProve that Z(R) is a commutative subring of R.\n\n34. Let p be prime. Prove that\n\nZ(p) = {a/b : a, b ∈ Z and gcd(b, p) = 1}\n\nis a ring. The ring Z(p) is called the ring of integers localized at p.\n\n35. Prove or disprove: Every finite integral domain is isomorphic to Zp.\n\n36. Let R be a ring with identity.\n(a) Let u be a unit in R. Define a map iu : R → R by r 7→ uru−1. Prove that iu is an\n\nautomorphism of R. Such an automorphism of R is called an inner automorphism of\nR. Denote the set of all inner automorphisms of R by Inn(R).\n\n(b) Denote the set of all automorphisms of R by Aut(R). Prove that Inn(R) is a normal\nsubgroup of Aut(R).\n\n(c) Let U(R) be the group of units in R. Prove that the map\n\nϕ : U(R) → Inn(R)\n\ndefined by u 7→ iu is a homomorphism. Determine the kernel of ϕ.\n(d) Compute Aut(Z), Inn(Z), and U(Z).\n\n\n\n16.7. PROGRAMMING EXERCISE 273\n\n37. Let R and S be arbitrary rings. Show that their Cartesian product is a ring if we define\naddition and multiplication in R× S by\n(a) (r, s) + (r′, s′) = (r + r′, s+ s′)\n\n(b) (r, s)(r′, s′) = (rr′, ss′)\n\n38. An element x in a ring is called an idempotent if x2 = x. Prove that the only\nidempotents in an integral domain are 0 and 1. Find a ring with a idempotent x not equal\nto 0 or 1.\n\n39. Let gcd(a, n) = d and gcd(b, d) ̸= 1. Prove that ax ≡ b (mod n) does not have a\nsolution.\n\n40. (The Chinese Remainder Theorem for Rings) Let R be a ring and I and J be ideals\nin R such that I + J = R.\n(a) Show that for any r and s in R, the system of equations\n\nx ≡ r (mod I)\n\nx ≡ s (mod J)\n\nhas a solution.\n(b) In addition, prove that any two solutions of the system are congruent modulo I ∩ J .\n(c) Let I and J be ideals in a ring R such that I + J = R. Show that there exists a ring\n\nisomorphism\nR/(I ∩ J) ∼= R/I ×R/J.\n\n16.7 Programming Exercise\n1. Write a computer program implementing fast addition and multiplication using the\nChinese Remainder Theorem and the method outlined in the text.\n\n16.8 References and Suggested Readings\n[1] Anderson, F. W. and Fuller, K. R. Rings and Categories of Modules. 2nd ed. Springer,\n\nNew York, 1992.\n[2] Atiyah, M. F. and MacDonald, I. G. Introduction to Commutative Algebra. Westview\n\nPress, Boulder, CO, 1994.\n[3] Herstein, I. N. Noncommutative Rings. Mathematical Association of America, Wash-\n\nington, DC, 1994.\n[4] Kaplansky, I. Commutative Rings. Revised edition. University of Chicago Press,\n\nChicago, 1974.\n[5] Knuth, D. E. The Art of Computer Programming: Semi-Numerical Algorithms, vol.\n\n2. 3rd ed. Addison-Wesley Professional, Boston, 1997.\n[6] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998. A\n\ngood source for applications.\n[7] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\n[8] McCoy, N. H. Rings and Ideals. Carus Monograph Series, No. 8. Mathematical\n\nAssociation of America, Washington, DC, 1968.\n\n\n\n274 CHAPTER 16. RINGS\n\n[9] McCoy, N. H. The Theory of Rings. Chelsea, New York, 1972.\n[10] Zariski, O. and Samuel, P. Commutative Algebra, vols. I and II. Springer, New York,\n\n1975, 1960.\n\n16.9 Sage\nRings are very important in your study of abstract algebra, and similarly, they are very\nimportant in the design and use of Sage. There is a lot of material in this chapter, and\nthere are many corresponding commands in Sage.\n\nCreating Rings\nHere is a list of various rings, domains and fields you can construct simply.\n\n1. Integers(), ZZ: the integral domain of positive and negative integers, Z.\n\n2. Integers(n): the integers mod n, Zn. A field when n is prime, but just a ring for\ncomposite n.\n\n3. QQ: the field of rational numbers, Q.\n\n4. RR, CC: the field of real numbers and the field of complex numbers, R, C. It is\nimpossible to create every real number inside a computer, so technically these sets do\nnot behave as fields, but only give a good imitiation of the real thing. We say they\nare inexact rings to make this point.\n\n5. QuadraticField(n): the field formed by combining the rationals with a solution to the\npolynomial equation x2 − n = 0. The notation in the text is Q[\n\n√\nn]. A functional\n\nequivalent can be made with the syntax QQ[sqrt(n)]. Note that n can be negative.\n\n6. CyclotomicField(n): the field formed by combining the rationals with the solutions to\nthe polynomial equation xn − 1 = 0.\n\n7. QQbar: the field formed by combining the rationals with the solutions to every poly-\nnomial equation with integer coefficients. This is known as a the field of algebraic\nnumbers, denoted as Q.\n\n8. FiniteField(p): for a prime p, the field of integers Zp.\n\nIf you print a description of some of the above rings, you will sometimes see a new\nsymbol introduced. Consider the following example:\n\nF = QuadraticField (7)\nF\n\nNumber Field in a with defining polynomial x^2 - 7\n\nroot = F.gen(0)\nroot^2\n\n7\n\nroot\n\na\n\n\n\n16.9. SAGE 275\n\n(2* root)^3\n\n56*a\n\nHere Number Field describes an object generally formed by combining the rationals with\nanother number (here\n\n√\n7). “a” is a new symbol which behaves as a root of the polynomial\n\nx2 − 7. We do not say which root,\n√\n7 or −\n\n√\n7, and as we understand the theory better we\n\nwill see that this does not really matter.\nWe can obtain this root as a generator of the number field, and then manipulate it.\n\nFirst squaring root yields 7. Notice that root prints as a. Notice, too, that computations\nwith root behave as if it was either root of x2 − 7, and results print using a.\n\nThis can get a bit confusing, inputing computations with root and getting output in\nterms of a. Fortunately, there is a better way. Consider the following example:\n\nF.<b> = QuadraticField (7)\nF\n\nNumber Field in b with defining polynomial x^2 - 7\n\nb^2\n\n7\n\n(2*b)^3\n\n56*b\n\nWith the syntax F.<b> we can create the field F along with specifying a generator b using\na name of our choosing. Then computations can use b in both input and output as a root\nof x2 − 7.\n\nHere are three new rings that are best created using this new syntax.\n\n1. F.<a> = FiniteField(p^n): We will later have a theorem that tells us that finite fields\nonly exist with orders equal to to a power of a prime. When the power is larger than\n1, then we need a generator, here given as a.\n\n2. P.<x>=R[]: the ring of all polynomials in the variable x, with coefficients from the ring\nR. Notice that R can be any ring, so this is a very general construction that uses one\nring to form another. See an example below.\n\n3. Q.<r,s,t> = QuaternionAlgebra(n, m): the rationals combined with indeterminates r,\ns and t such that r2 = n, s2 = m and t = rs = −sr. This is a generalization\nof the quaternions described in this chapter, though over the rationals rather than\nthe reals, so it is an exact ring. Notice that this is one of the few noncommuta-\ntive rings in Sage. The “usual” quaternions would be constructed with Q.<I,J,K> =\n\nQuaternionAlgebra(-1, -1). (Notice that using I here is not a good choice, because it\nwill then clobber the symbol I used for complex numbers.)\n\nSyntax specifying names for generators can be used for many of the above rings as well,\nsuch as demonstrated above for quadratic fields and below for cyclotomic fields.\n\nC.<t> = CyclotomicField (8)\nC.random_element ()\n\n-2/11*t^2 + t - 1\n\n\n\n276 CHAPTER 16. RINGS\n\nProperties of Rings\nThe examples below demonstrate how to query certain properties of rings. If you are playing\nalong, be sure to execute the first compute cell to define the various rings involved in the\nexamples.\n\nZ7 = Integers (7)\nZ9 = Integers (9)\nQ = QuadraticField (-11)\nF.<a> = FiniteField (3^2)\nP.<x> = Z7[]\nS.<f,g,h> = QuaternionAlgebra (-7, 3)\n\nExact versus inexact.\nQQ.is_exact ()\n\nTrue\n\nRR.is_exact ()\n\nFalse\n\nFinite versus infinite.\nZ7.is_finite ()\n\nTrue\n\nZ7.is_finite ()\n\nTrue\n\nIntegral domain?\nZ7.is_integral_domain ()\n\nTrue\n\nZ9.is_integral_domain ()\n\nFalse\n\nField?\nZ9.is_field ()\n\nFalse\n\nF.is_field ()\n\nTrue\n\nQ.is_field ()\n\nTrue\n\nCommutative?\n\n\n\n16.9. SAGE 277\n\nQ.is_commutative ()\n\nTrue\n\nS.is_commutative ()\n\nFalse\n\nCharacteristic.\nZ7.characteristic ()\n\n7\n\nZ9.characteristic ()\n\n9\n\nQ.characteristic ()\n\n0\n\nF.characteristic ()\n\n3\n\nP.characteristic ()\n\n7\n\nS.characteristic ()\n\n0\n\nAdditive and multiplicative identities print like you would expect, but notice that while\nthey may print identically, they could be different because of the ring they live in.\n\nb = Z9.zero(); b\n\n0\n\nb.parent ()\n\nRing of integers modulo 9\n\nc = Q.zero(); c\n\n0\n\nc.parent ()\n\nNumber Field in a with defining polynomial x^2 + 11\n\nb == c\n\n\n\n278 CHAPTER 16. RINGS\n\nFalse\n\nd = Z9.one(); d\n\n1\n\nd.parent ()\n\nRing of integers modulo 9\n\ne = Q.one(); e\n\n1\n\ne.parent ()\n\nNumber Field in a with defining polynomial x^2 + 11\n\nd == e\n\nFalse\n\nThere is some support for subrings. For example, Q and S are extensions of the rationals,\nwhile F is totally distinct from the rationals.\n\nQQ.is_subring(Q)\n\nTrue\n\nQQ.is_subring(S)\n\nTrue\n\nQQ.is_subring(F)\n\nFalse\n\nNot every element of a ring may have a multiplicative inverse, in other words, not every\nelement has to be a unit (unless the ring is a field). It would now be good practice to check\nif an element is a unit before you try computing its inverse.\n\nthree = Z9(3)\nthree.is_unit ()\n\nFalse\n\nthree*three\n\n0\n\nfour = Z9(4)\nfour.is_unit ()\n\nTrue\n\n\n\n16.9. SAGE 279\n\ng = four^-1; g\n\n7\n\nfour*g\n\n1\n\nQuotient Structure\nIdeals are the normal subgroups of rings and allow us to build “quotients” — basically new\nrings defined on equivalence classes of elements of the original ring. Sage support for ideals\nis variable. When they can be created, there is not always a lot you can do with them. But\nthey work well in certain very important cases.\n\nThe integers, Z, have ideals that are just multiples of a single integer. We can create\nthem with the .ideal() method or just by wrting a scalar multiple of ZZ. And then the\nquotient is isomorphic to a well-understood ring. (Notice that I is a bad name for an ideal\nif we want to work with complex numbers later.)\n\nI1 = ZZ.ideal (4)\nI2 = 4*ZZ\nI3 = (-4)*ZZ\nI1 == I2\n\nTrue\n\nI2 == I3\n\nTrue\n\nQ = ZZ.quotient(I1); Q\n\nRing of integers modulo 4\n\nQ == Integers (4)\n\nTrue\n\nWe might normally be more careful about the last statement. The quotient is a set of\nequivalence classes, each infinite, and certainly not a single integer. But the quotient is\nisomorphic to Z4, so Sage just makes this identification.\n\nZ7 = Integers (7)\nP.<y> = Z7[]\nM = P.ideal(y^2+4)\nQ = P.quotient(M)\nQ\n\nUnivariate Quotient Polynomial Ring in ybar over\nRing of integers modulo 7 with modulus y^2 + 4\n\nQ.random_element ()\n\n2*ybar + 6\n\n\n\n280 CHAPTER 16. RINGS\n\nQ.order()\n\n49\n\nQ.is_field ()\n\nTrue\n\nNotice that the construction of the quotient ring has created a new generator, converting\ny (y) to ybar (y). We can override this as before with the syntax demonstrated below.\n\nQ.<t> = P.quotient(M); Q\n\nUnivariate Quotient Polynomial Ring in t over\nRing of integers modulo 7 with modulus y^2 + 4\n\nQ.random_element ()\n\n4*t + 6\n\nSo from a quotient of an infinite ring and an ideal (which is also a ring), we create a\nfield, which is finite. Understanding this construction will be an important theme in the\nnext few chapters. To see how remarkable it is, consider what happens with just one little\nchange.\n\nZ7 = Integers (7)\nP.<y> = Z7[]\nM = P.ideal(y^2+3)\nQ.<t> = P.quotient(M)\nQ\n\nUnivariate Quotient Polynomial Ring in t over\nRing of integers modulo 7 with modulus y^2 + 3\n\nQ.random_element ()\n\n3*t + 1\n\nQ.order()\n\n49\n\nQ.is_field ()\n\nFalse\n\nThere are a few methods available which will give us properties of ideals. In particular,\nwe can check for prime and maximal ideals in rings of polynomials. Examine the results\nabove and below in the context of Theorem 16.35.\n\nZ7 = Integers (7)\nP.<y> = Z7[]\nM = P.ideal(y^2+4)\nN = P.ideal(y^2+3)\nM.is_maximal ()\n\n\n\n16.9. SAGE 281\n\nTrue\n\nN.is_maximal ()\n\nFalse\n\nThe fact that M is a prime ideal is verification of Corollary 16.40.\nM.is_prime ()\n\nTrue\n\nN.is_prime ()\n\nFalse\n\nRing Homomorphisms\nWhen Sage is presented with 3 + 4/3, how does it know that 3 is meant to be an integer?\nAnd then to add it to a rational, how does it know that we really want to view the compu-\ntation as 3/1 + 4/3? This is really easy for you and me, but devilishly hard for a program,\nand you can imagine it getting ever more complicated with the many possible rings in\nSage, subrings, matrices, etc. Part of the answer is that Sage uses ring homomorphisms to\n“translate” objects (numbers) between rings.\n\nWe will give an example below, but not pursue the topic much further. For the curious,\nreading the Sage documentation and experimenting would be a good exercise.\n\nH = Hom(ZZ , QQ)\nphi = H([1])\nphi\n\nRing morphism:\nFrom: Integer Ring\nTo: Rational Field\nDefn: 1 |--> 1\n\nphi.parent ()\n\nSet of Homomorphisms from Integer Ring to Rational Field\n\na = 3; a\n\n3\n\na.parent ()\n\nInteger Ring\n\nb = phi (3); b\n\n3\n\nb.parent ()\n\n\n\n282 CHAPTER 16. RINGS\n\nRational Field\n\nSo phi is a homomorphism (“morphism”) that converts integers (the domain is ZZ) into\nrationals (the codomain is QQ), whose parent is a set of homomorphisms that Sage calls\na “homset.” Even though a and b both print as 3, which is indistinguishable to our eyes,\nthe parents of a and b are different. Yet the numerical value of the two objects has not\nchanged.\n\n16.10 Sage Exercises\n1. Define the two rings Z11 and Z12 with the commands R = Integers(11) and S = Integers(12).\nFor each ring, use the relevant command to determine: if the ring is finite, if it is commu-\ntative, if it is an integral domain and if it is a field. Then use single Sage commands to find\nthe order of the ring, list the elements, and output the multiplicative identity (i.e. 1, if it\nexists).\n\n2. Define R to be the ring of integers, Z, by executing R = ZZ or R = Integers(). A command\nlike R.ideal(4) will create the principal ideal ⟨4⟩. The same command can accept more than\none generator, so for example, R.ideal(3, 5) will create the ideal {a · 3 + b · 5 | a, b ∈ Z}.\nCreate several ideals of Z with two generators and ask Sage to print each as you create it.\nExplain what you observe and then create code that will test your observation for thousands\nof different examples.\n\n3. Create a finite field F of order 81 with F.<t>=FiniteField(3^4).\n(a) List the elements of F .\n(b) Obtain the generators of F with F.gens().\n(c) Obtain the first generator of F and save it as u with u = F.0 (alternatively, u =\n\nF.gen(0)).\n(d) Compute the first 80 powers of u and comment.\n(e) The generator you have worked with above is a root of a polynomial over Z3. Obtain\n\nthis polynomial with F.modulus() and use this observation to explain the entry in your\nlist of powers that is the fourth power of the generator.\n\n4. Build and analyze a quotient ring as follows:\n(a) Use P.<z>=Integers(7)[] to construct a ring P of polynomials in z with coefficients\n\nfrom Z7.\n(b) Use K = P.ideal(z^2+z+3) to build a principal ideal K generated by the polynomial\n\nz2 + z + 3.\n(c) Use H = P.quotient(K) to build H, the quotient ring of P by K.\n(d) Use Sage to verify that H is a field.\n(e) As in the previous exercise, obtain a generator and examine the proper collection of\n\npowers of that generator.\n\n\n\n17\n\nPolynomials\n\nMost people are fairly familiar with polynomials by the time they begin to study abstract\nalgebra. When we examine polynomial expressions such as\n\np(x) = x3 − 3x+ 2\n\nq(x) = 3x2 − 6x+ 5,\n\nwe have a pretty good idea of what p(x) + q(x) and p(x)q(x) mean. We just add and\nmultiply polynomials as functions; that is,\n\n(p+ q)(x) = p(x) + q(x)\n\n= (x3 − 3x+ 2) + (3x2 − 6x+ 5)\n\n= x3 + 3x2 − 9x+ 7\n\nand\n\n(pq)(x) = p(x)q(x)\n\n= (x3 − 3x+ 2)(3x2 − 6x+ 5)\n\n= 3x5 − 6x4 − 4x3 + 24x2 − 27x+ 10.\n\nIt is probably no surprise that polynomials form a ring. In this chapter we shall emphasize\nthe algebraic structure of polynomials by studying polynomial rings. We can prove many\nresults for polynomial rings that are similar to the theorems we proved for the integers.\nAnalogs of prime numbers, the division algorithm, and the Euclidean algorithm exist for\npolynomials.\n\n17.1 Polynomial Rings\nThroughout this chapter we shall assume that R is a commutative ring with identity. Any\nexpression of the form\n\nf(x) =\n\nn∑\ni=0\n\naix\ni = a0 + a1x+ a2x\n\n2 + · · ·+ anx\nn,\n\nwhere ai ∈ R and an ̸= 0, is called a polynomial over R with indeterminate x. The\nelements a0, a1, . . . , an are called the coefficients of f . The coefficient an is called the\nleading coefficient. A polynomial is called monic if the leading coefficient is 1. If n is\nthe largest nonnegative number for which an ̸= 0, we say that the degree of f is n and write\ndeg f(x) = n. If no such n exists—that is, if f = 0 is the zero polynomial—then the degree\n\n283\n\n\n\n284 CHAPTER 17. POLYNOMIALS\n\nof f is defined to be −∞. We will denote the set of all polynomials with coefficients in a\nring R by R[x]. Two polynomials are equal exactly when their corresponding coefficients\nare equal; that is, if we let\n\np(x) = a0 + a1x+ · · ·+ anx\nn\n\nq(x) = b0 + b1x+ · · ·+ bmx\nm,\n\nthen p(x) = q(x) if and only if ai = bi for all i ≥ 0.\nTo show that the set of all polynomials forms a ring, we must first define addition and\n\nmultiplication. We define the sum of two polynomials as follows. Let\n\np(x) = a0 + a1x+ · · ·+ anx\nn\n\nq(x) = b0 + b1x+ · · ·+ bmx\nm.\n\nThen the sum of p(x) and q(x) is\n\np(x) + q(x) = c0 + c1x+ · · ·+ ckx\nk,\n\nwhere ci = ai + bi for each i. We define the product of p(x) and q(x) to be\n\np(x)q(x) = c0 + c1x+ · · ·+ cm+nx\nm+n,\n\nwhere\n\nci =\n\ni∑\nk=0\n\nakbi−k = a0bi + a1bi−1 + · · ·+ ai−1b1 + aib0\n\nfor each i. Notice that in each case some of the coefficients may be zero.\n\nExample 17.1. Suppose that\n\np(x) = 3 + 0x+ 0x2 + 2x3 + 0x4\n\nand\nq(x) = 2 + 0x− x2 + 0x3 + 4x4\n\nare polynomials in Z[x]. If the coefficient of some term in a polynomial is zero, then we\nusually just omit that term. In this case we would write p(x) = 3 + 2x3 and q(x) =\n2− x2 + 4x4. The sum of these two polynomials is\n\np(x) + q(x) = 5− x2 + 2x3 + 4x4.\n\nThe product,\n\np(x)q(x) = (3 + 2x3)(2− x2 + 4x4) = 6− 3x2 + 4x3 + 12x4 − 2x5 + 8x7,\n\ncan be calculated either by determining the ci’s in the definition or by simply multiplying\npolynomials in the same way as we have always done.\n\nExample 17.2. Let\n\np(x) = 3 + 3x3 and q(x) = 4 + 4x2 + 4x4\n\nbe polynomials in Z12[x]. The sum of p(x) and q(x) is 7+ 4x2 +3x3 +4x4. The product of\nthe two polynomials is the zero polynomial. This example tells us that we can not expect\nR[x] to be an integral domain if R is not an integral domain.\n\n\n\n17.1. POLYNOMIAL RINGS 285\n\nTheorem 17.3. Let R be a commutative ring with identity. Then R[x] is a commutative\nring with identity.\n\nProof. Our first task is to show that R[x] is an abelian group under polynomial addi-\ntion. The zero polynomial, f(x) = 0, is the additive identity. Given a polynomial p(x) =∑n\n\ni=0 aix\ni, the inverse of p(x) is easily verified to be −p(x) =\n\n∑n\ni=0(−ai)xi = −\n\n∑n\ni=0 aix\n\ni.\nCommutativity and associativity follow immediately from the definition of polynomial ad-\ndition and from the fact that addition in R is both commutative and associative.\n\nTo show that polynomial multiplication is associative, let\n\np(x) =\n\nm∑\ni=0\n\naix\ni,\n\nq(x) =\n\nn∑\ni=0\n\nbix\ni,\n\nr(x) =\n\np∑\ni=0\n\ncix\ni.\n\nThen\n\n[p(x)q(x)]r(x) =\n\n[(\nm∑\ni=0\n\naix\ni\n\n)(\nn∑\n\ni=0\n\nbix\ni\n\n)](\np∑\n\ni=0\n\ncix\ni\n\n)\n\n=\n\n\uf8ee\uf8f0m+n∑\ni=0\n\n\uf8eb\uf8ed i∑\nj=0\n\najbi−j\n\n\uf8f6\uf8f8xi\n\n\uf8f9\uf8fb( p∑\ni=0\n\ncix\ni\n\n)\n\n=\n\nm+n+p∑\ni=0\n\n\uf8ee\uf8f0 i∑\nj=0\n\n(\nj∑\n\nk=0\n\nakbj−k\n\n)\nci−j\n\n\uf8f9\uf8fbxi\n=\n\nm+n+p∑\ni=0\n\n\uf8eb\uf8ed ∑\nj+k+l=i\n\najbkcl\n\n\uf8f6\uf8f8xi\n\n=\n\nm+n+p∑\ni=0\n\n\uf8ee\uf8f0 i∑\nj=0\n\naj\n\n(\ni−j∑\nk=0\n\nbkci−j−k\n\n)\uf8f9\uf8fbxi\n=\n\n(\nm∑\ni=0\n\naix\ni\n\n)\uf8ee\uf8f0n+p∑\ni=0\n\n\uf8eb\uf8ed i∑\nj=0\n\nbjci−j\n\n\uf8f6\uf8f8xi\n\n\uf8f9\uf8fb\n=\n\n(\nm∑\ni=0\n\naix\ni\n\n)[(\nn∑\n\ni=0\n\nbix\ni\n\n)(\np∑\n\ni=0\n\ncix\ni\n\n)]\n= p(x)[q(x)r(x)]\n\nThe commutativity and distribution properties of polynomial multiplication are proved in\na similar manner. We shall leave the proofs of these properties as an exercise.\n\nProposition 17.4. Let p(x) and q(x) be polynomials in R[x], where R is an integral domain.\nThen deg p(x) + deg q(x) = deg(p(x)q(x)). Furthermore, R[x] is an integral domain.\n\nProof. Suppose that we have two nonzero polynomials\n\np(x) = amx\nm + · · ·+ a1x+ a0\n\n\n\n286 CHAPTER 17. POLYNOMIALS\n\nand\nq(x) = bnx\n\nn + · · ·+ b1x+ b0\n\nwith am ̸= 0 and bn ̸= 0. The degrees of p(x) and q(x) are m and n, respectively. The\nleading term of p(x)q(x) is ambnxm+n, which cannot be zero since R is an integral domain;\nhence, the degree of p(x)q(x) is m + n, and p(x)q(x) ̸= 0. Since p(x) ̸= 0 and q(x) ̸= 0\nimply that p(x)q(x) ̸= 0, we know that R[x] must also be an integral domain.\n\nWe also want to consider polynomials in two or more variables, such as x2 − 3xy+ 2y3.\nLet R be a ring and suppose that we are given two indeterminates x and y. Certainly\nwe can form the ring (R[x])[y]. It is straightforward but perhaps tedious to show that\n(R[x])[y] ∼= R([y])[x]. We shall identify these two rings by this isomorphism and simply write\nR[x, y]. The ring R[x, y] is called the ring of polynomials in two indeterminates x and\ny with coefficients in R. We can define the ring of polynomials in n indeterminates\nwith coefficients in R similarly. We shall denote this ring by R[x1, x2, . . . , xn].\n\nTheorem 17.5. Let R be a commutative ring with identity and α ∈ R. Then we have a\nring homomorphism ϕα : R[x] → R defined by\n\nϕα(p(x)) = p(α) = anα\nn + · · ·+ a1α+ a0,\n\nwhere p(x) = anx\nn + · · ·+ a1x+ a0.\n\nProof. Let p(x) =\n∑n\n\ni=0 aix\ni and q(x) =\n\n∑m\ni=0 bix\n\ni. It is easy to show that ϕα(p(x) +\nq(x)) = ϕα(p(x)) + ϕα(q(x)). To show that multiplication is preserved under the map ϕα,\nobserve that\n\nϕα(p(x))ϕα(q(x)) = p(α)q(α)\n\n=\n\n(\nn∑\n\ni=0\n\naiα\ni\n\n)(\nm∑\ni=0\n\nbiα\ni\n\n)\n\n=\nm+n∑\ni=0\n\n(\ni∑\n\nk=0\n\nakbi−k\n\n)\nαi\n\n= ϕα(p(x)q(x)).\n\nThe map ϕα : R[x] → R is called the evaluation homomorphism at α.\n\n17.2 The Division Algorithm\nRecall that the division algorithm for integers (Theorem 2.9) says that if a and b are integers\nwith b > 0, then there exist unique integers q and r such that a = bq + r, where 0 ≤ r < b.\nThe algorithm by which q and r are found is just long division. A similar theorem exists for\npolynomials. The division algorithm for polynomials has several important consequences.\nSince its proof is very similar to the corresponding proof for integers, it is worthwhile to\nreview Theorem 2.9 at this point.\n\nTheorem 17.6 (Division Algorithm). Let f(x) and g(x) be polynomials in F [x], where F\nis a field and g(x) is a nonzero polynomial. Then there exist unique polynomials q(x), r(x) ∈\nF [x] such that\n\nf(x) = g(x)q(x) + r(x),\n\nwhere either deg r(x) < deg g(x) or r(x) is the zero polynomial.\n\n\n\n17.2. THE DIVISION ALGORITHM 287\n\nProof. We will first consider the existence of q(x) and r(x). If f(x) is the zero polynomial,\nthen\n\n0 = 0 · g(x) + 0;\n\nhence, both q and r must also be the zero polynomial. Now suppose that f(x) is not the\nzero polynomial and that deg f(x) = n and deg g(x) = m. If m > n, then we can let\nq(x) = 0 and r(x) = f(x). Hence, we may assume that m ≤ n and proceed by induction on\nn. If\n\nf(x) = anx\nn + an−1x\n\nn−1 + · · ·+ a1x+ a0\n\ng(x) = bmx\nm + bm−1x\n\nm−1 + · · ·+ b1x+ b0\n\nthe polynomial\nf ′(x) = f(x)− an\n\nbm\nxn−mg(x)\n\nhas degree less than n or is the zero polynomial. By induction, there exist polynomials q′(x)\nand r(x) such that\n\nf ′(x) = q′(x)g(x) + r(x),\n\nwhere r(x) = 0 or the degree of r(x) is less than the degree of g(x). Now let\n\nq(x) = q′(x) +\nan\nbm\nxn−m.\n\nThen\nf(x) = g(x)q(x) + r(x),\n\nwith r(x) the zero polynomial or deg r(x) < deg g(x).\nTo show that q(x) and r(x) are unique, suppose that there exist two other polynomials\n\nq1(x) and r1(x) such that f(x) = g(x)q1(x) + r1(x) with deg r1(x) < deg g(x) or r1(x) = 0,\nso that\n\nf(x) = g(x)q(x) + r(x) = g(x)q1(x) + r1(x),\n\nand\ng(x)[q(x)− q1(x)] = r1(x)− r(x).\n\nIf g(x) is not the zero polynomial, then\n\ndeg(g(x)[q(x)− q1(x)]) = deg(r1(x)− r(x)) ≥ deg g(x).\n\nHowever, the degrees of both r(x) and r1(x) are strictly less than the degree of g(x);\ntherefore, r(x) = r1(x) and q(x) = q1(x).\n\nExample 17.7. The division algorithm merely formalizes long division of polynomials, a\ntask we have been familiar with since high school. For example, suppose that we divide\nx3 − x2 + 2x− 3 by x− 2.\n\nx2 + x + 4\n\nx − 2 x3 − x2 + 2x − 3\n\nx3 − 2x2\n\nx2 + 2x − 3\n\nx2 − 2x\n\n4x − 3\n\n4x − 8\n\n5\n\nHence, x3 − x2 + 2x− 3 = (x− 2)(x2 + x+ 4) + 5.\n\n\n\n288 CHAPTER 17. POLYNOMIALS\n\nLet p(x) be a polynomial in F [x] and α ∈ F . We say that α is a zero or root of p(x) if\np(x) is in the kernel of the evaluation homomorphism ϕα. All we are really saying here is\nthat α is a zero of p(x) if p(α) = 0.\n\nCorollary 17.8. Let F be a field. An element α ∈ F is a zero of p(x) ∈ F [x] if and only\nif x− α is a factor of p(x) in F [x].\n\nProof. Suppose that α ∈ F and p(α) = 0. By the division algorithm, there exist polyno-\nmials q(x) and r(x) such that\n\np(x) = (x− α)q(x) + r(x)\n\nand the degree of r(x) must be less than the degree of x − α. Since the degree of r(x) is\nless than 1, r(x) = a for a ∈ F ; therefore,\n\np(x) = (x− α)q(x) + a.\n\nBut\n0 = p(α) = 0 · q(α) + a = a;\n\nconsequently, p(x) = (x− α)q(x), and x− α is a factor of p(x).\nConversely, suppose that x − α is a factor of p(x); say p(x) = (x − α)q(x). Then\n\np(α) = 0 · q(α) = 0.\n\nCorollary 17.9. Let F be a field. A nonzero polynomial p(x) of degree n in F [x] can have\nat most n distinct zeros in F .\n\nProof. We will use induction on the degree of p(x). If deg p(x) = 0, then p(x) is a constant\npolynomial and has no zeros. Let deg p(x) = 1. Then p(x) = ax+ b for some a and b in F .\nIf α1 and α2 are zeros of p(x), then aα1 + b = aα2 + b or α1 = α2.\n\nNow assume that deg p(x) > 1. If p(x) does not have a zero in F , then we are done.\nOn the other hand, if α is a zero of p(x), then p(x) = (x− α)q(x) for some q(x) ∈ F [x] by\nCorollary 17.8. The degree of q(x) is n− 1 by Proposition 17.4. Let β be some other zero\nof p(x) that is distinct from α. Then p(β) = (β −α)q(β) = 0. Since α ̸= β and F is a field,\nq(β) = 0. By our induction hypothesis, p(x) can have at most n − 1 zeros in F that are\ndistinct from α. Therefore, p(x) has at most n distinct zeros in F .\n\nLet F be a field. A monic polynomial d(x) is a greatest common divisor of poly-\nnomials p(x), q(x) ∈ F [x] if d(x) evenly divides both p(x) and q(x); and, if for any other\npolynomial d′(x) dividing both p(x) and q(x), d′(x) | d(x). We write d(x) = gcd(p(x), q(x)).\nTwo polynomials p(x) and q(x) are relatively prime if gcd(p(x), q(x)) = 1.\n\nProposition 17.10. Let F be a field and suppose that d(x) is a greatest common divisor\nof two polynomials p(x) and q(x) in F [x]. Then there exist polynomials r(x) and s(x) such\nthat\n\nd(x) = r(x)p(x) + s(x)q(x).\n\nFurthermore, the greatest common divisor of two polynomials is unique.\n\nProof. Let d(x) be the monic polynomial of smallest degree in the set\n\nS = {f(x)p(x) + g(x)q(x) : f(x), g(x) ∈ F [x]}.\n\nWe can write d(x) = r(x)p(x) + s(x)q(x) for two polynomials r(x) and s(x) in F [x]. We\nneed to show that d(x) divides both p(x) and q(x). We shall first show that d(x) divides\n\n\n\n17.3. IRREDUCIBLE POLYNOMIALS 289\n\np(x). By the division algorithm, there exist polynomials a(x) and b(x) such that p(x) =\na(x)d(x)+ b(x), where b(x) is either the zero polynomial or deg b(x) < deg d(x). Therefore,\n\nb(x) = p(x)− a(x)d(x)\n\n= p(x)− a(x)(r(x)p(x) + s(x)q(x))\n\n= p(x)− a(x)r(x)p(x)− a(x)s(x)q(x)\n\n= p(x)(1− a(x)r(x)) + q(x)(−a(x)s(x))\n\nis a linear combination of p(x) and q(x) and therefore must be in S. However, b(x) must\nbe the zero polynomial since d(x) was chosen to be of smallest degree; consequently, d(x)\ndivides p(x). A symmetric argument shows that d(x) must also divide q(x); hence, d(x) is\na common divisor of p(x) and q(x).\n\nTo show that d(x) is a greatest common divisor of p(x) and q(x), suppose that d′(x)\nis another common divisor of p(x) and q(x). We will show that d′(x) | d(x). Since d′(x)\nis a common divisor of p(x) and q(x), there exist polynomials u(x) and v(x) such that\np(x) = u(x)d′(x) and q(x) = v(x)d′(x). Therefore,\n\nd(x) = r(x)p(x) + s(x)q(x)\n\n= r(x)u(x)d′(x) + s(x)v(x)d′(x)\n\n= d′(x)[r(x)u(x) + s(x)v(x)].\n\nSince d′(x) | d(x), d(x) is a greatest common divisor of p(x) and q(x).\nFinally, we must show that the greatest common divisor of p(x) and q(x) is unique.\n\nSuppose that d′(x) is another greatest common divisor of p(x) and q(x). We have just\nshown that there exist polynomials u(x) and v(x) in F [x] such that d(x) = d′(x)[r(x)u(x)+\ns(x)v(x)]. Since\n\ndeg d(x) = deg d′(x) + deg[r(x)u(x) + s(x)v(x)]\n\nand d(x) and d′(x) are both greatest common divisors, deg d(x) = deg d′(x). Since d(x) and\nd′(x) are both monic polynomials of the same degree, it must be the case that d(x) = d′(x).\n\nNotice the similarity between the proof of Proposition 17.10 and the proof of Theo-\nrem 2.10.\n\n17.3 Irreducible Polynomials\nA nonconstant polynomial f(x) ∈ F [x] is irreducible over a field F if f(x) cannot be\nexpressed as a product of two polynomials g(x) and h(x) in F [x], where the degrees of g(x)\nand h(x) are both smaller than the degree of f(x). Irreducible polynomials function as the\n“prime numbers” of polynomial rings.\n\nExample 17.11. The polynomial x2 − 2 ∈ Q[x] is irreducible since it cannot be factored\nany further over the rational numbers. Similarly, x2+1 is irreducible over the real numbers.\n\nExample 17.12. The polynomial p(x) = x3 + x2 + 2 is irreducible over Z3[x]. Suppose\nthat this polynomial was reducible over Z3[x]. By the division algorithm there would have\nto be a factor of the form x− a, where a is some element in Z3[x]. Hence, it would have to\nbe true that p(a) = 0. However,\n\np(0) = 2\n\np(1) = 1\n\np(2) = 2.\n\n\n\n290 CHAPTER 17. POLYNOMIALS\n\nTherefore, p(x) has no zeros in Z3 and must be irreducible.\nLemma 17.13. Let p(x) ∈ Q[x]. Then\n\np(x) =\nr\n\ns\n(a0 + a1x+ · · ·+ anx\n\nn),\n\nwhere r, s, a0, . . . , an are integers, the ai’s are relatively prime, and r and s are relatively\nprime.\nProof. Suppose that\n\np(x) =\nb0\nc0\n\n+\nb1\nc1\nx+ · · ·+ bn\n\ncn\nxn,\n\nwhere the bi’s and the ci’s are integers. We can rewrite p(x) as\n\np(x) =\n1\n\nc0 · · · cn\n(d0 + d1x+ · · ·+ dnx\n\nn),\n\nwhere d0, . . . , dn are integers. Let d be the greatest common divisor of d0, . . . , dn. Then\n\np(x) =\nd\n\nc0 · · · cn\n(a0 + a1x+ · · ·+ anx\n\nn),\n\nwhere di = dai and the ai’s are relatively prime. Reducing d/(c0 · · · cn) to its lowest terms,\nwe can write\n\np(x) =\nr\n\ns\n(a0 + a1x+ · · ·+ anx\n\nn),\n\nwhere gcd(r, s) = 1.\n\nTheorem 17.14 (Gauss’s Lemma). Let p(x) ∈ Z[x] be a monic polynomial such that p(x)\nfactors into a product of two polynomials α(x) and β(x) in Q[x], where the degrees of both\nα(x) and β(x) are less than the degree of p(x). Then p(x) = a(x)b(x), where a(x) and b(x)\nare monic polynomials in Z[x] with degα(x) = deg a(x) and degβ(x) = deg b(x).\nProof. By Lemma 17.13, we can assume that\n\nα(x) =\nc1\nd1\n\n(a0 + a1x+ · · ·+ amx\nm) =\n\nc1\nd1\nα1(x)\n\nβ(x) =\nc2\nd2\n\n(b0 + b1x+ · · ·+ bnx\nn) =\n\nc2\nd2\nβ1(x),\n\nwhere the ai’s are relatively prime and the bi’s are relatively prime. Consequently,\n\np(x) = α(x)β(x) =\nc1c2\nd1d2\n\nα1(x)β1(x) =\nc\n\nd\nα1(x)β1(x),\n\nwhere c/d is the product of c1/d1 and c2/d2 expressed in lowest terms. Hence, dp(x) =\ncα1(x)β1(x).\n\nIf d = 1, then cambn = 1 since p(x) is a monic polynomial. Hence, either c = 1\nor c = −1. If c = 1, then either am = bn = 1 or am = bn = −1. In the first case\np(x) = α1(x)β1(x), where α1(x) and β1(x) are monic polynomials with degα(x) = degα1(x)\nand degβ(x) = degβ1(x). In the second case a(x) = −α1(x) and b(x) = −β1(x) are the\ncorrect monic polynomials since p(x) = (−α1(x))(−β1(x)) = a(x)b(x). The case in which\nc = −1 can be handled similarly.\n\nNow suppose that d ̸= 1. Since gcd(c, d) = 1, there exists a prime p such that p | d and\np ̸ |c. Also, since the coefficients of α1(x) are relatively prime, there exists a coefficient ai\nsuch that p ̸ |ai. Similarly, there exists a coefficient bj of β1(x) such that p ̸ |bj . Let α′\n\n1(x)\nand β′1(x) be the polynomials in Zp[x] obtained by reducing the coefficients of α1(x) and\nβ1(x) modulo p. Since p | d, α′\n\n1(x)β\n′\n1(x) = 0 in Zp[x]. However, this is impossible since\n\nneither α′\n1(x) nor β′1(x) is the zero polynomial and Zp[x] is an integral domain. Therefore,\n\nd = 1 and the theorem is proven.\n\n\n\n17.3. IRREDUCIBLE POLYNOMIALS 291\n\nCorollary 17.15. Let p(x) = xn + an−1x\nn−1 + · · ·+ a0 be a polynomial with coefficients in\n\nZ and a0 ̸= 0. If p(x) has a zero in Q, then p(x) also has a zero α in Z. Furthermore, α\ndivides a0.\n\nProof. Let p(x) have a zero a ∈ Q. Then p(x) must have a linear factor x−a. By Gauss’s\nLemma, p(x) has a factorization with a linear factor in Z[x]. Hence, for some α ∈ Z\n\np(x) = (x− α)(xn−1 + · · · − a0/α).\n\nThus a0/α ∈ Z and so α | a0.\n\nExample 17.16. Let p(x) = x4 − 2x3 + x + 1. We shall show that p(x) is irreducible\nover Q[x]. Assume that p(x) is reducible. Then either p(x) has a linear factor, say p(x) =\n(x− α)q(x), where q(x) is a polynomial of degree three, or p(x) has two quadratic factors.\n\nIf p(x) has a linear factor in Q[x], then it has a zero in Z. By Corollary 17.15, any zero\nmust divide 1 and therefore must be ±1; however, p(1) = 1 and p(−1) = 3. Consequently,\nwe have eliminated the possibility that p(x) has any linear factors.\n\nTherefore, if p(x) is reducible it must factor into two quadratic polynomials, say\n\np(x) = (x2 + ax+ b)(x2 + cx+ d)\n\n= x4 + (a+ c)x3 + (ac+ b+ d)x2 + (ad+ bc)x+ bd,\n\nwhere each factor is in Z[x] by Gauss’s Lemma. Hence,\n\na+ c = −2\n\nac+ b+ d = 0\n\nad+ bc = 1\n\nbd = 1.\n\nSince bd = 1, either b = d = 1 or b = d = −1. In either case b = d and so\n\nad+ bc = b(a+ c) = 1.\n\nSince a+c = −2, we know that −2b = 1. This is impossible since b is an integer. Therefore,\np(x) must be irreducible over Q.\n\nTheorem 17.17 (Eisenstein’s Criterion). Let p be a prime and suppose that\n\nf(x) = anx\nn + · · ·+ a0 ∈ Z[x].\n\nIf p | ai for i = 0, 1, . . . , n− 1, but p ̸ |an and p2 ̸ |a0, then f(x) is irreducible over Q.\n\nProof. By Gauss’s Lemma, we need only show that f(x) does not factor into polynomials\nof lower degree in Z[x]. Let\n\nf(x) = (brx\nr + · · ·+ b0)(csx\n\ns + · · ·+ c0)\n\nbe a factorization in Z[x], with br and cs not equal to zero and r, s < n. Since p2 does not\ndivide a0 = b0c0, either b0 or c0 is not divisible by p. Suppose that p ̸ |b0 and p | c0. Since\np ̸ |an and an = brcs, neither br nor cs is divisible by p. Let m be the smallest value of k\nsuch that p ̸ |ck. Then\n\nam = b0cm + b1cm−1 + · · ·+ bmc0\n\nis not divisible by p, since each term on the right-hand side of the equation is divisible by p\nexcept for b0cm. Therefore, m = n since ai is divisible by p for m < n. Hence, f(x) cannot\nbe factored into polynomials of lower degree and therefore must be irreducible.\n\n\n\n292 CHAPTER 17. POLYNOMIALS\n\nExample 17.18. The polynomial\n\nf(x) = 16x5 − 9x4 + 3x2 + 6x− 21\n\nis easily seen to be irreducible over Q by Eisenstein’s Criterion if we let p = 3.\n\nEisenstein’s Criterion is more useful in constructing irreducible polynomials of a certain\ndegree over Q than in determining the irreducibility of an arbitrary polynomial in Q[x]:\ngiven an arbitrary polynomial, it is not very likely that we can apply Eisenstein’s Crite-\nrion. The real value of Theorem 17.17 is that we now have an easy method of generating\nirreducible polynomials of any degree.\n\nIdeals in F [x]\n\nLet F be a field. Recall that a principal ideal in F [x] is an ideal ⟨p(x)⟩ generated by some\npolynomial p(x); that is,\n\n⟨p(x)⟩ = {p(x)q(x) : q(x) ∈ F [x]}.\n\nExample 17.19. The polynomial x2 in F [x] generates the ideal ⟨x2⟩ consisting of all\npolynomials with no constant term or term of degree 1.\n\nTheorem 17.20. If F is a field, then every ideal in F [x] is a principal ideal.\n\nProof. Let I be an ideal of F [x]. If I is the zero ideal, the theorem is easily true. Suppose\nthat I is a nontrivial ideal in F [x], and let p(x) ∈ I be a nonzero element of minimal degree.\nIf deg p(x) = 0, then p(x) is a nonzero constant and 1 must be in I. Since 1 generates all\nof F [x], ⟨1⟩ = I = F [x] and I is again a principal ideal.\n\nNow assume that deg p(x) ≥ 1 and let f(x) be any element in I. By the division\nalgorithm there exist q(x) and r(x) in F [x] such that f(x) = p(x)q(x)+r(x) and deg r(x) <\ndeg p(x). Since f(x), p(x) ∈ I and I is an ideal, r(x) = f(x)−p(x)q(x) is also in I. However,\nsince we chose p(x) to be of minimal degree, r(x) must be the zero polynomial. Since we\ncan write any element f(x) in I as p(x)q(x) for some q(x) ∈ F [x], it must be the case that\nI = ⟨p(x)⟩.\n\nExample 17.21. It is not the case that every ideal in the ring F [x, y] is a principal ideal.\nConsider the ideal of F [x, y] generated by the polynomials x and y. This is the ideal of\nF [x, y] consisting of all polynomials with no constant term. Since both x and y are in the\nideal, no single polynomial can generate the entire ideal.\n\nTheorem 17.22. Let F be a field and suppose that p(x) ∈ F [x]. Then the ideal generated\nby p(x) is maximal if and only if p(x) is irreducible.\n\nProof. Suppose that p(x) generates a maximal ideal of F [x]. Then ⟨p(x)⟩ is also a prime\nideal of F [x]. Since a maximal ideal must be properly contained inside F [x], p(x) cannot\nbe a constant polynomial. Let us assume that p(x) factors into two polynomials of lesser\ndegree, say p(x) = f(x)g(x). Since ⟨p(x)⟩ is a prime ideal one of these factors, say f(x), is\nin ⟨p(x)⟩ and therefore be a multiple of p(x). But this would imply that ⟨p(x)⟩ ⊂ ⟨f(x)⟩,\nwhich is impossible since ⟨p(x)⟩ is maximal.\n\nConversely, suppose that p(x) is irreducible over F [x]. Let I be an ideal in F [x] contain-\ning ⟨p(x)⟩. By Theorem 17.20, I is a principal ideal; hence, I = ⟨f(x)⟩ for some f(x) ∈ F [x].\nSince p(x) ∈ I, it must be the case that p(x) = f(x)g(x) for some g(x) ∈ F [x]. However,\np(x) is irreducible; hence, either f(x) or g(x) is a constant polynomial. If f(x) is constant,\nthen I = F [x] and we are done. If g(x) is constant, then f(x) is a constant multiple of I\nand I = ⟨p(x)⟩. Thus, there are no proper ideals of F [x] that properly contain ⟨p(x)⟩.\n\n\n\n17.4. EXERCISES 293\n\nHistorical Note\n\nThroughout history, the solution of polynomial equations has been a challenging prob-\nlem. The Babylonians knew how to solve the equation ax2 + bx + c = 0. Omar Khayyam\n(1048–1131) devised methods of solving cubic equations through the use of geometric\nconstructions and conic sections. The algebraic solution of the general cubic equation\nax3 + bx2 + cx + d = 0 was not discovered until the sixteenth century. An Italian mathe-\nmatician, Luca Pacioli (ca. 1445–1509), wrote in Summa de Arithmetica that the solution\nof the cubic was impossible. This was taken as a challenge by the rest of the mathematical\ncommunity.\n\nScipione del Ferro (1465–1526), of the University of Bologna, solved the “depressed\ncubic,”\n\nax3 + cx+ d = 0.\n\nHe kept his solution an absolute secret. This may seem surprising today, when mathe-\nmaticians are usually very eager to publish their results, but in the days of the Italian\nRenaissance secrecy was customary. Academic appointments were not easy to secure and\ndepended on the ability to prevail in public contests. Such challenges could be issued at\nany time. Consequently, any major new discovery was a valuable weapon in such a contest.\nIf an opponent presented a list of problems to be solved, del Ferro could in turn present a\nlist of depressed cubics. He kept the secret of his discovery throughout his life, passing it\non only on his deathbed to his student Antonio Fior (ca. 1506–?).\n\nAlthough Fior was not the equal of his teacher, he immediately issued a challenge to\nNiccolo Fontana (1499–1557). Fontana was known as Tartaglia (the Stammerer). As a youth\nhe had suffered a blow from the sword of a French soldier during an attack on his village.\nHe survived the savage wound, but his speech was permanently impaired. Tartaglia sent\nFior a list of 30 various mathematical problems; Fior countered by sending Tartaglia a list\nof 30 depressed cubics. Tartaglia would either solve all 30 of the problems or absolutely fail.\nAfter much effort Tartaglia finally succeeded in solving the depressed cubic and defeated\nFior, who faded into obscurity.\n\nAt this point another mathematician, Gerolamo Cardano (1501–1576), entered the story.\nCardano wrote to Tartaglia, begging him for the solution to the depressed cubic. Tartaglia\nrefused several of his requests, then finally revealed the solution to Cardano after the latter\nswore an oath not to publish the secret or to pass it on to anyone else. Using the knowledge\nthat he had obtained from Tartaglia, Cardano eventually solved the general cubic\n\nax3 + bx2 + cx+ d = 0.\n\nCardano shared the secret with his student, Ludovico Ferrari (1522–1565), who solved the\ngeneral quartic equation,\n\nax4 + bx3 + cx2 + dx+ e = 0.\n\nIn 1543, Cardano and Ferrari examined del Ferro’s papers and discovered that he had also\nsolved the depressed cubic. Cardano felt that this relieved him of his obligation to Tartaglia,\nso he proceeded to publish the solutions in Ars Magna (1545), in which he gave credit to\ndel Ferro for solving the special case of the cubic. This resulted in a bitter dispute between\nCardano and Tartaglia, who published the story of the oath a year later.\n\n17.4 Exercises\n1. List all of the polynomials of degree 3 or less in Z2[x].\n\n\n\n294 CHAPTER 17. POLYNOMIALS\n\n2. Compute each of the following.\n(a) (5x2 + 3x− 4) + (4x2 − x+ 9) in Z12\n\n(b) (5x2 + 3x− 4)(4x2 − x+ 9) in Z12\n\n(c) (7x3 + 3x2 − x) + (6x2 − 8x+ 4) in Z9\n\n(d) (3x2 + 2x− 4) + (4x2 + 2) in Z5\n\n(e) (3x2 + 2x− 4)(4x2 + 2) in Z5\n\n(f) (5x2 + 3x− 2)2 in Z12\n\n3. Use the division algorithm to find q(x) and r(x) such that a(x) = q(x)b(x) + r(x) with\ndeg r(x) < deg b(x) for each of the following pairs of polynomials.\n(a) a(x) = 5x3 + 6x2 − 3x+ 4 and b(x) = x− 2 in Z7[x]\n\n(b) a(x) = 6x4 − 2x3 + x2 − 3x+ 1 and b(x) = x2 + x− 2 in Z7[x]\n\n(c) a(x) = 4x5 − x3 + x2 + 4 and b(x) = x3 − 2 in Z5[x]\n\n(d) a(x) = x5 + x3 − x2 − x and b(x) = x3 + x in Z2[x]\n\n4. Find the greatest common divisor of each of the following pairs p(x) and q(x) of polyno-\nmials. If d(x) = gcd(p(x), q(x)), find two polynomials a(x) and b(x) such that a(x)p(x) +\nb(x)q(x) = d(x).\n(a) p(x) = x3 − 6x2 + 14x− 15 and q(x) = x3 − 8x2 + 21x− 18, where p(x), q(x) ∈ Q[x]\n\n(b) p(x) = x3 + x2 − x+ 1 and q(x) = x3 + x− 1, where p(x), q(x) ∈ Z2[x]\n\n(c) p(x) = x3 + x2 − 4x+ 4 and q(x) = x3 + 3x− 2, where p(x), q(x) ∈ Z5[x]\n\n(d) p(x) = x3 − 2x+ 4 and q(x) = 4x3 + x+ 3, where p(x), q(x) ∈ Q[x]\n\n5. Find all of the zeros for each of the following polynomials.\n\n(a) 5x3 + 4x2 − x+ 9 in Z12\n\n(b) 3x3 − 4x2 − x+ 4 in Z5\n\n(c) 5x4 + 2x2 − 3 in Z7\n\n(d) x3 + x+ 1 in Z2\n\n6. Find all of the units in Z[x].\n\n7. Find a unit p(x) in Z4[x] such that deg p(x) > 1.\n\n8. Which of the following polynomials are irreducible over Q[x]?\n\n(a) x4 − 2x3 + 2x2 + x+ 4\n\n(b) x4 − 5x3 + 3x− 2\n\n(c) 3x5 − 4x3 − 6x2 + 6\n\n(d) 5x5 − 6x4 − 3x2 + 9x− 15\n\n9. Find all of the irreducible polynomials of degrees 2 and 3 in Z2[x].\n\n10. Give two different factorizations of x2 + x+ 8 in Z10[x].\n\n11. Prove or disprove: There exists a polynomial p(x) in Z6[x] of degree n with more than\nn distinct zeros.\n\n12. If F is a field, show that F [x1, . . . , xn] is an integral domain.\n\n13. Show that the division algorithm does not hold for Z[x]. Why does it fail?\n\n\n\n17.4. EXERCISES 295\n\n14. Prove or disprove: xp + a is irreducible for any a ∈ Zp, where p is prime.\n\n15. Let f(x) be irreducible in F [x], where F is a field. If f(x) | p(x)q(x), prove that either\nf(x) | p(x) or f(x) | q(x).\n\n16. Suppose that R and S are isomorphic rings. Prove that R[x] ∼= S[x].\n\n17. Let F be a field and a ∈ F . If p(x) ∈ F [x], show that p(a) is the remainder obtained\nwhen p(x) is divided by x− a.\n\n18. (The Rational Root Theorem) Let\n\np(x) = anx\nn + an−1x\n\nn−1 + · · ·+ a0 ∈ Z[x],\n\nwhere an ̸= 0. Prove that if p(r/s) = 0, where gcd(r, s) = 1, then r | a0 and s | an.\n\n19. Let Q∗ be the multiplicative group of positive rational numbers. Prove that Q∗ is\nisomorphic to (Z[x],+).\n\n20. (Cyclotomic Polynomials) The polynomial\n\nΦn(x) =\nxn − 1\n\nx− 1\n= xn−1 + xn−2 + · · ·+ x+ 1\n\nis called the cyclotomic polynomial. Show that Φp(x) is irreducible over Q for any prime\np.\n\n21. If F is a field, show that there are infinitely many irreducible polynomials in F [x].\n\n22. Let R be a commutative ring with identity. Prove that multiplication is commutative\nin R[x].\n\n23. Let R be a commutative ring with identity. Prove that multiplication is distributive in\nR[x].\n\n24. Show that xp − x has p distinct zeros in Zp, for any prime p. Conclude that\n\nxp − x = x(x− 1)(x− 2) · · · (x− (p− 1)).\n\n25. Let F be a field and f(x) = a0 + a1x + · · · + anx\nn be in F [x]. Define f ′(x) = a1 +\n\n2a2x+ · · ·+ nanx\nn−1 to be the derivative of f(x).\n\n(a) Prove that\n(f + g)′(x) = f ′(x) + g′(x).\n\nConclude that we can define a homomorphism of abelian groups D : F [x] → F [x] by\nD(f(x)) = f ′(x).\n\n(b) Calculate the kernel of D if charF = 0.\n(c) Calculate the kernel of D if charF = p.\n(d) Prove that\n\n(fg)′(x) = f ′(x)g(x) + f(x)g′(x).\n\n(e) Suppose that we can factor a polynomial f(x) ∈ F [x] into linear factors, say\n\nf(x) = a(x− a1)(x− a2) · · · (x− an).\n\nProve that f(x) has no repeated factors if and only if f(x) and f ′(x) are relatively\nprime.\n\n\n\n296 CHAPTER 17. POLYNOMIALS\n\n26. Let F be a field. Show that F [x] is never a field.\n\n27. Let R be an integral domain. Prove that R[x1, . . . , xn] is an integral domain.\n\n28. Let R be a commutative ring with identity. Show that R[x] has a subring R′ isomorphic\nto R.\n\n29. Let p(x) and q(x) be polynomials in R[x], where R is a commutative ring with identity.\nProve that deg(p(x) + q(x)) ≤ max(deg p(x),deg q(x)).\n\n17.5 Additional Exercises: Solving the Cubic and Quartic\nEquations\n\n1. Solve the general quadratic equation\n\nax2 + bx+ c = 0\n\nto obtain\nx =\n\n−b±\n√\nb2 − 4ac\n\n2a\n.\n\nThe discriminant of the quadratic equation ∆ = b2 − 4ac determines the nature of the\nsolutions of the equation. If ∆ > 0, the equation has two distinct real solutions. If ∆ = 0,\nthe equation has a single repeated real root. If ∆ < 0, there are two distinct imaginary\nsolutions.\n\n2. Show that any cubic equation of the form\n\nx3 + bx2 + cx+ d = 0\n\ncan be reduced to the form y3 + py + q = 0 by making the substitution x = y − b/3.\n\n3. Prove that the cube roots of 1 are given by\n\nω =\n−1 + i\n\n√\n3\n\n2\n\nω2 =\n−1− i\n\n√\n3\n\n2\nω3 = 1.\n\n4. Make the substitution\ny = z − p\n\n3z\n\nfor y in the equation y3 + py + q = 0 and obtain two solutions A and B for z3.\n\n5. Show that the product of the solutions obtained in (4) is −p3/27, deducing that 3\n√\nAB =\n\n−p/3.\n\n6. Prove that the possible solutions for z in (4) are given by\n3\n√\nA, ω\n\n3\n√\nA, ω2 3\n\n√\nA,\n\n3\n√\nB, ω\n\n3\n√\nB, ω2 3\n\n√\nB\n\nand use this result to show that the three possible solutions for y are\n\nωi 3\n\n√\n−q\n2\n+\n\n√\np3\n\n27\n+\nq2\n\n4\n+ ω2i 3\n\n√\n−q\n2\n−\n√\n\np3\n\n27\n+\nq2\n\n4\n,\n\nwhere i = 0, 1, 2.\n\n\n\n17.5. ADDITIONAL EXERCISES: SOLVING THE CUBIC AND QUARTIC EQUATIONS297\n\n7. The discriminant of the cubic equation is\n\n∆ =\np3\n\n27\n+\nq2\n\n4\n.\n\nShow that y3 + py + q = 0\n\n(a) has three real roots, at least two of which are equal, if ∆ = 0.\n(b) has one real root and two conjugate imaginary roots if ∆ > 0.\n(c) has three distinct real roots if ∆ < 0.\n\n8. Solve the following cubic equations.\n(a) x3 − 4x2 + 11x+ 30 = 0\n\n(b) x3 − 3x+ 5 = 0\n\n(c) x3 − 3x+ 2 = 0\n\n(d) x3 + x+ 3 = 0\n\n9. Show that the general quartic equation\n\nx4 + ax3 + bx2 + cx+ d = 0\n\ncan be reduced to\ny4 + py2 + qy + r = 0\n\nby using the substitution x = y − a/4.\n\n10. Show that (\ny2 +\n\n1\n\n2\nz\n\n)2\n\n= (z − p)y2 − qy +\n\n(\n1\n\n4\nz2 − r\n\n)\n.\n\n11. Show that the right-hand side of Exercise 17.5.10 can be put in the form (my + k)2 if\nand only if\n\nq2 − 4(z − p)\n\n(\n1\n\n4\nz2 − r\n\n)\n= 0.\n\n12. From Exercise 17.5.11 obtain the resolvent cubic equation\n\nz3 − pz2 − 4rz + (4pr − q2) = 0.\n\nSolving the resolvent cubic equation, put the equation found in Exercise 17.5.10 in the form(\ny2 +\n\n1\n\n2\nz\n\n)2\n\n= (my + k)2\n\nto obtain the solution of the quartic equation.\n\n13. Use this method to solve the following quartic equations.\n(a) x4 − x2 − 3x+ 2 = 0\n\n(b) x4 + x3 − 7x2 − x+ 6 = 0\n\n(c) x4 − 2x2 + 4x− 3 = 0\n\n(d) x4 − 4x3 + 3x2 − 5x+ 2 = 0\n\n\n\n298 CHAPTER 17. POLYNOMIALS\n\n17.6 Sage\nSage is particularly adept at building, analyzing and manipulating polynomial rings. We\nhave seen some of this in the previous chapter. Let’s begin by creating three polynomial\nrings and checking some of their basic properties. There are several ways to construct\npolynomial rings, but the syntax used here is the most straightforward.\n\nPolynomial Rings and their Elements\n\nR.<x> = Integers (8)[]; R\n\nUnivariate Polynomial Ring in x over Ring of integers modulo 8\n\nS.<y> = ZZ[]; S\n\nUnivariate Polynomial Ring in y over Integer Ring\n\nT.<z> = QQ[]; T\n\nUnivariate Polynomial Ring in z over Rational Field\n\nBasic properties of rings are availble for these examples.\nR.is_finite ()\n\nFalse\n\nR.is_integral_domain ()\n\nFalse\n\nS.is_integral_domain ()\n\nTrue\n\nT.is_field ()\n\nFalse\n\nR.characteristic ()\n\n8\n\nT.characteristic ()\n\n0\n\nWith the construction syntax used above, the variables can be used to create elements of\nthe polynomial ring without explicit coercion (though we need to be careful about constant\npolynomials).\n\ny in S\n\nTrue\n\n\n\n17.6. SAGE 299\n\nx in S\n\nFalse\n\nq = (3/2) + (5/4)*z^2\nq in T\n\nTrue\n\n3 in S\n\nTrue\n\nr = 3\nr.parent ()\n\nInteger Ring\n\ns = 3*y^0\ns.parent ()\n\nUnivariate Polynomial Ring in y over Integer Ring\n\nPolynomials can be evaluated like they are functions, so we can mimic the evaluation\nhomomorphism.\n\np = 3 + 5*x + 2*x^2\np.parent ()\n\nUnivariate Polynomial Ring in x over Ring of integers modulo 8\n\np(1)\n\n2\n\n[p(t) for t in Integers (8)]\n\n[3, 2, 5, 4, 7, 6, 1, 0]\n\nNotice that p is a degree two polynomial, yet through a brute-force examination we see\nthat the polynomial only has one root, contrary to our usual expectations. It can be even\nmore unusual.\n\nq = 4*x^2+4*x\n[q(t) for t in Integers (8)]\n\n[0, 0, 0, 0, 0, 0, 0, 0]\n\nSage can create and manipulate rings of polynomials in more than one variable, though\nwe will not have much occasion to use this functionality in this course.\n\nM.<s, t> = QQ[]; M\n\nMultivariate Polynomial Ring in s, t over Rational Field\n\n\n\n300 CHAPTER 17. POLYNOMIALS\n\nIrreducible Polynomials\nWhether or not a polynomial factors, taking into consideration the ring used for its coeffi-\ncients, is an important topic in this chapter and many of the following chapters. Sage can\nfactor, and determine irreducibility, over the integers, the rationals, and finite fields.\n\nFirst, over the rationals.\nR.<x> = QQ[]\np = 1/4*x^4 - x^3 + x^2 - x - 1/2\np.is_irreducible ()\n\nTrue\n\np.factor ()\n\n(1/4) * (x^4 - 4*x^3 + 4*x^2 - 4*x - 2)\n\nq = 2*x^5 + 5/2*x^4 + 3/4*x^3 - 25/24*x^2 - x - 1/2\nq.is_irreducible ()\n\nFalse\n\nq.factor ()\n\n(2) * (x^2 + 3/2*x + 3/4) * (x^3 - 1/4*x^2 - 1/3)\n\nFactoring over the integers is really no different than factoring over the rationals. This is\nthe content of Theorem 17.14 — finding a factorization over the integers can be converted\nto finding a factorization over the rationals. So it is with Sage, there is little difference\nbetween working over the rationals and the integers. It is a little different working over a\nfinite field. Commentary follows.\n\nF.<a> = FiniteField (5^2)\nS.<y> = F[]\np = 2*y^5 + 2*y^4 + 4*y^3 + 2*y^2 + 3*y + 1\np.is_irreducible ()\n\nTrue\n\np.factor ()\n\n(2) * (y^5 + y^4 + 2*y^3 + y^2 + 4*y + 3)\n\nq = 3*y^4+2*y^3-y+4; q.factor ()\n\n(3) * (y^2 + (a + 4)*y + 2*a + 3) * (y^2 + 4*a*y + 3*a)\n\nr = y^4+2*y^3+3*y^2+4; r.factor ()\n\n(y + 4) * (y^3 + 3*y^2 + y + 1)\n\ns = 3*y^4+2*y^3-y+3; s.factor ()\n\n(3) * (y + 1) * (y + 3) * (y + 2*a + 4) * (y + 3*a + 1)\n\n\n\n17.6. SAGE 301\n\nTo check these factorizations, we need to compute in the finite field, F, and so we need\nto know how the symbol a behaves. This symbol is considered as a root of a degree two\npolynomial over the integers mod 5, which we can get with the .modulus() method.\n\nF.modulus ()\n\nx^2 + 4*x + 2\n\nSo a2 + 4a + 2 = 0, or a2 = −4a − 3 = a + 2. So when checking the factorizations,\nanytime you see an a2 you can replace it by a+ 2. Notice that by Corollary 17.8 we could\nfind the one linear factor of r, and the four linear factors of s, through a brute-force search\nfor roots. This is feasible because the field is finite.\n\n[t for t in F if r(t)==0]\n\n[1]\n\n[t for t in F if s(t)==0]\n\n[2, 3*a + 1, 4, 2*a + 4]\n\nHowever, q factors into a pair of degree 2 polynomials, so no amount of testing for roots\nwill discover a factor.\n\nWith Eisenstein’s Criterion, we can create irreducible polynomials, such as in Exam-\nple 17.18.\n\nW.<w> = QQ[]\np = 16*w^5 - 9*w^4 +3*w^2 + 6*w -21\np.is_irreducible ()\n\nTrue\n\nOver the field Zp, the field of integers mod a prime p, Conway polynomials are canonical\nchoices of a polynomial of degree n that is irreducible over Zp. See the exercises for more\nabout these polynomials.\n\nPolynomials over Fields\nIf F is a field, then every ideal of F [x] is principal (Theorem 17.20). Nothing stops you\nfrom giving Sage two (or more) generators to construct an ideal, but Sage will determine\nthe element to use in a description of the ideal as a principal ideal.\n\nW.<w> = QQ[]\nr = -w^5 + 5*w^4 - 4*w^3 + 14*w^2 - 67*w + 17\ns = 3*w^5 - 14*w^4 + 12*w^3 - 6*w^2 + w\nS = W.ideal(r, s)\nS\n\nPrincipal ideal (w^2 - 4*w + 1) of\nUnivariate Polynomial Ring in w over Rational Field\n\n(w^2)*r + (3*w-6)*s in S\n\nTrue\n\nTheorem 17.22 is the key fact that allows us to easily construct finite fields. Here is a\nconstruction of a finite field of order 75 = 16 807. All we need is a polynomial of degree 5\nthat is irreducible over Z7.\n\n\n\n302 CHAPTER 17. POLYNOMIALS\n\nF = Integers (7)\nR.<x> = F[]\np = x^5+ x + 4\np.is_irreducible ()\n\nTrue\n\nid = R.ideal(p)\nQ = R.quotient(id); Q\n\nUnivariate Quotient Polynomial Ring in xbar over\nRing of integers modulo 7 with modulus x^5 + x + 4\n\nQ.is_field ()\n\nTrue\n\nQ.order() == 7^5\n\nTrue\n\nThe symbol xbar is a generator of the field, but right now it is not accessible. xbar is\nthe coset x+ ⟨x5 + x+ 4⟩. A better construction would include specifying this generator.\n\nQ.gen (0)\n\nxbar\n\nQ.<t> = R.quotient(id); Q\n\nUnivariate Quotient Polynomial Ring in t over\nRing of integers modulo 7 with modulus x^5 + x + 4\n\nt^5 + t + 4\n\n0\n\nt^5 == -(t+4)\n\nTrue\n\nt^5\n\n6*t + 3\n\n(3*t^3 + t + 5)*(t^2 + 4*t + 2)\n\n5*t^4 + 2*t^2 + 5*t + 5\n\na = 3*t^4 - 6*t^3 + 3*t^2 + 5*t + 2\nainv = a^-1; ainv\n\n6*t^4 + 5*t^2 + 4\n\na*ainv\n\n1\n\n\n\n17.7. SAGE EXERCISES 303\n\n17.7 Sage Exercises\n1. Consider the polynomial x3 − 3x+ 4. Compute the most thorough factorization of this\npolynomial over each of the following fields: (a) the finite field Z5, (b) a finite field with\n125 elements, (c) the rationals, (d) the real numbers and (e) the complex numbers. To do\nthis, build the appropriate polynomial ring, and construct the polynomial as a member of\nthis ring, and use the .factor() method.\n\n2. “Conway polynomials” are irreducible polynomials over Zp that Sage (and other soft-\nware) uses to build maximal ideals in polynomial rings, and thus quotient rings that are\nfields. Roughly speaking, they are “canonical” choices for each degree and each prime.\nThe command conway_polynomial(p, n) will return a database entry that is an irreducible\npolynomial of degree n over Zp.\nExecute the command conway_polynomial(5, 4) to obtain an allegedly irreducible polynomial\nof degree 4 over Z5: p = x4+4x2+4x+2. Construct the right polynomial ring (i.e., in the\nindeterminate x) and verify that p is really an element of your polynomial ring.\nFirst determine that p has no linear factors. The only possibility left is that p factors as\ntwo quadratic polynomials over Z5. Use a list comprehension with three for statements\nto create every possible quadratic polynomial over Z5. Now use this list to create every\npossible product of two quadratic polynomials and check to see if p is in this list.\nMore on Conway polynomials is available at Frank Lübeck’s site.\n\n3. Construct a finite field of order 729 as a quotient of a polynomial ring by a principal\nideal generated with a Conway polynomial.\n\n4. Define the polynomials p = x3 + 2x2 + 2x + 4 and q = x4 + 2x2 as polynomials with\ncoefficients from the integers. Compute gcd(p, q) and verify that the result divides both p\n\nand q (just form a fraction in Sage and see that it simplifies cleanly, or use the .quo_rem()\n\nmethod).\nProposition 17.10 says there are polynomials r(x) and s(x) such that the greatest common\ndivisor equals r(x)p(x) + s(x)q(x), if the coefficients come from a field. Since here we have\ntwo polynomials over the integers, investigate the results returned by Sage for the extended\ngcd, xgcd(p, q). In particular, show that the first result of the returned triple is a multiple\nof the gcd. Then verify the “linear combination” property of the result.\n\n5. For a polynomial ring over a field, every ideal is principal. Begin with the ring of\npolynomials over the rationals. Experiment with constructing ideals using two generators\nand then see that Sage converts the ideal to a principal ideal with a single generator. (You\ncan get this generator with the ideal method .gen().) Can you explain how this single\ngenerator is computed?\n\nhttp://www.math.rwth-aachen.de/~Frank.Luebeck/data/ConwayPol/index.html\n\n\n18\n\nIntegral Domains\n\nOne of the most important rings we study is the ring of integers. It was our first example of\nan algebraic structure: the first polynomial ring that we examined was Z[x]. We also know\nthat the integers sit naturally inside the field of rational numbers, Q. The ring of integers\nis the model for all integral domains. In this chapter we will examine integral domains in\ngeneral, answering questions about the ideal structure of integral domains, polynomial rings\nover integral domains, and whether or not an integral domain can be embedded in a field.\n\n18.1 Fields of Fractions\nEvery field is also an integral domain; however, there are many integral domains that are\nnot fields. For example, the integers Z form an integral domain but not a field. A question\nthat naturally arises is how we might associate an integral domain with a field. There is a\nnatural way to construct the rationals Q from the integers: the rationals can be represented\nas formal quotients of two integers. The rational numbers are certainly a field. In fact, it\ncan be shown that the rationals are the smallest field that contains the integers. Given an\nintegral domain D, our question now becomes how to construct a smallest field F containing\nD. We will do this in the same way as we constructed the rationals from the integers.\n\nAn element p/q ∈ Q is the quotient of two integers p and q; however, different pairs of\nintegers can represent the same rational number. For instance, 1/2 = 2/4 = 3/6. We know\nthat\n\na\n\nb\n=\nc\n\nd\n\nif and only if ad = bc. A more formal way of considering this problem is to examine fractions\nin terms of equivalence relations. We can think of elements in Q as ordered pairs in Z×Z.\nA quotient p/q can be written as (p, q). For instance, (3, 7) would represent the fraction\n3/7. However, there are problems if we consider all possible pairs in Z × Z. There is no\nfraction 5/0 corresponding to the pair (5, 0). Also, the pairs (3, 6) and (2, 4) both represent\nthe fraction 1/2. The first problem is easily solved if we require the second coordinate to\nbe nonzero. The second problem is solved by considering two pairs (a, b) and (c, d) to be\nequivalent if ad = bc.\n\nIf we use the approach of ordered pairs instead of fractions, then we can study integral\ndomains in general. Let D be any integral domain and let\n\nS = {(a, b) : a, b ∈ D and b ̸= 0}.\n\nDefine a relation on S by (a, b) ∼ (c, d) if ad = bc.\n\nLemma 18.1. The relation ∼ between elements of S is an equivalence relation.\n\n304\n\n\n\n18.1. FIELDS OF FRACTIONS 305\n\nProof. Since D is commutative, ab = ba; hence, ∼ is reflexive on D. Now suppose that\n(a, b) ∼ (c, d). Then ad = bc or cb = da. Therefore, (c, d) ∼ (a, b) and the relation is\nsymmetric. Finally, to show that the relation is transitive, let (a, b) ∼ (c, d) and (c, d) ∼\n(e, f). In this case ad = bc and cf = de. Multiplying both sides of ad = bc by f yields\n\nafd = adf = bcf = bde = bed.\n\nSince D is an integral domain, we can deduce that af = be or (a, b) ∼ (e, f).\n\nWe will denote the set of equivalence classes on S by FD. We now need to define\nthe operations of addition and multiplication on FD. Recall how fractions are added and\nmultiplied in Q:\n\na\n\nb\n+\nc\n\nd\n=\nad+ bc\n\nbd\n;\n\na\n\nb\n· c\nd\n=\nac\n\nbd\n.\n\nIt seems reasonable to define the operations of addition and multiplication on FD in a\nsimilar manner. If we denote the equivalence class of (a, b) ∈ S by [a, b], then we are led to\ndefine the operations of addition and multiplication on FD by\n\n[a, b] + [c, d] = [ad+ bc, bd]\n\nand\n[a, b] · [c, d] = [ac, bd],\n\nrespectively. The next lemma demonstrates that these operations are independent of the\nchoice of representatives from each equivalence class.\n\nLemma 18.2. The operations of addition and multiplication on FD are well-defined.\n\nProof. We will prove that the operation of addition is well-defined. The proof that mul-\ntiplication is well-defined is left as an exercise. Let [a1, b1] = [a2, b2] and [c1, d1] = [c2, d2].\nWe must show that\n\n[a1d1 + b1c1, b1d1] = [a2d2 + b2c2, b2d2]\n\nor, equivalently, that\n\n(a1d1 + b1c1)(b2d2) = (b1d1)(a2d2 + b2c2).\n\nSince [a1, b1] = [a2, b2] and [c1, d1] = [c2, d2], we know that a1b2 = b1a2 and c1d2 = d1c2.\nTherefore,\n\n(a1d1 + b1c1)(b2d2) = a1d1b2d2 + b1c1b2d2\n\n= a1b2d1d2 + b1b2c1d2\n\n= b1a2d1d2 + b1b2d1c2\n\n= (b1d1)(a2d2 + b2c2).\n\nLemma 18.3. The set of equivalence classes of S, FD, under the equivalence relation ∼,\ntogether with the operations of addition and multiplication defined by\n\n[a, b] + [c, d] = [ad+ bc, bd]\n\n[a, b] · [c, d] = [ac, bd],\n\nis a field.\n\n\n\n306 CHAPTER 18. INTEGRAL DOMAINS\n\nProof. The additive and multiplicative identities are [0, 1] and [1, 1], respectively. To show\nthat [0, 1] is the additive identity, observe that\n\n[a, b] + [0, 1] = [a1 + b0, b1] = [a, b].\n\nIt is easy to show that [1, 1] is the multiplicative identity. Let [a, b] ∈ FD such that a ̸= 0.\nThen [b, a] is also in FD and [a, b] · [b, a] = [1, 1]; hence, [b, a] is the multiplicative inverse for\n[a, b]. Similarly, [−a, b] is the additive inverse of [a, b]. We leave as exercises the verification\nof the associative and commutative properties of multiplication in FD. We also leave it to\nthe reader to show that FD is an abelian group under addition.\n\nIt remains to show that the distributive property holds in FD; however,\n\n[a, b][e, f ] + [c, d][e, f ] = [ae, bf ] + [ce, df ]\n\n= [aedf + bfce, bdf2]\n\n= [aed+ bce, bdf ]\n\n= [ade+ bce, bdf ]\n\n= ([a, b] + [c, d])[e, f ]\n\nand the lemma is proved.\n\nThe field FD in Lemma 18.3 is called the field of fractions or field of quotients of\nthe integral domain D.\n\nTheorem 18.4. Let D be an integral domain. Then D can be embedded in a field of\nfractions FD, where any element in FD can be expressed as the quotient of two elements\nin D. Furthermore, the field of fractions FD is unique in the sense that if E is any field\ncontaining D, then there exists a map ψ : FD → E giving an isomorphism with a subfield\nof E such that ψ(a) = a for all elements a ∈ D.\n\nProof. We will first demonstrate that D can be embedded in the field FD. Define a map\nϕ : D → FD by ϕ(a) = [a, 1]. Then for a and b in D,\n\nϕ(a+ b) = [a+ b, 1] = [a, 1] + [b, 1] = ϕ(a) + ϕ(b)\n\nand\nϕ(ab) = [ab, 1] = [a, 1][b, 1] = ϕ(a)ϕ(b);\n\nhence, ϕ is a homomorphism. To show that ϕ is one-to-one, suppose that ϕ(a) = ϕ(b).\nThen [a, 1] = [b, 1], or a = a1 = 1b = b. Finally, any element of FD can be expressed as the\nquotient of two elements in D, since\n\nϕ(a)[ϕ(b)]−1 = [a, 1][b, 1]−1 = [a, 1] · [1, b] = [a, b].\n\nNow let E be a field containing D and define a map ψ : FD → E by ψ([a, b]) = ab−1.\nTo show that ψ is well-defined, let [a1, b1] = [a2, b2]. Then a1b2 = b1a2. Therefore, a1b−1\n\n1 =\na2b\n\n−1\n2 and ψ([a1, b1]) = ψ([a2, b2]).\nIf [a, b] and [c, d] are in FD, then\n\nψ([a, b] + [c, d]) = ψ([ad+ bc, bd])\n\n= (ad+ bc)(bd)−1\n\n= ab−1 + cd−1\n\n= ψ([a, b]) + ψ([c, d])\n\n\n\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 307\n\nand\n\nψ([a, b] · [c, d]) = ψ([ac, bd])\n\n= (ac)(bd)−1\n\n= ab−1cd−1\n\n= ψ([a, b])ψ([c, d]).\n\nTherefore, ψ is a homomorphism.\nTo complete the proof of the theorem, we need to show that ψ is one-to-one. Suppose\n\nthat ψ([a, b]) = ab−1 = 0. Then a = 0b = 0 and [a, b] = [0, b]. Therefore, the kernel of ψ is\nthe zero element [0, b] in FD, and ψ is injective.\n\nExample 18.5. Since Q is a field, Q[x] is an integral domain. The field of fractions of Q[x]\nis the set of all rational expressions p(x)/q(x), where p(x) and q(x) are polynomials over\nthe rationals and q(x) is not the zero polynomial. We will denote this field by Q(x).\n\nWe will leave the proofs of the following corollaries of Theorem 18.4 as exercises.\n\nCorollary 18.6. Let F be a field of characteristic zero. Then F contains a subfield iso-\nmorphic to Q.\n\nCorollary 18.7. Let F be a field of characteristic p. Then F contains a subfield isomorphic\nto Zp.\n\n18.2 Factorization in Integral Domains\nThe building blocks of the integers are the prime numbers. If F is a field, then irreducible\npolynomials in F [x] play a role that is very similar to that of the prime numbers in the\nring of integers. Given an arbitrary integral domain, we are led to the following series of\ndefinitions.\n\nLet R be a commutative ring with identity, and let a and b be elements in R. We say\nthat a divides b, and write a | b, if there exists an element c ∈ R such that b = ac. A unit\nin R is an element that has a multiplicative inverse. Two elements a and b in R are said to\nbe associates if there exists a unit u in R such that a = ub.\n\nLet D be an integral domain. A nonzero element p ∈ D that is not a unit is said to\nbe irreducible provided that whenever p = ab, either a or b is a unit. Furthermore, p is\nprime if whenever p | ab either p | a or p | b.\n\nExample 18.8. It is important to notice that prime and irreducible elements do not always\ncoincide. Let R be the subring (with identity) of Q[x, y] generated by x2, y2, and xy. Each\nof these elements is irreducible in R; however, xy is not prime, since xy divides x2y2 but\ndoes not divide either x2 or y2.\n\nThe Fundamental Theorem of Arithmetic states that every positive integer n > 1 can\nbe factored into a product of prime numbers p1 · · · pk, where the pi’s are not necessarily\ndistinct. We also know that such factorizations are unique up to the order of the pi’s. We\ncan easily extend this result to the integers. The question arises of whether or not such\nfactorizations are possible in other rings. Generalizing this definition, we say an integral\ndomain D is a unique factorization domain, or ufd, if D satisfies the following criteria.\n\n1. Let a ∈ D such that a ̸= 0 and a is not a unit. Then a can be written as the product\nof irreducible elements in D.\n\n\n\n308 CHAPTER 18. INTEGRAL DOMAINS\n\n2. Let a = p1 · · · pr = q1 · · · qs, where the pi’s and the qi’s are irreducible. Then r = s\nand there is a π ∈ Sr such that pi and qπ(j) are associates for j = 1, . . . , r.\n\nExample 18.9. The integers are a unique factorization domain by the Fundamental The-\norem of Arithmetic.\nExample 18.10. Not every integral domain is a unique factorization domain. The subring\nZ[\n√\n3 i] = {a + b\n\n√\n3 i} of the complex numbers is an integral domain (Exercise 16.6.12,\n\nChapter 16). Let z = a+ b\n√\n3 i and define ν : Z[\n\n√\n3 i] → N ∪ {0} by ν(z) = |z|2 = a2 + 3b2.\n\nIt is clear that ν(z) ≥ 0 with equality when z = 0. Also, from our knowledge of complex\nnumbers we know that ν(zw) = ν(z)ν(w). It is easy to show that if ν(z) = 1, then z is a\nunit, and that the only units of Z[\n\n√\n3 i] are 1 and −1.\n\nWe claim that 4 has two distinct factorizations into irreducible elements:\n\n4 = 2 · 2 = (1−\n√\n3 i)(1 +\n\n√\n3 i).\n\nWe must show that each of these factors is an irreducible element in Z[\n√\n3 i]. If 2 is not\n\nirreducible, then 2 = zw for elements z, w in Z[\n√\n3 i] where ν(z) = ν(w) = 2. However,\n\nthere does not exist an element in z in Z[\n√\n3 i] such that ν(z) = 2 because the equation\n\na2+3b2 = 2 has no integer solutions. Therefore, 2 must be irreducible. A similar argument\nshows that both 1−\n\n√\n3 i and 1+\n\n√\n3 i are irreducible. Since 2 is not a unit multiple of either\n\n1−\n√\n3 i or 1 +\n\n√\n3 i, 4 has at least two distinct factorizations into irreducible elements.\n\nPrincipal Ideal Domains\nLet R be a commutative ring with identity. Recall that a principal ideal generated by\na ∈ R is an ideal of the form ⟨a⟩ = {ra : r ∈ R}. An integral domain in which every ideal\nis principal is called a principal ideal domain, or pid.\nLemma 18.11. Let D be an integral domain and let a, b ∈ D. Then\n\n1. a | b if and only if ⟨b⟩ ⊂ ⟨a⟩.\n\n2. a and b are associates if and only if ⟨b⟩ = ⟨a⟩.\n\n3. a is a unit in D if and only if ⟨a⟩ = D.\nProof. (1) Suppose that a | b. Then b = ax for some x ∈ D. Hence, for every r in D,\nbr = (ax)r = a(xr) and ⟨b⟩ ⊂ ⟨a⟩. Conversely, suppose that ⟨b⟩ ⊂ ⟨a⟩. Then b ∈ ⟨a⟩.\nConsequently, b = ax for some x ∈ D. Thus, a | b.\n\n(2) Since a and b are associates, there exists a unit u such that a = ub. Therefore,\nb | a and ⟨a⟩ ⊂ ⟨b⟩. Similarly, ⟨b⟩ ⊂ ⟨a⟩. It follows that ⟨a⟩ = ⟨b⟩. Conversely, suppose\nthat ⟨a⟩ = ⟨b⟩. By part (1), a | b and b | a. Then a = bx and b = ay for some x, y ∈ D.\nTherefore, a = bx = ayx. Since D is an integral domain, xy = 1; that is, x and y are units\nand a and b are associates.\n\n(3) An element a ∈ D is a unit if and only if a is an associate of 1. However, a is an\nassociate of 1 if and only if ⟨a⟩ = ⟨1⟩ = D.\n\nTheorem 18.12. Let D be a pid and ⟨p⟩ be a nonzero ideal in D. Then ⟨p⟩ is a maximal\nideal if and only if p is irreducible.\nProof. Suppose that ⟨p⟩ is a maximal ideal. If some element a in D divides p, then\n⟨p⟩ ⊂ ⟨a⟩. Since ⟨p⟩ is maximal, either D = ⟨a⟩ or ⟨p⟩ = ⟨a⟩. Consequently, either a and p\nare associates or a is a unit. Therefore, p is irreducible.\n\nConversely, let p be irreducible. If ⟨a⟩ is an ideal in D such that ⟨p⟩ ⊂ ⟨a⟩ ⊂ D, then\na | p. Since p is irreducible, either a must be a unit or a and p are associates. Therefore,\neither D = ⟨a⟩ or ⟨p⟩ = ⟨a⟩. Thus, ⟨p⟩ is a maximal ideal.\n\n\n\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 309\n\nCorollary 18.13. Let D be a pid. If p is irreducible, then p is prime.\n\nProof. Let p be irreducible and suppose that p | ab. Then ⟨ab⟩ ⊂ ⟨p⟩. By Corollary 16.40,\nsince ⟨p⟩ is a maximal ideal, ⟨p⟩ must also be a prime ideal. Thus, either a ∈ ⟨p⟩ or b ∈ ⟨p⟩.\nHence, either p | a or p | b.\n\nLemma 18.14. Let D be a pid. Let I1, I2, . . . be a set of ideals such that I1 ⊂ I2 ⊂ · · ·.\nThen there exists an integer N such that In = IN for all n ≥ N .\n\nProof. We claim that I =\n∪∞\n\ni=1 Ii is an ideal of D. Certainly I is not empty, since I1 ⊂ I\nand 0 ∈ I. If a, b ∈ I, then a ∈ Ii and b ∈ Ij for some i and j in N. Without loss of\ngenerality we can assume that i ≤ j. Hence, a and b are both in Ij and so a − b is also in\nIj . Now let r ∈ D and a ∈ I. Again, we note that a ∈ Ii for some positive integer i. Since\nIi is an ideal, ra ∈ Ii and hence must be in I. Therefore, we have shown that I is an ideal\nin D.\n\nSince D is a principal ideal domain, there exists an element a ∈ D that generates I.\nSince a is in IN for some N ∈ N, we know that IN = I = ⟨a⟩. Consequently, In = IN for\nn ≥ N .\n\nAny commutative ring satisfying the condition in Lemma 18.14 is said to satisfy the\nascending chain condition, or ACC. Such rings are called Noetherian rings, after\nEmmy Noether.\n\nTheorem 18.15. Every pid is a ufd.\n\nProof. Existence of a factorization. Let D be a pid and a be a nonzero element in D that\nis not a unit. If a is irreducible, then we are done. If not, then there exists a factorization\na = a1b1, where neither a1 nor b1 is a unit. Hence, ⟨a⟩ ⊂ ⟨a1⟩. By Lemma 18.11, we know\nthat ⟨a⟩ ̸= ⟨a1⟩; otherwise, a and a1 would be associates and b1 would be a unit, which\nwould contradict our assumption. Now suppose that a1 = a2b2, where neither a2 nor b2 is a\nunit. By the same argument as before, ⟨a1⟩ ⊂ ⟨a2⟩. We can continue with this construction\nto obtain an ascending chain of ideals\n\n⟨a⟩ ⊂ ⟨a1⟩ ⊂ ⟨a2⟩ ⊂ · · · .\n\nBy Lemma 18.14, there exists a positive integer N such that ⟨an⟩ = ⟨aN ⟩ for all n ≥ N .\nConsequently, aN must be irreducible. We have now shown that a is the product of two\nelements, one of which must be irreducible.\n\nNow suppose that a = c1p1, where p1 is irreducible. If c1 is not a unit, we can repeat\nthe preceding argument to conclude that ⟨a⟩ ⊂ ⟨c1⟩. Either c1 is irreducible or c1 = c2p2,\nwhere p2 is irreducible and c2 is not a unit. Continuing in this manner, we obtain another\nchain of ideals\n\n⟨a⟩ ⊂ ⟨c1⟩ ⊂ ⟨c2⟩ ⊂ · · · .\n\nThis chain must satisfy the ascending chain condition; therefore,\n\na = p1p2 · · · pr\n\nfor irreducible elements p1, . . . , pr.\nUniqueness of the factorization. To show uniqueness, let\n\na = p1p2 · · · pr = q1q2 · · · qs,\n\n\n\n310 CHAPTER 18. INTEGRAL DOMAINS\n\nwhere each pi and each qi is irreducible. Without loss of generality, we can assume that\nr < s. Since p1 divides q1q2 · · · qs, by Corollary 18.13 it must divide some qi. By rearranging\nthe qi’s, we can assume that p1 | q1; hence, q1 = u1p1 for some unit u1 in D. Therefore,\n\na = p1p2 · · · pr = u1p1q2 · · · qs\n\nor\np2 · · · pr = u1q2 · · · qs.\n\nContinuing in this manner, we can arrange the qi’s such that p2 = q2, p3 = q3, . . . , pr = qr,\nto obtain\n\nu1u2 · · ·urqr+1 · · · qs = 1.\n\nIn this case qr+1 · · · qs is a unit, which contradicts the fact that qr+1, . . . , qs are irreducibles.\nTherefore, r = s and the factorization of a is unique.\n\nCorollary 18.16. Let F be a field. Then F [x] is a ufd.\n\nExample 18.17. Every pid is a ufd, but it is not the case that every ufd is a pid.\nIn Corollary 18.31, we will prove that Z[x] is a ufd. However, Z[x] is not a pid. Let\nI = {5f(x) + xg(x) : f(x), g(x) ∈ Z[x]}. We can easily show that I is an ideal of Z[x].\nSuppose that I = ⟨p(x)⟩. Since 5 ∈ I, 5 = f(x)p(x). In this case p(x) = p must be a\nconstant. Since x ∈ I, x = pg(x); consequently, p = ±1. However, it follows from this\nfact that ⟨p(x)⟩ = Z[x]. But this would mean that 3 is in I. Therefore, we can write\n3 = 5f(x) + xg(x) for some f(x) and g(x) in Z[x]. Examining the constant term of this\npolynomial, we see that 3 = 5f(x), which is impossible.\n\nEuclidean Domains\nWe have repeatedly used the division algorithm when proving results about either Z or\nF [x], where F is a field. We should now ask when a division algorithm is available for an\nintegral domain.\n\nLet D be an integral domain such that for each a ∈ D there is a nonnegative integer\nν(a) satisfying the following conditions.\n\n1. If a and b are nonzero elements in D, then ν(a) ≤ ν(ab).\n\n2. Let a, b ∈ D and suppose that b ̸= 0. Then there exist elements q, r ∈ D such that\na = bq + r and either r = 0 or ν(r) < ν(b).\n\nThen D is called a Euclidean domain and ν is called a Euclidean valuation.\n\nExample 18.18. Absolute value on Z is a Euclidean valuation.\n\nExample 18.19. Let F be a field. Then the degree of a polynomial in F [x] is a Euclidean\nvaluation.\n\nExample 18.20. Recall that the Gaussian integers in Example 16.12 of Chapter 16 are\ndefined by\n\nZ[i] = {a+ bi : a, b ∈ Z}.\n\nWe usually measure the size of a complex number a + bi by its absolute value, |a + bi| =√\na2 + b2; however,\n\n√\na2 + b2 may not be an integer. For our valuation we will let ν(a+bi) =\n\na2 + b2 to ensure that we have an integer.\n\n\n\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 311\n\nWe claim that ν(a + bi) = a2 + b2 is a Euclidean valuation on Z[i]. Let z, w ∈ Z[i].\nThen ν(zw) = |zw|2 = |z|2|w|2 = ν(z)ν(w). Since ν(z) ≥ 1 for every nonzero z ∈ Z[i],\nν(z) ≤ ν(z)ν(w).\n\nNext, we must show that for any z = a + bi and w = c + di in Z[i] with w ̸= 0, there\nexist elements q and r in Z[i] such that z = qw + r with either r = 0 or ν(r) < ν(w). We\ncan view z and w as elements in Q(i) = {p + qi : p, q ∈ Q}, the field of fractions of Z[i].\nObserve that\n\nzw−1 = (a+ bi)\nc− di\n\nc2 + d2\n\n=\nac+ bd\n\nc2 + d2\n+\nbc− ad\n\nc2 + d2\ni\n\n=\n\n(\nm1 +\n\nn1\nc2 + d2\n\n)\n+\n\n(\nm2 +\n\nn2\nc2 + d2\n\n)\ni\n\n= (m1 +m2i) +\n\n(\nn1\n\nc2 + d2\n+\n\nn2\nc2 + d2\n\ni\n\n)\n= (m1 +m2i) + (s+ ti)\n\nin Q(i). In the last steps we are writing the real and imaginary parts as an integer plus a\nproper fraction. That is, we take the closest integer mi such that the fractional part satisfies\n|ni/(a2 + b2)| ≤ 1/2. For example, we write\n\n9\n\n8\n= 1 +\n\n1\n\n8\n15\n\n8\n= 2− 1\n\n8\n.\n\nThus, s and t are the “fractional parts” of zw−1 = (m1 +m2i) + (s + ti). We also know\nthat s2 + t2 ≤ 1/4 + 1/4 = 1/2. Multiplying by w, we have\n\nz = zw−1w = w(m1 +m2i) + w(s+ ti) = qw + r,\n\nwhere q = m1 + m2i and r = w(s + ti). Since z and qw are in Z[i], r must be in Z[i].\nFinally, we need to show that either r = 0 or ν(r) < ν(w). However,\n\nν(r) = ν(w)ν(s+ ti) ≤ 1\n\n2\nν(w) < ν(w).\n\nTheorem 18.21. Every Euclidean domain is a principal ideal domain.\n\nProof. Let D be a Euclidean domain and let ν be a Euclidean valuation on D. Suppose\nI is a nontrivial ideal in D and choose a nonzero element b ∈ I such that ν(b) is minimal\nfor all a ∈ I. Since D is a Euclidean domain, there exist elements q and r in D such that\na = bq + r and either r = 0 or ν(r) < ν(b). But r = a − bq is in I since I is an ideal;\ntherefore, r = 0 by the minimality of b. It follows that a = bq and I = ⟨b⟩.\n\nCorollary 18.22. Every Euclidean domain is a unique factorization domain.\n\nFactorization in D[x]\n\nOne of the most important polynomial rings is Z[x]. One of the first questions that come\nto mind about Z[x] is whether or not it is a ufd. We will prove a more general statement\nhere. Our first task is to obtain a more general version of Gauss’s Lemma (Theorem 17.14).\n\n\n\n312 CHAPTER 18. INTEGRAL DOMAINS\n\nLet D be a unique factorization domain and suppose that\n\np(x) = anx\nn + · · ·+ a1x+ a0\n\nin D[x]. Then the content of p(x) is the greatest common divisor of a0, . . . , an. We say\nthat p(x) is primitive if gcd(a0, . . . , an) = 1.\n\nExample 18.23. In Z[x] the polynomial p(x) = 5x4−3x3+x−4 is a primitive polynomial\nsince the greatest common divisor of the coefficients is 1; however, the polynomial q(x) =\n4x2 − 6x+ 8 is not primitive since the content of q(x) is 2.\n\nTheorem 18.24 (Gauss’s Lemma). Let D be a ufd and let f(x) and g(x) be primitive\npolynomials in D[x]. Then f(x)g(x) is primitive.\n\nProof. Let f(x) =\n∑m\n\ni=0 aix\ni and g(x) =\n\n∑n\ni=0 bix\n\ni. Suppose that p is a prime dividing\nthe coefficients of f(x)g(x). Let r be the smallest integer such that p ̸ |ar and s be the\nsmallest integer such that p ̸ |bs. The coefficient of xr+s in f(x)g(x) is\n\ncr+s = a0br+s + a1br+s−1 + · · ·+ ar+s−1b1 + ar+sb0.\n\nSince p divides a0, . . . , ar−1 and b0, . . . , bs−1, p divides every term of cr+s except for the term\narbs. However, since p | cr+s, either p divides ar or p divides bs. But this is impossible.\n\nLemma 18.25. Let D be a ufd, and let p(x) and q(x) be in D[x]. Then the content of\np(x)q(x) is equal to the product of the contents of p(x) and q(x).\n\nProof. Let p(x) = cp1(x) and q(x) = dq1(x), where c and d are the contents of p(x)\nand q(x), respectively. Then p1(x) and q1(x) are primitive. We can now write p(x)q(x) =\ncdp1(x)q1(x). Since p1(x)q1(x) is primitive, the content of p(x)q(x) must be cd.\n\nLemma 18.26. Let D be a ufd and F its field of fractions. Suppose that p(x) ∈ D[x] and\np(x) = f(x)g(x), where f(x) and g(x) are in F [x]. Then p(x) = f1(x)g1(x), where f1(x)\nand g1(x) are in D[x]. Furthermore, deg f(x) = deg f1(x) and deg g(x) = deg g1(x).\n\nProof. Let a and b be nonzero elements of D such that af(x), bg(x) are in D[x]. We can\nfind a1, b2 ∈ D such that af(x) = a1f1(x) and bg(x) = b1g1(x), where f1(x) and g1(x) are\nprimitive polynomials in D[x]. Therefore, abp(x) = (a1f1(x))(b1g1(x)). Since f1(x) and\ng1(x) are primitive polynomials, it must be the case that ab | a1b1 by Gauss’s Lemma.\nThus there exists a c ∈ D such that p(x) = cf1(x)g1(x). Clearly, deg f(x) = deg f1(x) and\ndeg g(x) = deg g1(x).\n\nThe following corollaries are direct consequences of Lemma 18.26.\n\nCorollary 18.27. Let D be a ufd and F its field of fractions. A primitive polynomial p(x)\nin D[x] is irreducible in F [x] if and only if it is irreducible in D[x].\n\nCorollary 18.28. Let D be a ufd and F its field of fractions. If p(x) is a monic polynomial\nin D[x] with p(x) = f(x)g(x) in F [x], then p(x) = f1(x)g1(x), where f1(x) and g1(x) are\nin D[x]. Furthermore, deg f(x) = deg f1(x) and deg g(x) = deg g1(x).\n\nTheorem 18.29. If D is a ufd, then D[x] is a ufd.\n\nProof. Let p(x) be a nonzero polynomial in D[x]. If p(x) is a constant polynomial,\nthen it must have a unique factorization since D is a ufd. Now suppose that p(x) is\na polynomial of positive degree in D[x]. Let F be the field of fractions of D, and let\np(x) = f1(x)f2(x) · · · fn(x) by a factorization of p(x), where each fi(x) is irreducible. Choose\n\n\n\n18.2. FACTORIZATION IN INTEGRAL DOMAINS 313\n\nai ∈ D such that aifi(x) is in D[x]. There exist b1, . . . , bn ∈ D such that aifi(x) = bigi(x),\nwhere gi(x) is a primitive polynomial in D[x]. By Corollary 18.27, each gi(x) is irreducible\nin D[x]. Consequently, we can write\n\na1 · · · anp(x) = b1 · · · bng1(x) · · · gn(x).\n\nLet b = b1 · · · bn. Since g1(x) · · · gn(x) is primitive, a1 · · · an divides b. Therefore, p(x) =\nag1(x) · · · gn(x), where a ∈ D. Since D is a ufd, we can factor a as uc1 · · · ck, where u is a\nunit and each of the ci’s is irreducible in D.\n\nWe will now show the uniqueness of this factorization. Let\n\np(x) = a1 · · · amf1(x) · · · fn(x) = b1 · · · brg1(x) · · · gs(x)\n\nbe two factorizations of p(x), where all of the factors are irreducible in D[x]. By Corol-\nlary 18.27, each of the fi’s and gi’s is irreducible in F [x]. The ai’s and the bi’s are units\nin F . Since F [x] is a pid, it is a ufd; therefore, n = s. Now rearrange the gi(x)’s so that\nfi(x) and gi(x) are associates for i = 1, . . . , n. Then there exist c1, . . . , cn and d1, . . . , dn in\nD such that (ci/di)fi(x) = gi(x) or cifi(x) = digi(x). The polynomials fi(x) and gi(x) are\nprimitive; hence, ci and di are associates in D. Thus, a1 · · · am = ub1 · · · br in D, where u is\na unit in D. Since D is a unique factorization domain, m = s. Finally, we can reorder the\nbi’s so that ai and bi are associates for each i. This completes the uniqueness part of the\nproof.\n\nThe theorem that we have just proven has several obvious but important corollaries.\n\nCorollary 18.30. Let F be a field. Then F [x] is a ufd.\n\nCorollary 18.31. The ring of polynomials over the integers, Z[x], is a ufd.\n\nCorollary 18.32. Let D be a ufd. Then D[x1, . . . , xn] is a ufd.\n\nRemark 18.33. It is important to notice that every Euclidean domain is a pid and every\npid is a ufd. However, as demonstrated by our examples, the converse of each of these\nstatements fails. There are principal ideal domains that are not Euclidean domains, and\nthere are unique factorization domains that are not principal ideal domains (Z[x]).\n\nHistorical Note\n\nKarl Friedrich Gauss, born in Brunswick, Germany on April 30, 1777, is considered to\nbe one of the greatest mathematicians who ever lived. Gauss was truly a child prodigy. At\nthe age of three he was able to detect errors in the books of his father’s business. Gauss\nentered college at the age of 15. Before the age of 20, Gauss was able to construct a regular\n17-sided polygon with a ruler and compass. This was the first new construction of a regular\nn-sided polygon since the time of the ancient Greeks. Gauss succeeded in showing that if\nN = 22\n\nn\n+ 1 was prime, then it was possible to construct a regular N -sided polygon.\n\nGauss obtained his Ph.D. in 1799 under the direction of Pfaff at the University of\nHelmstedt. In his dissertation he gave the first complete proof of the Fundamental Theorem\nof Algebra, which states that every polynomial with real coefficients can be factored into\nlinear factors over the complex numbers. The acceptance of complex numbers was brought\nabout by Gauss, who was the first person to use the notation of i for\n\n√\n−1.\n\nGauss then turned his attention toward number theory; in 1801, he published his fa-\nmous book on number theory, Disquisitiones Arithmeticae. Throughout his life Gauss was\n\n\n\n314 CHAPTER 18. INTEGRAL DOMAINS\n\nintrigued with this branch of mathematics. He once wrote, “Mathematics is the queen of\nthe sciences, and the theory of numbers is the queen of mathematics.”\n\nIn 1807, Gauss was appointed director of the Observatory at the University of Göttingen,\na position he held until his death. This position required him to study applications of math-\nematics to the sciences. He succeeded in making contributions to fields such as astronomy,\nmechanics, optics, geodesy, and magnetism. Along with Wilhelm Weber, he coinvented the\nfirst practical electric telegraph some years before a better version was invented by Samuel\nF. B. Morse.\n\nGauss was clearly the most prominent mathematician in the world in the early nineteenth\ncentury. His status naturally made his discoveries subject to intense scrutiny. Gauss’s cold\nand distant personality many times led him to ignore the work of his contemporaries, mak-\ning him many enemies. He did not enjoy teaching very much, and young mathematicians\nwho sought him out for encouragement were often rebuffed. Nevertheless, he had many\noutstanding students, including Eisenstein, Riemann, Kummer, Dirichlet, and Dedekind.\nGauss also offered a great deal of encouragement to Sophie Germain (1776–1831), who\novercame the many obstacles facing women in her day to become a very prominent mathe-\nmatician. Gauss died at the age of 78 in Göttingen on February 23, 1855.\n\n18.3 Exercises\n1. Let z = a+ b\n\n√\n3 i be in Z[\n\n√\n3 i]. If a2 + 3b2 = 1, show that z must be a unit. Show that\n\nthe only units of Z[\n√\n3 i] are 1 and −1.\n\n2. The Gaussian integers, Z[i], are a ufd. Factor each of the following elements in Z[i] into\na product of irreducibles.\n\n(a) 5\n(b) 1 + 3i\n\n(c) 6 + 8i\n\n(d) 2\n\n3. Let D be an integral domain.\n(a) Prove that FD is an abelian group under the operation of addition.\n(b) Show that the operation of multiplication is well-defined in the field of fractions, FD.\n(c) Verify the associative and commutative properties for multiplication in FD.\n\n4. Prove or disprove: Any subring of a field F containing 1 is an integral domain.\n\n5. Prove or disprove: If D is an integral domain, then every prime element in D is also\nirreducible in D.\n\n6. Let F be a field of characteristic zero. Prove that F contains a subfield isomorphic to\nQ.\n\n7. Let F be a field.\n(a) Prove that the field of fractions of F [x], denoted by F (x), is isomorphic to the set all\n\nrational expressions p(x)/q(x), where q(x) is not the zero polynomial.\n(b) Let p(x1, . . . , xn) and q(x1, . . . , xn) be polynomials in F [x1, . . . , xn]. Show that the\n\nset of all rational expressions p(x1, . . . , xn)/q(x1, . . . , xn) is isomorphic to the field\nof fractions of F [x1, . . . , xn]. We denote the field of fractions of F [x1, . . . , xn] by\nF (x1, . . . , xn).\n\n\n\n18.3. EXERCISES 315\n\n8. Let p be prime and denote the field of fractions of Zp[x] by Zp(x). Prove that Zp(x) is\nan infinite field of characteristic p.\n\n9. Prove that the field of fractions of the Gaussian integers, Z[i], is\n\nQ(i) = {p+ qi : p, q ∈ Q}.\n\n10. A field F is called a prime field if it has no proper subfields. If E is a subfield of F\nand E is a prime field, then E is a prime subfield of F .\n\n(a) Prove that every field contains a unique prime subfield.\n(b) If F is a field of characteristic 0, prove that the prime subfield of F is isomorphic to\n\nthe field of rational numbers, Q.\n(c) If F is a field of characteristic p, prove that the prime subfield of F is isomorphic to\n\nZp.\n\n11. Let Z[\n√\n2 ] = {a+ b\n\n√\n2 : a, b ∈ Z}.\n\n(a) Prove that Z[\n√\n2 ] is an integral domain.\n\n(b) Find all of the units in Z[\n√\n2 ].\n\n(c) Determine the field of fractions of Z[\n√\n2 ].\n\n(d) Prove that Z[\n√\n2i] is a Euclidean domain under the Euclidean valuation ν(a+b\n\n√\n2 i) =\n\na2 + 2b2.\n\n12. Let D be a ufd. An element d ∈ D is a greatest common divisor of a and b in D\nif d | a and d | b and d is divisible by any other element dividing both a and b.\n\n(a) If D is a pid and a and b are both nonzero elements of D, prove there exists a unique\ngreatest common divisor of a and b up to associates. That is, if d and d′ are both\ngreatest common divisors of a and b, then d and d′ are associates. We write gcd(a, b)\nfor the greatest common divisor of a and b.\n\n(b) Let D be a pid and a and b be nonzero elements of D. Prove that there exist elements\ns and t in D such that gcd(a, b) = as+ bt.\n\n13. Let D be an integral domain. Define a relation on D by a ∼ b if a and b are associates\nin D. Prove that ∼ is an equivalence relation on D.\n\n14. Let D be a Euclidean domain with Euclidean valuation ν. If u is a unit in D, show\nthat ν(u) = ν(1).\n\n15. Let D be a Euclidean domain with Euclidean valuation ν. If a and b are associates in\nD, prove that ν(a) = ν(b).\n\n16. Show that Z[\n√\n5 i] is not a unique factorization domain.\n\n17. Prove or disprove: Every subdomain of a ufd is also a ufd.\n\n18. An ideal of a commutative ring R is said to be finitely generated if there exist\nelements a1, . . . , an in R such that every element r ∈ R can be written as a1r1 + · · ·+ anrn\nfor some r1, . . . , rn in R. Prove that R satisfies the ascending chain condition if and only if\nevery ideal of R is finitely generated.\n\n\n\n316 CHAPTER 18. INTEGRAL DOMAINS\n\n19. Let D be an integral domain with a descending chain of ideals I1 ⊃ I2 ⊃ I3 ⊃ · · ·.\nSuppose that there exists an N such that Ik = IN for all k ≥ N . A ring satisfying this\ncondition is said to satisfy the descending chain condition, or DCC. Rings satisfying the\nDCC are called Artinian rings, after Emil Artin. Show that if D satisfies the descending\nchain condition, it must satisfy the ascending chain condition.\n\n20. Let R be a commutative ring with identity. We define a multiplicative subset of R\nto be a subset S such that 1 ∈ S and ab ∈ S if a, b ∈ S.\n(a) Define a relation ∼ on R × S by (a, s) ∼ (a′, s′) if there exists an s∗ ∈ S such that\n\ns∗(s′a− sa′) = 0. Show that ∼ is an equivalence relation on R× S.\n(b) Let a/s denote the equivalence class of (a, s) ∈ R × S and let S−1R be the set of all\n\nequivalence classes with respect to ∼. Define the operations of addition and multipli-\ncation on S−1R by\n\na\n\ns\n+\nb\n\nt\n=\nat+ bs\n\nst\na\n\ns\n\nb\n\nt\n=\nab\n\nst\n,\n\nrespectively. Prove that these operations are well-defined on S−1R and that S−1R\nis a ring with identity under these operations. The ring S−1R is called the ring of\nquotients of R with respect to S.\n\n(c) Show that the map ψ : R→ S−1R defined by ψ(a) = a/1 is a ring homomorphism.\n(d) If R has no zero divisors and 0 /∈ S, show that ψ is one-to-one.\n(e) Prove that P is a prime ideal of R if and only if S = R \\ P is a multiplicative subset\n\nof R.\n(f) If P is a prime ideal of R and S = R \\ P , show that the ring of quotients S−1R has\n\na unique maximal ideal. Any ring that has a unique maximal ideal is called a local\nring.\n\n18.4 References and Suggested Readings\n[1] Atiyah, M. F. and MacDonald, I. G. Introduction to Commutative Algebra. Westview\n\nPress, Boulder, CO, 1994.\n[2] Zariski, O. and Samuel, P. Commutative Algebra, vols. I and II. Springer, New York,\n\n1975, 1960.\n\n18.5 Sage\nWe have already seen some integral domains and unique factorizations in the previous two\nchapters. In addition to what we have already seen, Sage has support for some of the topics\nfrom this section, but the coverage is limited. Some functions will work for some rings and\nnot others, while some functions are not yet part of Sage. So we will give some examples,\nbut this is far from comprehensive.\n\nField of Fractions\nSage is frequently able to construct a field of fractions, or identify a certain field as the field\nof fractions. For example, the ring of integers and the field of rational numbers are both\nimplemented in Sage, and the integers “know” that the rationals is it’s field of fractions.\n\n\n\n18.5. SAGE 317\n\nQ = ZZ.fraction_field (); Q\n\nRational Field\n\nQ == QQ\n\nTrue\n\nIn other cases Sage will construct a fraction field, in the spirit of Lemma 18.3. So it is\nthen possible to do basic calculations in the constructed field.\n\nR.<x> = ZZ[]\nP = R.fraction_field ();P\n\nFraction Field of Univariate Polynomial Ring in x over Integer Ring\n\nf = P((x^2+3) /(7*x+4))\ng = P((4*x^2) /(3*x^2-5*x+4))\nh = P((-2*x^3+4*x^2+3)/(x^2+1))\n((f+g)/h).numerator ()\n\n3*x^6 + 23*x^5 + 32*x^4 + 8*x^3 + 41*x^2 - 15*x + 12\n\n((f+g)/h).denominator ()\n\n-42*x^6 + 130*x^5 - 108*x^4 + 63*x^3 - 5*x^2 + 24*x + 48\n\nPrime Subfields\nCorollary 18.7 says every field of characteristic p has a subfield isomorphic to Zp. For a\nfinite field, the exact nature of this subfield is not a surprise, but Sage will allow us to\nextract it easily.\n\nF.<c> = FiniteField (3^5)\nF.characteristic ()\n\n3\n\nG = F.prime_subfield (); G\n\nFinite Field of size 3\n\nG.list()\n\n[0, 1, 2]\n\nMore generally, the fields mentioned in the conclusions of Corollary 18.6 and Corol-\nlary 18.7 are known as the “prime subfield” of the ring containing them. Here is an example\nof the characteristic zero case.\n\nK.<y>= QuadraticField (-7); K\n\nNumber Field in y with defining polynomial x^2 + 7\n\n\n\n318 CHAPTER 18. INTEGRAL DOMAINS\n\nK.prime_subfield ()\n\nRational Field\n\nIn a rough sense, every characteristic zero field contains a copy of the rational numbers\n(the fraction field of the integers), which can explain Sage’s extensive support for rings and\nfields that extend the integers and the rationals.\n\nIntegral Domains\n\nSage can determine if some rings are integral domains and we can test products in them.\nHowever, notions of units, irreducibles or prime elements are not generally supported (out-\nside of what we have seen for polynomials in the previous chapter). Worse, the construction\nbelow creates a ring within a larger field and so some functions (such as .is_unit()) pass\nthrough and give misleading results. This is because the construction below creates a ring\nknown as an “order in a number field.”\n\nK.<x> = ZZ[sqrt(-3)]; K\n\nOrder in Number Field in a with defining polynomial x^2 + 3\n\nK.is_integral_domain ()\n\nTrue\n\nK.basis()\n\n[1, a]\n\nx\n\na\n\n(1+x)*(1-x) == 2*2\n\nTrue\n\nThe following is a bit misleading, since 4, as an element of Z[\n√\n3i] does not have a\n\nmultiplicative inverse, though seemingly we can compute one.\n\nfour = K(4)\nfour.is_unit ()\n\nFalse\n\nfour^-1\n\n1/4\n\n\n\n18.6. SAGE EXERCISES 319\n\nPrincipal Ideals\nWhen a ring is a principal ideal domain, such as the integers, or polynomials over a field,\nSage works well. Beyond that, support begins to weaken.\n\nT.<x>=ZZ[]\nT.is_integral_domain ()\n\nTrue\n\nJ = T.ideal(5, x); J\n\nIdeal (5, x) of Univariate Polynomial Ring in x over Integer Ring\n\nQ = T.quotient(J); Q\n\nQuotient of Univariate Polynomial Ring in x over\nInteger Ring by the ideal (5, x)\n\nJ.is_principal ()\n\nTraceback (most recent call last):\n...\nNotImplementedError\n\nQ.is_field ()\n\nTraceback (most recent call last):\n...\nNotImplementedError\n\n18.6 Sage Exercises\nThere are no Sage exercises for this section.\n\n\n\n19\n\nLattices and Boolean Algebras\n\nThe axioms of a ring give structure to the operations of addition and multiplication on a set.\nHowever, we can construct algebraic structures, known as lattices and Boolean algebras,\nthat generalize other types of operations. For example, the important operations on sets\nare inclusion, union, and intersection. Lattices are generalizations of order relations on\nalgebraic spaces, such as set inclusion in set theory and inequality in the familiar number\nsystems N, Z, Q, and R. Boolean algebras generalize the operations of intersection and\nunion. Lattices and Boolean algebras have found applications in logic, circuit theory, and\nprobability.\n\n19.1 Lattices\nPartially Ordered Sets\nWe begin by the study of lattices and Boolean algebras by generalizing the idea of inequality.\nRecall that a relation on a set X is a subset of X × X. A relation P on X is called a\npartial order of X if it satisfies the following axioms.\n\n1. The relation is reflexive: (a, a) ∈ P for all a ∈ X.\n\n2. The relation is antisymmetric: if (a, b) ∈ P and (b, a) ∈ P , then a = b.\n\n3. The relation is transitive: if (a, b) ∈ P and (b, c) ∈ P , then (a, c) ∈ P .\n\nWe will usually write a ⪯ b to mean (a, b) ∈ P unless some symbol is naturally associated\nwith a particular partial order, such as a ≤ b with integers a and b, or A ⊆ B with sets\nA and B. A set X together with a partial order ⪯ is called a partially ordered set, or\nposet.\n\nExample 19.1. The set of integers (or rationals or reals) is a poset where a ≤ b has the\nusual meaning for two integers a and b in Z.\n\nExample 19.2. Let X be any set. We will define the power set of X to be the set of\nall subsets of X. We denote the power set of X by P(X). For example, let X = {a, b, c}.\nThen P(X) is the set of all subsets of the set {a, b, c}:\n\n∅ {a} {b} {c}\n{a, b} {a, c} {b, c} {a, b, c}.\n\nOn any power set of a set X, set inclusion, ⊆, is a partial order. We can represent the order\non {a, b, c} schematically by a diagram such as the one in Figure 19.3.\n\n320\n\n\n\n19.1. LATTICES 321\n\n{a, b, c}\n\n{a, b} {a, c} {b, c}\n\n{a} {b} {c}\n\n∅\n\nFigure 19.3: Partial order on P({a, b, c})\n\nExample 19.4. Let G be a group. The set of subgroups of G is a poset, where the partial\norder is set inclusion.\n\nExample 19.5. There can be more than one partial order on a particular set. We can\nform a partial order on N by a ⪯ b if a | b. The relation is certainly reflexive since a | a for\nall a ∈ N. If m | n and n | m, then m = n; hence, the relation is also antisymmetric. The\nrelation is transitive, because if m | n and n | p, then m | p.\n\nExample 19.6. Let X = {1, 2, 3, 4, 6, 8, 12, 24} be the set of divisors of 24 with the partial\norder defined in Example 19.5. Figure 19.7 shows the partial order on X.\n\n24\n\n12\n\n6\n\n3\n\n8\n\n4\n\n2\n\n1\n\nFigure 19.7: A partial order on the divisors of 24\n\nLet Y be a subset of a poset X. An element u in X is an upper bound of Y if a ⪯ u\nfor every element a ∈ Y . If u is an upper bound of Y such that u ⪯ v for every other upper\nbound v of Y , then u is called a least upper bound or supremum of Y . An element l in\nX is said to be a lower bound of Y if l ⪯ a for all a ∈ Y . If l is a lower bound of Y such\nthat k ⪯ l for every other lower bound k of Y , then l is called a greatest lower bound or\ninfimum of Y .\n\nExample 19.8. Let Y = {2, 3, 4, 6} be contained in the set X of Example 19.6. Then Y\nhas upper bounds 12 and 24, with 12 as a least upper bound. The only lower bound is 1;\nhence, it must be a greatest lower bound.\n\nAs it turns out, least upper bounds and greatest lower bounds are unique if they exist.\n\n\n\n322 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\nTheorem 19.9. Let Y be a nonempty subset of a poset X. If Y has a least upper bound,\nthen Y has a unique least upper bound. If Y has a greatest lower bound, then Y has a\nunique greatest lower bound.\n\nProof. Let u1 and u2 be least upper bounds for Y . By the definition of the least upper\nbound, u1 ⪯ u for all upper bounds u of Y . In particular, u1 ⪯ u2. Similarly, u2 ⪯ u1.\nTherefore, u1 = u2 by antisymmetry. A similar argument show that the greatest lower\nbound is unique.\n\nOn many posets it is possible to define binary operations by using the greatest lower\nbound and the least upper bound of two elements. A lattice is a poset L such that every\npair of elements in L has a least upper bound and a greatest lower bound. The least upper\nbound of a, b ∈ L is called the join of a and b and is denoted by a ∨ b. The greatest lower\nbound of a, b ∈ L is called the meet of a and b and is denoted by a ∧ b.\n\nExample 19.10. Let X be a set. Then the power set of X, P(X), is a lattice. For two sets\nA and B in P(X), the least upper bound of A and B is A∪B. Certainly A∪B is an upper\nbound of A and B, since A ⊆ A∪B and B ⊆ A∪B. If C is some other set containing both\nA and B, then C must contain A ∪B; hence, A ∪B is the least upper bound of A and B.\nSimilarly, the greatest lower bound of A and B is A ∩B.\n\nExample 19.11. Let G be a group and suppose that X is the set of subgroups of G. Then\nX is a poset ordered by set-theoretic inclusion, ⊆. The set of subgroups of G is also a\nlattice. If H and K are subgroups of G, the greatest lower bound of H and K is H ∩K.\nThe set H ∪K may not be a subgroup of G. We leave it as an exercise to show that the\nleast upper bound of H and K is the subgroup generated by H ∪K.\n\nIn set theory we have certain duality conditions. For example, by De Morgan’s laws,\nany statement about sets that is true about (A ∪B)′ must also be true about A′ ∩B′. We\nalso have a duality principle for lattices.\n\nAxiom 19.12 (Principle of Duality). Any statement that is true for all lattices remains\ntrue when ⪯ is replaced by ⪰ and ∨ and ∧ are interchanged throughout the statement.\n\nThe following theorem tells us that a lattice is an algebraic structure with two binary\noperations that satisfy certain axioms.\n\nTheorem 19.13. If L is a lattice, then the binary operations ∨ and ∧ satisfy the following\nproperties for a, b, c ∈ L.\n\n1. Commutative laws: a ∨ b = b ∨ a and a ∧ b = b ∧ a.\n\n2. Associative laws: a ∨ (b ∨ c) = (a ∨ b) ∨ c and a ∧ (b ∧ c) = (a ∧ b) ∧ c.\n\n3. Idempotent laws: a ∨ a = a and a ∧ a = a.\n\n4. Absorption laws: a ∨ (a ∧ b) = a and a ∧ (a ∨ b) = a.\n\nProof. By the Principle of Duality, we need only prove the first statement in each part.\n(1) By definition a ∨ b is the least upper bound of {a, b}, and b ∨ a is the least upper\n\nbound of {b, a}; however, {a, b} = {b, a}.\n(2) We will show that a∨ (b∨ c) and (a∨ b)∨ c are both least upper bounds of {a, b, c}.\n\nLet d = a ∨ b. Then c ⪯ d ∨ c = (a ∨ b) ∨ c. We also know that\n\na ⪯ a ∨ b = d ⪯ d ∨ c = (a ∨ b) ∨ c.\n\n\n\n19.2. BOOLEAN ALGEBRAS 323\n\nA similar argument demonstrates that b ⪯ (a ∨ b) ∨ c. Therefore, (a ∨ b) ∨ c is an upper\nbound of {a, b, c}. We now need to show that (a∨ b)∨ c is the least upper bound of {a, b, c}.\nLet u be some other upper bound of {a, b, c}. Then a ⪯ u and b ⪯ u; hence, d = a ∨ b ⪯ u.\nSince c ⪯ u, it follows that (a ∨ b) ∨ c = d ∨ c ⪯ u. Therefore, (a ∨ b) ∨ c must be the least\nupper bound of {a, b, c}. The argument that shows a ∨ (b ∨ c) is the least upper bound of\n{a, b, c} is the same. Consequently, a ∨ (b ∨ c) = (a ∨ b) ∨ c.\n\n(3) The join of a and a is the least upper bound of {a}; hence, a ∨ a = a.\n(4) Let d = a ∧ b. Then a ⪯ a ∨ d. On the other hand, d = a ∧ b ⪯ a, and so a ∨ d ⪯ a.\n\nTherefore, a ∨ (a ∧ b) = a.\n\nGiven any arbitrary set L with operations ∨ and ∧, satisfying the conditions of the\nprevious theorem, it is natural to ask whether or not this set comes from some lattice. The\nfollowing theorem says that this is always the case.\n\nTheorem 19.14. Let L be a nonempty set with two binary operations ∨ and ∧ satisfying\nthe commutative, associative, idempotent, and absorption laws. We can define a partial\norder on L by a ⪯ b if a ∨ b = b. Furthermore, L is a lattice with respect to ⪯ if for all\na, b ∈ L, we define the least upper bound and greatest lower bound of a and b by a ∨ b and\na ∧ b, respectively.\n\nProof. We first show that L is a poset under ⪯. Since a∨ a = a, a ⪯ a and ⪯ is reflexive.\nTo show that ⪯ is antisymmetric, let a ⪯ b and b ⪯ a. Then a∨ b = b and b∨ a = a.By the\ncommutative law, b = a ∨ b = b ∨ a = a. Finally, we must show that ⪯ is transitive. Let\na ⪯ b and b ⪯ c. Then a ∨ b = b and b ∨ c = c. Thus,\n\na ∨ c = a ∨ (b ∨ c) = (a ∨ b) ∨ c = b ∨ c = c,\n\nor a ⪯ c.\nTo show that L is a lattice, we must prove that a∨ b and a∧ b are, respectively, the least\n\nupper and greatest lower bounds of a and b. Since a = (a ∨ b) ∧ a = a ∧ (a ∨ b), it follows\nthat a ⪯ a ∨ b. Similarly, b ⪯ a ∨ b. Therefore, a ∨ b is an upper bound for a and b. Let u\nbe any other upper bound of both a and b. Then a ⪯ u and b ⪯ u. But a ∨ b ⪯ u since\n\n(a ∨ b) ∨ u = a ∨ (b ∨ u) = a ∨ u = u.\n\nThe proof that a ∧ b is the greatest lower bound of a and b is left as an exercise.\n\n19.2 Boolean Algebras\nLet us investigate the example of the power set, P(X), of a set X more closely. The power\nset is a lattice that is ordered by inclusion. By the definition of the power set, the largest\nelement in P(X) is X itself and the smallest element is ∅, the empty set. For any set A\nin P(X), we know that A ∩X = A and A ∪ ∅ = A. This suggests the following definition\nfor lattices. An element I in a poset X is a largest element if a ⪯ I for all a ∈ X. An\nelement O is a smallest element of X if O ⪯ a for all a ∈ X.\n\nLet A be in P(X). Recall that the complement of A is\n\nA′ = X \\A = {x : x ∈ X and x /∈ A}.\n\nWe know that A ∪ A′ = X and A ∩ A′ = ∅. We can generalize this example for lattices. A\nlattice L with a largest element I and a smallest element O is complemented if for each\na ∈ X, there exists an a′ such that a ∨ a′ = I and a ∧ a′ = O.\n\n\n\n324 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\nIn a lattice L, the binary operations ∨ and ∧ satisfy commutative and associative laws;\nhowever, they need not satisfy the distributive law\n\na ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c);\n\nhowever, in P(X) the distributive law is satisfied since\n\nA ∩ (B ∪ C) = (A ∩B) ∪ (A ∩ C)\n\nfor A,B,C ∈ P(X). We will say that a lattice L is distributive if the following distributive\nlaw holds:\n\na ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c)\n\nfor all a, b, c ∈ L.\n\nTheorem 19.15. A lattice L is distributive if and only if\n\na ∨ (b ∧ c) = (a ∨ b) ∧ (a ∨ c)\n\nfor all a, b, c ∈ L.\n\nProof. Let us assume that L is a distributive lattice.\n\na ∨ (b ∧ c) = [a ∨ (a ∧ c)] ∨ (b ∧ c)\n= a ∨ [(a ∧ c) ∨ (b ∧ c)]\n= a ∨ [(c ∧ a) ∨ (c ∧ b)]\n= a ∨ [c ∧ (a ∨ b)]\n= a ∨ [(a ∨ b) ∧ c]\n= [(a ∨ b) ∧ a] ∨ [(a ∨ b) ∧ c]\n= (a ∨ b) ∧ (a ∨ c).\n\nThe converse follows directly from the Duality Principle.\n\nA Boolean algebra is a lattice B with a greatest element I and a smallest element\nO such that B is both distributive and complemented. The power set of X, P(X), is our\nprototype for a Boolean algebra. As it turns out, it is also one of the most important\nBoolean algebras. The following theorem allows us to characterize Boolean algebras in\nterms of the binary relations ∨ and ∧ without mention of the fact that a Boolean algebra\nis a poset.\n\nTheorem 19.16. A set B is a Boolean algebra if and only if there exist binary operations\n∨ and ∧ on B satisfying the following axioms.\n\n1. a ∨ b = b ∨ a and a ∧ b = b ∧ a for a, b ∈ B.\n\n2. a ∨ (b ∨ c) = (a ∨ b) ∨ c and a ∧ (b ∧ c) = (a ∧ b) ∧ c for a, b, c ∈ B.\n\n3. a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c) and a ∨ (b ∧ c) = (a ∨ b) ∧ (a ∨ c) for a, b, c ∈ B.\n\n4. There exist elements I and O such that a ∨O = a and a ∧ I = a for all a ∈ B.\n\n5. For every a ∈ B there exists an a′ ∈ B such that a ∨ a′ = I and a ∧ a′ = O.\n\n\n\n19.2. BOOLEAN ALGEBRAS 325\n\nProof. Let B be a set satisfying (1)–(5) in the theorem. One of the idempotent laws is\nsatisfied since\n\na = a ∨O\n= a ∨ (a ∧ a′)\n= (a ∨ a) ∧ (a ∨ a′)\n= (a ∨ a) ∧ I\n= a ∨ a.\n\nObserve that\nI ∨ b = (I ∨ b) ∧ I = (I ∧ I) ∨ (b ∧ I) = I ∨ I = I.\n\nConsequently, the first of the two absorption laws holds, since\n\na ∨ (a ∧ b) = (a ∧ I) ∨ (a ∧ b)\n= a ∧ (I ∨ b)\n= a ∧ I\n= a.\n\nThe other idempotent and absorption laws are proven similarly. Since B also satisfies (1)–\n(3), the conditions of Theorem 19.14 are met; therefore, B must be a lattice. Condition (4)\ntells us that B is a distributive lattice.\n\nFor a ∈ B, O ∨ a = a; hence, O ⪯ a and O is the smallest element in B. To show that\nI is the largest element in B, we will first show that a ∨ b = b is equivalent to a ∧ b = a.\nSince a ∨ I = a for all a ∈ B, using the absorption laws we can determine that\n\na ∨ I = (a ∧ I) ∨ I = I ∨ (I ∧ a) = I\n\nor a ⪯ I for all a in B. Finally, since we know that B is complemented by (5), B must be\na Boolean algebra.\n\nConversely, suppose that B is a Boolean algebra. Let I and O be the greatest and least\nelements in B, respectively. If we define a ∨ b and a ∧ b as least upper and greatest lower\nbounds of {a, b}, then B is a Boolean algebra by Theorem 19.14, Theorem 19.15, and our\nhypothesis.\n\nMany other identities hold in Boolean algebras. Some of these identities are listed in\nthe following theorem.\n\nTheorem 19.17. Let B be a Boolean algebra. Then\n\n1. a ∨ I = I and a ∧O = O for all a ∈ B.\n\n2. If a ∨ b = a ∨ c and a ∧ b = a ∧ c for a, b, c ∈ B, then b = c.\n\n3. If a ∨ b = I and a ∧ b = O, then b = a′.\n\n4. (a′)′ = a for all a ∈ B.\n\n5. I ′ = O and O′ = I.\n\n6. (a ∨ b)′ = a′ ∧ b′ and (a ∧ b)′ = a′ ∨ b′ (De Morgan’s Laws).\n\n\n\n326 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\nProof. We will prove only (2). The rest of the identities are left as exercises. For a ∨ b =\na ∨ c and a ∧ b = a ∧ c, we have\n\nb = b ∨ (b ∧ a)\n= b ∨ (a ∧ b)\n= b ∨ (a ∧ c)\n= (b ∨ a) ∧ (b ∨ c)\n= (a ∨ b) ∧ (b ∨ c)\n= (a ∨ c) ∧ (b ∨ c)\n= (c ∨ a) ∧ (c ∨ b)\n= c ∨ (a ∧ b)\n= c ∨ (a ∧ c)\n= c ∨ (c ∧ a)\n= c.\n\nFinite Boolean Algebras\nA Boolean algebra is a finite Boolean algebra if it contains a finite number of elements\nas a set. Finite Boolean algebras are particularly nice since we can classify them up to\nisomorphism.\n\nLet B and C be Boolean algebras. A bijective map ϕ : B → C is an isomorphism of\nBoolean algebras if\n\nϕ(a ∨ b) = ϕ(a) ∨ ϕ(b)\nϕ(a ∧ b) = ϕ(a) ∧ ϕ(b)\n\nfor all a and b in B.\nWe will show that any finite Boolean algebra is isomorphic to the Boolean algebra\n\nobtained by taking the power set of some finite set X. We will need a few lemmas and\ndefinitions before we prove this result. Let B be a finite Boolean algebra. An element\na ∈ B is an atom of B if a ̸= O and a ∧ b = a for all nonzero b ∈ B. Equivalently, a is an\natom of B if there is no nonzero b ∈ B distinct from a such that O ⪯ b ⪯ a.\n\nLemma 19.18. Let B be a finite Boolean algebra. If b is a nonzero element of B, then\nthere is an atom a in B such that a ⪯ b.\n\nProof. If b is an atom, let a = b. Otherwise, choose an element b1, not equal to O or b,\nsuch that b1 ⪯ b. We are guaranteed that this is possible since b is not an atom. If b1 is an\natom, then we are done. If not, choose b2, not equal to O or b1, such that b2 ⪯ b1. Again,\nif b2 is an atom, let a = b2. Continuing this process, we can obtain a chain\n\nO ⪯ · · · ⪯ b3 ⪯ b2 ⪯ b1 ⪯ b.\n\nSince B is a finite Boolean algebra, this chain must be finite. That is, for some k, bk is an\natom. Let a = bk.\n\nLemma 19.19. Let a and b be atoms in a finite Boolean algebra B such that a ̸= b. Then\na ∧ b = O.\n\n\n\n19.2. BOOLEAN ALGEBRAS 327\n\nProof. Since a∧ b is the greatest lower bound of a and b, we know that a∧ b ⪯ a. Hence,\neither a ∧ b = a or a ∧ b = O. However, if a ∧ b = a, then either a ⪯ b or a = O. In either\ncase we have a contradiction because a and b are both atoms; therefore, a ∧ b = O.\n\nLemma 19.20. Let B be a Boolean algebra and a, b ∈ B. The following statements are\nequivalent.\n\n1. a ⪯ b.\n\n2. a ∧ b′ = O.\n\n3. a′ ∨ b = I.\n\nProof. (1) ⇒ (2). If a ⪯ b, then a ∨ b = b. Therefore,\n\na ∧ b′ = a ∧ (a ∨ b)′\n\n= a ∧ (a′ ∧ b′)\n= (a ∧ a′) ∧ b′\n\n= O ∧ b′\n\n= O.\n\n(2) ⇒ (3). If a ∧ b′ = O, then a′ ∨ b = (a ∧ b′)′ = O′ = I.\n(3) ⇒ (1). If a′ ∨ b = I, then\n\na = a ∧ (a′ ∨ b)\n= (a ∧ a′) ∨ (a ∧ b)\n= O ∨ (a ∧ b)\n= a ∧ b.\n\nThus, a ⪯ b.\n\nLemma 19.21. Let B be a Boolean algebra and b and c be elements in B such that b ̸⪯ c.\nThen there exists an atom a ∈ B such that a ⪯ b and a ̸⪯ c.\n\nProof. By Lemma 19.20, b ∧ c′ ̸= O. Hence, there exists an atom a such that a ⪯ b ∧ c′.\nConsequently, a ⪯ b and a ̸⪯ c.\n\nLemma 19.22. Let b ∈ B and a1, . . . , an be the atoms of B such that ai ⪯ b. Then\nb = a1 ∨ · · · ∨ an. Furthermore, if a, a1, . . . , an are atoms of B such that a ⪯ b, ai ⪯ b, and\nb = a ∨ a1 ∨ · · · ∨ an, then a = ai for some i = 1, . . . , n.\n\nProof. Let b1 = a1∨ · · ·∨an. Since ai ⪯ b for each i, we know that b1 ⪯ b. If we can show\nthat b ⪯ b1, then the lemma is true by antisymmetry. Assume b ̸⪯ b1. Then there exists\nan atom a such that a ⪯ b and a ̸⪯ b1. Since a is an atom and a ⪯ b, we can deduce that\na = ai for some ai. However, this is impossible since a ⪯ b1. Therefore, b ⪯ b1.\n\nNow suppose that b = a1 ∨ · · · ∨ an. If a is an atom less than b,\n\na = a ∧ b = a ∧ (a1 ∨ · · · ∨ an) = (a ∧ a1) ∨ · · · ∨ (a ∧ an).\n\nBut each term is O or a with a ∧ ai occurring for only one ai. Hence, by Lemma 19.19,\na = ai for some i.\n\nTheorem 19.23. Let B be a finite Boolean algebra. Then there exists a set X such that\nB is isomorphic to P(X).\n\n\n\n328 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\nProof. We will show that B is isomorphic to P(X), where X is the set of atoms of B. Let\na ∈ B. By Lemma 19.22, we can write a uniquely as a = a1 ∨ · · · ∨ an for a1, . . . , an ∈ X.\nConsequently, we can define a map ϕ : B → P(X) by\n\nϕ(a) = ϕ(a1 ∨ · · · ∨ an) = {a1, . . . , an}.\n\nClearly, ϕ is onto.\nNow let a = a1 ∨ · · · ∨an and b = b1 ∨ · · · ∨ bm be elements in B, where each ai and each\n\nbi is an atom. If ϕ(a) = ϕ(b), then {a1, . . . , an} = {b1, . . . , bm} and a = b. Consequently, ϕ\nis injective.\n\nThe join of a and b is preserved by ϕ since\n\nϕ(a ∨ b) = ϕ(a1 ∨ · · · ∨ an ∨ b1 ∨ · · · ∨ bm)\n\n= {a1, . . . , an, b1, . . . , bm}\n= {a1, . . . , an} ∪ {b1, . . . , bm}\n= ϕ(a1 ∨ · · · ∨ an) ∪ ϕ(b1 ∧ · · · ∨ bm)\n\n= ϕ(a) ∪ ϕ(b).\n\nSimilarly, ϕ(a ∧ b) = ϕ(a) ∩ ϕ(b).\n\nWe leave the proof of the following corollary as an exercise.\n\nCorollary 19.24. The order of any finite Boolean algebra must be 2n for some positive\ninteger n.\n\n19.3 The Algebra of Electrical Circuits\nThe usefulness of Boolean algebras has become increasingly apparent over the past several\ndecades with the development of the modern computer. The circuit design of computer\nchips can be expressed in terms of Boolean algebras. In this section we will develop the\nBoolean algebra of electrical circuits and switches; however, these results can easily be\ngeneralized to the design of integrated computer circuitry.\n\nA switch is a device, located at some point in an electrical circuit, that controls the\nflow of current through the circuit. Each switch has two possible states: it can be open,\nand not allow the passage of current through the circuit, or a it can be closed, and allow\nthe passage of current. These states are mutually exclusive. We require that every switch\nbe in one state or the other—a switch cannot be open and closed at the same time. Also, if\none switch is always in the same state as another, we will denote both by the same letter;\nthat is, two switches that are both labeled with the same letter a will always be open at\nthe same time and closed at the same time.\n\nGiven two switches, we can construct two fundamental types of circuits. Two switches a\nand b are in series if they make up a circuit of the type that is illustrated in Figure 19.25.\nCurrent can pass between the terminals A and B in a series circuit only if both of the\nswitches a and b are closed. We will denote this combination of switches by a ∧ b. Two\nswitches a and b are in parallel if they form a circuit of the type that appears in Fig-\nure 19.26. In the case of a parallel circuit, current can pass between A and B if either one\nof the switches is closed. We denote a parallel combination of circuits a and b by a ∨ b.\n\nA a b B\n\nFigure 19.25: a ∧ b\n\n\n\n19.3. THE ALGEBRA OF ELECTRICAL CIRCUITS 329\n\nA\n\na\n\nb\n\nB\n\nFigure 19.26: a ∨ b\n\nWe can build more complicated electrical circuits out of series and parallel circuits by\nreplacing any switch in the circuit with one of these two fundamental types of circuits.\nCircuits constructed in this manner are called series-parallel circuits.\n\nWe will consider two circuits equivalent if they act the same. That is, if we set the\nswitches in equivalent circuits exactly the same we will obtain the same result. For example,\nin a series circuit a∧b is exactly the same as b∧a. Notice that this is exactly the commutative\nlaw for Boolean algebras. In fact, the set of all series-parallel circuits forms a Boolean algebra\nunder the operations of ∨ and ∧. We can use diagrams to verify the different axioms of\na Boolean algebra. The distributive law, a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c), is illustrated in\nFigure 19.27. If a is a switch, then a′ is the switch that is always open when a is closed and\nalways closed when a is open. A circuit that is always closed is I in our algebra; a circuit\nthat is always open is O. The laws for a∧ a′ = O and a∨ a′ = I are shown in Figure 19.28.\n\na\n\nb\n\nc\n\na b\n\na c\n\nFigure 19.27: a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c)\n\na a′\n\na\n\na′\n\nFigure 19.28: a ∧ a′ = O and a ∨ a′ = I\n\nExample 19.29. Every Boolean expression represents a switching circuit. For example,\ngiven the expression (a∨ b)∧ (a∨ b′)∧ (a∨ b), we can construct the circuit in Figure 19.32.\n\nTheorem 19.30. The set of all circuits is a Boolean algebra.\n\nWe leave as an exercise the proof of this theorem for the Boolean algebra axioms not\nyet verified. We can now apply the techniques of Boolean algebras to switching theory.\n\nExample 19.31. Given a complex circuit, we can now apply the techniques of Boolean\n\n\n\n330 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\nalgebra to reduce it to a simpler one. Consider the circuit in Figure 19.32. Since\n\n(a ∨ b) ∧ (a ∨ b′) ∧ (a ∨ b) = (a ∨ b) ∧ (a ∨ b) ∧ (a ∨ b′)\n= (a ∨ b) ∧ (a ∨ b′)\n= a ∨ (b ∧ b′)\n= a ∨O\n= a,\n\nwe can replace the more complicated circuit with a circuit containing the single switch a\nand achieve the same function.\n\na\n\nb\n\na\n\nb′\n\na\n\nb\n\nFigure 19.32: (a ∨ b) ∧ (a ∨ b′) ∧ (a ∨ b)\n\nHistorical Note\n\nGeorge Boole (1815–1864) was the first person to study lattices. In 1847, he published\nThe Investigation of the Laws of Thought, a book in which he used lattices to formalize logic\nand the calculus of propositions. Boole believed that mathematics was the study of form\nrather than of content; that is, he was not so much concerned with what he was calculating\nas with how he was calculating it. Boole’s work was carried on by his friend Augustus De\nMorgan (1806–1871). De Morgan observed that the principle of duality often held in set\ntheory, as is illustrated by De Morgan’s laws for set theory. He believed, as did Boole, that\nmathematics was the study of symbols and abstract operations.\n\nSet theory and logic were further advanced by such mathematicians as Alfred North\nWhitehead (1861–1947), Bertrand Russell (1872–1970), and David Hilbert (1862–1943). In\nPrincipia Mathematica, Whitehead and Russell attempted to show the connection between\nmathematics and logic by the deduction of the natural number system from the rules of\nformal logic. If the natural numbers could be determined from logic itself, then so could\nmuch of the rest of existing mathematics. Hilbert attempted to build up mathematics\nby using symbolic logic in a way that would prove the consistency of mathematics. His\napproach was dealt a mortal blow by Kurt Gödel (1906–1978), who proved that there will\nalways be “undecidable” problems in any sufficiently rich axiomatic system; that is, that\nin any mathematical system of any consequence, there will always be statements that can\nnever be proven either true or false.\n\nAs often occurs, this basic research in pure mathematics later became indispensable in a\nwide variety of applications. Boolean algebras and logic have become essential in the design\nof the large-scale integrated circuitry found on today’s computer chips. Sociologists have\nused lattices and Boolean algebras to model social hierarchies; biologists have used them to\ndescribe biosystems.\n\n\n\n19.4. EXERCISES 331\n\n19.4 Exercises\n1. Draw the lattice diagram for the power set of X = {a, b, c, d} with the set inclusion\nrelation, ⊆.\n\n2. Draw the diagram for the set of positive integers that are divisors of 30. Is this poset a\nBoolean algebra?\n\n3. Draw a diagram of the lattice of subgroups of Z12.\n\n4. Let B be the set of positive integers that are divisors of 36. Define an order on B by\na ⪯ b if a | b. Prove that B is a Boolean algebra. Find a set X such that B is isomorphic\nto P(X).\n\n5. Prove or disprove: Z is a poset under the relation a ⪯ b if a | b.\n\n6. Draw the switching circuit for each of the following Boolean expressions.\n\n(a) (a ∨ b ∨ a′) ∧ a\n(b) (a ∨ b)′ ∧ (a ∨ b)\n\n(c) a ∨ (a ∧ b)\n(d) (c ∨ a ∨ b) ∧ c′ ∧ (a ∨ b)′\n\n7. Draw a circuit that will be closed exactly when only one of three switches a, b, and c\nare closed.\n\n8. Prove or disprove that the two circuits shown are equivalent.\n\na b c\n\na′ b\n\na c′\n\na\n\na\n\nb\n\nc′\n\n9. Let X be a finite set containing n elements. Prove that P(X) = 2n. Conclude that the\norder of any finite Boolean algebra must be 2n for some n ∈ N.\n\n10. For each of the following circuits, write a Boolean expression. If the circuit can be\nreplaced by one with fewer switches, give the Boolean expression and draw a diagram for\nthe new circuit.\n\na b c\n\na′ b′ c\n\na b′ c′\n\na a b\n\na′\n\nb a′ b\n\na′\n\na b′\n\nb\n\n\n\n332 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\n11. Prove or disprove: The set of all nonzero integers is a lattice, where a ⪯ b is defined by\na | b.\n\n12. Let L be a nonempty set with two binary operations ∨ and ∧ satisfying the commuta-\ntive, associative, idempotent, and absorption laws. We can define a partial order on L, as\nin Theorem 19.14, by a ⪯ b if a ∨ b = b. Prove that the greatest lower bound of a and b is\na ∧ b.\n\n13. Let G be a group and X be the set of subgroups of G ordered by set-theoretic inclusion.\nIf H and K are subgroups of G, show that the least upper bound of H and K is the subgroup\ngenerated by H ∪K.\n\n14. Let R be a ring and suppose that X is the set of ideals of R. Show that X is a poset\nordered by set-theoretic inclusion, ⊆. Define the meet of two ideals I and J in X by I ∩ J\nand the join of I and J by I + J . Prove that the set of ideals of R is a lattice under these\noperations.\n\n15. Let B be a Boolean algebra. Prove each of the following identities.\n(a) a ∨ I = I and a ∧O = O for all a ∈ B.\n(b) If a ∨ b = I and a ∧ b = O, then b = a′.\n(c) (a′)′ = a for all a ∈ B.\n(d) I ′ = O and O′ = I.\n(e) (a ∨ b)′ = a′ ∧ b′ and (a ∧ b)′ = a′ ∨ b′ (De Morgan’s laws).\n\n16. By drawing the appropriate diagrams, complete the proof of Theorem 19.30 to show\nthat the switching functions form a Boolean algebra.\n\n17. Let B be a Boolean algebra. Define binary operations + and · on B by\n\na+ b = (a ∧ b′) ∨ (a′ ∧ b)\na · b = a ∧ b.\n\nProve that B is a commutative ring under these operations satisfying a2 = a for all a ∈ B.\n\n18. Let X be a poset such that for every a and b in X, either a ⪯ b or b ⪯ a. Then X is\nsaid to be a totally ordered set.\n(a) Is a | b a total order on N?\n(b) Prove that N, Z, Q, and R are totally ordered sets under the usual ordering ≤.\n\n19. Let X and Y be posets. A map ϕ : X → Y is order-preserving if a ⪯ b implies that\nϕ(a) ⪯ ϕ(b). Let L and M be lattices. A map ψ : L→M is a lattice homomorphism if\nψ(a∨ b) = ψ(a)∨ψ(b) and ψ(a∧ b) = ψ(a)∧ψ(b). Show that every lattice homomorphism\nis order-preserving, but that it is not the case that every order-preserving homomorphism\nis a lattice homomorphism.\n\n20. Let B be a Boolean algebra. Prove that a = b if and only if (a ∧ b′) ∨ (a′ ∧ b) = O for\na, b ∈ B.\n\n21. Let B be a Boolean algebra. Prove that a = O if and only if (a ∧ b′) ∨ (a′ ∧ b) = b for\nall b ∈ B.\n\n22. Let L and M be lattices. Define an order relation on L×M by (a, b) ⪯ (c, d) if a ⪯ c\nand b ⪯ d. Show that L×M is a lattice under this partial order.\n\n\n\n19.5. PROGRAMMING EXERCISES 333\n\n19.5 Programming Exercises\n1. A Boolean or switching function on n variables is a map f : {O, I}n → {0, I}.\nA Boolean polynomial is a special type of Boolean function: it is any type of Boolean\nexpression formed from a finite combination of variables x1, . . . , xn together with O and I,\nusing the operations ∨, ∧, and ′. The values of the functions are defined in Table 19.33.\nWrite a program to evaluate Boolean polynomials.\n\nx y x′ x ∨ y x ∧ y\n0 0 1 0 0\n0 1 1 1 0\n1 0 0 1 0\n1 1 0 1 1\n\nTable 19.33: Boolean polynomials\n\n19.6 References and Suggested Readings\n[1] Donnellan, T. Lattice Theory. Pergamon Press, Oxford, 1968.\n[2] Halmos, P. R. “The Basic Concepts of Algebraic Logic,” American Mathematical\n\nMonthly 53(1956), 363–87.\n[3] Hohn, F. “Some Mathematical Aspects of Switching,” American Mathematical Monthly\n\n62(1955), 75–90.\n[4] Hohn, F. Applied Boolean Algebra. 2nd ed. Macmillan, New York, 1966.\n[5] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\n[6] Whitesitt, J. Boolean Algebra and Its Applications. Dover, Mineola, NY, 2010.\n\n19.7 Sage\nSage has support for both partially ordered sets (“posets”) and lattices, and does an excellent\njob of providing visual depictions of both.\n\nCreating Partially Ordered Sets\n\nExample 19.6 in the text is a good example to replicate as a demonstration of Sage com-\nmands. We first define the elements of the set X.\n\nX = (24).divisors ()\nX\n\n[1, 2, 3, 4, 6, 8, 12, 24]\n\nOne approach to creating the relation is to specify every instance where one element is\ncomparable to the another. So we build a list of pairs, where each pair contains comparable\nelements, with the lesser one first. This is the set of relations.\n\nR = [(a,b) for a in X for b in X if a.divides(b)]; R\n\n\n\n334 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\n[(1, 1), (1, 2), (1, 3), (1, 4), (1, 6), (1, 8), (1, 12), (1, 24),\n(2, 2), (2, 4), (2, 6), (2, 8), (2, 12), (2, 24), (3, 3), (3, 6),\n(3, 12), (3, 24), (4, 4), (4, 8), (4, 12), (4, 24), (6, 6),\n(6, 12), (6, 24), (8, 8), (8, 24), (12, 12), (12, 24), (24, 24)]\n\nWe construct the poset by giving the the Poset constructor a list containing the elements\nand the relations. We can then easily get a “plot” of the poset. Notice the plot just shows\nthe “cover relations” — a minimal set of comparisons which the assumption of transitivity\nwould expand into the set of all the relations.\n\nD = Poset([X, R])\nD.plot()\n\nAnother approach to creating a Poset is to let the poset constructor run over all the\npairs of elements, and all we do is give the constructor a way to test if two elements are\ncomparable. Our comparison function should expect two elements and then return True or\nFalse. A “lambda” function is one way to quickly build such a function. This may be a\nnew idea for you, but mastering lambda functions can be a great convenience. Notice that\n“lambda” is a word reserved for just this purpose (so, for example, lambda is a bad choice\nfor the name of an eigenvalue of a matrix). There are other ways to make functions in Sage,\nbut a lambda function is quickest when the function is simple.\n\ndivisible = lambda x, y: x.divides(y)\nL = Poset([X, divisible ])\nL == D\n\nTrue\n\nL.plot()\n\nSage also has a collection of stock posets. Some are one-shot constructions, while others\nare members of parameterized families. Use tab-completion on Posets. to see the full list.\nHere are some examples.\n\nA one-shot construction. Perhaps what you would expect, though there might be other,\nequally plausible, alternatives.\n\nQ = Posets.PentagonPoset ()\nQ.plot()\n\nA parameterized family. This is the classic example where the elements are subsets of a\nset with n elements and the relation is “subset of.”\n\nS = Posets.BooleanLattice (4)\nS.plot()\n\nAnd random posets. These can be useful for testing and experimenting, but are unlikely\nto exhibit special cases that may be important. You might run the following command many\ntimes and vary the second argument, which is a rough upper bound on the probability any\ntwo elements are comparable. Remember that the plot only shows the cover relations. The\nmore elements that are comparable, the more “vertically stretched” the plot will be.\n\nT = Posets.RandomPoset (20 ,0.05)\nT.plot()\n\n\n\n19.7. SAGE 335\n\nProperties of a Poset\nOnce you have a poset, what can you do with it? Let’s return to our first example, D.\nWe can of course determine if one element is less than another, which is the fundamental\nstructure of a poset.\n\nD.is_lequal(4, 8)\n\nTrue\n\nD.is_lequal(4, 4)\n\nTrue\n\nD.is_less_than (4, 8)\n\nTrue\n\nD.is_less_than (4, 4)\n\nFalse\n\nD.is_lequal(6, 8)\n\nFalse\n\nD.is_lequal(8, 6)\n\nFalse\n\nNotice that 6 and 8 are not comparable in this poset (it is a partial order). The methods\n.is_gequal() and .is_greater_than() work similarly, but returns True if the first element is\ngreater (or equal).\n\nD.is_gequal(8, 4)\n\nTrue\n\nD.is_greater_than (4, 8)\n\nFalse\n\nWe can find the largest and smallest elements of a poset. This is a random poset built\nwith a 10%probability, but copied here to be repeatable.\n\nX = range (20)\nC = [[18, 7], [9, 11], [9, 10], [11, 8], [6, 10],\n\n[10, 2], [0, 2], [2, 1], [1, 8], [8, 12],\n[8, 3], [3, 15], [15, 7], [7, 16], [7, 4],\n[16, 17], [16, 13], [4, 19], [4, 14], [14, 5]]\n\nP = Poset([X, C])\nP.plot()\n\nP.minimal_elements ()\n\n[18, 9, 6, 0]\n\n\n\n336 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\nP.maximal_elements ()\n\n[5, 19, 13, 17, 12]\n\nElements of a poset can be partioned into level sets. In plots of posets, elements at the\nsame level are plotted vertically at the same height. Each level set is obtained by removing\nall of the previous level sets and then taking the minimal elements of the result.\n\nP.level_sets ()\n\n[[18, 9, 6, 0], [11, 10], [2], [1], [8], [3, 12],\n[15], [7], [16, 4], [13, 17, 14, 19], [5]]\n\nIf we make two elements in R comparable when they had not previously been, this is an\nextension of R. Consider all possible extensions of one poset — we can make a poset from\nall of these, where set inclusion is the relation. A linear extension is a maximal element in\nthis poset of posets. Informally, we are adding as many new relations as possible, consistent\nwith the original poset and so that the result is a total order. In other words, there is an\nordering of the elements that is consistent with the order in the poset. We can build such a\nthing, but the output is just a list of the elements in the linear order. A computer scientist\nwould be inclined to call this a “topological sort.”\n\nlinear = P.linear_extension (); linear\n\n[18, 9, 11, 6, 10, 0, 2, 1, 8, 3, 15,\n7, 4, 14, 5, 19, 16, 13, 17, 12]\n\nWe can construct subposets by giving a set of elements to induce the new poset. Here\nwe take roughly the “bottom half” of the random poset P by inducing the subposet on a\nunion of some of the level sets.\n\nlevel = P.level_sets ()\nbottomhalf = sum([level[i] for i in range (5)], [])\nB = P.subposet(bottomhalf)\nB.plot()\n\nThe dual of a poset retains the same set of elements, but reverses any comparisons.\nPdual = P.dual()\nPdual.plot()\n\nTaking the dual of the divisibility poset from Example 19.6 would be like changing the\nrelation to “is a multiple of.”\n\nDdual = D.dual()\nDdual.plot()\n\nLattices\nEvery lattice is a poset, so all the commands above will perform equally well for a lattice.\nBut how do you create a lattice? Simple — first create a poset and then feed it into the\nLatticePoset() constructor. But realize that just because you give this constructor a poset,\nit does not mean a lattice will always come back out. Only if the poset is already a lattice will\nit get upgraded from a poset to a lattice for Sage’s purposes, and you will get a ValueError\n\nif the upgrade is not possible. Finally, notice that some of the posets Sage constructs are\nalready recognized as lattices, such as the prototypical BooleanLattice.\n\n\n\n19.7. SAGE 337\n\nP = Posets.AntichainPoset (8)\nP.is_lattice ()\n\nFalse\n\nLatticePoset(P)\n\nTraceback (most recent call last):\n...\nValueError: Not a lattice.\n\nAn integer composition of n is a list of positive integers that sum to n. A composition\nC1 covers a composition C2 if C2 can be formed from C1 by adding consecutive parts. For\nexample, C1 = [2, 1, 2] ⪰ [3, 2] = C2. With this relation, the set of all integer compositions\nof a fixed integer n is a poset that is also a lattice.\n\nCP = Posets.IntegerCompositions (5)\nC = LatticePoset(CP)\nC.plot()\n\nA meet or a join is a fundamental operation in a lattice.\npar = C.an_element ().parent ()\na = par([1, 1, 1, 2])\nb = par([2, 1, 1, 1])\na, b\n\n([1, 1, 1, 2], [2, 1, 1, 1])\n\nC.meet(a, b)\n\n[2, 1, 2]\n\nc = par([1, 4])\nd = par([2, 3])\nc, d\n\n([1, 4], [2, 3])\n\nC.join(c, d)\n\n[1, 1, 3]\n\nOnce a poset is upgraded to lattice status, then additional commands become available,\nor the character of their results changes.\n\nAn example of the former is the .is_distributive() method.\nC.is_distributive ()\n\nTrue\n\nAn example of the latter is the .top() method. What your text calls a largest element\nand a smallest element of a lattice, Sage calls a top and a bottom. For a poset, .top()\n\nand .bottom() may return an element or may not (returning None), but for a lattice it is\nguaranteed to return exactly one element.\n\n\n\n338 CHAPTER 19. LATTICES AND BOOLEAN ALGEBRAS\n\nC.top()\n\n[1, 1, 1, 1, 1]\n\nC.bottom ()\n\n[5]\n\nNotice that the returned values are all elements of the lattice, in this case ordered lists\nof integers summing to 5.\n\nComplements now make sense in a lattice. The result of the .complements() method is a\ndictionary that uses elements of the lattice as the keys. We say the dictionary is “indexed”\nby the elements of the lattice. The result is a list of the complements of the element. We call\nthis the “value” of the key-value pair. (You may know dictionaries as “associative arrays”,\nbut they are really just fancy functions.)\n\ncomp = C.complements ()\ncomp[par([1, 1, 1, 2])]\n\n[[4, 1]]\n\nThe lattice of integer compositions is a complemented lattice, as we can see by the result\nthat each element has a single (unique) complement, evidenced by the lists of length 1 in the\nvalues of the dictionary. Or we can just ask Sage via .is_complemented(). Dictionaries have\nno inherent order, so you may get different output each time you inspect the dictionary.\n\ncomp\n\n{[1, 1, 1, 1, 1]: [[5]],\n[1, 1, 1, 2]: [[4, 1]],\n[1, 1, 2, 1]: [[3, 2]],\n[1, 1, 3]: [[3, 1, 1]],\n[1, 2, 1, 1]: [[2, 3]],\n[1, 2, 2]: [[2, 2, 1]],\n[1, 3, 1]: [[2, 1, 2]],\n[1, 4]: [[2, 1, 1, 1]],\n[2, 1, 1, 1]: [[1, 4]],\n[2, 1, 2]: [[1, 3, 1]],\n[2, 2, 1]: [[1, 2, 2]],\n[2, 3]: [[1, 2, 1, 1]],\n[3, 1, 1]: [[1, 1, 3]],\n[3, 2]: [[1, 1, 2, 1]],\n[4, 1]: [[1, 1, 1, 2]],\n[5]: [[1, 1, 1, 1, 1]]}\n\n[len(e[1]) for e in comp.items()]\n\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\nC.is_complemented ()\n\nTrue\n\nThere are many more commands which apply to posets and lattices, so build a few and\nuse tab-completion liberally to explore. There is more to discover than we can cover in just\na single chapter, but you now have the basic tools to profitably study posets and lattices in\nSage.\n\n\n\n19.8. SAGE EXERCISES 339\n\n19.8 Sage Exercises\n1. Use R = Posets.RandomPoset(30,0.05) to construct a random poset. Use R.plot() to get\nan idea of what you have built.\n(a) Illustrate the use of the poset methods: .is_lequal(), .is_less_than(), .is_gequal(),\n\nand .is_greater_than() to determine if two specific elements (of your choice) are related\nor incomparable.\n\n(b) Use .minimal_elements() and .maximal_elements() to find the smallest and largest ele-\nments of your poset.\n\n(c) Use LatticePoset(R) to see if the poset R is a lattice by attempting to convert it into a\nlattice.\n\n(d) Find a linear extension of your poset. Confirm that any pair of elements that are\ncomparable in the poset will be similarly comparable in the linear extension.\n\n2. Construct the poset on the positive divisors of 72 = 23 ·32 with divisiblity as the relation,\nand then convert to a lattice.\n(a) Determine the one and zero element using .top() and .bottom().\n(b) Determine all the pairs of elements of the lattice that are complements of each other\n\nwithout using the .complement() method, but rather just use the .meet() and .join()\n\nmethods. Extra credit if you can output each pair just once.\n(c) Determine if the lattice is distributive using just the .meet() and .join() methods, and\n\nnot the .is_distributive() method.\n\n3. Construct several specific diamond lattices with Posets.DiamondPoset(n) by varying the\nvalue of n. Once you feel you have enough empirical evidence, give answers, with justifi-\ncations, to the following questions for general values of n, based on observations obtained\nfrom your experiments with Sage.\n(a) Which elements have complements and which do not, and why?\n(b) Read the documentation of the .antichains() method to learn what an antichain is.\n\nHow many antichains are there?\n(c) Is the lattice distributive?\n\n4. Use Posets.BooleanLattice(4) to construct an instance of the prototypical Boolean alge-\nbra on 16 elements (i.e., all subsets of a 4-set).\nThen use Posets.IntegerCompositions(5) to construct the poset whose 16 elements are the\ncompositions of the integer 5. We have seen above that the integer composition lattice is\ndistributive and complemented, making it a Boolean algebra. And by Theorem 19.23 we\ncan conclude that these two Boolean algebras are isomorphic.\nUse the .plot() method to see the similarity visually. Then use the method .hasse_diagram()\n\non each poset to obtain a directed graph (which you can also plot, though the embedding\ninto the plane may not be as informative). Employ the graph method .is_isomorphic() to\nsee that the two Hasse diagrams really are the “same.”\n\n5. (Advanced) For the previous question, construct an explicit isomorphism between the two\nBoolean algebras. This would be a bijective function (constructed with the def command)\nthat converts compositions into sets (or if, you choose, sets into compositions) and which\nrespects the meet and join operations. You can test and illustrate your function by its\ninteraction with specific elements evaluated in the meet and join operations, as described\nin the definition of an isomorphism of Boolean algebras.\n\n\n\n20\n\nVector Spaces\n\nIn a physical system a quantity can often be described with a single number. For example,\nwe need to know only a single number to describe temperature, mass, or volume. However,\nfor some quantities, such as location, we need several numbers. To give the location of\na point in space, we need x, y, and z coordinates. Temperature distribution over a solid\nobject requires four numbers: three to identify each point within the object and a fourth\nto describe the temperature at that point. Often n-tuples of numbers, or vectors, also have\ncertain algebraic properties, such as addition or scalar multiplication.\n\nIn this chapter we will examine mathematical structures called vector spaces. As with\ngroups and rings, it is desirable to give a simple list of axioms that must be satisfied to\nmake a set of vectors a structure worth studying.\n\n20.1 Definitions and Examples\nA vector space V over a field F is an abelian group with a scalar product α · v or αv\ndefined for all α ∈ F and all v ∈ V satisfying the following axioms.\n\n• α(βv) = (αβ)v;\n\n• (α+ β)v = αv + βv;\n\n• α(u+ v) = αu+ αv;\n\n• 1v = v;\n\nwhere α, β ∈ F and u, v ∈ V .\nThe elements of V are called vectors; the elements of F are called scalars. It is\n\nimportant to notice that in most cases two vectors cannot be multiplied. In general, it is\nonly possible to multiply a vector with a scalar. To differentiate between the scalar zero\nand the vector zero, we will write them as 0 and 0, respectively.\n\nLet us examine several examples of vector spaces. Some of them will be quite familiar;\nothers will seem less so.\n\nExample 20.1. The n-tuples of real numbers, denoted by Rn, form a vector space over R.\nGiven vectors u = (u1, . . . , un) and v = (v1, . . . , vn) in Rn and α in R, we can define vector\naddition by\n\nu+ v = (u1, . . . , un) + (v1, . . . , vn) = (u1 + v1, . . . , un + vn)\n\nand scalar multiplication by\n\nαu = α(u1, . . . , un) = (αu1, . . . , αun).\n\n340\n\n\n\n20.2. SUBSPACES 341\n\nExample 20.2. If F is a field, then F [x] is a vector space over F . The vectors in F [x]\nare simply polynomials, and vector addition is just polynomial addition. If α ∈ F and\np(x) ∈ F [x], then scalar multiplication is defined by αp(x).\n\nExample 20.3. The set of all continuous real-valued functions on a closed interval [a, b] is\na vector space over R. If f(x) and g(x) are continuous on [a, b], then (f + g)(x) is defined\nto be f(x) + g(x). Scalar multiplication is defined by (αf)(x) = αf(x) for α ∈ R. For\nexample, if f(x) = sinx and g(x) = x2, then (2f + 5g)(x) = 2 sinx+ 5x2.\n\nExample 20.4. Let V = Q(\n√\n2 ) = {a + b\n\n√\n2 : a, b ∈ Q}. Then V is a vector space over\n\nQ. If u = a+ b\n√\n2 and v = c+ d\n\n√\n2, then u+ v = (a+ c) + (b+ d)\n\n√\n2 is again in V . Also,\n\nfor α ∈ Q, αv is in V . We will leave it as an exercise to verify that all of the vector space\naxioms hold for V .\n\nProposition 20.5. Let V be a vector space over F . Then each of the following statements\nis true.\n\n1. 0v = 0 for all v ∈ V .\n\n2. α0 = 0 for all α ∈ F .\n\n3. If αv = 0, then either α = 0 or v = 0.\n\n4. (−1)v = −v for all v ∈ V .\n\n5. −(αv) = (−α)v = α(−v) for all α ∈ F and all v ∈ V .\n\nProof. To prove (1), observe that\n\n0v = (0 + 0)v = 0v + 0v;\n\nconsequently, 0 + 0v = 0v + 0v. Since V is an abelian group, 0 = 0v.\nThe proof of (2) is almost identical to the proof of (1). For (3), we are done if α = 0.\n\nSuppose that α ̸= 0. Multiplying both sides of αv = 0 by 1/α, we have v = 0.\nTo show (4), observe that\n\nv + (−1)v = 1v + (−1)v = (1− 1)v = 0v = 0,\n\nand so −v = (−1)v. We will leave the proof of (5) as an exercise.\n\n20.2 Subspaces\nJust as groups have subgroups and rings have subrings, vector spaces also have substruc-\ntures. Let V be a vector space over a field F , and W a subset of V . Then W is a subspace\nof V if it is closed under vector addition and scalar multiplication; that is, if u, v ∈W and\nα ∈ F , it will always be the case that u+ v and αv are also in W .\n\nExample 20.6. Let W be the subspace of R3 defined by W = {(x1, 2x1 + x2, x1 − x2) :\nx1, x2 ∈ R}. We claim that W is a subspace of R3. Since\n\nα(x1, 2x1 + x2, x1 − x2) = (αx1, α(2x1 + x2), α(x1 − x2))\n\n= (αx1, 2(αx1) + αx2, αx1 − αx2),\n\nW is closed under scalar multiplication. To show that W is closed under vector addition,\nlet u = (x1, 2x1 + x2, x1 − x2) and v = (y1, 2y1 + y2, y1 − y2) be vectors in W . Then\n\nu+ v = (x1 + y1, 2(x1 + y1) + (x2 + y2), (x1 + y1)− (x2 + y2)).\n\n\n\n342 CHAPTER 20. VECTOR SPACES\n\nExample 20.7. Let W be the subset of polynomials of F [x] with no odd-power terms. If\np(x) and q(x) have no odd-power terms, then neither will p(x)+ q(x). Also, αp(x) ∈W for\nα ∈ F and p(x) ∈W .\n\nLet V be any vector space over a field F and suppose that v1, v2, . . . , vn are vectors in\nV and α1, α2, . . . , αn are scalars in F . Any vector w in V of the form\n\nw =\n\nn∑\ni=1\n\nαivi = α1v1 + α2v2 + · · ·+ αnvn\n\nis called a linear combination of the vectors v1, v2, . . . , vn. The spanning set of vec-\ntors v1, v2, . . . , vn is the set of vectors obtained from all possible linear combinations of\nv1, v2, . . . , vn. If W is the spanning set of v1, v2, . . . , vn, then we say that W is spanned by\nv1, v2, . . . , vn.\n\nProposition 20.8. Let S = {v1, v2, . . . , vn} be vectors in a vector space V . Then the span\nof S is a subspace of V .\n\nProof. Let u and v be in S. We can write both of these vectors as linear combinations of\nthe vi’s:\n\nu = α1v1 + α2v2 + · · ·+ αnvn\n\nv = β1v1 + β2v2 + · · ·+ βnvn.\n\nThen\nu+ v = (α1 + β1)v1 + (α2 + β2)v2 + · · ·+ (αn + βn)vn\n\nis a linear combination of the vi’s. For α ∈ F ,\n\nαu = (αα1)v1 + (αα2)v2 + · · ·+ (ααn)vn\n\nis in the span of S.\n\n20.3 Linear Independence\nLet S = {v1, v2, . . . , vn} be a set of vectors in a vector space V . If there exist scalars\nα1, α2 . . . αn ∈ F such that not all of the αi’s are zero and\n\nα1v1 + α2v2 + · · ·+ αnvn = 0,\n\nthen S is said to be linearly dependent. If the set S is not linearly dependent, then it is\nsaid to be linearly independent. More specifically, S is a linearly independent set if\n\nα1v1 + α2v2 + · · ·+ αnvn = 0\n\nimplies that\nα1 = α2 = · · · = αn = 0\n\nfor any set of scalars {α1, α2 . . . αn}.\n\nProposition 20.9. Let {v1, v2, . . . , vn} be a set of linearly independent vectors in a vector\nspace. Suppose that\n\nv = α1v1 + α2v2 + · · ·+ αnvn = β1v1 + β2v2 + · · ·+ βnvn.\n\nThen α1 = β1, α2 = β2, . . . , αn = βn.\n\n\n\n20.3. LINEAR INDEPENDENCE 343\n\nProof. If\nv = α1v1 + α2v2 + · · ·+ αnvn = β1v1 + β2v2 + · · ·+ βnvn,\n\nthen\n(α1 − β1)v1 + (α2 − β2)v2 + · · ·+ (αn − βn)vn = 0.\n\nSince v1, . . . , vn are linearly independent, αi − βi = 0 for i = 1, . . . , n.\n\nThe definition of linear dependence makes more sense if we consider the following propo-\nsition.\n\nProposition 20.10. A set {v1, v2, . . . , vn} of vectors in a vector space V is linearly depen-\ndent if and only if one of the vi’s is a linear combination of the rest.\n\nProof. Suppose that {v1, v2, . . . , vn} is a set of linearly dependent vectors. Then there\nexist scalars α1, . . . , αn such that\n\nα1v1 + α2v2 + · · ·+ αnvn = 0,\n\nwith at least one of the αi’s not equal to zero. Suppose that αk ̸= 0. Then\n\nvk = −α1\n\nαk\nv1 − · · · − αk−1\n\nαk\nvk−1 −\n\nαk+1\n\nαk\nvk+1 − · · · − αn\n\nαk\nvn.\n\nConversely, suppose that\n\nvk = β1v1 + · · ·+ βk−1vk−1 + βk+1vk+1 + · · ·+ βnvn.\n\nThen\nβ1v1 + · · ·+ βk−1vk−1 − vk + βk+1vk+1 + · · ·+ βnvn = 0.\n\nThe following proposition is a consequence of the fact that any system of homogeneous\nlinear equations with more unknowns than equations will have a nontrivial solution. We\nleave the details of the proof for the end-of-chapter exercises.\n\nProposition 20.11. Suppose that a vector space V is spanned by n vectors. If m > n,\nthen any set of m vectors in V must be linearly dependent.\n\nA set {e1, e2, . . . , en} of vectors in a vector space V is called a basis for V if {e1, e2, . . . , en}\nis a linearly independent set that spans V .\n\nExample 20.12. The vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1) form a basis\nfor R3. The set certainly spans R3, since any arbitrary vector (x1, x2, x3) in R3 can be\nwritten as x1e1 + x2e2 + x3e3. Also, none of the vectors e1, e2, e3 can be written as a linear\ncombination of the other two; hence, they are linearly independent. The vectors e1, e2, e3\nare not the only basis of R3: the set {(3, 2, 1), (3, 2, 0), (1, 1, 1)} is also a basis for R3.\n\nExample 20.13. Let Q(\n√\n2 ) = {a+b\n\n√\n2 : a, b ∈ Q}. The sets {1,\n\n√\n2 } and {1+\n\n√\n2, 1−\n\n√\n2 }\n\nare both bases of Q(\n√\n2 ).\n\nFrom the last two examples it should be clear that a given vector space has several bases.\nIn fact, there are an infinite number of bases for both of these examples. In general, there\nis no unique basis for a vector space. However, every basis of R3 consists of exactly three\nvectors, and every basis of Q(\n\n√\n2 ) consists of exactly two vectors. This is a consequence of\n\nthe next proposition.\n\n\n\n344 CHAPTER 20. VECTOR SPACES\n\nProposition 20.14. Let {e1, e2, . . . , em} and {f1, f2, . . . , fn} be two bases for a vector space\nV . Then m = n.\n\nProof. Since {e1, e2, . . . , em} is a basis, it is a linearly independent set. By Proposi-\ntion 20.11, n ≤ m. Similarly, {f1, f2, . . . , fn} is a linearly independent set, and the last\nproposition implies that m ≤ n. Consequently, m = n.\n\nIf {e1, e2, . . . , en} is a basis for a vector space V , then we say that the dimension of\nV is n and we write dimV = n. We will leave the proof of the following theorem as an\nexercise.\n\nTheorem 20.15. Let V be a vector space of dimension n.\n\n1. If S = {v1, . . . , vn} is a set of linearly independent vectors for V , then S is a basis for\nV .\n\n2. If S = {v1, . . . , vn} spans V , then S is a basis for V .\n\n3. If S = {v1, . . . , vk} is a set of linearly independent vectors for V with k < n, then\nthere exist vectors vk+1, . . . , vn such that\n\n{v1, . . . , vk, vk+1, . . . , vn}\n\nis a basis for V .\n\n20.4 Exercises\n1. If F is a field, show that F [x] is a vector space over F , where the vectors in F [x] are\npolynomials. Vector addition is polynomial addition, and scalar multiplication is defined\nby αp(x) for α ∈ F .\n\n2. Prove that Q(\n√\n2 ) is a vector space.\n\n3. Let Q(\n√\n2,\n√\n3 ) be the field generated by elements of the form a + b\n\n√\n2 + c\n\n√\n3, where\n\na, b, c are in Q. Prove that Q(\n√\n2,\n√\n3 ) is a vector space of dimension 4 over Q. Find a basis\n\nfor Q(\n√\n2,\n√\n3 ).\n\n4. Prove that the complex numbers are a vector space of dimension 2 over R.\n\n5. Prove that the set Pn of all polynomials of degree less than n form a subspace of the\nvector space F [x]. Find a basis for Pn and compute the dimension of Pn.\n\n6. Let F be a field and denote the set of n-tuples of F by Fn. Given vectors u = (u1, . . . , un)\nand v = (v1, . . . , vn) in Fn and α in F , define vector addition by\n\nu+ v = (u1, . . . , un) + (v1, . . . , vn) = (u1 + v1, . . . , un + vn)\n\nand scalar multiplication by\n\nαu = α(u1, . . . , un) = (αu1, . . . , αun).\n\nProve that Fn is a vector space of dimension n under these operations.\n\n7. Which of the following sets are subspaces of R3? If the set is indeed a subspace, find a\nbasis for the subspace and compute its dimension.\n(a) {(x1, x2, x3) : 3x1 − 2x2 + x3 = 0}\n\n\n\n20.4. EXERCISES 345\n\n(b) {(x1, x2, x3) : 3x1 + 4x3 = 0, 2x1 − x2 + x3 = 0}\n(c) {(x1, x2, x3) : x1 − 2x2 + 2x3 = 2}\n(d) {(x1, x2, x3) : 3x1 − 2x22 = 0}\n\n8. Show that the set of all possible solutions (x, y, z) ∈ R3 of the equations\n\nAx+By + Cz = 0\n\nDx+ Ey + Cz = 0\n\nform a subspace of R3.\n\n9. Let W be the subset of continuous functions on [0, 1] such that f(0) = 0. Prove that W\nis a subspace of C[0, 1].\n\n10. Let V be a vector space over F . Prove that −(αv) = (−α)v = α(−v) for all α ∈ F and\nall v ∈ V .\n\n11. Let V be a vector space of dimension n. Prove each of the following statements.\n(a) If S = {v1, . . . , vn} is a set of linearly independent vectors for V , then S is a basis for\n\nV .\n(b) If S = {v1, . . . , vn} spans V , then S is a basis for V .\n(c) If S = {v1, . . . , vk} is a set of linearly independent vectors for V with k < n, then there\n\nexist vectors vk+1, . . . , vn such that\n\n{v1, . . . , vk, vk+1, . . . , vn}\n\nis a basis for V .\n\n12. Prove that any set of vectors containing 0 is linearly dependent.\n\n13. Let V be a vector space. Show that {0} is a subspace of V of dimension zero.\n\n14. If a vector space V is spanned by n vectors, show that any set of m vectors in V must\nbe linearly dependent for m > n.\n\n15. (Linear Transformations) Let V and W be vector spaces over a field F , of dimensions\nm and n, respectively. If T : V →W is a map satisfying\n\nT (u+ v) = T (u) + T (v)\n\nT (αv) = αT (v)\n\nfor all α ∈ F and all u, v ∈ V , then T is called a linear transformation from V into W .\n(a) Prove that the kernel of T , ker(T ) = {v ∈ V : T (v) = 0}, is a subspace of V . The\n\nkernel of T is sometimes called the null space of T .\n(b) Prove that the range or range space of T , R(V ) = {w ∈W : T (v) = w for some v ∈\n\nV }, is a subspace of W .\n(c) Show that T : V →W is injective if and only if ker(T ) = {0}.\n(d) Let {v1, . . . , vk} be a basis for the null space of T . We can extend this basis to be\n\na basis {v1, . . . , vk, vk+1, . . . , vm} of V . Why? Prove that {T (vk+1), . . . , T (vm)} is a\nbasis for the range of T . Conclude that the range of T has dimension m− k.\n\n(e) Let dimV = dimW . Show that a linear transformation T : V →W is injective if and\nonly if it is surjective.\n\n\n\n346 CHAPTER 20. VECTOR SPACES\n\n16. Let V and W be finite dimensional vector spaces of dimension n over a field F . Suppose\nthat T : V → W is a vector space isomorphism. If {v1, . . . , vn} is a basis of V , show that\n{T (v1), . . . , T (vn)} is a basis of W . Conclude that any vector space over a field F of\ndimension n is isomorphic to Fn.\n\n17. (Direct Sums) Let U and V be subspaces of a vector space W . The sum of U and V ,\ndenoted U + V , is defined to be the set of all vectors of the form u + v, where u ∈ U and\nv ∈ V .\n(a) Prove that U + V and U ∩ V are subspaces of W .\n(b) If U + V = W and U ∩ V = 0, then W is said to be the direct sum. In this case,\n\nwe write W = U ⊕ V . Show that every element w ∈ W can be written uniquely as\nw = u+ v, where u ∈ U and v ∈ V .\n\n(c) Let U be a subspace of dimension k of a vector space W of dimension n. Prove that\nthere exists a subspace V of dimension n− k such that W = U ⊕ V . Is the subspace\nV unique?\n\n(d) If U and V are arbitrary subspaces of a vector space W , show that\n\ndim(U + V ) = dimU + dimV − dim(U ∩ V ).\n\n18. (Dual Spaces) Let V and W be finite dimensional vector spaces over a field F .\n(a) Show that the set of all linear transformations from V into W , denoted by Hom(V,W ),\n\nis a vector space over F , where we define vector addition as follows:\n\n(S + T )(v) = S(v) + T (v)\n\n(αS)(v) = αS(v),\n\nwhere S, T ∈ Hom(V,W ), α ∈ F , and v ∈ V .\n(b) Let V be an F -vector space. Define the dual space of V to be V ∗ = Hom(V, F ).\n\nElements in the dual space of V are called linear functionals. Let v1, . . . , vn be\nan ordered basis for V . If v = α1v1 + · · · + αnvn is any vector in V , define a linear\nfunctional ϕi : V → F by ϕi(v) = αi. Show that the ϕi’s form a basis for V ∗. This\nbasis is called the dual basis of v1, . . . , vn (or simply the dual basis if the context\nmakes the meaning clear).\n\n(c) Consider the basis {(3, 1), (2,−2)} for R2. What is the dual basis for (R2)∗?\n(d) Let V be a vector space of dimension n over a field F and let V ∗∗ be the dual space\n\nV ∗. Show that each element v ∈ V gives rise to an element λv in V ∗∗ and that the\nmap v 7→ λv is an isomorphism of V with V ∗∗.\n\n20.5 References and Suggested Readings\n[1] Beezer, R. A First Course in Linear Algebra. Available online at http://linear.ups.\n\nedu/. 2004–2014.\n[2] Bretscher, O. Linear Algebra with Applications. 4th ed. Pearson, Upper Saddle River,\n\nNJ, 2009.\n[3] Curtis, C. W. Linear Algebra: An Introductory Approach. 4th ed. Springer, New\n\nYork, 1984.\n\nhttp://linear.ups.edu/\nhttp://linear.ups.edu/\n\n\n20.6. SAGE 347\n\n[4] Hoffman, K. and Kunze, R. Linear Algebra. 2nd ed. Prentice-Hall, Englewood Cliffs,\nNJ, 1971.\n\n[5] Johnson, L. W., Riess, R. D., and Arnold, J. T. Introduction to Linear Algebra. 6th\ned. Pearson, Upper Saddle River, NJ, 2011.\n\n[6] Leon, S. J. Linear Algebra with Applications. 8th ed. Pearson, Upper Saddle River,\nNJ, 2010.\n\n20.6 Sage\nMany computations, in seemingly very different areas of mathematics, can be translated\ninto questions about linear combinations, or other areas of linear algebra. So Sage has\nextensive and thorough support for topics such as vector spaces.\n\nVector Spaces\nThe simplest way to create a vector space is to begin with a field and use an exponent to\nindicate the number of entries in the vectors of the space.\n\nV = QQ^4; V\n\nVector space of dimension 4 over Rational Field\n\nF.<a> = FiniteField (3^4)\nW = F^5; W\n\nVector space of dimension 5 over Finite Field in a of size 3^4\n\nElements can be built with the vector constructor.\nv = vector(QQ, [1, 1/2, 1/3, 1/4]); v\n\n(1, 1/2, 1/3, 1/4)\n\nv in V\n\nTrue\n\nw = vector(F, [1, a^2, a^4, a^6, a^8]); w\n\n(1, a^2, a^3 + 1, a^3 + a^2 + a + 1, a^2 + a + 2)\n\nw in W\n\nTrue\n\nNotice that vectors are printed with parentheses, which helps distinguish them from\nlists (though they alos look like tuples). Vectors print horizontally, but in Sage there is no\nsuch thing as a “row vector” or a “column vector,” though once matrices get involved we\nneed to address this distinction. Finally, notice how the elements of the finite field have\nbeen converted to an alternate representation.\n\nOnce we have vector spaces full of vectors, we can perform computations with them.\nUltimately, all the action in a vector space comes back to vector addition and scalar mul-\ntiplication, which together create linear combinations.\n\n\n\n348 CHAPTER 20. VECTOR SPACES\n\nu = vector(QQ, [ 1, 2, 3, 4, 5, 6])\nv = vector(QQ, [-1, 2, -4, 8, -16, 32])\n3*u - 2*v\n\n(5, 2, 17, -4, 47, -46)\n\nw = vector(F, [1, a^2, a^4, a^6, a^8])\nx = vector(F, [1, a, 2*a, a, 1])\ny = vector(F, [1, a^3, a^6, a^9, a^12])\na^25*w + a^43*x + a^66*y\n\n(a^3 + a^2 + a + 2, a^2 + 2*a, 2*a^3 + a^2 + 2, 2*a^3 + a^2 + a,\na^3 + 2*a^2 + a + 2)\n\nSubspaces\nSage can create subspaces in a variety of ways, such as in the creation of row or column\nspaces of matrices. However, the most direct way is to begin with a set of vectors to use as\na spanning set.\n\nu = vector(QQ, [1, -1, 3])\nv = vector(QQ, [2, 1, -1])\nw = vector(QQ, [3, 0, 2])\nS = (QQ^3).subspace ([u, v, w]); S\n\nVector space of degree 3 and dimension 2 over Rational Field\nBasis matrix:\n[ 1 0 2/3]\n[ 0 1 -7/3]\n\n3*u - 6*v + (1/2)*w in S\n\nTrue\n\nvector(QQ, [4, -1, -2]) in S\n\nFalse\n\nNotice that the information printed about S includes a “basis matrix.” The rows of this\nmatrix are a basis for the vector space. We can get the basis, as a list of vectors (not rows\nof a matrix), with the .basis() method.\n\nS.basis()\n\n[\n(1, 0, 2/3),\n(0, 1, -7/3)\n]\n\nNotice that Sage has converted the spanning set of three vectors into a basis with two\nvectors. This is partially due to the fact that the original set of three vectors is linearly\ndependent, but a more substantial change has occurred.\n\nThis is a good place to discuss some of the mathematics behind what makes Sage work.\nA vector space over an infinite field, like the rationals or the reals, is an infinite set. No\nmatter how expansive computer memory may seem, it is still finite. How does Sage fit\n\n\n\n20.6. SAGE 349\n\nan infinite set into our finite machines? The main idea is that a finite-dimensional vector\nspace has a finite set of generators, which we know as a basis. So Sage really only needs\nthe elements of a basis (two vectors in the previous example) to be able to work with the\ninfinitely many possibilities for elements of the subspace.\n\nFurthermore, for every basis associated with a vector space, Sage performs linear com-\nbinations to convert the given basis into another “standard” basis. This new basis has the\nproperty that as the rows of a matrix, the matrix is in reduced row-echelon form. You can\nsee this in the basis matrix above. The reduced row-echelon form of a matrix is unique, so\nthis standard basis allows Sage to recognize when two vector spaces are equal. Here is an\nexample.\n\nu = vector(QQ, [1, -1, 3])\nv = vector(QQ, [2, 1, -1])\nw = vector(QQ, [3, 0, 2])\nu + v == w\n\nTrue\n\nS1 = (QQ^3).subspace ([u, v, w])\nS2 = (QQ^3).subspace ([u-v, v-w, w-u])\nS1 == S2\n\nTrue\n\nAs you might expect, it is easy to determine the dimension of a vector space.\nu = vector(QQ, [1, -1, 3, 4])\nv = vector(QQ, [2, 1, -1, -2])\nS = (QQ^4).subspace ([u, v, 2*u + 3*v, -u + 2*v])\nS.dimension ()\n\n2\n\nLinear Independence\nThere are a variety of ways in Sage to determine if a set of vectors is linearly independent\nor not, and to find relations of linear dependence if they exist. The technique we will show\nhere is a simple test to see if a set of vectors is linearly independent or not. Simply use the\nvectors as a spanning set for a subspace, and check the dimension of the subspace. The\ndimension equals the number of vectors in the spanning set if and only if the spanning set\nis linearly independent.\n\nF.<a> = FiniteField (3^4)\nu = vector(F, [a^i for i in range(0, 7, 1)])\nv = vector(F, [a^i for i in range(0, 14, 2)])\nw = vector(F, [a^i for i in range(0, 21, 3)])\nS = (F^7).subspace ([u, v, w])\nS.dimension ()\n\n3\n\nS = (F^7).subspace ([u, v, a^3*u + a^11*v])\nS.dimension ()\n\n2\n\nSo the first set of vectors, [u, v, w], is linearly independent, while the second set, [u,\nv, a^3*u + a^11*v], is not.\n\n\n\n350 CHAPTER 20. VECTOR SPACES\n\nAbstract Vector Spaces\nSage does not implement many abstract vector spaces directly, such as Pn, the vector space\nof polynomials of degree n or less. This is due in part to the fact that a finite-dimensional\nvector space over a field F is isomorphic to the vector space Fn. So Sage captures all\nthe functionality of finite-dimensional vector spaces, and it is left to the user to perform\nthe conversions according to the isomorphism (which is often trivial with the choice of an\nobvious basis).\n\nHowever, there are instances where rings behave naturally as vector spaces and we can\nexploit this extra structure. We will see much more of this in the chapters on fields and\nGalois theory. As an example, finite fields have a single generator, and the first few powers\nof the generator form a basis. Consider creating a vector space from the elements of a finite\nfield of order 76 = 117 649. As elements of a field we know they can be added, so we will\ndefine this to be the addition in our vector space. For any element of the integers mod 7,\nwe can multiply an element of the field by the integer, so we define this to be our scalar\nmultiplication. Later, we will be certain that these two definitions lead to a vector space,\nbut take that for granted now. So here are some operations in our new vector space.\n\nF.<a> = FiniteField (7^6)\nu = 2*a^5 + 6*a^4 + 2*a^3 + 3*a^2 + 2*a + 3\nv = 4*a^5 + 4*a^4 + 4*a^3 + 6*a^2 + 5*a + 6\nu + v\n\n6*a^5 + 3*a^4 + 6*a^3 + 2*a^2 + 2\n\n4*u\n\na^5 + 3*a^4 + a^3 + 5*a^2 + a + 5\n\n2*u + 5*v\n\n3*a^5 + 4*a^4 + 3*a^3 + a^2 + a + 1\n\nYou might recognize that this looks very familiar to how we add polynomials, and\nmultiply polynomials by scalars. You would be correct. However, notice that in this vector\nspace construction, we are totally ignoring the possibility of multiplying two field elements\ntogether. As a vector space with scalars from Z7, a basis is the first six powers of the\ngenerator, {1, a, a2, a3, a4, a5}. (Notice how counting from zero is natural here.) You may\nhave noticed how Sage consistently rewrites elements of fields as linear combinations — now\nyou have a good explanation.\n\nHere is what Sage knows about a finite field as a vector space. First, it knows that the\nfinite field is a vector space, and what the field of scalars is.\n\nV = F.vector_space (); V\n\nVector space of dimension 6 over Finite Field of size 7\n\nR = V.base_ring (); R\n\nFinite Field of size 7\n\nR == FiniteField (7)\n\nTrue\n\n\n\n20.7. SAGE EXERCISES 351\n\nV.dimension ()\n\n6\n\nSo the finite field (as a vector space) is isomorphic to the vector space (Z7)\n6. Notice this\n\nis not a ring or field isomorphism, as it does not fully address multiplication of elements,\neven though that is possible in the field.\n\nSecond, elements of the field can be converted to elements of the vector space easily.\nx = V(u); x\n\n(3, 2, 3, 2, 6, 2)\n\ny = V(v); y\n\n(6, 5, 6, 4, 4, 4)\n\nNotice that Sage writes field elements with high powers of the generator first, while\nthe basis in use is ordered with low powers first. The computations below illustrate the\nisomorphism preserving the structure between the finite field itself and its interpretation as\na vector space, (Z7)\n\n6.\nV(u + v) == V(u) + V(v)\n\nTrue\n\ntwo = R(2)\nV(two*u) == two*V(u)\n\nTrue\n\nLinear Algebra\nSage has extensive support for linear algebra, well beyond what we have described here,\nor what we will need for the remaining chapters. Create vector spaces and vectors (with\ndifferent fields of scalars), and then use tab-completion on these objects to explore the large\nsets of available commands.\n\n20.7 Sage Exercises\n1. Given two subspaces U and W of a vector space V , their sum U +W can be defined as\nthe set U +W = {u + w | u ∈ U, w ∈ W}, in other words, the set of all possible sums of\nan element from U and an element from W .\nNotice this is not the direct sum of your text, nor the direct_sum() method in Sage. However,\nyou can build this subspace in Sage as follows. Grab the bases of U and W individually,\nas lists of vectors. Join the two lists together by just using a plus sign between them.\nNow build the sum subspace by creating a subspace of V spanned by this set, by using the\n.subspace() method.\nIn the vector space (QQ^10) construct two subspaces that you expect to (a) have dimension 5\nor 6 or so, and (b) have an intersection that is a vector space of dimension 2 or so. Compare\ntheir individual dimensions with the dimensions of the intersection of U and W (U ∩W ,\n.intersection() in Sage) and the sum U +W .\n\n\n\n352 CHAPTER 20. VECTOR SPACES\n\nRepeat the experiment with the two original vector spaces having dimension 8 or so, and\nwith the intersection as small as possible. Form a general conjecture relating these four\ndimensions based on the results of your two (or more)experiments.\n\n2. We can construct a field in Sage that extends the rationals by adding in a fourth root\nof two, Q[ 4\n\n√\n2], with the command F.<c> = QQ[2^(1/4)]. This is a vector space of dimension\n\n4 over the rationals, with a basis that is the first four powers of c = 4\n√\n2 (starting with the\n\nzero power).\nThe command F.vector_space() will return three items in a triple (so be careful how you\nhandle this output to extract what you need). The first part of the output is a vector\nspace over the rationals that is isomorphic to F. The next is a vector space isomorphism\n(invertible linear transformation) from the provided vector space to the field, while the\nthird is an isomorphism in the opposite direction. These two isomorphisms can then be\nused like functions. Notice that this is different behavior than for .vector_space() applied\nto finite fields. Create non-trivial examples that show that these vector space isomorphisms\nbehave as an isomorphism should. (You will have at least four such examples in a complete\nsolution.)\n\n3. Build a finite field F of order pn in the usual way. Then construct the (multiplicative)\ngroup of all invertible (nonsingular) m ×m matrices over this field with the command G\n\n= GL(m, F) (“the general linear group”). What is the order of this group? In other words,\nfind a general expression for the order of this group.\nYour answer should be a function of m, p and n. Provide a complete explanation of the\nlogic behind your solution (i.e. something resembling a proof). Also provide tests in Sage\nthat your answer is correct.\nHints: G.order() will help you test and verify your hypotheses. Small examples in Sage\n(listing all the elements of the group) might aid your intuition—which is why this is a Sage\nexercise. Small means 2 × 2 and 3 × 3 matrices and finite fields with 2, 3, 4, 5 elements, at\nmost. Results do not really depend on each of p and n, but rather just on pn.\nRealize this group is interesting because it contains representations of all the invertible (i.e.\n1-1 and onto) linear transformations from the (finite) vector space Fm to itself.\n\n4. What happens if we try to do linear algebra over a ring that is not also a field? The\nobject that resembles a vector space, but with this one distinction, is known as a module.\nYou can build one easily with a construction like ZZ^3. Evaluate the following to create a\nmodule and a submodule.\n\nM = ZZ^3\nu = M([1, 0, 0])\nv = M([2, 2, 0])\nw = M([0, 0, 4])\nN = M.submodule ([u, v, w])\n\nExamine the bases and dimensions (aka “rank”) of the module and submodule, and check\nthe equality of the module and submodule. How is this different than the situation for\nvector spaces? Can you create a third module, P, that is a proper subset of M and properly\ncontains N?\n\n5. A finite field, F , of order 53 is a vector space of dimension 3 over Z5. Suppose a is a\ngenerator of F . Let M be any 3×3 matrix with entries from Z5 (carefule here, the elements\nare from th field of scalars, not from the vector space). If we convert an element x ∈ F\nto a vector (relative to the basis {1, a, a2}), then we can multiply it by M (with M on the\nleft) to create another vector, which we can translate to a linear combination of the basis\nelements, and hence another element of F . This function is a vector space homomorphism,\n\n\n\n20.7. SAGE EXERCISES 353\n\nbetter known as a linear transformation (implemented with a matrix representation relative\nto the basis {1, a, a2}. Notice that each part below becomes less general and more specific.\n(a) Create a non-invertible matrix R and give examples to show that the mapping de-\n\nscribed by R is a vector space homomorphism of F into F .\n(b) Create an invertible matrix M . The mapping will now be an invertible homomorphism.\n\nDetermine the inverse function and give examples to verify its properties.\n(c) Since a is a generator of the field, the mapping a 7→ a5 can be extended to a vector\n\nspace homomorphism (i.e. a linear transformation). Find a matrix M which effects this\nlinear transformation, and from this, determine that the homomorphism is invertible.\n\n(d) None of the previous three parts applies to properties of multiplication in the field.\nHowever, the mapping from the third part also preserves multiplication in the field,\nthough a proof of this may not be obvious right now. So we are saying this mapping\nis a field automorphism, preserving both addition and multiplication. Give a nontriv-\nial example of the multiplication-preserving properties of this mapping. (This is the\nFrobenius map which will be discussed further in Chapter 21.)\n\n\n\n21\n\nFields\n\nIt is natural to ask whether or not some field F is contained in a larger field. We think of\nthe rational numbers, which reside inside the real numbers, while in turn, the real numbers\nlive inside the complex numbers. We can also study the fields between Q and R and inquire\nas to the nature of these fields.\n\nMore specifically if we are given a field F and a polynomial p(x) ∈ F [x], we can ask\nwhether or not we can find a field E containing F such that p(x) factors into linear factors\nover E[x]. For example, if we consider the polynomial\n\np(x) = x4 − 5x2 + 6\n\nin Q[x], then p(x) factors as (x2− 2)(x2− 3). However, both of these factors are irreducible\nin Q[x]. If we wish to find a zero of p(x), we must go to a larger field. Certainly the field\nof real numbers will work, since\n\np(x) = (x−\n√\n2)(x+\n\n√\n2)(x−\n\n√\n3)(x+\n\n√\n3).\n\nIt is possible to find a smaller field in which p(x) has a zero, namely\n\nQ(\n√\n2) = {a+ b\n\n√\n2 : a, b ∈ Q}.\n\nWe wish to be able to compute and study such fields for arbitrary polynomials over a field\nF .\n\n21.1 Extension Fields\nA field E is an extension field of a field F if F is a subfield of E. The field F is called\nthe base field. We write F ⊂ E.\n\nExample 21.1. For example, let\n\nF = Q(\n√\n2 ) = {a+ b\n\n√\n2 : a, b ∈ Q}\n\nand let E = Q(\n√\n2 +\n\n√\n3 ) be the smallest field containing both Q and\n\n√\n2 +\n\n√\n3. Both\n\nE and F are extension fields of the rational numbers. We claim that E is an extension\nfield of F . To see this, we need only show that\n\n√\n2 is in E. Since\n\n√\n2 +\n\n√\n3 is in E,\n\n1/(\n√\n2 +\n\n√\n3 ) =\n\n√\n3 −\n\n√\n2 must also be in E. Taking linear combinations of\n\n√\n2 +\n\n√\n3 and√\n\n3−\n√\n2, we find that\n\n√\n2 and\n\n√\n3 must both be in E.\n\nExample 21.2. Let p(x) = x2 + x + 1 ∈ Z2[x]. Since neither 0 nor 1 is a root of this\npolynomial, we know that p(x) is irreducible over Z2. We will construct a field extension\nof Z2 containing an element α such that p(α) = 0. By Theorem 17.22, the ideal ⟨p(x)⟩\n\n354\n\n\n\n21.1. EXTENSION FIELDS 355\n\ngenerated by p(x) is maximal; hence, Z2[x]/⟨p(x)⟩ is a field. Let f(x) + ⟨p(x)⟩ be an\narbitrary element of Z2[x]/⟨p(x)⟩. By the division algorithm,\n\nf(x) = (x2 + x+ 1)q(x) + r(x),\n\nwhere the degree of r(x) is less than the degree of x2 + x+ 1. Therefore,\n\nf(x) + ⟨x2 + x+ 1⟩ = r(x) + ⟨x2 + x+ 1⟩.\n\nThe only possibilities for r(x) are then 0, 1, x, and 1+x. Consequently, E = Z2[x]/⟨x2+x+1⟩\nis a field with four elements and must be a field extension of Z2, containing a zero α of p(x).\nThe field Z2(α) consists of elements\n\n0 + 0α = 0\n\n1 + 0α = 1\n\n0 + 1α = α\n\n1 + 1α = 1 + α.\n\nNotice that α2 + α+ 1 = 0; hence, if we compute (1 + α)2,\n\n(1 + α)(1 + α) = 1 + α+ α+ (α)2 = α.\n\nOther calculations are accomplished in a similar manner. We summarize these computations\nin the following tables, which tell us how to add and multiply elements in E.\n\n+ 0 1 α 1 + α\n\n0 0 1 α 1 + α\n\n1 1 0 1 + α α\n\nα α 1 + α 0 1\n\n1 + α 1 + α α 1 0\n\nTable 21.3: Addition Table for Z2(α)\n\n· 0 1 α 1 + α\n\n0 0 0 0 0\n\n1 0 1 α 1 + α\n\nα 0 α 1 + α 1\n\n1 + α 0 1 + α 1 α\n\nTable 21.4: Multiplication Table for Z2(α)\n\nThe following theorem, due to Kronecker, is so important and so basic to our under-\nstanding of fields that it is often known as the Fundamental Theorem of Field Theory.\n\nTheorem 21.5. Let F be a field and let p(x) be a nonconstant polynomial in F [x]. Then\nthere exists an extension field E of F and an element α ∈ E such that p(α) = 0.\n\n\n\n356 CHAPTER 21. FIELDS\n\nProof. To prove this theorem, we will employ the method that we used to construct\nExample 21.2. Clearly, we can assume that p(x) is an irreducible polynomial. We wish to\nfind an extension field E of F containing an element α such that p(α) = 0. The ideal ⟨p(x)⟩\ngenerated by p(x) is a maximal ideal in F [x] by Theorem 17.22; hence, F [x]/⟨p(x)⟩ is a\nfield. We claim that E = F [x]/⟨p(x)⟩ is the desired field.\n\nWe first show that E is a field extension of F . We can define a homomorphism of\ncommutative rings by the map ψ : F → F [x]/⟨p(x)⟩, where ψ(a) = a+ ⟨p(x)⟩ for a ∈ F . It\nis easy to check that ψ is indeed a ring homomorphism. Observe that\n\nψ(a) + ψ(b) = (a+ ⟨p(x)⟩) + (b+ ⟨p(x)⟩) = (a+ b) + ⟨p(x)⟩ = ψ(a+ b)\n\nand\nψ(a)ψ(b) = (a+ ⟨p(x)⟩)(b+ ⟨p(x)⟩) = ab+ ⟨p(x)⟩ = ψ(ab).\n\nTo prove that ψ is one-to-one, assume that\n\na+ ⟨p(x)⟩ = ψ(a) = ψ(b) = b+ ⟨p(x)⟩.\n\nThen a−b is a multiple of p(x), since it lives in the ideal ⟨p(x)⟩. Since p(x) is a nonconstant\npolynomial, the only possibility is that a − b = 0. Consequently, a = b and ψ is injective.\nSince ψ is one-to-one, we can identify F with the subfield {a + ⟨p(x)⟩ : a ∈ F} of E and\nview E as an extension field of F .\n\nIt remains for us to prove that p(x) has a zero α ∈ E. Set α = x+ ⟨p(x)⟩. Then α is in\nE. If p(x) = a0 + a1x+ · · ·+ anx\n\nn, then\n\np(α) = a0 + a1(x+ ⟨p(x)⟩) + · · ·+ an(x+ ⟨p(x)⟩)n\n\n= a0 + (a1x+ ⟨p(x)⟩) + · · ·+ (anx\nn + ⟨p(x)⟩)\n\n= a0 + a1x+ · · ·+ anx\nn + ⟨p(x)⟩\n\n= 0 + ⟨p(x)⟩.\n\nTherefore, we have found an element α ∈ E = F [x]/⟨p(x)⟩ such that α is a zero of p(x).\n\nExample 21.6. Let p(x) = x5+x4+1 ∈ Z2[x]. Then p(x) has irreducible factors x2+x+1\nand x3 + x+ 1. For a field extension E of Z2 such that p(x) has a root in E, we can let E\nbe either Z2[x]/⟨x2 + x+ 1⟩ or Z2[x]/⟨x3 + x+ 1⟩. We will leave it as an exercise to show\nthat Z2[x]/⟨x3 + x+ 1⟩ is a field with 23 = 8 elements.\n\nAlgebraic Elements\nAn element α in an extension field E over F is algebraic over F if f(α) = 0 for some nonzero\npolynomial f(x) ∈ F [x]. An element in E that is not algebraic over F is transcendental\nover F . An extension field E of a field F is an algebraic extension of F if every element\nin E is algebraic over F . If E is a field extension of F and α1, . . . , αn are contained in E,\nwe denote the smallest field containing F and α1, . . . , αn by F (α1, . . . , αn). If E = F (α)\nfor some α ∈ E, then E is a simple extension of F .\n\nExample 21.7. Both\n√\n2 and i are algebraic over Q since they are zeros of the polynomials\n\nx2−2 and x2+1, respectively. Clearly π and e are algebraic over the real numbers; however,\nit is a nontrivial fact that they are transcendental over Q. Numbers in R that are algebraic\nover Q are in fact quite rare. Almost all real numbers are transcendental over Q.1 (In many\ncases we do not know whether or not a particular number is transcendental; for example,\nit is still not known whether π + e is transcendental or algebraic.)\n\n1The probability that a real number chosen at random from the interval [0, 1] will be transcendental over\nthe rational numbers is one.\n\n\n\n21.1. EXTENSION FIELDS 357\n\nA complex number that is algebraic over Q is an algebraic number. A transcendental\nnumber is an element of C that is transcendental over Q.\nExample 21.8. We will show that\n\n√\n2 +\n\n√\n3 is algebraic over Q. If α =\n\n√\n2 +\n\n√\n3, then\n\nα2 = 2 +\n√\n3. Hence, α2 − 2 =\n\n√\n3 and (α2 − 2)2 = 3. Since α4 − 4α2 + 1 = 0, it must be\n\ntrue that α is a zero of the polynomial x4 − 4x2 + 1 ∈ Q[x].\nIt is very easy to give an example of an extension field E over a field F , where E contains\n\nan element transcendental over F . The following theorem characterizes transcendental\nextensions.\nTheorem 21.9. Let E be an extension field of F and α ∈ E. Then α is transcendental\nover F if and only if F (α) is isomorphic to F (x), the field of fractions of F [x].\nProof. Let ϕα : F [x] → E be the evaluation homomorphism for α. Then α is transcenden-\ntal over F if and only if ϕα(p(x)) = p(α) ̸= 0 for all nonconstant polynomials p(x) ∈ F [x].\nThis is true if and only if kerϕα = {0}; that is, it is true exactly when ϕα is one-to-one.\nHence, E must contain a copy of F [x]. The smallest field containing F [x] is the field of\nfractions F (x). By Theorem 18.4, E must contain a copy of this field.\n\nWe have a more interesting situation in the case of algebraic extensions.\nTheorem 21.10. Let E be an extension field of a field F and α ∈ E with α algebraic over\nF . Then there is a unique irreducible monic polynomial p(x) ∈ F [x] of smallest degree such\nthat p(α) = 0. If f(x) is another polynomial in F [x] such that f(α) = 0, then p(x) divides\nf(x).\nProof. Let ϕα : F [x] → E be the evaluation homomorphism. The kernel of ϕα is a\nprincipal ideal generated by some p(x) ∈ F [x] with deg p(x) ≥ 1. We know that such a\npolynomial exists, since F [x] is a principal ideal domain and α is algebraic. The ideal ⟨p(x)⟩\nconsists exactly of those elements of F [x] having α as a zero. If f(α) = 0 and f(x) is not\nthe zero polynomial, then f(x) ∈ ⟨p(x)⟩ and p(x) divides f(x). So p(x) is a polynomial of\nminimal degree having α as a zero. Any other polynomial of the same degree having α as\na zero must have the form βp(x) for some β ∈ F .\n\nSuppose now that p(x) = r(x)s(x) is a factorization of p(x) into polynomials of lower\ndegree. Since p(α) = 0, r(α)s(α) = 0; consequently, either r(α) = 0 or s(α) = 0, which\ncontradicts the fact that p is of minimal degree. Therefore, p(x) must be irreducible.\n\nLet E be an extension field of F and α ∈ E be algebraic over F . The unique monic\npolynomial p(x) of the last theorem is called the minimal polynomial for α over F . The\ndegree of p(x) is the degree of α over F .\nExample 21.11. Let f(x) = x2 − 2 and g(x) = x4 − 4x2 + 1. These polynomials are the\nminimal polynomials of\n\n√\n2 and\n\n√\n2 +\n\n√\n3, respectively.\n\nProposition 21.12. Let E be a field extension of F and α ∈ E be algebraic over F . Then\nF (α) ∼= F [x]/⟨p(x)⟩, where p(x) is the minimal polynomial of α over F .\nProof. Let ϕα : F [x] → E be the evaluation homomorphism. The kernel of this map is\n⟨p(x)⟩, where p(x) is the minimal polynomial of α. By the First Isomorphism Theorem for\nrings, the image of ϕα in E is isomorphic to F (α) since it contains both F and α.\n\nTheorem 21.13. Let E = F (α) be a simple extension of F , where α ∈ E is algebraic over\nF . Suppose that the degree of α over F is n. Then every element β ∈ E can be expressed\nuniquely in the form\n\nβ = b0 + b1α+ · · ·+ bn−1α\nn−1\n\nfor bi ∈ F .\n\n\n\n358 CHAPTER 21. FIELDS\n\nProof. Since ϕα(F [x]) ∼= F (α), every element in E = F (α) must be of the form ϕα(f(x)) =\nf(α), where f(α) is a polynomial in α with coefficients in F . Let\n\np(x) = xn + an−1x\nn−1 + · · ·+ a0\n\nbe the minimal polynomial of α. Then p(α) = 0; hence,\n\nαn = −an−1α\nn−1 − · · · − a0.\n\nSimilarly,\n\nαn+1 = ααn\n\n= −an−1α\nn − an−2α\n\nn−1 − · · · − a0α\n\n= −an−1(−an−1α\nn−1 − · · · − a0)− an−2α\n\nn−1 − · · · − a0α.\n\nContinuing in this manner, we can express every monomial αm, m ≥ n, as a linear combi-\nnation of powers of α that are less than n. Hence, any β ∈ F (α) can be written as\n\nβ = b0 + b1α+ · · ·+ bn−1α\nn−1.\n\nTo show uniqueness, suppose that\n\nβ = b0 + b1α+ · · ·+ bn−1α\nn−1 = c0 + c1α+ · · ·+ cn−1α\n\nn−1\n\nfor bi and ci in F . Then\n\ng(x) = (b0 − c0) + (b1 − c1)x+ · · ·+ (bn−1 − cn−1)x\nn−1\n\nis in F [x] and g(α) = 0. Since the degree of g(x) is less than the degree of p(x), the\nirreducible polynomial of α, g(x) must be the zero polynomial. Consequently,\n\nb0 − c0 = b1 − c1 = · · · = bn−1 − cn−1 = 0,\n\nor bi = ci for i = 0, 1, . . . , n− 1. Therefore, we have shown uniqueness.\n\nExample 21.14. Since x2+1 is irreducible over R, ⟨x2+1⟩ is a maximal ideal in R[x]. So\nE = R[x]/⟨x2+1⟩ is a field extension of R that contains a root of x2+1. Let α = x+⟨x2+1⟩.\nWe can identify E with the complex numbers. By Proposition 21.12, E is isomorphic to\nR(α) = {a+ bα : a, b ∈ R}. We know that α2 = −1 in E, since\n\nα2 + 1 = (x+ ⟨x2 + 1⟩)2 + (1 + ⟨x2 + 1⟩)\n= (x2 + 1) + ⟨x2 + 1⟩\n= 0.\n\nHence, we have an isomorphism of R(α) with C defined by the map that takes a + bα to\na+ bi.\n\nLet E be a field extension of a field F . If we regard E as a vector space over F , then we\ncan bring the machinery of linear algebra to bear on the problems that we will encounter in\nour study of fields. The elements in the field E are vectors; the elements in the field F are\nscalars. We can think of addition in E as adding vectors. When we multiply an element in\nE by an element of F , we are multiplying a vector by a scalar. This view of field extensions\nis especially fruitful if a field extension E of F is a finite dimensional vector space over F ,\n\n\n\n21.1. EXTENSION FIELDS 359\n\nand Theorem 21.13 states that E = F (α) is finite dimensional vector space over F with\nbasis {1, α, α2, . . . , αn−1}.\n\nIf an extension field E of a field F is a finite dimensional vector space over F of dimension\nn, then we say that E is a finite extension of degree n over F . We write\n\n[E : F ] = n.\n\nto indicate the dimension of E over F .\n\nTheorem 21.15. Every finite extension field E of a field F is an algebraic extension.\n\nProof. Let α ∈ E. Since [E : F ] = n, the elements\n\n1, α, . . . , αn\n\ncannot be linearly independent. Hence, there exist ai ∈ F , not all zero, such that\n\nanα\nn + an−1α\n\nn−1 + · · ·+ a1α+ a0 = 0.\n\nTherefore,\np(x) = anx\n\nn + · · ·+ a0 ∈ F [x]\n\nis a nonzero polynomial with p(α) = 0.\n\nRemark 21.16. Theorem 21.15 says that every finite extension of a field F is an algebraic\nextension. The converse is false, however. We will leave it as an exercise to show that the\nset of all elements in R that are algebraic over Q forms an infinite field extension of Q.\n\nThe next theorem is a counting theorem, similar to Lagrange’s Theorem in group theory.\nTheorem 21.17 will prove to be an extremely useful tool in our investigation of finite field\nextensions.\n\nTheorem 21.17. If E is a finite extension of F and K is a finite extension of E, then K\nis a finite extension of F and\n\n[K : F ] = [K : E][E : F ].\n\nProof. Let {α1, . . . , αn} be a basis for E as a vector space over F and {β1, . . . , βm} be a\nbasis for K as a vector space over E. We claim that {αiβj} is a basis for K over F . We will\nfirst show that these vectors span K. Let u ∈ K. Then u =\n\n∑m\nj=1 bjβj and bj =\n\n∑n\ni=1 aijαi,\n\nwhere bj ∈ E and aij ∈ F . Then\n\nu =\n\nm∑\nj=1\n\n(\nn∑\n\ni=1\n\naijαi\n\n)\nβj =\n\n∑\ni,j\n\naij(αiβj).\n\nSo the mn vectors αiβj must span K over F .\nWe must show that {αiβj} are linearly independent. Recall that a set of vectors\n\nv1, v2, . . . , vn in a vector space V are linearly independent if\n\nc1v1 + c2v2 + · · ·+ cnvn = 0\n\nimplies that\nc1 = c2 = · · · = cn = 0.\n\nLet\nu =\n\n∑\ni,j\n\ncij(αiβj) = 0\n\n\n\n360 CHAPTER 21. FIELDS\n\nfor cij ∈ F . We need to prove that all of the cij ’s are zero. We can rewrite u as\nm∑\nj=1\n\n(\nn∑\n\ni=1\n\ncijαi\n\n)\nβj = 0,\n\nwhere\n∑\n\ni cijαi ∈ E. Since the βj ’s are linearly independent over E, it must be the case\nthat\n\nn∑\ni=1\n\ncijαi = 0\n\nfor all j. However, the αj are also linearly independent over F . Therefore, cij = 0 for all i\nand j, which completes the proof.\n\nThe following corollary is easily proved using mathematical induction.\n\nCorollary 21.18. If Fi is a field for i = 1, . . . , k and Fi+1 is a finite extension of Fi, then\nFk is a finite extension of F1 and\n\n[Fk : F1] = [Fk : Fk−1] · · · [F2 : F1].\n\nCorollary 21.19. Let E be an extension field of F . If α ∈ E is algebraic over F with\nminimal polynomial p(x) and β ∈ F (α) with minimal polynomial q(x), then deg q(x) divides\ndeg p(x).\n\nProof. We know that deg p(x) = [F (α) : F ] and deg q(x) = [F (β) : F ]. Since F ⊂ F (β) ⊂\nF (α),\n\n[F (α) : F ] = [F (α) : F (β)][F (β) : F ].\n\nExample 21.20. Let us determine an extension field of Q containing\n√\n3 +\n\n√\n5. It is easy\n\nto determine that the minimal polynomial of\n√\n3 +\n\n√\n5 is x4 − 16x2 + 4. It follows that\n\n[Q(\n√\n3 +\n\n√\n5 ) : Q] = 4.\n\nWe know that {1,\n√\n3 } is a basis for Q(\n\n√\n3 ) over Q. Hence,\n\n√\n3 +\n\n√\n5 cannot be in\n\nQ(\n√\n3 ). It follows that\n\n√\n5 cannot be in Q(\n\n√\n3 ) either. Therefore, {1,\n\n√\n5 } is a basis\n\nfor Q(\n√\n3,\n√\n5 ) = (Q(\n\n√\n3 ))(\n\n√\n5 ) over Q(\n\n√\n3 ) and {1,\n\n√\n3,\n√\n5,\n√\n3\n√\n5 =\n\n√\n15 } is a basis for\n\nQ(\n√\n3,\n√\n5 ) = Q(\n\n√\n3 +\n\n√\n5 ) over Q. This example shows that it is possible that some\n\nextension F (α1, . . . , αn) is actually a simple extension of F even though n > 1.\n\nExample 21.21. Let us compute a basis for Q( 3\n√\n5,\n√\n5 i), where\n\n√\n5 is the positive square\n\nroot of 5 and 3\n√\n5 is the real cube root of 5. We know that\n\n√\n5 i /∈ Q( 3\n\n√\n5 ), so\n\n[Q(\n3\n√\n5,\n√\n5 i) : Q(\n\n3\n√\n5 )] = 2.\n\nIt is easy to determine that {1,\n√\n5i } is a basis for Q( 3\n\n√\n5,\n√\n5 i) over Q( 3\n\n√\n5 ). We also know\n\nthat {1, 3\n√\n5, ( 3\n\n√\n5 )2} is a basis for Q( 3\n\n√\n5 ) over Q. Hence, a basis for Q( 3\n\n√\n5,\n√\n5 i) over Q is\n\n{1,\n√\n5 i,\n\n3\n√\n5, (\n\n3\n√\n5 )2, (\n\n6\n√\n5 )5i, (\n\n6\n√\n5 )7i = 5\n\n6\n√\n5 i or 6\n\n√\n5 i}.\n\nNotice that 6\n√\n5 i is a zero of x6 + 5. We can show that this polynomial is irreducible over\n\nQ using Eisenstein’s Criterion, where we let p = 5. Consequently,\n\nQ ⊂ Q(\n6\n√\n5 i) ⊂ Q(\n\n3\n√\n5,\n√\n5 i).\n\nBut it must be the case that Q( 6\n√\n5 i) = Q( 3\n\n√\n5,\n√\n5 i), since the degree of both of these\n\nextensions is 6.\n\n\n\n21.1. EXTENSION FIELDS 361\n\nTheorem 21.22. Let E be a field extension of F . Then the following statements are\nequivalent.\n\n1. E is a finite extension of F .\n\n2. There exists a finite number of algebraic elements α1, . . . , αn ∈ E such that E =\nF (α1, . . . , αn).\n\n3. There exists a sequence of fields\n\nE = F (α1, . . . , αn) ⊃ F (α1, . . . , αn−1) ⊃ · · · ⊃ F (α1) ⊃ F,\n\nwhere each field F (α1, . . . , αi) is algebraic over F (α1, . . . , αi−1).\n\nProof. (1) ⇒ (2). Let E be a finite algebraic extension of F . Then E is a finite dimensional\nvector space over F and there exists a basis consisting of elements α1, . . . , αn in E such that\nE = F (α1, . . . , αn). Each αi is algebraic over F by Theorem 21.15.\n\n(2) ⇒ (3). Suppose that E = F (α1, . . . , αn), where every αi is algebraic over F . Then\n\nE = F (α1, . . . , αn) ⊃ F (α1, . . . , αn−1) ⊃ · · · ⊃ F (α1) ⊃ F,\n\nwhere each field F (α1, . . . , αi) is algebraic over F (α1, . . . , αi−1).\n(3) ⇒ (1). Let\n\nE = F (α1, . . . , αn) ⊃ F (α1, . . . , αn−1) ⊃ · · · ⊃ F (α1) ⊃ F,\n\nwhere each field F (α1, . . . , αi) is algebraic over F (α1, . . . , αi−1). Since\n\nF (α1, . . . , αi) = F (α1, . . . , αi−1)(αi)\n\nis simple extension and αi is algebraic over F (α1, . . . , αi−1), it follows that\n\n[F (α1, . . . , αi) : F (α1, . . . , αi−1)]\n\nis finite for each i. Therefore, [E : F ] is finite.\n\nAlgebraic Closure\nGiven a field F , the question arises as to whether or not we can find a field E such that\nevery polynomial p(x) has a root in E. This leads us to the following theorem.\n\nTheorem 21.23. Let E be an extension field of F . The set of elements in E that are\nalgebraic over F form a field.\n\nProof. Let α, β ∈ E be algebraic over F . Then F (α, β) is a finite extension of F . Since\nevery element of F (α, β) is algebraic over F , α ± β, αβ, and α/β (β ̸= 0) are all algebraic\nover F . Consequently, the set of elements in E that are algebraic over F form a field.\n\nCorollary 21.24. The set of all algebraic numbers forms a field; that is, the set of all\ncomplex numbers that are algebraic over Q makes up a field.\n\nLet E be a field extension of a field F . We define the algebraic closure of a field F\nin E to be the field consisting of all elements in E that are algebraic over F . A field F is\nalgebraically closed if every nonconstant polynomial in F [x] has a root in F .\n\nTheorem 21.25. A field F is algebraically closed if and only if every nonconstant polyno-\nmial in F [x] factors into linear factors over F [x].\n\n\n\n362 CHAPTER 21. FIELDS\n\nProof. Let F be an algebraically closed field. If p(x) ∈ F [x] is a nonconstant polynomial,\nthen p(x) has a zero in F , say α. Therefore, x − α must be a factor of p(x) and so\np(x) = (x − α)q1(x), where deg q1(x) = deg p(x) − 1. Continue this process with q1(x) to\nfind a factorization\n\np(x) = (x− α)(x− β)q2(x),\n\nwhere deg q2(x) = deg p(x)− 2. The process must eventually stop since the degree of p(x)\nis finite.\n\nConversely, suppose that every nonconstant polynomial p(x) in F [x] factors into linear\nfactors. Let ax − b be such a factor. Then p(b/a) = 0. Consequently, F is algebraically\nclosed.\n\nCorollary 21.26. An algebraically closed field F has no proper algebraic extension E.\n\nProof. Let E be an algebraic extension of F ; then F ⊂ E. For α ∈ E, the minimal\npolynomial of α is x− α. Therefore, α ∈ F and F = E.\n\nTheorem 21.27. Every field F has a unique algebraic closure.\n\nIt is a nontrivial fact that every field has a unique algebraic closure. The proof is not\nextremely difficult, but requires some rather sophisticated set theory. We refer the reader\nto [3], [4], or [8] for a proof of this result.\n\nWe now state the Fundamental Theorem of Algebra, first proven by Gauss at the age\nof 22 in his doctoral thesis. This theorem states that every polynomial with coefficients in\nthe complex numbers has a root in the complex numbers. The proof of this theorem will\nbe given in Chapter 23.\n\nTheorem 21.28 (Fundamental Theorem of Algebra). The field of complex numbers is\nalgebraically closed.\n\n21.2 Splitting Fields\nLet F be a field and p(x) be a nonconstant polynomial in F [x]. We already know that we\ncan find a field extension of F that contains a root of p(x). However, we would like to know\nwhether an extension E of F containing all of the roots of p(x) exists. In other words, can\nwe find a field extension of F such that p(x) factors into a product of linear polynomials?\nWhat is the “smallest” extension containing all the roots of p(x)?\n\nLet F be a field and p(x) = a0 + a1x+ · · ·+ anx\nn be a nonconstant polynomial in F [x].\n\nAn extension field E of F is a splitting field of p(x) if there exist elements α1, . . . , αn in\nE such that E = F (α1, . . . , αn) and\n\np(x) = (x− α1)(x− α2) · · · (x− αn).\n\nA polynomial p(x) ∈ F [x] splits in E if it is the product of linear factors in E[x].\n\nExample 21.29. Let p(x) = x4 + 2x2 − 8 be in Q[x]. Then p(x) has irreducible factors\nx2 − 2 and x2 + 4. Therefore, the field Q(\n\n√\n2, i) is a splitting field for p(x).\n\nExample 21.30. Let p(x) = x3 − 3 be in Q[x]. Then p(x) has a root in the field Q( 3\n√\n3 ).\n\nHowever, this field is not a splitting field for p(x) since the complex cube roots of 3,\n\n− 3\n√\n3± ( 6\n\n√\n3 )5i\n\n2\n,\n\nare not in Q( 3\n√\n3 ).\n\n\n\n21.2. SPLITTING FIELDS 363\n\nTheorem 21.31. Let p(x) ∈ F [x] be a nonconstant polynomial. Then there exists a splitting\nfield E for p(x).\n\nProof. We will use mathematical induction on the degree of p(x). If deg p(x) = 1, then\np(x) is a linear polynomial and E = F . Assume that the theorem is true for all polynomials\nof degree k with 1 ≤ k < n and let deg p(x) = n. We can assume that p(x) is irreducible;\notherwise, by our induction hypothesis, we are done. By Theorem 21.5, there exists a field\nK such that p(x) has a zero α1 in K. Hence, p(x) = (x − α1)q(x), where q(x) ∈ K[x].\nSince deg q(x) = n− 1, there exists a splitting field E ⊃ K of q(x) that contains the zeros\nα2, . . . , αn of p(x) by our induction hypothesis. Consequently,\n\nE = K(α2, . . . , αn) = F (α1, . . . , αn)\n\nis a splitting field of p(x).\n\nThe question of uniqueness now arises for splitting fields. This question is answered in\nthe affirmative. Given two splitting fields K and L of a polynomial p(x) ∈ F [x], there exists\na field isomorphism ϕ : K → L that preserves F . In order to prove this result, we must first\nprove a lemma.\n\nLemma 21.32. Let ϕ : E → F be an isomorphism of fields. Let K be an extension field\nof E and α ∈ K be algebraic over E with minimal polynomial p(x). Suppose that L is\nan extension field of F such that β is root of the polynomial in F [x] obtained from p(x)\nunder the image of ϕ. Then ϕ extends to a unique isomorphism ϕ : E(α) → F (β) such that\nϕ(α) = β and ϕ agrees with ϕ on E.\n\nProof. If p(x) has degree n, then by Theorem 21.13 we can write any element in E(α)\nas a linear combination of 1, α, . . . , αn−1. Therefore, the isomorphism that we are seeking\nmust be\n\nϕ(a0 + a1α+ · · ·+ an−1α\nn−1) = ϕ(a0) + ϕ(a1)β + · · ·+ ϕ(an−1)β\n\nn−1,\n\nwhere\na0 + a1α+ · · ·+ an−1α\n\nn−1\n\nis an element in E(α). The fact that ϕ is an isomorphism could be checked by direct\ncomputation; however, it is easier to observe that ϕ is a composition of maps that we\nalready know to be isomorphisms.\n\nWe can extend ϕ to be an isomorphism from E[x] to F [x], which we will also denote by\nϕ, by letting\n\nϕ(a0 + a1x+ · · ·+ anx\nn) = ϕ(a0) + ϕ(a1)x+ · · ·+ ϕ(an)x\n\nn.\n\nThis extension agrees with the original isomorphism ϕ : E → F , since constant polynomials\nget mapped to constant polynomials. By assumption, ϕ(p(x)) = q(x); hence, ϕ maps ⟨p(x)⟩\nonto ⟨q(x)⟩. Consequently, we have an isomorphism ψ : E[x]/⟨p(x)⟩ → F [x]/⟨q(x)⟩. By\nProposition 21.12, we have isomorphisms σ : E[x]/⟨p(x)⟩ → E(α) and τ : F [x]/⟨q(x)⟩ →\nF (β), defined by evaluation at α and β, respectively. Therefore, ϕ = τψσ−1 is the required\nisomorphism.\n\n\n\n364 CHAPTER 21. FIELDS\n\nE F\n\nE(α) F (β)\n\nE[x]/⟨p(x)⟩ F [x]/⟨q(x)⟩\n\nϕ\n\nϕ\n\nψ\n\nσ τ\n\nWe leave the proof of uniqueness as a exercise.\n\nTheorem 21.33. Let ϕ : E → F be an isomorphism of fields and let p(x) be a nonconstant\npolynomial in E[x] and q(x) the corresponding polynomial in F [x] under the isomorphism.\nIf K is a splitting field of p(x) and L is a splitting field of q(x), then ϕ extends to an\nisomorphism ψ : K → L.\n\nProof. We will use mathematical induction on the degree of p(x). We can assume that\np(x) is irreducible over E. Therefore, q(x) is also irreducible over F . If deg p(x) = 1, then\nby the definition of a splitting field, K = E and L = F and there is nothing to prove.\n\nAssume that the theorem holds for all polynomials of degree less than n. Since K is a\nsplitting field of p(x), all of the roots of p(x) are in K. Choose one of these roots, say α, such\nthat E ⊂ E(α) ⊂ K. Similarly, we can find a root β of q(x) in L such that F ⊂ F (β) ⊂ L.\nBy Lemma 21.32, there exists an isomorphism ϕ : E(α) → F (β) such that ϕ(α) = β and ϕ\nagrees with ϕ on E.\n\nE F\n\nE(α) F (β)\n\nK L\n\nϕ\n\nϕ\n\nψ\n\nNow write p(x) = (x − α)f(x) and q(x) = (x − β)g(x), where the degrees of f(x) and\ng(x) are less than the degrees of p(x) and q(x), respectively. The field extension K is a\nsplitting field for f(x) over E(α), and L is a splitting field for g(x) over F (β). By our\ninduction hypothesis there exists an isomorphism ψ : K → L such that ψ agrees with ϕ on\nE(α). Hence, there exists an isomorphism ψ : K → L such that ψ agrees with ϕ on E.\n\nCorollary 21.34. Let p(x) be a polynomial in F [x]. Then there exists a splitting field K\nof p(x) that is unique up to isomorphism.\n\n21.3 Geometric Constructions\nIn ancient Greece, three classic problems were posed. These problems are geometric in\nnature and involve straightedge-and-compass constructions from what is now high school\ngeometry; that is, we are allowed to use only a straightedge and compass to solve them.\nThe problems can be stated as follows.\n\n\n\n21.3. GEOMETRIC CONSTRUCTIONS 365\n\n1. Given an arbitrary angle, can one trisect the angle into three equal subangles using\nonly a straightedge and compass?\n\n2. Given an arbitrary circle, can one construct a square with the same area using only a\nstraightedge and compass?\n\n3. Given a cube, can one construct the edge of another cube having twice the volume of\nthe original? Again, we are only allowed to use a straightedge and compass to do the\nconstruction.\n\nAfter puzzling mathematicians for over two thousand years, each of these constructions\nwas finally shown to be impossible. We will use the theory of fields to provide a proof that\nthe solutions do not exist. It is quite remarkable that the long-sought solution to each of\nthese three geometric problems came from abstract algebra.\n\nFirst, let us determine more specifically what we mean by a straightedge and compass,\nand also examine the nature of these problems in a bit more depth. To begin with, a\nstraightedge is not a ruler. We cannot measure arbitrary lengths with a straightedge. It is\nmerely a tool for drawing a line through two points. The statement that the trisection of\nan arbitrary angle is impossible means that there is at least one angle that is impossible to\ntrisect with a straightedge-and-compass construction. Certainly it is possible to trisect an\nangle in special cases. We can construct a 30◦ angle; hence, it is possible to trisect a 90◦\n\nangle. However, we will show that it is impossible to construct a 20◦ angle. Therefore, we\ncannot trisect a 60◦ angle.\n\nConstructible Numbers\nA real number α is constructible if we can construct a line segment of length |α| in a finite\nnumber of steps from a segment of unit length by using a straightedge and compass.\n\nTheorem 21.35. The set of all constructible real numbers forms a subfield F of the field\nof real numbers.\n\nProof. Let α and β be constructible numbers. We must show that α+ β, α− β, αβ, and\nα/β (β ̸= 0) are also constructible numbers. We can assume that both α and β are positive\nwith α > β. It is quite obvious how to construct α + β and α − β. To find a line segment\nwith length αβ, we assume that β > 1 and construct the triangle in Figure 21.36 such that\ntriangles △ABC and △ADE are similar. Since α/1 = x/β, the line segment x has length\nαβ. A similar construction can be made if β < 1. We will leave it as an exercise to show\nthat the same triangle can be used to construct α/β for β ̸= 0.\n\nA E\n\nB\n\nC\n\nD\n\n1\n\nα\n\nβ\n\nx\n\nFigure 21.36: Construction of products\n\nLemma 21.37. If α is a constructible number, then\n√\nα is a constructible number.\n\n\n\n366 CHAPTER 21. FIELDS\n\nProof. In Figure 21.38 the triangles △ABD, △BCD, and △ABC are similar; hence,\n1/x = x/α, or x2 = α.\n\nDA C\n\nB\n\nα1\n\nx\n\nFigure 21.38: Construction of roots\n\nBy Theorem 21.35, we can locate in the plane any point P = (p, q) that has rational\ncoordinates p and q. We need to know what other points can be constructed with a compass\nand straightedge from points with rational coordinates.\n\nLemma 21.39. Let F be a subfield of R.\n\n1. If a line contains two points in F , then it has the equation ax+ by + c = 0, where a,\nb, and c are in F .\n\n2. If a circle has a center at a point with coordinates in F and a radius that is also in\nF , then it has the equation x2 + y2 + dx+ ey + f = 0, where d, e, and f are in F .\n\nProof. Let (x1, y1) and (x2, y2) be points on a line whose coordinates are in F . If x1 = x2,\nthen the equation of the line through the two points is x − x1 = 0, which has the form\nax+ by+ c = 0. If x1 ̸= x2, then the equation of the line through the two points is given by\n\ny − y1 =\n\n(\ny2 − y1\nx2 − x1\n\n)\n(x− x1),\n\nwhich can also be put into the proper form.\nTo prove the second part of the lemma, suppose that (x1, y1) is the center of a circle of\n\nradius r. Then the circle has the equation\n\n(x− x1)\n2 + (y − y1)\n\n2 − r2 = 0.\n\nThis equation can easily be put into the appropriate form.\n\nStarting with a field of constructible numbers F , we have three possible ways of con-\nstructing additional points in R with a compass and straightedge.\n\n1. To find possible new points in R, we can take the intersection of two lines, each of\nwhich passes through two known points with coordinates in F .\n\n2. The intersection of a line that passes through two points that have coordinates in F\nand a circle whose center has coordinates in F with radius of a length in F will give\nnew points in R.\n\n3. We can obtain new points in R by intersecting two circles whose centers have coordi-\nnates in F and whose radii are of lengths in F .\n\n\n\n21.3. GEOMETRIC CONSTRUCTIONS 367\n\nThe first case gives no new points in R, since the solution of two equations of the form\nax+ by+ c = 0 having coefficients in F will always be in F . The third case can be reduced\nto the second case. Let\n\nx2 + y2 + d1x+ e1y + f1 = 0\n\nx2 + y2 + d2x+ e2y + f2 = 0\n\nbe the equations of two circles, where di, ei, and fi are in F for i = 1, 2. These circles have\nthe same intersection as the circle\n\nx2 + y2 + d1x+ e1x+ f1 = 0\n\nand the line\n(d1 − d2)x+ b(e2 − e1)y + (f2 − f1) = 0.\n\nThe last equation is that of the chord passing through the intersection points of the two\ncircles. Hence, the intersection of two circles can be reduced to the case of an intersection\nof a line with a circle.\n\nConsidering the case of the intersection of a line and a circle, we must determine the\nnature of the solutions of the equations\n\nax+ by + c = 0\n\nx2 + y2 + dx+ ey + f = 0.\n\nIf we eliminate y from these equations, we obtain an equation of the form Ax2+Bx+C = 0,\nwhere A, B, and C are in F . The x coordinate of the intersection points is given by\n\nx =\n−B ±\n\n√\nB2 − 4AC\n\n2A\n\nand is in F (\n√\nα ), where α = B2 − 4AC > 0. We have proven the following lemma.\n\nLemma 21.40. Let F be a field of constructible numbers. Then the points determined by\nthe intersections of lines and circles in F lie in the field F (\n\n√\nα ) for some α in F .\n\nTheorem 21.41. A real number α is a constructible number if and only if there exists a\nsequence of fields\n\nQ = F0 ⊂ F1 ⊂ · · · ⊂ Fk\n\nsuch that Fi = Fi−1(\n√\nαi ) with αi ∈ Fi and α ∈ Fk. In particular, there exists an integer\n\nk > 0 such that [Q(α) : Q] = 2k.\n\nProof. The existence of the Fi’s and the αi’s is a direct consequence of Lemma 21.40 and\nof the fact that\n\n[Fk : Q] = [Fk : Fk−1][Fk−1 : Fk−2] · · · [F1 : Q] = 2k.\n\nCorollary 21.42. The field of all constructible numbers is an algebraic extension of Q.\n\nAs we can see by the field of constructible numbers, not every algebraic extension of a\nfield is a finite extension.\n\n\n\n368 CHAPTER 21. FIELDS\n\nDoubling the Cube and Squaring the Circle\nWe are now ready to investigate the classical problems of doubling the cube and squaring\nthe circle. We can use the field of constructible numbers to show exactly when a particular\ngeometric construction can be accomplished.\n\nDoubling the cube is impossible. Given the edge of the cube, it is impossible to construct\nwith a straightedge and compass the edge of the cube that has twice the volume of the\noriginal cube. Let the original cube have an edge of length 1 and, therefore, a volume of 1.\nIf we could construct a cube having a volume of 2, then this new cube would have an edge\nof length 3\n\n√\n2. However, 3\n\n√\n2 is a zero of the irreducible polynomial x3 − 2 over Q; hence,\n\n[Q(\n3\n√\n2 ) : Q] = 3\n\nThis is impossible, since 3 is not a power of 2.\nSquaring the circle. Suppose that we have a circle of radius 1. The area of the circle\n\nis π; therefore, we must be able to construct a square with side\n√\nπ. This is impossible\n\nsince π and consequently\n√\nπ are both transcendental. Therefore, using a straightedge and\n\ncompass, it is not possible to construct a square with the same area as the circle.\n\nTrisecting an Angle\nTrisecting an arbitrary angle is impossible. We will show that it is impossible to construct\na 20◦ angle. Consequently, a 60◦ angle cannot be trisected. We first need to calculate the\ntriple-angle formula for the cosine:\n\ncos 3θ = cos(2θ + θ)\n\n= cos 2θ cos θ − sin 2θ sin θ\n= (2 cos2 θ − 1) cos θ − 2 sin2 θ cos θ\n= (2 cos2 θ − 1) cos θ − 2(1− cos2 θ) cos θ\n= 4 cos3 θ − 3 cos θ.\n\nThe angle θ can be constructed if and only if α = cos θ is constructible. Let θ = 20◦. Then\ncos 3θ = cos 60◦ = 1/2. By the triple-angle formula for the cosine,\n\n4α3 − 3α =\n1\n\n2\n.\n\nTherefore, α is a zero of 8x3 − 6x − 1. This polynomial has no factors in Z[x], and hence\nis irreducible over Q[x]. Thus, [Q(α) : Q] = 3. Consequently, α cannot be a constructible\nnumber.\n\nHistorical Note\n\nAlgebraic number theory uses the tools of algebra to solve problems in number theory.\nModern algebraic number theory began with Pierre de Fermat (1601–1665). Certainly we\ncan find many positive integers that satisfy the equation x2 + y2 = z2; Fermat conjectured\nthat the equation xn+ yn = zn has no positive integer solutions for n ≥ 3. He stated in the\nmargin of his copy of the Latin translation of Diophantus’ Arithmetica that he had found a\nmarvelous proof of this theorem, but that the margin of the book was too narrow to contain\nit. Building on work of other mathematicians, it was Andrew Wiles who finally succeeded\nin proving Fermat’s Last Theorem in the 1990s. Wiles’s achievement was reported on the\nfront page of the New York Times.\n\n\n\n21.4. EXERCISES 369\n\nAttempts to prove Fermat’s Last Theorem have led to important contributions to al-\ngebraic number theory by such notable mathematicians as Leonhard Euler (1707–1783).\nSignificant advances in the understanding of Fermat’s Last Theorem were made by Ernst\nKummer (1810–1893). Kummer’s student, Leopold Kronecker (1823–1891), became one of\nthe leading algebraists of the nineteenth century. Kronecker’s theory of ideals and his study\nof algebraic number theory added much to the understanding of fields.\n\nDavid Hilbert (1862–1943) and Hermann Minkowski (1864–1909) were among the math-\nematicians who led the way in this subject at the beginning of the twentieth century. Hilbert\nand Minkowski were both mathematicians at Göttingen University in Germany. Göttingen\nwas truly one the most important centers of mathematical research during the last two cen-\nturies. The large number of exceptional mathematicians who studied there included Gauss,\nDirichlet, Riemann, Dedekind, Noether, and Weyl.\n\nAndré Weil answered questions in number theory using algebraic geometry, a field of\nmathematics that studies geometry by studying commutative rings. From about 1955 to\n1970, Alexander Grothendieck dominated the field of algebraic geometry. Pierre Deligne,\na student of Grothendieck, solved several of Weil’s number-theoretic conjectures. One of\nthe most recent contributions to algebra and number theory is Gerd Falting’s proof of the\nMordell-Weil conjecture. This conjecture of Mordell and Weil essentially says that certain\npolynomials p(x, y) in Z[x, y] have only a finite number of integral solutions.\n\n21.4 Exercises\n1. Show that each of the following numbers is algebraic over Q by finding the minimal\npolynomial of the number over Q.\n\n(a)\n√\n\n1/3 +\n√\n7\n\n(b)\n√\n3 + 3\n\n√\n5\n\n(c)\n√\n3 +\n\n√\n2 i\n\n(d) cos θ + i sin θ for θ = 2π/n with n ∈ N\n\n(e)\n√\n\n3\n√\n2− i\n\n2. Find a basis for each of the following field extensions. What is the degree of each\nextension?\n(a) Q(\n\n√\n3,\n√\n6 ) over Q\n\n(b) Q( 3\n√\n2, 3\n\n√\n3 ) over Q\n\n(c) Q(\n√\n2, i) over Q\n\n(d) Q(\n√\n3,\n√\n5,\n√\n7 ) over Q\n\n(e) Q(\n√\n2, 3\n\n√\n2 ) over Q\n\n(f) Q(\n√\n8 ) over Q(\n\n√\n2 )\n\n(g) Q(i,\n√\n2 + i,\n\n√\n3 + i) over Q\n\n(h) Q(\n√\n2 +\n\n√\n5 ) over Q(\n\n√\n5 )\n\n(i) Q(\n√\n2,\n√\n6 +\n\n√\n10 ) over Q(\n\n√\n3 +\n\n√\n5 )\n\n3. Find the splitting field for each of the following polynomials.\n\n\n\n370 CHAPTER 21. FIELDS\n\n(a) x4 − 10x2 + 21 over Q\n(b) x4 + 1 over Q\n\n(c) x3 + 2x+ 2 over Z3\n\n(d) x3 − 3 over Q\n\n4. Consider the field extension Q( 4\n√\n3, i) over Q.\n\n(a) Find a basis for the field extension Q( 4\n√\n3, i) over Q. Conclude that [Q( 4\n\n√\n3, i) : Q] = 8.\n\n(b) Find all subfields F of Q( 4\n√\n3, i) such that [F : Q] = 2.\n\n(c) Find all subfields F of Q( 4\n√\n3, i) such that [F : Q] = 4.\n\n5. Show that Z2[x]/⟨x3 + x+ 1⟩ is a field with eight elements. Construct a multiplication\ntable for the multiplicative group of the field.\n\n6. Show that the regular 9-gon is not constructible with a straightedge and compass, but\nthat the regular 20-gon is constructible.\n\n7. Prove that the cosine of one degree (cos 1◦) is algebraic over Q but not constructible.\n\n8. Can a cube be constructed with three times the volume of a given cube?\n\n9. Prove that Q(\n√\n3, 4\n\n√\n3, 8\n\n√\n3, . . .) is an algebraic extension of Q but not a finite extension.\n\n10. Prove or disprove: π is algebraic over Q(π3).\n\n11. Let p(x) be a nonconstant polynomial of degree n in F [x]. Prove that there exists a\nsplitting field E for p(x) such that [E : F ] ≤ n!.\n\n12. Prove or disprove: Q(\n√\n2 ) ∼= Q(\n\n√\n3 ).\n\n13. Prove that the fields Q( 4\n√\n3 ) and Q( 4\n\n√\n3 i) are isomorphic but not equal.\n\n14. Let K be an algebraic extension of E, and E an algebraic extension of F . Prove that\nK is algebraic over F . [ Caution: Do not assume that the extensions are finite.]\n\n15. Prove or disprove: Z[x]/⟨x3 − 2⟩ is a field.\n\n16. Let F be a field of characteristic p. Prove that p(x) = xp − a either is irreducible over\nF or splits in F .\n\n17. Let E be the algebraic closure of a field F . Prove that every polynomial p(x) in F [x]\nsplits in E.\n\n18. If every irreducible polynomial p(x) in F [x] is linear, show that F is an algebraically\nclosed field.\n\n19. Prove that if α and β are constructible numbers such that β ̸= 0, then so is α/β.\n\n20. Show that the set of all elements in R that are algebraic over Q form a field extension\nof Q that is not finite.\n\n21. Let E be an algebraic extension of a field F , and let σ be an automorphism of E leaving\nF fixed. Let α ∈ E. Show that σ induces a permutation of the set of all zeros of the minimal\npolynomial of α that are in E.\n\n22. Show that Q(\n√\n3,\n√\n7 ) = Q(\n\n√\n3 +\n\n√\n7 ). Extend your proof to show that Q(\n\n√\na,\n√\nb ) =\n\nQ(\n√\na+\n\n√\nb ), where gcd(a, b) = 1.\n\n23. Let E be a finite extension of a field F . If [E : F ] = 2, show that E is a splitting field\nof F .\n\n\n\n21.5. REFERENCES AND SUGGESTED READINGS 371\n\n24. Prove or disprove: Given a polynomial p(x) in Z6[x], it is possible to construct a ring\nR such that p(x) has a root in R.\n\n25. Let E be a field extension of F and α ∈ E. Determine [F (α) : F (α3)].\n\n26. Let α, β be transcendental over Q. Prove that either αβ or α+β is also transcendental.\n\n27. Let E be an extension field of F and α ∈ E be transcendental over F . Prove that every\nelement in F (α) that is not in F is also transcendental over F .\n\n21.5 References and Suggested Readings\n[1] Dean, R. A. Elements of Abstract Algebra. Wiley, New York, 1966.\n[2] Dudley, U. A Budget of Trisections. Springer-Verlag, New York, 1987. An interesting\n\nand entertaining account of how not to trisect an angle.\n[3] Fraleigh, J. B. A First Course in Abstract Algebra. 7th ed. Pearson, Upper Saddle\n\nRiver, NJ, 2003.\n[4] Kaplansky, I. Fields and Rings, 2nd ed. University of Chicago Press, Chicago, 1972.\n[5] Klein, F. Famous Problems of Elementary Geometry. Chelsea, New York, 1955.\n[6] Martin, G. Geometric Constructions. Springer, New York, 1998.\n[7] H. Pollard and H. G. Diamond. Theory of Algebraic Numbers, Dover, Mineola, NY,\n\n2010.\n[8] Walker, E. A. Introduction to Abstract Algebra. Random House, New York, 1987.\n\nThis work contains a proof showing that every field has an algebraic closure.\n\n21.6 Sage\nIn Sage, and other places, an extension of the rationals is called a “number field.” They are\none of Sage’s most mature features.\n\nNumber Fields\nThere are several ways to create a number field. We are familiar with the syntax where we\nadjoin an irrational number that we can write with traditional combinations of arithmetic\nand roots.\n\nM.<a> = QQ[sqrt (2)+sqrt (3)]; M\n\nNumber Field in a with defining polynomial x^4 - 10*x^2 + 1\n\nWe can also specify the element we want to adjoin as the root of a monic irreducible\npolynomial. One approach is to construct the polynomial ring first so that the polynomial\nhas the location of its coefficients specified properly.\n\nF.<y> = QQ[]\np = y^3 - 1/4*y^2 - 1/16*y + 1/4\np.is_irreducible ()\n\nTrue\n\nN.<b> = NumberField(p, \' b \' ); N\n\n\n\n372 CHAPTER 21. FIELDS\n\nNumber Field in b with\ndefining polynomial y^3 - 1/4*y^2 - 1/16*y + 1/4\n\nRather than building the whole polynomial ring, we can simply introduce a variable as\nthe generator of a polynomial ring and then create polynomials from this variable. This\nspares us naming the polynomial ring. Notice in the example that both instances of z are\nnecessary.\n\nz = polygen(QQ, \' z \' )\nq = z^3 - 1/4*z^2 - 1/16*z + 1/4\nq.parent ()\n\nUnivariate Polynomial Ring in z over Rational Field\n\nP.<c> = NumberField(q, \' c \' ); P\n\nNumber Field in c with\ndefining polynomial z^3 - 1/4*z^2 - 1/16*z + 1/4\n\nWe can recover the polynomial used to create a number field, even if we constructed\nit by giving an expression for an irrational element. In this case, the polynomial is the\nminimal polynomial of the element.\n\nM.polynomial ()\n\nx^4 - 10*x^2 + 1\n\nN.polynomial ()\n\ny^3 - 1/4*y^2 - 1/16*y + 1/4\n\nFor any element of a number field, Sage will obligingly compute its minimal polynomial.\n\nelement = -b^2 + 1/3*b + 4\nelement.parent ()\n\nNumber Field in b with\ndefining polynomial y^3 - 1/4*y^2 - 1/16*y + 1/4\n\nr = element.minpoly( \' t \' ); r\n\nt^3 - 571/48*t^2 + 108389/2304*t - 13345/216\n\nr.parent ()\n\nUnivariate Polynomial Ring in t over Rational Field\n\nr.subs(t=element)\n\n0\n\nSubstituting element back into the alleged minimal polynomial and getting back zero is\nnot convincing evidence that it is the minimal polynomial, but it is heartening.\n\n\n\n21.6. SAGE 373\n\nRelative and Absolute Number Fields\nWith Sage we can adjoin several elements at once and we can build nested towers of number\nfields. Sage uses the term “absolute” to refer to a number field viewed as an extension of\nthe rationals themselves, and the term “relative” to refer to a number field constructed, or\nviewed, as an extension of another (nontrivial) number field.\n\nA.<a,b> = QQ[sqrt (2), sqrt (3)]\nA\n\nNumber Field in sqrt2 with defining polynomial x^2 - 2 over\nits base field\n\nB = A.base_field (); B\n\nNumber Field in sqrt3 with defining polynomial x^2 - 3\n\nA.is_relative ()\n\nTrue\n\nB.is_relative ()\n\nFalse\n\nThe number field A has been constructed mathematically as what we would write as\nQ ⊂ Q[\n\n√\n3] ⊂ Q[\n\n√\n3,\n√\n2]. Notice the slight difference in ordering of the elements we are\n\nadjoining, and notice how the number fields use slightly fancier internal names (sqrt2, sqrt3)\nfor the new elements.\n\nWe can “flatten” a relative field to view it as an absolute field, which may have been our\nintention from the start. Here we create a new number field from A that makes it a pure\nabsolute number field.\n\nC.<c> = A.absolute_field ()\nC\n\nNumber Field in c with defining polynomial x^4 - 10*x^2 + 1\n\nOnce we construct an absolute number field this way, we can recover isomorphisms to\nand from the absolute field. Recall that our tower was built with generators a and b, while\nthe flattened tower is generated by c. The .structure() method returns a pair of functions,\nwith the absolute number field as the domain and codomain (in that order).\n\nfromC , toC = C.structure ()\nfromC(c)\n\nsqrt2 - sqrt3\n\ntoC(a)\n\n1/2*c^3 - 9/2*c\n\ntoC(b)\n\n1/2*c^3 - 11/2*c\n\n\n\n374 CHAPTER 21. FIELDS\n\nThis tells us that the single generator of the flattened tower, c, is equal to\n√\n2 −\n\n√\n3,\n\nand further, each of\n√\n2 and\n\n√\n3 can be expressed as polynomial functions of c. With\n\nthese connections, you might want to compute the final two expressions in c by hand, and\nappreciate the work Sage does to determine these for us. This computation is an example\nof the conclusion of the upcoming Theorem 23.12.\n\nMany number field methods have both relative and absolute versions, and we will also\nfind it more convenient to work in a tower or a flattened version, thus the isomorphisms\nbetween the two can be invaluable for translating both questions and answers.\n\nAs a vector space over Q, or over another number field, number fields that are finite\nextensions have a dimension, called the degree. These are easy to get from Sage, though\nfor a relative field, we need to be more precise about which degree we desire.\n\nB.degree ()\n\n2\n\nA.absolute_degree ()\n\n4\n\nA.relative_degree ()\n\n2\n\nSplitting Fields\nHere is a concrete example of how to use Sage to construct a splitting field of a polynomial.\nConsider p(x) = x4 + x2 − 1. We first build a number field with a single root, and then\nfactor the polynomial over this new, larger, field.\n\nx = polygen(QQ, \' x \' )\np = x^4 + x^2 - 1\np.parent ()\n\nUnivariate Polynomial Ring in x over Rational Field\n\np.is_irreducible ()\n\nTrue\n\nM.<a> = NumberField(p, \' a \' )\ny = polygen(M, \' y \' )\np = p.subs(x = y)\np\n\ny^4 + y^2 - 1\n\np.parent ()\n\nUnivariate Polynomial Ring in y over Number Field in a with\ndefining polynomial x^4 + x^2 - 1\n\np.factor ()\n\n\n\n21.6. SAGE 375\n\n(y - a) * (y + a) * (y^2 + a^2 + 1)\n\na^2 + 1 in QQ\n\nFalse\n\nSo our polynomial factors partially into two linear factors and a quadratic factor. But\nnotice that the quadratic factor has a coefficient that is irrational, a2 + 1, so the quadratic\nfactor properly belongs in the polynomial ring over M and not over QQ.\n\nWe build an extension containing a root of the quadratic factor, called q here. Then,\nrather than using the polygen() function, we build an entire polynomial ring R over N with\nthe indeterminate z. The reason for doing this is we can illustrate how we “upgrade” the\npolynomial p with the syntax R(p) to go from having coefficients in M to having coefficients\nin N.\n\nq = y^2 + a^2 + 1\nN.<b> = NumberField(q, \' b \' )\nR.<z> = N[]\ns = R(p)\ns\n\nz^4 + z^2 - 1\n\ns.parent ()\n\nUnivariate Polynomial Ring in z over Number Field in b with\ndefining polynomial y^2 + a^2 + 1 over its base field\n\ns.factor ()\n\n(z + b) * (z + a) * (z - a) * (z - b)\n\na in N, b in N\n\n(True , True)\n\nSo we have a field, N, where our polynomial factors into linear factors with coefficients\nfrom the field. We can get another factorization by converting N to an absolute number\nfield and factoring there. We need to recreate the polynomial over N, since a substitution\nwill carry coefficients from the wrong ring.\n\nP.<c> = N.absolute_field ()\nw = polygen(P, \' w \' )\np = w^4 + w^2- 1\np.factor ()\n\n(w - 7/18966*c^7 + 110/9483*c^5 + 923/9483*c^3 + 3001/6322*c) *\n(w - 7/37932*c^7 + 55/9483*c^5 + 923/18966*c^3 - 3321/12644*c) *\n(w + 7/37932*c^7 - 55/9483*c^5 - 923/18966*c^3 + 3321/12644*c) *\n(w + 7/18966*c^7 - 110/9483*c^5 - 923/9483*c^3 - 3001/6322*c)\n\nThis is an interesting alternative, in that the roots of the polynomial are expressions\nin terms of the single generator c. Since the roots involve a seventh power of c, we might\nsuspect (but not be certain) that the minimal polynomial of c has degree 8 and that P is a\ndegree 8 extension of the rationals. Indeed P (or N) is a splitting field for p(x) = x4+x2−1.\n\n\n\n376 CHAPTER 21. FIELDS\n\nThe roots are not really as bad as they appear — lets convert them back to the relative\nnumber field.\n\nFirst we want to rewrite a single factor (the first) in the form (w − r) to identify the\nroot with the correct signs.\n\n(w - 7/18966*c^7 + 110/9483*c^5 + 923/9483*c^3 + 3001/6322*c) =\n(w - (7/18966*c^7 - 110/9483*c^5 - 923/9483*c^3 - 3001/6322*c))\n\nWith the conversion isomorphisms, we can recognize the roots for what they are.\nfromP , toP = P.structure ()\nfromP (7/18966*c^7 - 110/9483*c^5 - 923/9483*c^3 - 3001/6322*c)\n\n-b\n\nSo the rather complicated expression in c is just the negative of the root we adjoined\nin the second step of constructing the tower of number fields. It would be a good exercise\nto see what happens to the other three roots (being careful to get the signs right on each\nroot).\n\nThis is a good opportunity to illustrate Theorem 21.17.\nM.degree ()\n\n4\n\nN.relative_degree ()\n\n2\n\nP.degree ()\n\n8\n\nM.degree ()*N.relative_degree () == P.degree ()\n\nTrue\n\nAlgebraic Numbers\nCorollary 21.24 says that the set of all algebraic numbers forms a field. This field is imple-\nmented in Sage as QQbar. This allows for finding roots of polynomials as exact quantities\nwhich display as inexact numbers.\n\nx = polygen(QQ, \' x \' )\np = x^4 + x^2 - 1\nr = p.roots(ring=QQbar); r\n\n[( -0.7861513777574233? , 1), (0.7861513777574233? , 1),\n( -1.272019649514069?*I, 1), (1.272019649514069?*I, 1)]\n\nSo we asked for the roots of a polynomial over the rationals, but requested any root\nthat may lie outside the rationals and within the field of algebraic numbers. Since the field\nof algebraic numbers contains all such roots, we get a full four roots of the fourth-degree\npolynomial. These roots are computed to lie within an interval and the question mark\nindicates that the preceding digits are accurate. (The integers paired with each root are the\nmultiplicities of that root. Use the keyword multiplicities=False to turn them off.) Let us\ntake a look under the hood and see how Sage manages the field of algebraic numbers.\n\n\n\n21.7. SAGE EXERCISES 377\n\nr1 = r[0][0]; r1\n\n-0.7861513777574233?\n\nr1.as_number_field_element ()\n\n(Number Field in a with defining polynomial y^4 + y^2 - 1, a, Ring\nmorphism:\nFrom: Number Field in a with defining polynomial y^4 + y^2 - 1\nTo: Algebraic Real Field\nDefn: a |--> -0.7861513777574233?)\n\nThree items are associated with this initial root. First is a number field, with generator\na and a defining polynomial similar to the polynomial we are finding the roots of, but not\nidentical. Second is an expression in the generator a, which is the actual root. In this\nexample, the expression is simple, but it could be more complex in other examples. Finally,\nthere is a ring homomorphism from the number field to the “Algebraic Real Field”, AA, the\nsubfield of QQbar with just real elements, which associates the generator a with the number\n-0.7861513777574233?. Let us verify, in two ways, that the root given is really a root.\n\nr1^4 + r1^2 - 1\n\n0\n\nN, rexact , homomorphism = r1.as_number_field_element ()\n(rexact)^4 + rexact ^2 - 1\n\n0\n\nNow that we have enough theory to understand the field of algebraic numbers, and a\nnatural way to represent them exactly, you might consider the operations in the field. If\nwe take two algebraic numbers and add them together, we get another algebraic number\n(Corollary 21.24). So what is the resulting minimal polynomial? How is it computed in\nSage? You could read the source code if you wanted the answer.\n\nGeometric Constructions\nSage can do a lot of things, but it is not yet able to lay out lines with a straightedge\nand compass. However, we can very quickly determine that trisecting a 60 degree angle\nis impossible. We adjoin the cosine of a 20 degree angle (in radians) to the rationals,\ndetermine the degree of the extension, and check that it is not an integer power of 2. In\none line. Sweet.\n\nlog(QQ[cos(pi/9)]. degree (), 2) in ZZ\n\nFalse\n\n21.7 Sage Exercises\n1. Create the polynomial p(x) = x5+2x4+1 over Z3. Verify that it does not have any linear\nfactors by evaluating p(x) with each element of Z3, and then check that p(x) is irreducible.\nCreate a finite field of order 35 with the FiniteField() command, but include the modulus\n\nkeyword set to the polynomial p(x) to override the default choice.\n\n\n\n378 CHAPTER 21. FIELDS\n\nRecreate p(x) as a polynomial over this field. Check each of the 35 = 243 elements of the\nfield to see if they are roots of the polynomial and list all of the elements which are roots.\nFinally, request that Sage give a factorization of p(x) over the field, and comment on the\nrelationship between your list of roots and your factorization.\n\n2. This problem continues the previous one. Build the ring of polynomials over Z3 and\nwithin this ring use p(x) to generate a principal ideal. Finally construct the quotient of\nthe polynomial ring by the ideal. Since the polynomial is irreducible, this quotient ring is\na field, and by Proposition 21.12 this quotient ring is isomorphic to the number field in the\nprevious problem.\nBorrowing from your results in the previous question, construct five roots of the polynomial\np(x) within this quotient ring, but now as expressions in the generator of the quotient\nring (which is technically a coset). Use Sage to verify that they are indeed roots. This\ndemonstrates using a quotient ring to create a splitting field for an irreducible polynomial\nover a finite field.\n\n3. The subsection “⟨⟨Unresolved xref, reference ”subsection-algebraic-elements”; check spelling\nor use ”provisional” attribute⟩⟩” relies on techniques from linear algebra and contains The-\norem 21.15: every finite extension is an algebraic extension. This exercise will help you\nunderstand this proof.\nThe polynomial r(x) = x4 + 2x + 2 is irreducible over the rationals (Eisenstein’s criterion\nwith prime p = 2). Create a number field that contains a root of r(x). By Theorem 21.15,\nand the remark following, every element of this finite field extension is an algebraic number,\nand hence satisfies some polynomial over the base field (it is this polynomial that Sage will\nproduce with the .minpoly() method). This exercise will show how we can use just linear\nalgebra to determine this minimal polynomial.\nSuppose that a is the generator of the number field you just created with r(x). Then we\nwill determine the minimal polynomial of t = 3a + 1 using just linear algebra. According\nto the proof, the first five powers of t (start counting from zero) will be linearly dependent.\n(Why?) So a nontrivial relation of linear dependence on these powers will provide the\ncoefficients of a polynomial with t as a root. Compute these five powers, then construct\nthe correct linear system to determine the coefficients of the minimal polynomial, solve the\nsystem, and suitably interpret its solutions.\nHints: The vector() and matrix() commands will create vectors and matrices, and the\n.solve_right() method for matrices can be used to find solutions. Given an element of\nthe number field, which will necessarily be a polynomial in the generator a, the .vector()\n\nmethod of the element will provide the coefficients of this polynomial in a list.\n\n4. Construct the splitting field of s(x) = x4 + x2 + 1 and find a factorization of s(x) over\nthis field into linear factors.\n\n5. Form the number field, K, which contains a root of the irreducible polynomial q(x) =\nx3 + 3x2 + 3x− 2. Name your root a. Verify that q(x) factors, but does not split, over K.\nWith K now as the base field, form an extension of K where the quadratic factor of q(x)\nhas a root. Name this root b, and call this second extension of the tower L.\nUse M.<c> = L.absolute_field() to form the flattened tower that is the absolute number field\nM. Find the defining polynomial of M with the .polynomial() method. From this polynomial,\nwhich must have the generator c as a root, you should be able to use elementary algebra to\nwrite the generator as a fairly simple expression.\nM should be the splitting field of q(x). To see this, start over, and build from scratch a new\nnumber field, P , using the simple expression for c that you just found. Use d as the name\nof the root used to construct P. Since d is a root of the simple minimal polynomial for c,\n\n\n\n21.7. SAGE EXERCISES 379\n\nyou should be able to write an expression for d that a pre-calculus student would recognize.\nNow factor the original polynomial q(x) (with rational coefficients) over P , to see the\npolynomial split (as expected). Using this factorization, and your simple expression for d\n\nwrite simplified expressions for the three roots of q(x). See if you can convert between the\ntwo versions of the roots “by hand”, and without using the isomorphisms provided by the\n.structure() method on M.\n\n\n\n22\n\nFinite Fields\n\nFinite fields appear in many applications of algebra, including coding theory and cryptog-\nraphy. We already know one finite field, Zp, where p is prime. In this chapter we will show\nthat a unique finite field of order pn exists for every prime p, where n is a positive integer.\nFinite fields are also called Galois fields in honor of Évariste Galois, who was one of the\nfirst mathematicians to investigate them.\n\n22.1 Structure of a Finite Field\nRecall that a field F has characteristic p if p is the smallest positive integer such that\nfor every nonzero element α in F , we have pα = 0. If no such integer exists, then F has\ncharacteristic 0. From Theorem 16.19 we know that p must be prime. Suppose that F is a\nfinite field with n elements. Then nα = 0 for all α in F . Consequently, the characteristic of\nF must be p, where p is a prime dividing n. This discussion is summarized in the following\nproposition.\n\nProposition 22.1. If F is a finite field, then the characteristic of F is p, where p is prime.\n\nThroughout this chapter we will assume that p is a prime number unless otherwise\nstated.\n\nProposition 22.2. If F is a finite field of characteristic p, then the order of F is pn for\nsome n ∈ N.\n\nProof. Let ϕ : Z → F be the ring homomorphism defined by ϕ(n) = n · 1. Since the\ncharacteristic of F is p, the kernel of ϕ must be pZ and the image of ϕ must be a subfield\nof F isomorphic to Zp. We will denote this subfield by K. Since F is a finite field, it\nmust be a finite extension of K and, therefore, an algebraic extension of K. Suppose that\n[F : K] = n is the dimension of F , where F is a K vector space. There must exist elements\nα1, . . . , αn ∈ F such that any element α in F can be written uniquely in the form\n\nα = a1α1 + · · ·+ anαn,\n\nwhere the ai’s are in K. Since there are p elements in K, there are pn possible linear\ncombinations of the αi’s. Therefore, the order of F must be pn.\n\nLemma 22.3 (Freshman’s Dream). Let p be prime and D be an integral domain of char-\nacteristic p. Then\n\nap\nn\n+ bp\n\nn\n= (a+ b)p\n\nn\n\nfor all positive integers n.\n\n380\n\n\n\n22.1. STRUCTURE OF A FINITE FIELD 381\n\nProof. We will prove this lemma using mathematical induction on n. We can use the\nbinomial formula (see Chapter 2, Example 2.4) to verify the case for n = 1; that is,\n\n(a+ b)p =\n\np∑\nk=0\n\n(\np\n\nk\n\n)\nakbp−k.\n\nIf 0 < k < p, then (\np\n\nk\n\n)\n=\n\np!\n\nk!(p− k)!\n\nmust be divisible by p, since p cannot divide k!(p− k)!. Note that D is an integral domain\nof characteristic p, so all but the first and last terms in the sum must be zero. Therefore,\n(a+ b)p = ap + bp.\n\nNow suppose that the result holds for all k, where 1 ≤ k ≤ n. By the induction\nhypothesis,\n\n(a+ b)p\nn+1\n\n= ((a+ b)p)p\nn\n= (ap + bp)p\n\nn\n= (ap)p\n\nn\n+ (bp)p\n\nn\n= ap\n\nn+1\n+ bp\n\nn+1\n.\n\nTherefore, the lemma is true for n+ 1 and the proof is complete.\n\nLet F be a field. A polynomial f(x) ∈ F [x] of degree n is separable if it has n distinct\nroots in the splitting field of f(x); that is, f(x) is separable when it factors into distinct\nlinear factors over the splitting field of f . An extension E of F is a separable extension\nof F if every element in E is the root of a separable polynomial in F [x].\n\nExample 22.4. The polynomial x2−2 is separable over Q since it factors as (x−\n√\n2 )(x+√\n\n2 ). In fact, Q(\n√\n2 ) is a separable extension of Q. Let α = a + b\n\n√\n2 be any element in\n\nQ(\n√\n2 ). If b = 0, then α is a root of x − a. If b ̸= 0, then α is the root of the separable\n\npolynomial\nx2 − 2ax+ a2 − 2b2 = (x− (a+ b\n\n√\n2 ))(x− (a− b\n\n√\n2 )).\n\nFortunately, we have an easy test to determine the separability of any polynomial. Let\n\nf(x) = a0 + a1x+ · · ·+ anx\nn\n\nbe any polynomial in F [x]. Define the derivative of f(x) to be\n\nf ′(x) = a1 + 2a2x+ · · ·+ nanx\nn−1.\n\nLemma 22.5. Let F be a field and f(x) ∈ F [x]. Then f(x) is separable if and only if f(x)\nand f ′(x) are relatively prime.\n\nProof. Let f(x) be separable. Then f(x) factors over some extension field of F as f(x) =\n(x−α1)(x−α2) · · · (x−αn), where αi ̸= αj for i ̸= j. Taking the derivative of f(x), we see\nthat\n\nf ′(x) = (x− α2) · · · (x− αn)\n\n+ (x− α1)(x− α3) · · · (x− αn)\n\n+ · · ·+ (x− α1) · · · (x− αn−1).\n\nHence, f(x) and f ′(x) can have no common factors.\nTo prove the converse, we will show that the contrapositive of the statement is true.\n\nSuppose that f(x) = (x− α)kg(x), where k > 1. Differentiating, we have\n\nf ′(x) = k(x− α)k−1g(x) + (x− α)kg′(x).\n\nTherefore, f(x) and f ′(x) have a common factor.\n\n\n\n382 CHAPTER 22. FINITE FIELDS\n\nTheorem 22.6. For every prime p and every positive integer n, there exists a finite field\nF with pn elements. Furthermore, any field of order pn is isomorphic to the splitting field\nof xpn − x over Zp.\n\nProof. Let f(x) = xp\nn − x and let F be the splitting field of f(x). Then by Lemma 22.5,\n\nf(x) has pn distinct zeros in F , since f ′(x) = pnxp\nn−1 − 1 = −1 is relatively prime to\n\nf(x). We claim that the roots of f(x) form a subfield of F . Certainly 0 and 1 are zeros\nof f(x). If α and β are zeros of f(x), then α + β and αβ are also zeros of f(x), since\nαpn +βp\n\nn\n= (α+β)p\n\nn and αpnβp\nn\n= (αβ)p\n\nn . We also need to show that the additive inverse\nand the multiplicative inverse of each root of f(x) are roots of f(x). For any zero α of f(x),\n−α = (p − 1)α is also a zero of f(x). If α ̸= 0, then (α−1)p\n\nn\n= (αpn)−1 = α−1. Since the\n\nzeros of f(x) form a subfield of F and f(x) splits in this subfield, the subfield must be all\nof F .\n\nLet E be any other field of order pn. To show that E is isomorphic to F , we must show\nthat every element in E is a root of f(x). Certainly 0 is a root of f(x). Let α be a nonzero\nelement of E. The order of the multiplicative group of nonzero elements of E is pn − 1;\nhence, αpn−1 = 1 or αpn −α = 0. Since E contains pn elements, E must be a splitting field\nof f(x); however, by Corollary 21.34, the splitting field of any polynomial is unique up to\nisomorphism.\n\nThe unique finite field with pn elements is called the Galois field of order pn. We will\ndenote this field by GF(pn).\n\nTheorem 22.7. Every subfield of the Galois field GF(pn) has pm elements, where m divides\nn. Conversely, if m | n for m > 0, then there exists a unique subfield of GF(pn) isomorphic\nto GF(pm).\n\nProof. Let F be a subfield of E = GF(pn). Then F must be a field extension of K that\ncontains pm elements, where K is isomorphic to Zp. Then m | n, since [E : K] = [E : F ][F :\nK].\n\nTo prove the converse, suppose that m | n for some m > 0. Then pm − 1 divides pn − 1.\nConsequently, xpm−1 − 1 divides xpn−1 − 1. Therefore, xpm − x must divide xpn − x, and\nevery zero of xpm − x is also a zero of xpn − x. Thus, GF(pn) contains, as a subfield, a\nsplitting field of xpm − x, which must be isomorphic to GF(pm).\n\nExample 22.8. The lattice of subfields of GF(p24) is given in Figure 22.9.\n\nGF(p24)\n\nGF(p12)\n\nGF(p6)\n\nGF(p3)\n\nGF(p8)\n\nGF(p4)\n\nGF(p2)\n\nGF(p)\n\nFigure 22.9: Subfields of GF(p24)\n\n\n\n22.2. POLYNOMIAL CODES 383\n\nWith each field F we have a multiplicative group of nonzero elements of F which we\nwill denote by F ∗. The multiplicative group of any finite field is cyclic. This result follows\nfrom the more general result that we will prove in the next theorem.\n\nTheorem 22.10. If G is a finite subgroup of F ∗, the multiplicative group of nonzero\nelements of a field F , then G is cyclic.\n\nProof. Let G be a finite subgroup of F ∗ of order n. By the Fundamental Theorem of\nFinite Abelian Groups (Theorem 13.4),\n\nG ∼= Zp\ne1\n1\n\n× · · · × Zp\nek\nk\n,\n\nwhere n = pe11 · · · pekk and the p1, . . . , pk are (not necessarily distinct) primes. Let m be the\nleast common multiple of pe11 , . . . , p\n\nek\nk . Then G contains an element of order m. Since every\n\nα in G satisfies xr − 1 for some r dividing m, α must also be a root of xm− 1. Since xm− 1\nhas at most m roots in F , n ≤ m. On the other hand, we know that m ≤ |G|; therefore,\nm = n. Thus, G contains an element of order n and must be cyclic.\n\nCorollary 22.11. The multiplicative group of all nonzero elements of a finite field is cyclic.\n\nCorollary 22.12. Every finite extension E of a finite field F is a simple extension of F .\n\nProof. Let α be a generator for the cyclic group E∗ of nonzero elements of E. Then\nE = F (α).\n\nExample 22.13. The finite field GF(24) is isomorphic to the field Z2/⟨1+x+x4⟩. Therefore,\nthe elements of GF(24) can be taken to be\n\n{a0 + a1α+ a2α\n2 + a3α\n\n3 : ai ∈ Z2 and 1 + α+ α4 = 0}.\n\nRemembering that 1 + α+ α4 = 0, we add and multiply elements of GF(24) exactly as we\nadd and multiply polynomials. The multiplicative group of GF(24) is isomorphic to Z15\n\nwith generator α:\n\nα1 = α α6 = α2 + α3 α11 = α+ α2 + α3\n\nα2 = α2 α7 = 1 + α+ α3 α12 = 1 + α+ α2 + α3\n\nα3 = α3 α8 = 1 + α2 α13 = 1 + α2 + α3\n\nα4 = 1 + α α9 = α+ α3 α14 = 1 + α3\n\nα5 = α+ α2 α10 = 1 + α+ α2 α15 = 1.\n\n22.2 Polynomial Codes\nWith knowledge of polynomial rings and finite fields, it is now possible to derive more\nsophisticated codes than those of Chapter 8. First let us recall that an (n, k)-block code\nconsists of a one-to-one encoding function E : Zk\n\n2 → Zn\n2 and a decoding function D : Zn\n\n2 →\nZk\n2. The code is error-correcting if D is onto. A code is a linear code if it is the null space\n\nof a matrix H ∈ Mk×n(Z2).\nWe are interested in a class of codes known as cyclic codes. Let ϕ : Zk\n\n2 → Zn\n2 be a\n\nbinary (n, k)-block code. Then ϕ is a cyclic code if for every codeword (a1, a2, . . . , an),\nthe cyclically shifted n-tuple (an, a1, a2, . . . , an−1) is also a codeword. Cyclic codes are\nparticularly easy to implement on a computer using shift registers [2, 3].\n\n\n\n384 CHAPTER 22. FINITE FIELDS\n\nExample 22.14. Consider the (6, 3)-linear codes generated by the two matrices\n\nG1 =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 0 0\n\n0 1 0\n\n0 0 1\n\n1 0 0\n\n0 1 0\n\n0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand G2 =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 0 0\n\n1 1 0\n\n1 1 1\n\n1 1 1\n\n0 1 1\n\n0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\n\nMessages in the first code are encoded as follows:\n\n(000) 7→ (000000) (100) 7→ (100100)\n\n(001) 7→ (001001) (101) 7→ (101101)\n\n(010) 7→ (010010) (110) 7→ (110110)\n\n(011) 7→ (011011) (111) 7→ (111111).\n\nIt is easy to see that the codewords form a cyclic code. In the second code, 3-tuples are\nencoded in the following manner:\n\n(000) 7→ (000000) (100) 7→ (111100)\n\n(001) 7→ (001111) (101) 7→ (110011)\n\n(010) 7→ (011110) (110) 7→ (100010)\n\n(011) 7→ (010001) (111) 7→ (101101).\n\nThis code cannot be cyclic, since (101101) is a codeword but (011011) is not a codeword.\n\nPolynomial Codes\nWe would like to find an easy method of obtaining cyclic linear codes. To accomplish this, we\ncan use our knowledge of finite fields and polynomial rings over Z2. Any binary n-tuple can\nbe interpreted as a polynomial in Z2[x]. Stated another way, the n-tuple (a0, a1, . . . , an−1)\ncorresponds to the polynomial\n\nf(x) = a0 + a1x+ · · ·+ an−1x\nn−1,\n\nwhere the degree of f(x) is at most n − 1. For example, the polynomial corresponding to\nthe 5-tuple (10011) is\n\n1 + 0x+ 0x2 + 1x3 + 1x4 = 1 + x3 + x4.\n\nConversely, with any polynomial f(x) ∈ Z2[x] with deg f(x) < n we can associate a binary\nn-tuple. The polynomial x+ x2 + x4 corresponds to the 5-tuple (01101).\n\nLet us fix a nonconstant polynomial g(x) in Z2[x] of degree n− k. We can define an\n(n, k)-code C in the following manner. If (a0, . . . , ak−1) is a k-tuple to be encoded, then\nf(x) = a0+a1x+ · · ·+ak−1x\n\nk−1 is the corresponding polynomial in Z2[x]. To encode f(x),\nwe multiply by g(x). The codewords in C are all those polynomials in Z2[x] of degree less\nthan n that are divisible by g(x). Codes obtained in this manner are called polynomial\ncodes.\n\nExample 22.15. If we let g(x) = 1+x3, we can define a (6, 3)-code C as follows. To encode\na 3-tuple (a0, a1, a2), we multiply the corresponding polynomial f(x) = a0 + a1x+ a2x\n\n2 by\n1+x3. We are defining a map ϕ : Z3\n\n2 → Z6\n2 by ϕ : f(x) 7→ g(x)f(x). It is easy to check that\n\nthis map is a group homomorphism. In fact, if we regard Zn\n2 as a vector space over Z2, ϕ is\n\n\n\n22.2. POLYNOMIAL CODES 385\n\na linear transformation of vector spaces (see Exercise 20.4.15, Chapter 20). Let us compute\nthe kernel of ϕ. Observe that ϕ(a0, a1, a2) = (000000) exactly when\n\n0 + 0x+ 0x2 + 0x3 + 0x4 + 0x5 = (1 + x3)(a0 + a1x+ a2x\n2)\n\n= a0 + a1x+ a2x\n2 + a0x\n\n3 + a1x\n4 + a2x\n\n5.\n\nSince the polynomials over a field form an integral domain, a0 + a1x + a2x\n2 must be the\n\nzero polynomial. Therefore, kerϕ = {(000)} and ϕ is one-to-one.\nTo calculate a generator matrix for C, we merely need to examine the way the polyno-\n\nmials 1, x, and x2 are encoded:\n\n(1 + x3) · 1 = 1 + x3\n\n(1 + x3)x = x+ x4\n\n(1 + x3)x2 = x2 + x5.\n\nWe obtain the code corresponding to the generator matrix G1 in Example 22.14. The\nparity-check matrix for this code is\n\nH =\n\n\uf8eb\uf8ed1 0 0 1 0 0\n\n0 1 0 0 1 0\n\n0 0 1 0 0 1\n\n\uf8f6\uf8f8 .\n\nSince the smallest weight of any nonzero codeword is 2, this code has the ability to detect\nall single errors.\n\nRings of polynomials have a great deal of structure; therefore, our immediate goal\nis to establish a link between polynomial codes and ring theory. Recall that xn − 1 =\n(x− 1)(xn−1 + · · ·+ x+ 1). The factor ring\n\nRn = Z2[x]/⟨xn − 1⟩\n\ncan be considered to be the ring of polynomials of the form\n\nf(t) = a0 + a1t+ · · ·+ an−1t\nn−1\n\nthat satisfy the condition tn = 1. It is an easy exercise to show that Zn\n2 and Rn are isomor-\n\nphic as vector spaces. We will often identify elements in Zn\n2 with elements in Z[x]/⟨xn− 1⟩.\n\nIn this manner we can interpret a linear code as a subset of Z[x]/⟨xn − 1⟩.\nThe additional ring structure on polynomial codes is very powerful in describing cyclic\n\ncodes. A cyclic shift of an n-tuple can be described by polynomial multiplication. If\nf(t) = a0 + a1t+ · · ·+ an−1t\n\nn−1 is a code polynomial in Rn, then\n\ntf(t) = an−1 + a0t+ · · ·+ an−2t\nn−1\n\nis the cyclically shifted word obtained from multiplying f(t) by t. The following theorem\ngives a beautiful classification of cyclic codes in terms of the ideals of Rn.\n\nTheorem 22.16. A linear code C in Zn\n2 is cyclic if and only if it is an ideal in Rn =\n\nZ[x]/⟨xn − 1⟩.\n\nProof. Let C be a linear cyclic code and suppose that f(t) is in C. Then tf(t) must also\nbe in C. Consequently, tkf(t) is in C for all k ∈ N. Since C is a linear code, any linear\ncombination of the codewords f(t), tf(t), t2f(t), . . . , tn−1f(t) is also a codeword; therefore,\nfor every polynomial p(t), p(t)f(t) is in C. Hence, C is an ideal.\n\nConversely, let C be an ideal in Z2[x]/⟨xn + 1⟩. Suppose that f(t) = a0 + a1t + · · · +\nan−1t\n\nn−1 is a codeword in C. Then tf(t) is a codeword in C; that is, (a1, . . . , an−1, a0) is\nin C.\n\n\n\n386 CHAPTER 22. FINITE FIELDS\n\nTheorem 22.16 tells us that knowing the ideals of Rn is equivalent to knowing the linear\ncyclic codes in Zn\n\n2 . Fortunately, the ideals in Rn are easy to describe. The natural ring\nhomomorphism ϕ : Z2[x] → Rn defined by ϕ[f(x)] = f(t) is a surjective homomorphism.\nThe kernel of ϕ is the ideal generated by xn − 1. By Theorem 16.34, every ideal C in Rn is\nof the form ϕ(I), where I is an ideal in Z2[x] that contains ⟨xn− 1⟩. By Theorem 17.20, we\nknow that every ideal I in Z2[x] is a principal ideal, since Z2 is a field. Therefore, I = ⟨g(x)⟩\nfor some unique monic polynomial in Z2[x]. Since ⟨xn− 1⟩ is contained in I, it must be the\ncase that g(x) divides xn − 1. Consequently, every ideal C in Rn is of the form\n\nC = ⟨g(t)⟩ = {f(t)g(t) : f(t) ∈ Rn and g(x) | (xn − 1) in Z2[x]}.\n\nThe unique monic polynomial of the smallest degree that generates C is called the minimal\ngenerator polynomial of C.\n\nExample 22.17. If we factor x7 − 1 into irreducible components, we have\n\nx7 − 1 = (1 + x)(1 + x+ x3)(1 + x2 + x3).\n\nWe see that g(t) = (1 + t+ t3) generates an ideal C in R7. This code is a (7, 4)-block code.\nAs in Example 22.15, it is easy to calculate a generator matrix by examining what g(t) does\nto the polynomials 1, t, t2, and t3. A generator matrix for C is\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 0 0 0\n\n1 1 0 0\n\n0 1 1 0\n\n1 0 1 1\n\n0 1 0 1\n\n0 0 1 0\n\n0 0 0 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\n\nIn general, we can determine a generator matrix for an (n, k)-code C by the manner in\nwhich the elements tk are encoded. Let xn − 1 = g(x)h(x) in Z2[x]. If g(x) = g0 + g1x +\n· · ·+ gn−kx\n\nn−k and h(x) = h0 + h1x+ · · ·+ hkx\nk, then the n× k matrix\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\ng0 0 · · · 0\n\ng1 g0 · · · 0\n...\n\n... . . . ...\ngn−k gn−k−1 · · · g0\n0 gn−k · · · g1\n...\n\n... . . . ...\n0 0 · · · gn−k\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nis a generator matrix for the code C with generator polynomial g(t). The parity-check\nmatrix for C is the (n− k)× n matrix\n\nH =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n0 · · · 0 0 hk · · · h0\n0 · · · 0 hk · · · h0 0\n\n· · · · · · · · · · · · · · · · · · · · ·\nhk · · · h0 0 0 · · · 0\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\n\nWe will leave the details of the proof of the following proposition as an exercise.\n\n\n\n22.2. POLYNOMIAL CODES 387\n\nProposition 22.18. Let C = ⟨g(t)⟩ be a cyclic code in Rn and suppose that xn − 1 =\ng(x)h(x). Then G and H are generator and parity-check matrices for C, respectively.\nFurthermore, HG = 0.\n\nExample 22.19. In Example 22.17,\n\nx7 − 1 = g(x)h(x) = (1 + x+ x3)(1 + x+ x2 + x4).\n\nTherefore, a parity-check matrix for this code is\n\nH =\n\n\uf8eb\uf8ed0 0 1 0 1 1 1\n\n0 1 0 1 1 1 0\n\n1 0 1 1 1 0 0\n\n\uf8f6\uf8f8 .\n\nTo determine the error-detecting and error-correcting capabilities of a cyclic code, we\nneed to know something about determinants. If α1, . . . , αn are elements in a field F , then\nthe n× n matrix \uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 1 · · · 1\n\nα1 α2 · · · αn\n\nα2\n1 α2\n\n2 · · · α2\nn\n\n...\n... . . . ...\n\nαn−1\n1 αn−1\n\n2 · · · αn−1\nn\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nis called the Vandermonde matrix. The determinant of this matrix is called the Van-\ndermonde determinant. We will need the following lemma in our investigation of cyclic\ncodes.\n\nLemma 22.20. Let α1, . . . , αn be elements in a field F with n ≥ 2. Then\n\ndet\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 1 · · · 1\n\nα1 α2 · · · αn\n\nα2\n1 α2\n\n2 · · · α2\nn\n\n...\n... . . . ...\n\nαn−1\n1 αn−1\n\n2 · · · αn−1\nn\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 =\n∏\n\n1≤j<i≤n\n\n(αi − αj).\n\nIn particular, if the αi’s are distinct, then the determinant is nonzero.\n\nProof. We will induct on n. If n = 2, then the determinant is α2−α1. Let us assume the\nresult for n− 1 and consider the polynomial p(x) defined by\n\np(x) = det\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 1 · · · 1 1\n\nα1 α2 · · · αn−1 x\n\nα2\n1 α2\n\n2 · · · α2\nn−1 x2\n\n...\n... . . . ...\n\n...\nαn−1\n1 αn−1\n\n2 · · · αn−1\nn−1 xn−1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\nExpanding this determinant by cofactors on the last column, we see that p(x) is a polynomial\nof at most degree n−1. Moreover, the roots of p(x) are α1, . . . , αn−1, since the substitution\nof any one of these elements in the last column will produce a column identical to the last\ncolumn in the matrix. Remember that the determinant of a matrix is zero if it has two\nidentical columns. Therefore,\n\np(x) = (x− α1)(x− α2) · · · (x− αn−1)β,\n\n\n\n388 CHAPTER 22. FINITE FIELDS\n\nwhere\n\nβ = (−1)n+n det\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 1 · · · 1\n\nα1 α2 · · · αn−1\n\nα2\n1 α2\n\n2 · · · α2\nn−1\n\n...\n... . . . ...\n\nαn−2\n1 αn−2\n\n2 · · · αn−2\nn−1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\nBy our induction hypothesis,\n\nβ = (−1)n+n\n∏\n\n1≤j<i≤n−1\n\n(αi − αj).\n\nIf we let x = αn, the result now follows immediately.\n\nThe following theorem gives us an estimate on the error detection and correction capa-\nbilities for a particular generator polynomial.\n\nTheorem 22.21. Let C = ⟨g(t)⟩ be a cyclic code in Rn and suppose that ω is a primitive\nnth root of unity over Z2. If s consecutive powers of ω are roots of g(x), then the minimum\ndistance of C is at least s+ 1.\n\nProof. Suppose that\n\ng(ωr) = g(ωr+1) = · · · = g(ωr+s−1) = 0.\n\nLet f(x) be some polynomial in C with s or fewer nonzero coefficients. We can assume that\n\nf(x) = ai0x\ni0 + ai1x\n\ni1 + · · ·+ ais−1x\nis−1\n\nbe some polynomial in C. It will suffice to show that all of the ai’s must be 0. Since\n\ng(ωr) = g(ωr+1) = · · · = g(ωr+s−1) = 0\n\nand g(x) divides f(x),\n\nf(ωr) = f(ωr+1) = · · · = f(ωr+s−1) = 0.\n\nEquivalently, we have the following system of equations:\n\nai0(ω\nr)i0 + ai1(ω\n\nr)i1 + · · ·+ ais−1(ω\nr)is−1 = 0\n\nai0(ω\nr+1)i0 + ai1(ω\n\nr+1)i2 + · · ·+ ais−1(ω\nr+1)is−1 = 0\n\n...\nai0(ω\n\nr+s−1)i0 + ai1(ω\nr+s−1)i1 + · · ·+ ais−1(ω\n\nr+s−1)is−1 = 0.\n\nTherefore, (ai0 , ai1 , . . . , ais−1) is a solution to the homogeneous system of linear equations\n\n(ωi0)rx0 + (ωi1)rx1 + · · ·+ (ωis−1)rxn−1 = 0\n\n(ωi0)r+1x0 + (ωi1)r+1x1 + · · ·+ (ωis−1)r+1xn−1 = 0\n\n...\n(ωi0)r+s−1x0 + (ωi1)r+s−1x1 + · · ·+ (ωis−1)r+s−1xn−1 = 0.\n\n\n\n22.2. POLYNOMIAL CODES 389\n\nHowever, this system has a unique solution, since the determinant of the matrix\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n(ωi0)r (ωi1)r · · · (ωis−1)r\n\n(ωi0)r+1 (ωi1)r+1 · · · (ωis−1)r+1\n\n...\n... . . . ...\n\n(ωi0)r+s−1 (ωi1)r+s−1 · · · (ωis−1)r+s−1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8\ncan be shown to be nonzero using Lemma 22.20 and the basic properties of determinants\n(Exercise). Therefore, this solution must be ai0 = ai1 = · · · = ais−1 = 0.\n\nBCH Codes\nSome of the most important codes, discovered independently by A. Hocquenghem in 1959\nand by R. C. Bose and D. V. Ray-Chaudhuri in 1960, are bch codes. The European\nand transatlantic communication systems both use bch codes. Information words to be\nencoded are of length 231, and a polynomial of degree 24 is used to generate the code.\nSince 231 + 24 = 255 = 28 − 1, we are dealing with a (255, 231)-block code. This bch code\nwill detect six errors and has a failure rate of 1 in 16 million. One advantage of bch codes\nis that efficient error correction algorithms exist for them.\n\nThe idea behind bch codes is to choose a generator polynomial of smallest degree that\nhas the largest error detection and error correction capabilities. Let d = 2r + 1 for some\nr ≥ 0. Suppose that ω is a primitive nth root of unity over Z2, and let mi(x) be the minimal\npolynomial over Z2 of ωi. If\n\ng(x) = lcm[m1(x),m2(x), . . . ,m2r(x)],\n\nthen the cyclic code ⟨g(t)⟩ in Rn is called the bch code of length n and distance d. By\nTheorem 22.21, the minimum distance of C is at least d.\nTheorem 22.22. Let C = ⟨g(t)⟩ be a cyclic code in Rn. The following statements are\nequivalent.\n\n1. The code C is a bch code whose minimum distance is at least d.\n\n2. A code polynomial f(t) is in C if and only if f(ωi) = 0 for 1 ≤ i < d.\n\n3. The matrix\n\nH =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 ω ω2 · · · ωn−1\n\n1 ω2 ω4 · · · ω(n−1)(2)\n\n1 ω3 ω6 · · · ω(n−1)(3)\n\n...\n...\n\n... . . . ...\n1 ω2r ω4r · · · ω(n−1)(2r)\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nis a parity-check matrix for C.\n\nProof. (1) ⇒ (2). If f(t) is in C, then g(x) | f(x) in Z2[x]. Hence, for i = 1, . . . , 2r,\nf(ωi) = 0 since g(ωi) = 0. Conversely, suppose that f(ωi) = 0 for 1 ≤ i ≤ d. Then f(x) is\ndivisible by each mi(x), since mi(x) is the minimal polynomial of ωi. Therefore, g(x) | f(x)\nby the definition of g(x). Consequently, f(x) is a codeword.\n\n(2) ⇒ (3). Let f(t) = a0 + a1t+ · · ·+ an−1vt\nn−1 be in Rn. The corresponding n-tuple\n\nin Zn\n2 is x = (a0a1 · · · an−1)\n\nt. By (2),\n\nHx =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\na0 + a1ω + · · ·+ an−1ω\n\nn−1\n\na0 + a1ω\n2 + · · ·+ an−1(ω\n\n2)n−1\n\n...\na0 + a1ω\n\n2r + · · ·+ an−1(ω\n2r)n−1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\nf(ω)\n\nf(ω2)\n...\n\nf(ω2r)\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 = 0\n\n\n\n390 CHAPTER 22. FINITE FIELDS\n\nexactly when f(t) is in C. Thus, H is a parity-check matrix for C.\n(3) ⇒ (1). By (3), a code polynomial f(t) = a0+a1t+· · ·+an−1t\n\nn−1 is in C exactly when\nf(ωi) = 0 for i = 1, . . . , 2r. The smallest such polynomial is g(t) = lcm[m1(t), . . . ,m2r(t)].\nTherefore, C = ⟨g(t)⟩.\n\nExample 22.23. It is easy to verify that x15 − 1 ∈ Z2[x] has a factorization\n\nx15 − 1 = (x+ 1)(x2 + x+ 1)(x4 + x+ 1)(x4 + x3 + 1)(x4 + x3 + x2 + x+ 1),\n\nwhere each of the factors is an irreducible polynomial. Let ω be a root of 1 + x+ x4. The\nGalois field GF(24) is\n\n{a0 + a1ω + a2ω\n2 + a3ω\n\n3 : ai ∈ Z2 and 1 + ω + ω4 = 0}.\n\nBy Example 22.8, ω is a primitive 15th root of unity. The minimal polynomial of ω is\nm1(x) = 1 + x+ x4. It is easy to see that ω2 and ω4 are also roots of m1(x). The minimal\npolynomial of ω3 is m2(x) = 1 + x+ x2 + x3 + x4. Therefore,\n\ng(x) = m1(x)m2(x) = 1 + x4 + x6 + x7 + x8\n\nhas roots ω, ω2, ω3, ω4. Since both m1(x) and m2(x) divide x15 − 1, the bch code is a\n(15, 7)-code. If x15 − 1 = g(x)h(x), then h(x) = 1 + x4 + x6 + x7; therefore, a parity-check\nmatrix for this code is\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n0 0 0 0 0 0 0 1 1 0 1 0 0 0 1\n\n0 0 0 0 0 0 1 1 0 1 0 0 0 1 0\n\n0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n\n0 0 0 0 1 1 0 1 0 0 0 1 0 0 0\n\n0 0 0 1 1 0 1 0 0 0 1 0 0 0 0\n\n0 0 1 1 0 1 0 0 0 1 0 0 0 0 0\n\n0 1 1 0 1 0 0 0 1 0 0 0 0 0 0\n\n1 1 0 1 0 0 0 1 0 0 0 0 0 0 0\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\n\n22.3 Exercises\n1. Calculate each of the following.\n\n(a) [GF(36) : GF(33)]\n(b) [GF(128) : GF(16)]\n\n(c) [GF(625) : GF(25)]\n(d) [GF(p12) : GF(p2)]\n\n2. Calculate [GF(pm) : GF(pn)], where n | m.\n\n3. What is the lattice of subfields for GF(p30)?\n\n4. Let α be a zero of x3 + x2 + 1 over Z2. Construct a finite field of order 8. Show that\nx3 + x2 + 1 splits in Z2(α).\n\n5. Construct a finite field of order 27.\n\n6. Prove or disprove: Q∗ is cyclic.\n\n7. Factor each of the following polynomials in Z2[x].\n\n\n\n22.3. EXERCISES 391\n\n(a) x5 − 1\n\n(b) x6 + x5 + x4 + x3 + x2 + x+ 1\n\n(c) x9 − 1\n\n(d) x4 + x3 + x2 + x+ 1\n\n8. Prove or disprove: Z2[x]/⟨x3 + x+ 1⟩ ∼= Z2[x]/⟨x3 + x2 + 1⟩.\n\n9. Determine the number of cyclic codes of length n for n = 6, 7, 8, 10.\n\n10. Prove that the ideal ⟨t+1⟩ in Rn is the code in Zn\n2 consisting of all words of even parity.\n\n11. Construct all bch codes of\n\n(a) length 7. (b) length 15.\n\n12. Prove or disprove: There exists a finite field that is algebraically closed.\n\n13. Let p be prime. Prove that the field of rational functions Zp(x) is an infinite field of\ncharacteristic p.\n\n14. Let D be an integral domain of characteristic p. Prove that (a − b)p\nn\n= ap\n\nn − bp\nn for\n\nall a, b ∈ D.\n\n15. Show that every element in a finite field can be written as the sum of two squares.\n\n16. Let E and F be subfields of a finite field K. If E is isomorphic to F , show that E = F .\n\n17. Let F ⊂ E ⊂ K be fields. If K is separable over F , show that K is also separable over\nE.\n\n18. Let E be an extension of a finite field F , where F has q elements. Let α ∈ E be\nalgebraic over F of degree n. Prove that F (α) has qn elements.\n\n19. Show that every finite extension of a finite field F is simple; that is, if E is a finite\nextension of a finite field F , prove that there exists an α ∈ E such that E = F (α).\n\n20. Show that for every n there exists an irreducible polynomial of degree n in Zp[x].\n\n21. Prove that the Frobenius map Φ : GF(pn) → GF(pn) given by Φ : α 7→ αp is an\nautomorphism of order n.\n\n22. Show that every element in GF(pn) can be written in the form ap for some unique\na ∈ GF(pn).\n\n23. Let E and F be subfields of GF(pn). If |E| = pr and |F | = ps, what is the order of\nE ∩ F?\n\n24. (Wilson’s Theorem) Let p be prime. Prove that (p− 1)! ≡ −1 (mod p).\n\n25. If g(t) is the minimal generator polynomial for a cyclic code C in Rn, prove that the\nconstant term of g(x) is 1.\n\n26. Often it is conceivable that a burst of errors might occur during transmission, as in\nthe case of a power surge. Such a momentary burst of interference might alter several\nconsecutive bits in a codeword. Cyclic codes permit the detection of such error bursts. Let\nC be an (n, k)-cyclic code. Prove that any error burst up to n− k digits can be detected.\n\n27. Prove that the rings Rn and Zn\n2 are isomorphic as vector spaces.\n\n\n\n392 CHAPTER 22. FINITE FIELDS\n\n28. Let C be a code in Rn that is generated by g(t). If ⟨f(t)⟩ is another code in Rn, show\nthat ⟨g(t)⟩ ⊂ ⟨f(t)⟩ if and only if f(x) divides g(x) in Z2[x].\n\n29. Let C = ⟨g(t)⟩ be a cyclic code in Rn and suppose that xn − 1 = g(x)h(x), where\ng(x) = g0 + g1x + · · · + gn−kx\n\nn−k and h(x) = h0 + h1x + · · · + hkx\nk. Define G to be the\n\nn× k matrix\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\ng0 0 · · · 0\n\ng1 g0 · · · 0\n...\n\n... . . . ...\ngn−k gn−k−1 · · · g0\n0 gn−k · · · g1\n...\n\n... . . . ...\n0 0 · · · gn−k\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand H to be the (n− k)× n matrix\n\nH =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n0 · · · 0 0 hk · · · h0\n0 · · · 0 hk · · · h0 0\n\n· · · · · · · · · · · · · · · · · · · · ·\nhk · · · h0 0 0 · · · 0\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n(a) Prove that G is a generator matrix for C.\n(b) Prove that H is a parity-check matrix for C.\n(c) Show that HG = 0.\n\n22.4 Additional Exercises: Error Correction for BCH Codes\nbch codes have very attractive error correction algorithms. Let C be a bch code in Rn,\nand suppose that a code polynomial c(t) = c0 + c1t + · · · + cn−1t\n\nn−1 is transmitted. Let\nw(t) = w0 + w1t + · · ·wn−1t\n\nn−1 be the polynomial in Rn that is received. If errors have\noccurred in bits a1, . . . , ak, then w(t) = c(t) + e(t), where e(t) = ta1 + ta2 + · · ·+ tak is the\nerror polynomial. The decoder must determine the integers ai and then recover c(t) from\nw(t) by flipping the aith bit. From w(t) we can compute w(ωi) = si for i = 1, . . . , 2r, where\nω is a primitive nth root of unity over Z2. We say the syndrome of w(t) is s1, . . . , s2r.\n1. Show that w(t) is a code polynomial if and only if si = 0 for all i.\n\n2. Show that\nsi = w(ωi) = e(ωi) = ωia1 + ωia2 + · · ·+ ωiak\n\nfor i = 1, . . . , 2r. The error-locator polynomial is defined to be\n\ns(x) = (x+ ωa1)(x+ ωa2) · · · (x+ ωak).\n\n3. Recall the (15, 7)-block bch code in Example 22.19. By Theorem 8.13, this code is\ncapable of correcting two errors. Suppose that these errors occur in bits a1 and a2. The\nerror-locator polynomial is s(x) = (x+ ωa1)(x+ ωa2). Show that\n\ns(x) = x2 + s1x+\n\n(\ns21 +\n\ns3\ns1\n\n)\n.\n\n4. Let w(t) = 1 + t2 + t4 + t5 + t7 + t12 + t13. Determine what the originally transmitted\ncode polynomial was.\n\n\n\n22.5. REFERENCES AND SUGGESTED READINGS 393\n\n22.5 References and Suggested Readings\n[1] Childs, L. A Concrete Introduction to Higher Algebra. 2nd ed. Springer-Verlag, New\n\nYork, 1995.\n[2] Gåding, L. and Tambour, T. Algebra for Computer Science. Springer-Verlag, New\n\nYork, 1988.\n[3] Lidl, R. and Pilz, G. Applied Abstract Algebra. 2nd ed. Springer, New York, 1998.\n\nAn excellent presentation of finite fields and their applications.\n[4] Mackiw, G. Applications of Abstract Algebra. Wiley, New York, 1985.\n[5] Roman, S. Coding and Information Theory. Springer-Verlag, New York, 1992.\n[6] van Lint, J. H. Introduction to Coding Theory. Springer, New York, 1999.\n\n22.6 Sage\nYou have noticed in this chapter that finite fields have a great deal of structure. We have\nalso seen finite fields in Sage regularly as examples of rings and fields. Now we can combine\nthe two, mostly using commands we already know, plus a few new ones.\n\nCreating Finite Fields\n\nBy Theorem 22.6 we know that all finite fields of a given order are isomorphic and that\npossible orders are limited to powers of primes. We can use the FiniteField() command, as\nbefore, or a shorter equivalent is GF(). Optionally, we can specify an irreducible polynomial\nfor the contruction of the field. We can view this polynomial as the generator of the principal\nideal of a polynomial ring, or we can view it as a “re-writing” rule for powers of the field’s\ngenerator that allow us to multiply elements and reformulate them as linear combinations\nof lesser powers.\n\nAbsent providing an irreducible polynomial, Sage will use a Conway polynomial. You\ncan determine these with the conway_polynomial() command, or just build a finite field and\nrequest the defining polynomial with the .polynomial() method.\n\nF.<a> = GF (7^15); F\n\nFinite Field in a of size 7^15\n\nF.polynomial ()\n\na^15 + 5*a^6 + 6*a^5 + 6*a^4 + 4*a^3 + a^2 + 2*a + 4\n\na^15 + 5*a^6 + 6*a^5 + 6*a^4 + 4*a^3 + a^2 + 2*a + 4\n\n0\n\nconway_polynomial (7, 15)\n\nx^15 + 5*x^6 + 6*x^5 + 6*x^4 + 4*x^3 + x^2 + 2*x + 4\n\nJust to be more readable, we coerce a list of coefficients into the set of polynomials\n(obtained with the .parent() method on a simple polynomial) to define a polynomial.\n\n\n\n394 CHAPTER 22. FINITE FIELDS\n\ny = polygen(Integers (7), \' y \' )\nP = y.parent ()\np = P([4, 5, 2, 6, 3, 3, 6, 2, 1, 1, 2, 5, 6, 3, 5, 1]); p\n\ny^15 + 5*y^14 + 3*y^13 + 6*y^12 + 5*y^11 + 2*y^10 + y^9 +\ny^8 + 2*y^7 + 6*y^6 + 3*y^5 + 3*y^4 + 6*y^3 + 2*y^2 + 5*y + 4\n\np.is_irreducible ()\n\nTrue\n\nT.<b> = GF(7^15, modulus=p); T\n\nFinite Field in b of size 7^15\n\nLogarithms in Finite Fields\nOne useful command we have not described is the .log() method for elements of a finite\nfield. Since we now know that the multiplicative group of nonzero elements is cyclic, we can\nexpress every element as a power of the generator. The log method will return that power.\n\nUsually we will want to use the generator as the base of a lograithm computation in a\nfinite field. However, other bases may be used, wih the understanding that if the base is\nnot a generator, then the logarithm may not exist (i.e. there may not be a solution to the\nrelevant equation).\n\nF.<a> = GF (5^4)\na^458\n\n3*a^3 + 2*a^2 + a + 3\n\n(3*a^3 + 2*a^2 + a + 3).log(a)\n\n458\n\nexponent = (3*a^3 + 2*a^2 + a + 3).log(2*a^3 + 4*a^2 + 4*a)\nexponent\n\n211\n\n(2*a^3 + 4*a^2 + 4*a)^exponent == 3*a^3 + 2*a^2 + a + 3\n\nTrue\n\n(3*a^3 + 2*a^2 + a + 3).log(a^2 + 4*a + 4)\n\nTraceback (most recent call last):\n...\nValueError: No discrete log of 3*a^3 + 2*a^2 + a + 3 found\nto base a^2 + 4*a + 4\n\nSince we already know many Sage commands, there is not much else worth introducing\nbefore we can work profitably with finite fields. The exercises explore the ways we can\nexamine and exploit the structure of finite fields in Sage.\n\n\n\n22.7. SAGE EXERCISES 395\n\n22.7 Sage Exercises\n1. Create a finite field of order 52 and then factor p(x) = x25 − x over this field. Comment\non what is interesting about this result and why it is not a surprise.\n\n2. Corollary 22.11 says that the nonzero elements of a finite field are a cyclic group under\nmultiplication. The generator used in Sage is also a generator of this multiplicative group.\nTo see this, create a finite field of order 27. Create two lists of the elements of the field:\nfirst, use the .list() method, then use a list comprehension to generate the proper powers\nof the generator you specified when you created the field.\nThe second list should be the whole field, but will be missing zero. Create the zero element\nof the field (perhaps by coercing 0 into the field) and .append() it to the list of powers.\nApply the sorted() command to each list and then test the lists for equality.\n\n3. Subfields of a finite field are completely classified by Theorem 22.7. It is possible to\ncreate two finite fields of the correct orders for the supefield/subfield realtionship to hold,\nand to translate between one and the other. However, in this exercise we will create a\nsubfield of a finite field from scratch. Since the group of nonzero elements in a finite field\nis cyclic, the nonzero elements of a subfield will form a subgroup of the cyclic group, and\nnecessarily will be cyclic.\nCreate a finite field of order 36. Theory says there is a subfield of order 32, since 2|6.\nDetermine a generator of multiplicative order 8 for the nonzero elements of this subfield,\nand construct these 8 elements. Add in the field’s zero element to this list. It should be\nclear that this set of 9 elements is closed under multiplication. Absent our theorems about\nfinite fields and cyclic groups, the closure under addition is not a given. Write a single\nstatement that checks if this set is also closed under addition, by considering all possible\nsums of elements from the set.\n\n4. This problem investigates the “separableness” of Q(\n√\n3,\n√\n7). You can create this number\n\nfield quickly with the NumberFieldTower constructor, along with the polynomials x2 − 3 and\nx2 − 7. Flatten the tower with the .absolute_field() method and use the .structure()\n\nmethod to retrieve mappings between the tower and the flattened version. Name the tower\nN and use a and b as generators. Name the flattened version L with c as a generator.\nCreate a nontrivial (“random”) element of L using as many powers of c as possible (check\nthe degree of L to see how many linearly independent powers there are). Request from\nSage the minimum polynomial of your random element, thus ensuring the element is a root.\nConstruct the minimum polynomial as a polynomial over N, the field tower, and find its\nfactorization. Your factorization should have only linear factors. Each root should be an\nexpression in a and b, so convert each root into an expression with mathematical notation\ninvolving\n\n√\n3 and\n\n√\n7. Use one of the mappings to verify that one of the roots is indeed the\n\noriginal random element.\nCreate a few more random elements, and find a factorization (in N or in L). For a field to be\nseparable, every element of the field should be a root of some separable polynomial. The\nminimal polynomial is a good polynomial to test. (Why?) Based on the evidence, does it\nappear that Q(\n\n√\n3,\n√\n7) is a separable extension?\n\n5. Exercise 22.3.21 describes the Frobenius Map, an automorphism of a finite field. If F is\na finite field in Sage, then End(F) will create the automorphism group of F, the set of all\nbijective mappings between the field and itself.\n(a) Work Exercise 22.3.21 to gain an understanding of how and why the Frobenius mapping\n\nis a field automorphism. (Do not include any of this in your answer to this question,\nbut understand that the following will be much easier if you do this problem first.)\n\n\n\n396 CHAPTER 22. FINITE FIELDS\n\n(b) For some small, but not trivial, finite fields locate the Frobenius map in the automor-\nphism group. Small might mean p = 2, 3, 5, 7 and 3 ≤ n ≤ 10, with n prime versus\ncomposite.\n\n(c) Once you have located the Frobenius map, describe the other automorphisms. In other\nwords, with a bit of investigation, you should find a description of the automorphisms\nwhich will allow you to accurately predict the entire automorphism group for a finite\nfield you have not already explored. (Hint: the automorphism group is a group.\nWhat if you “do the operation” between the Frobenius map and itself? Just what\nis the operation? Try using Sage’s multiplicative notation with the elements of the\nautomorphism group.)\n\n(d) What is the “structure” of the automorphism group? What special status does the\nFrobenius map have in this group?\n\n(e) For any field, the subfield known as the fixed field is an important construction, and\nwill be especially important in the next chapter. Given an automorphism τ of a field\nE, the subset, K = {b ∈ E | τ(b) = b}, can be shown to be a subfield of E. It is known\nas the fixed field of τ in E. For each automorphism of E = GF (36) identify the fixed\nfield of the automorphism. Since we understand the structure of subfields of a finite\nfield, it is enough to just determine the order of the fixed field to be able to identify\nthe subfield precisely.\n\n6. Exercise 22.3.15 suggests that every element of a finite field may be written (expressed)\nas a sum of squares. This exercise suggests computational experiments which might help\nyou formulate a proof for the exercise.\n(a) Construct two small, but not too small, finite fields, one with p = 2 and the other with\n\nan odd prime. Repeat the following for each field, F .\n(b) Choose a “random” element of the field, say a ∈ F . Construct the sets\n\n{x2|x ∈ F} {a− x2|x ∈ F}\n\nusing Sage sets with the Set() constructor. (Be careful: set() is a Python command\nwhich behaves differently in fundamental ways.)\n\n(c) Examine the size of the two sets and the size of their intersection (.intersection()).\nTry different elements for a, perhaps writing a loop to try all possible values. Note\nthat p = 2 will behave quite differently.\n\n(d) Suppose you have an element of the intersection. (You can get one with .an_element().)\nHow does this lead to the sum of squares proposed in the exercise?\n\n(e) Can you write a Python function that accepts a finite field whose order is a power of\nan odd prime and then lists each element as a sum of squares?\n\n\n\n23\n\nGalois Theory\n\nA classic problem of algebra is to find the solutions of a polynomial equation. The solution\nto the quadratic equation was known in antiquity. Italian mathematicians found general\nsolutions to the general cubic and quartic equations in the sixteenth century; however,\nattempts to solve the general fifth-degree, or quintic, polynomial were repulsed for the next\nthree hundred years. Certainly, equations such as x5 − 1 = 0 or x6 − x3 − 6 = 0 could be\nsolved, but no solution like the quadratic formula was found for the general quintic,\n\nax5 + bx4 + cx3 + dx2 + ex+ f = 0.\n\nFinally, at the beginning of the nineteenth century, Ruffini and Abel both found quintics\nthat could not be solved with any formula. It was Galois, however, who provided the full\nexplanation by showing which polynomials could and could not be solved by formulas. He\ndiscovered the connection between groups and field extensions. Galois theory demonstrates\nthe strong interdependence of group and field theory, and has had far-reaching implications\nbeyond its original purpose.\n\nIn this chapter we will prove the Fundamental Theorem of Galois Theory. This result\nwill be used to establish the insolvability of the quintic and to prove the Fundamental\nTheorem of Algebra.\n\n23.1 Field Automorphisms\nOur first task is to establish a link between group theory and field theory by examining\nautomorphisms of fields.\n\nProposition 23.1. The set of all automorphisms of a field F is a group under composition\nof functions.\n\nProof. If σ and τ are automorphisms of E, then so are στ and σ−1. The identity is\ncertainly an automorphism; hence, the set of all automorphisms of a field F is indeed a\ngroup.\n\nProposition 23.2. Let E be a field extension of F . Then the set of all automorphisms of\nE that fix F elementwise is a group; that is, the set of all automorphisms σ : E → E such\nthat σ(α) = α for all α ∈ F is a group.\n\nProof. We need only show that the set of automorphisms of E that fix F elementwise is\na subgroup of the group of all automorphisms of E. Let σ and τ be two automorphisms\nof E such that σ(α) = α and τ(α) = α for all α ∈ F . Then στ(α) = σ(α) = α and\nσ−1(α) = α. Since the identity fixes every element of E, the set of automorphisms of E\nthat leave elements of F fixed is a subgroup of the entire group of automorphisms of E.\n\n397\n\n\n\n398 CHAPTER 23. GALOIS THEORY\n\nLet E be a field extension of F . We will denote the full group of automorphisms of E\nby Aut(E). We define the Galois group of E over F to be the group of automorphisms\nof E that fix F elementwise; that is,\n\nG(E/F ) = {σ ∈ Aut(E) : σ(α) = α for all α ∈ F}.\n\nIf f(x) is a polynomial in F [x] and E is the splitting field of f(x) over F , then we define\nthe Galois group of f(x) to be G(E/F ).\n\nExample 23.3. Complex conjugation, defined by σ : a+ bi 7→ a− bi, is an automorphism\nof the complex numbers. Since\n\nσ(a) = σ(a+ 0i) = a− 0i = a,\n\nthe automorphism defined by complex conjugation must be in G(C/R).\n\nExample 23.4. Consider the fields Q ⊂ Q(\n√\n5 ) ⊂ Q(\n\n√\n3,\n√\n5 ). Then for a, b ∈ Q(\n\n√\n5 ),\n\nσ(a+ b\n√\n3 ) = a− b\n\n√\n3\n\nis an automorphism of Q(\n√\n3,\n√\n5 ) leaving Q(\n\n√\n5 ) fixed. Similarly,\n\nτ(a+ b\n√\n5 ) = a− b\n\n√\n5\n\nis an automorphism of Q(\n√\n3,\n√\n5 ) leaving Q(\n\n√\n3 ) fixed. The automorphism µ = στ moves\n\nboth\n√\n3 and\n\n√\n5. It will soon be clear that {id, σ, τ, µ} is the Galois group of Q(\n\n√\n3,\n√\n5 )\n\nover Q. The following table shows that this group is isomorphic to Z2 × Z2.\n\nid σ τ µ\n\nid id σ τ µ\n\nσ σ id µ τ\n\nτ τ µ id σ\n\nµ µ τ σ id\n\nWe may also regard the field Q(\n√\n3,\n√\n5 ) as a vector space over Q that has basis {1,\n\n√\n3,\n√\n5,\n√\n15 }.\n\nIt is no coincidence that |G(Q(\n√\n3,\n√\n5 )/Q)| = [Q(\n\n√\n3,\n√\n5 ) : Q)] = 4.\n\nProposition 23.5. Let E be a field extension of F and f(x) be a polynomial in F [x]. Then\nany automorphism in G(E/F ) defines a permutation of the roots of f(x) that lie in E.\n\nProof. Let\nf(x) = a0 + a1x+ a2x\n\n2 + · · ·+ anx\nn\n\nand suppose that α ∈ E is a zero of f(x). Then for σ ∈ G(E/F ),\n\n0 = σ(0)\n\n= σ(f(α))\n\n= σ(a0 + a1α+ a2α\n2 + · · ·+ anα\n\nn)\n\n= a0 + a1σ(α) + a2[σ(α)]\n2 + · · ·+ an[σ(α)]\n\nn;\n\ntherefore, σ(α) is also a zero of f(x).\n\nLet E be an algebraic extension of a field F . Two elements α, β ∈ E are conjugate over\nF if they have the same minimal polynomial. For example, in the field Q(\n\n√\n2 ) the elements√\n\n2 and −\n√\n2 are conjugate over Q since they are both roots of the irreducible polynomial\n\nx2 − 2.\nA converse of the last proposition exists. The proof follows directly from Lemma 21.32.\n\n\n\n23.1. FIELD AUTOMORPHISMS 399\n\nProposition 23.6. If α and β are conjugate over F , there exists an isomorphism σ :\nF (α) → F (β) such that σ is the identity when restricted to F .\n\nTheorem 23.7. Let f(x) be a polynomial in F [x] and suppose that E is the splitting field\nfor f(x) over F . If f(x) has no repeated roots, then\n\n|G(E/F )| = [E : F ].\n\nProof. We will use mathematical induction on the degree of f(x). If the degree of f(x)\nis 0 or 1, then E = F and there is nothing to show. Assume that the result holds for all\npolynomials of degree k with 0 ≤ k < n. Suppose that the degree of f(x) is n. Let p(x)\nbe an irreducible factor of f(x) of degree r. Since all of the roots of p(x) are in E, we can\nchoose one of these roots, say α, so that F ⊂ F (α) ⊂ E. Then\n\n[E : F (α)] = n/r and [F (α) : F ] = r.\n\nIf β is any other root of p(x), then F ⊂ F (β) ⊂ E. By Lemma 21.32, there exists a\nunique isomorphism σ : F (α) → F (β) for each such β that fixes F elementwise. Since\nE is a splitting field of F (β), there are exactly r such isomorphisms. For each of these\nautomorphisms, we can use our induction hypothesis on [E : F (α)] = n/r < n to conclude\nthat\n\n|G(E/F (α))| = [E : F (α)].\n\nConsequently, there are\n[E : F ] = [E : F (α)][F (α) : F ] = n\n\npossible automorphisms of E that fix F , or |G(E/F )| = [E : F ].\n\nCorollary 23.8. Let F be a finite field with a finite extension E such that [E : F ] = k.\nThen G(E/F ) is cyclic of order k.\n\nProof. Let p be the characteristic of E and F and assume that the orders of E and F are\npm and pn, respectively. Then nk = m. We can also assume that E is the splitting field of\nxp\n\nm − x over a subfield of order p. Therefore, E must also be the splitting field of xpm − x\nover F . Applying Theorem 23.7, we find that |G(E/F )| = k.\n\nTo prove that G(E/F ) is cyclic, we must find a generator for G(E/F ). Let σ : E → E\nbe defined by σ(α) = αpn . We claim that σ is the element in G(E/F ) that we are seeking.\nWe first need to show that σ is in Aut(E). If α and β are in E,\n\nσ(α+ β) = (α+ β)p\nn\n= αpn + βp\n\nn\n= σ(α) + σ(β)\n\nby Lemma 22.3 Also, it is easy to show that σ(αβ) = σ(α)σ(β). Since σ is a nonzero\nhomomorphism of fields, it must be injective. It must also be onto, since E is a finite field.\nWe know that σ must be in G(E/F ), since F is the splitting field of xpn − x over the base\nfield of order p. This means that σ leaves every element in F fixed. Finally, we must show\nthat the order of σ is k. By Theorem 23.7, we know that\n\nσk(α) = αpnk\n= αpm = α\n\nis the identity of G(E/F ). However, σr cannot be the identity for 1 ≤ r < k; otherwise,\nxp\n\nnr − x would have pm roots, which is impossible.\n\nExample 23.9. We can now confirm that the Galois group of Q(\n√\n3,\n√\n5 ) over Q in Exam-\n\nple 23.4 is indeed isomorphic to Z2×Z2. Certainly the group H = {id, σ, τ, µ} is a subgroup\nof G(Q(\n\n√\n3,\n√\n5 )/Q); however, H must be all of G(Q(\n\n√\n3,\n√\n5 )/Q), since\n\n|H| = [Q(\n√\n3,\n√\n5 ) : Q] = |G(Q(\n\n√\n3,\n√\n5 )/Q)| = 4.\n\n\n\n400 CHAPTER 23. GALOIS THEORY\n\nExample 23.10. Let us compute the Galois group of\n\nf(x) = x4 + x3 + x2 + x+ 1\n\nover Q. We know that f(x) is irreducible by Exercise 17.4.20 in Chapter 17. Furthermore,\nsince (x− 1)f(x) = x5 − 1, we can use DeMoivre’s Theorem to determine that the roots of\nf(x) are ωi, where i = 1, . . . , 4 and\n\nω = cos(2π/5) + i sin(2π/5).\n\nHence, the splitting field of f(x) must be Q(ω). We can define automorphisms σi of Q(ω) by\nσi(ω) = ωi for i = 1, . . . , 4. It is easy to check that these are indeed distinct automorphisms\nin G(Q(ω)/Q). Since\n\n[Q(ω) : Q] = |G(Q(ω)/Q)| = 4,\n\nthe σi’s must be all of G(Q(ω)/Q). Therefore, G(Q(ω)/Q) ∼= Z4 since ω is a generator for\nthe Galois group.\n\nSeparable Extensions\nMany of the results that we have just proven depend on the fact that a polynomial f(x) in\nF [x] has no repeated roots in its splitting field. It is evident that we need to know exactly\nwhen a polynomial factors into distinct linear factors in its splitting field. Let E be the\nsplitting field of a polynomial f(x) in F [x]. Suppose that f(x) factors over E as\n\nf(x) = (x− α1)\nn1(x− α2)\n\nn2 · · · (x− αr)\nnr =\n\nr∏\ni=1\n\n(x− αi)\nni .\n\nWe define the multiplicity of a root αi of f(x) to be ni. A root with multiplicity 1 is\ncalled a simple root. Recall that a polynomial f(x) ∈ F [x] of degree n is separable if it\nhas n distinct roots in its splitting field E. Equivalently, f(x) is separable if it factors into\ndistinct linear factors over E[x]. An extension E of F is a separable extension of F if\nevery element in E is the root of a separable polynomial in F [x]. Also recall that f(x) is\nseparable if and only if gcd(f(x), f ′(x)) = 1 (Lemma 22.5).\n\nProposition 23.11. Let f(x) be an irreducible polynomial over F . If the characteristic of\nF is 0, then f(x) is separable. If the characteristic of F is p and f(x) ̸= g(xp) for some\ng(x) in F [x], then f(x) is also separable.\n\nProof. First assume that charF = 0. Since deg f ′(x) < deg f(x) and f(x) is irreducible,\nthe only way gcd(f(x), f ′(x)) ̸= 1 is if f ′(x) is the zero polynomial; however, this is impos-\nsible in a field of characteristic zero. If charF = p, then f ′(x) can be the zero polynomial\nif every coefficient of f(x) is a multiple of p. This can happen only if we have a polynomial\nof the form f(x) = a0 + a1x\n\np + a2x\n2p + · · ·+ anx\n\nnp.\n\nCertainly extensions of a field F of the form F (α) are some of the easiest to study\nand understand. Given a field extension E of F , the obvious question to ask is when it\nis possible to find an element α ∈ E such that E = F (α). In this case, α is called a\nprimitive element. We already know that primitive elements exist for certain extensions.\nFor example,\n\nQ(\n√\n3,\n√\n5 ) = Q(\n\n√\n3 +\n\n√\n5 )\n\nand\nQ(\n\n3\n√\n5,\n√\n5 i) = Q(\n\n6\n√\n5 i).\n\nCorollary 22.12 tells us that there exists a primitive element for any finite extension of a\nfinite field. The next theorem tells us that we can often find a primitive element.\n\n\n\n23.2. THE FUNDAMENTAL THEOREM 401\n\nTheorem 23.12 (Primitive Element Theorem). Let E be a finite separable extension of a\nfield F . Then there exists an α ∈ E such that E = F (α).\n\nProof. We already know that there is no problem if F is a finite field. Suppose that E\nis a finite extension of an infinite field. We will prove the result for F (α, β). The general\ncase easily follows when we use mathematical induction. Let f(x) and g(x) be the minimal\npolynomials of α and β, respectively. Let K be the field in which both f(x) and g(x) split.\nSuppose that f(x) has zeros α = α1, . . . , αn in K and g(x) has zeros β = β1, . . . , βm in K.\nAll of these zeros have multiplicity 1, since E is separable over F . Since F is infinite, we\ncan find an a in F such that\n\na ̸= αi − α\n\nβ − βj\n\nfor all i and j with j ̸= 1. Therefore, a(β − βj) ̸= αi − α. Let γ = α+ aβ. Then\n\nγ = α+ aβ ̸= αi + aβj ;\n\nhence, γ − aβj ̸= αi for all i, j with j ̸= 1. Define h(x) ∈ F (γ)[x] by h(x) = f(γ − ax).\nThen h(β) = f(α) = 0. However, h(βj) ̸= 0 for j ̸= 1. Hence, h(x) and g(x) have a single\ncommon factor in F (γ)[x]; that is, the irreducible polynomial of β over F (γ) must be linear,\nsince β is the only zero common to both g(x) and h(x). So β ∈ F (γ) and α = γ − aβ is in\nF (γ). Hence, F (α, β) = F (γ).\n\n23.2 The Fundamental Theorem\nThe goal of this section is to prove the Fundamental Theorem of Galois Theory. This\ntheorem explains the connection between the subgroups of G(E/F ) and the intermediate\nfields between E and F .\n\nProposition 23.13. Let {σi : i ∈ I} be a collection of automorphisms of a field F . Then\n\nF{σi} = {a ∈ F : σi(a) = a for all σi}\n\nis a subfield of F .\n\nProof. Let σi(a) = a and σi(b) = b. Then\n\nσi(a± b) = σi(a)± σi(b) = a± b\n\nand\nσi(ab) = σi(a)σi(b) = ab.\n\nIf a ̸= 0, then σi(a\n−1) = [σi(a)]\n\n−1 = a−1. Finally, σi(0) = 0 and σi(1) = 1 since σi is an\nautomorphism.\n\nCorollary 23.14. Let F be a field and let G be a subgroup of Aut(F ). Then\n\nFG = {α ∈ F : σ(α) = α for all σ ∈ G}\n\nis a subfield of F .\n\nThe subfield F{σi} of F is called the fixed field of {σi}. The field fixed for a subgroup\nG of Aut(F ) will be denoted by FG.\n\nExample 23.15. Let σ : Q(\n√\n3,\n√\n5 ) → Q(\n\n√\n3,\n√\n5 ) be the automorphism that maps\n\n√\n3 to\n\n−\n√\n3. Then Q(\n\n√\n5 ) is the subfield of Q(\n\n√\n3,\n√\n5 ) left fixed by σ.\n\n\n\n402 CHAPTER 23. GALOIS THEORY\n\nProposition 23.16. Let E be a splitting field over F of a separable polynomial. Then\nEG(E/F ) = F .\n\nProof. Let G = G(E/F ). Clearly, F ⊂ EG ⊂ E. Also, E must be a splitting field of EG\n\nand G(E/F ) = G(E/EG). By Theorem 23.7,\n\n|G| = [E : EG] = [E : F ].\n\nTherefore, [EG : F ] = 1. Consequently, EG = F .\n\nA large number of mathematicians first learned Galois theory from Emil Artin’s mono-\ngraph on the subject [1]. The very clever proof of the following lemma is due to Artin.\n\nLemma 23.17. Let G be a finite group of automorphisms of E and let F = EG. Then\n[E : F ] ≤ |G|.\n\nProof. Let |G| = n. We must show that any set of n + 1 elements α1, . . . , αn+1 in E is\nlinearly dependent over F ; that is, we need to find elements ai ∈ F , not all zero, such that\n\na1α1 + a2α2 + · · ·+ an+1αn+1 = 0.\n\nSuppose that σ1 = id, σ2, . . . , σn are the automorphisms in G. The homogeneous system of\nlinear equations\n\nσ1(α1)x1 + σ1(α2)x2 + · · ·+ σ1(αn+1)xn+1 = 0\n\nσ2(α1)x1 + σ2(α2)x2 + · · ·+ σ2(αn+1)xn+1 = 0\n\n...\nσn(α1)x1 + σn(α2)x2 + · · ·+ σn(αn+1)xn+1 = 0\n\nhas more unknowns than equations. From linear algebra we know that this system has a\nnontrivial solution, say xi = ai for i = 1, 2, . . . , n + 1. Since σ1 is the identity, the first\nequation translates to\n\na1α1 + a2α2 + · · ·+ an+1αn+1 = 0.\n\nThe problem is that some of the ai’s may be in E but not in F . We must show that this is\nimpossible.\n\nSuppose that at least one of the ai’s is in E but not in F . By rearranging the αi’s we\nmay assume that a1 is nonzero. Since any nonzero multiple of a solution is also a solution,\nwe can also assume that a1 = 1. Of all possible solutions fitting this description, we choose\nthe one with the smallest number of nonzero terms. Again, by rearranging α2, . . . , αn+1 if\nnecessary, we can assume that a2 is in E but not in F . Since F is the subfield of E that\nis fixed elementwise by G, there exists a σi in G such that σi(a2) ̸= a2. Applying σi to\neach equation in the system, we end up with the same homogeneous system, since G is a\ngroup. Therefore, x1 = σi(a1) = 1, x2 = σi(a2), . . ., xn+1 = σi(an+1) is also a solution of\nthe original system. We know that a linear combination of two solutions of a homogeneous\nsystem is also a solution; consequently,\n\nx1 = 1− 1 = 0\n\nx2 = a2 − σi(a2)\n\n...\nxn+1 = an+1 − σi(an+1)\n\n\n\n23.2. THE FUNDAMENTAL THEOREM 403\n\nmust be another solution of the system. This is a nontrivial solution because σi(a2) ̸= a2,\nand has fewer nonzero entries than our original solution. This is a contradiction, since the\nnumber of nonzero solutions to our original solution was assumed to be minimal. We can\ntherefore conclude that a1, . . . , an+1 ∈ F .\n\nLet E be an algebraic extension of F . If every irreducible polynomial in F [x] with a\nroot in E has all of its roots in E, then E is called a normal extension of F ; that is,\nevery irreducible polynomial in F [x] containing a root in E is the product of linear factors\nin E[x].\n\nTheorem 23.18. Let E be a field extension of F . Then the following statements are\nequivalent.\n\n1. E is a finite, normal, separable extension of F .\n\n2. E is a splitting field over F of a separable polynomial.\n\n3. F = EG for some finite group G of automorphisms of E.\n\nProof. (1) ⇒ (2). Let E be a finite, normal, separable extension of F . By the Primitive\nElement Theorem, we can find an α in E such that E = F (α). Let f(x) be the minimal\npolynomial of α over F . The field E must contain all of the roots of f(x) since it is a normal\nextension F ; hence, E is a splitting field for f(x).\n\n(2) ⇒ (3). Let E be the splitting field over F of a separable polynomial. By Proposi-\ntion 23.16, EG(E/F ) = F . Since |G(E/F )| = [E : F ], this is a finite group.\n\n(3) ⇒ (1). Let F = EG for some finite group of automorphisms G of E. Since [E : F ] ≤\n|G|, E is a finite extension of F . To show that E is a finite, normal extension of F , let\nf(x) ∈ F [x] be an irreducible monic polynomial that has a root α in E. We must show that\nf(x) is the product of distinct linear factors in E[x]. By Proposition 23.5, automorphisms in\nG permute the roots of f(x) lying in E. Hence, if we let G act on α, we can obtain distinct\nroots α1 = α, α2, . . . , αn in E. Let g(x) =\n\n∏n\ni=1(x − αi). Then g(x) is separable over F\n\nand g(α) = 0. Any automorphism σ in G permutes the factors of g(x) since it permutes\nthese roots; hence, when σ acts on g(x), it must fix the coefficients of g(x). Therefore,\nthe coefficients of g(x) must be in F . Since deg g(x) ≤ deg f(x) and f(x) is the minimal\npolynomial of α, f(x) = g(x).\n\nCorollary 23.19. Let K be a field extension of F such that F = KG for some finite group\nof automorphisms G of K. Then G = G(K/F ).\n\nProof. Since F = KG, G is a subgroup of G(K/F ). Hence,\n\n[K : F ] ≤ |G| ≤ |G(K/F )| = [K : F ].\n\nIt follows that G = G(K/F ), since they must have the same order.\n\nBefore we determine the exact correspondence between field extensionsand automor-\nphisms of fields, let us return to a familiar example.\n\nExample 23.20. In Example 23.4 we examined the automorphisms of Q(\n√\n3,\n√\n5 ) fixing\n\nQ. Figure 23.21 compares the lattice of field extensions of Q with the lattice of subgroups\nof G(Q(\n\n√\n3,\n√\n5 )/Q). The Fundamental Theorem of Galois Theory tells us what the rela-\n\ntionship is between the two lattices.\n\n\n\n404 CHAPTER 23. GALOIS THEORY\n\n{id, σ} {id, τ} {id, µ}\n\n{id, σ, τ, µ}\n\n{id}\n\nQ(\n√\n3 ) Q(\n\n√\n5 ) Q(\n\n√\n15 )\n\nQ(\n√\n3,\n√\n5 )\n\nQ\n\nFigure 23.21: G(Q(\n√\n3,\n√\n5 )/Q)\n\nWe are now ready to state and prove the Fundamental Theorem of Galois Theory.\n\nTheorem 23.22 (Fundamental Theorem of Galois Theory). Let F be a finite field or a field\nof characteristic zero. If E is a finite normal extension of F with Galois group G(E/F ),\nthen the following statements are true.\n\n1. The map K 7→ G(E/K) is a bijection of subfields K of E containing F with the\nsubgroups of G(E/F ).\n\n2. If F ⊂ K ⊂ E, then\n\n[E : K] = |G(E/K)| and [K : F ] = [G(E/F ) : G(E/K)].\n\n3. F ⊂ K ⊂ L ⊂ E if and only if {id} ⊂ G(E/L) ⊂ G(E/K) ⊂ G(E/F ).\n\n4. K is a normal extension of F if and only if G(E/K) is a normal subgroup of G(E/F ).\nIn this case\n\nG(K/F ) ∼= G(E/F )/G(E/K).\n\nProof. (1) Suppose that G(E/K) = G(E/L) = G. Both K and L are fixed fields of\nG; hence, K = L and the map defined by K 7→ G(E/K) is one-to-one. To show that\nthe map is onto, let G be a subgroup of G(E/F ) and K be the field fixed by G. Then\nF ⊂ K ⊂ E; consequently, E is a normal extension of K. Thus, G(E/K) = G and the map\nK 7→ G(E/K) is a bijection.\n\n(2) By Theorem 23.7, |G(E/K)| = [E : K]; therefore,\n\n|G(E/F )| = [G(E/F ) : G(E/K)] · |G(E/K)| = [E : F ] = [E : K][K : F ].\n\nThus, [K : F ] = [G(E/F ) : G(E/K)].\n(3) Statement (3) is illustrated in Figure 23.23. We leave the proof of this property as\n\nan exercise.\n(4) This part takes a little more work. Let K be a normal extension of F . If σ is in\n\nG(E/F ) and τ is in G(E/K), we need to show that σ−1τσ is in G(E/K); that is, we need\nto show that σ−1τσ(α) = α for all α ∈ K. Suppose that f(x) is the minimal polynomial of\nα over F . Then σ(α) is also a root of f(x) lying in K, since K is a normal extension of F .\nHence, τ(σ(α)) = σ(α) or σ−1τσ(α) = α.\n\nConversely, let G(E/K) be a normal subgroup of G(E/F ). We need to show that\nF = KG(K/F ). Let τ ∈ G(E/K). For all σ ∈ G(E/F ) there exists a τ ∈ G(E/K) such that\nτσ = στ . Consequently, for all α ∈ K\n\nτ(σ(α)) = σ(τ(α)) = σ(α);\n\n\n\n23.2. THE FUNDAMENTAL THEOREM 405\n\nhence, σ(α) must be in the fixed field of G(E/K). Let σ be the restriction of σ to K. Then\nσ is an automorphism of K fixing F , since σ(α) ∈ K for all α ∈ K; hence, σ ∈ G(K/F ).\nNext, we will show that the fixed field of G(K/F ) is F . Let β be an element in K that\nis fixed by all automorphisms in G(K/F ). In particular, σ(β) = β for all σ ∈ G(E/F ).\nTherefore, β belongs to the fixed field F of G(E/F ).\n\nFinally, we must show that when K is a normal extension of F ,\n\nG(K/F ) ∼= G(E/F )/G(E/K).\n\nFor σ ∈ G(E/F ), let σK be the automorphism of K obtained by restricting σ to K. Since K\nis a normal extension, the argument in the preceding paragraph shows that σK ∈ G(K/F ).\nConsequently, we have a map ϕ : G(E/F ) → G(K/F ) defined by σ 7→ σK . This map is a\ngroup homomorphism since\n\nϕ(στ) = (στ)K = σKτK = ϕ(σ)ϕ(τ).\n\nThe kernel of ϕ is G(E/K). By (2),\n\n|G(E/F )|/|G(E/K)| = [K : F ] = |G(K/F )|.\n\nHence, the image of ϕ is G(K/F ) and ϕ is onto. Applying the First Isomorphism Theorem,\nwe have\n\nG(K/F ) ∼= G(E/F )/G(E/K).\n\nF G(E/F )\n\nK G(E/K)\n\nL G(E/L)\n\nE {id}\n\nFigure 23.23: Subgroups of G(E/F ) and subfields of E\n\nExample 23.24. In this example we will illustrate the Fundamental Theorem of Galois\nTheory by determining the lattice of subgroups of the Galois group of f(x) = x4 − 2. We\nwill compare this lattice to the lattice of field extensions of Q that are contained in the\nsplitting field of x4 − 2. The splitting field of f(x) is Q( 4\n\n√\n2, i). To see this, notice that\n\nf(x) factors as (x2 +\n√\n2 )(x2 −\n\n√\n2 ); hence, the roots of f(x) are ± 4\n\n√\n2 and ± 4\n\n√\n2 i. We first\n\nadjoin the root 4\n√\n2 to Q and then adjoin the root i of x2 + 1 to Q( 4\n\n√\n2 ). The splitting field\n\nof f(x) is then Q( 4\n√\n2 )(i) = Q( 4\n\n√\n2, i).\n\nSince [Q( 4\n√\n2 ) : Q] = 4 and i is not in Q( 4\n\n√\n2 ), it must be the case that [Q( 4\n\n√\n2, i) :\n\nQ( 4\n√\n2 )] = 2. Hence, [Q( 4\n\n√\n2, i) : Q] = 8. The set\n\n{1, 4\n√\n2, (\n\n4\n√\n2 )2, (\n\n4\n√\n2 )3, i, i\n\n4\n√\n2, i(\n\n4\n√\n2 )2, i(\n\n4\n√\n2 )3}\n\n\n\n406 CHAPTER 23. GALOIS THEORY\n\nis a basis of Q( 4\n√\n2, i) over Q. The lattice of field extensions of Q contained in Q( 4\n\n√\n2, i) is\n\nillustrated in Figure 23.25(a).\nThe Galois group G of f(x) must be of order 8. Let σ be the automorphism defined by\n\nσ( 4\n√\n2 ) = i 4\n\n√\n2 and σ(i) = i, and τ be the automorphism defined by complex conjugation;\n\nthat is, τ(i) = −i. Then G has an element of order 4 and an element of order 2. It is easy\nto verify by direct computation that the elements of G are {id, σ, σ2, σ3, τ, στ, σ2τ, σ3τ}\nand that the relations τ2 = id, σ4 = id, and τστ = σ−1 are satisfied; hence, G must be\nisomorphic to D4. The lattice of subgroups of G is illustrated in Figure 23.25(b).\n\n{id}\n\n{id, τ} {id, σ2τ} {id, σ2} {id, στ} {id, σ3τ}\n\n{id, σ2, τ, σ2τ} {id, σ, σ2, σ3}{id, σ2, στ, σ3τ}\n\nD4\n\n(b)\n\nQ\n\nQ(\n√\n2 ) Q(i) Q(\n\n√\n2 i)\n\nQ( 4\n√\n2 ) Q( 4\n\n√\n2 i) Q(\n\n√\n2, i) Q((1 + i) 4\n\n√\n2 ) Q((1− i) 4\n\n√\n2 )\n\nQ( 4\n√\n2, i)\n\n(a)\n\nFigure 23.25: Galois group of x4 − 2\n\nHistorical Note\n\nSolutions for the cubic and quartic equations were discovered in the 1500s. Attempts to\nfind solutions for the quintic equations puzzled some of history’s best mathematicians. In\n1798, P. Ruffini submitted a paper that claimed no such solution could be found; however,\nthe paper was not well received. In 1826, Niels Henrik Abel (1802–1829) finally offered the\nfirst correct proof that quintics are not always solvable by radicals.\n\nAbel inspired the work of Évariste Galois. Born in 1811, Galois began to display ex-\ntraordinary mathematical talent at the age of 14. He applied for entrance to the École\nPolytechnique several times; however, he had great difficulty meeting the formal entrance\nrequirements, and the examiners failed to recognize his mathematical genius. He was finally\naccepted at the École Normale in 1829.\n\n\n\n23.3. APPLICATIONS 407\n\nGalois worked to develop a theory of solvability for polynomials. In 1829, at the age of\n17, Galois presented two papers on the solution of algebraic equations to the Académie des\nSciences de Paris. These papers were sent to Cauchy, who subsequently lost them. A third\npaper was submitted to Fourier, who died before he could read the paper. Another paper\nwas presented, but was not published until 1846.\n\nGalois’ democratic sympathies led him into the Revolution of 1830. He was expelled\nfrom school and sent to prison for his part in the turmoil. After his release in 1832, he was\ndrawn into a duel possibly over a love affair. Certain that he would be killed, he spent the\nevening before his death outlining his work and his basic ideas for research in a long letter\nto his friend Chevalier. He was indeed dead the next day, at the age of 20.\n\n23.3 Applications\nSolvability by Radicals\nThroughout this section we shall assume that all fields have characteristic zero to ensure\nthat irreducible polynomials do not have multiple roots. The immediate goal of this section\nis to determine when the roots of a polynomial f(x) can be computed with a finite number of\noperations on the coefficients of f(x). The allowable operations are addition, subtraction,\nmultiplication, division, and the extraction of nth roots. Certainly the solution to the\nquadratic equation, ax2 + bx+ c = 0, illustrates this process:\n\nx =\n−b±\n\n√\nb2 − 4ac\n\n2a\n.\n\nThe only one of these operations that might demand a larger field is the taking of nth roots.\nWe are led to the following definition.\n\nAn extension field E of a field F is an extension by radicals if there exists a chain of\nsubfields\n\nF = F0 ⊆ F1 ⊆ F2 ⊆ · · · ⊆ Fr = E\n\nsuch for i = 1, 2, . . . , r, we have Fi = Fi−1(αi) and αni\ni ∈ Fi−1 for some positive integer ni.\n\nA polynomial f(x) is solvable by radicals over F if the splitting field K of f(x) over F\nis contained in an extension of F by radicals. Our goal is to arrive at criteria that will tell\nus whether or not a polynomial f(x) is solvable by radicals by examining the Galois group\nf(x).\n\nThe easiest polynomial to solve by radicals is one of the form xn−a. As we discussed in\nChapter 4, the roots of xn − 1 are called the nth roots of unity. These roots are a finite\nsubgroup of the splitting field of xn − 1. By Corollary 22.11, the nth roots of unity form a\ncyclic group. Any generator of this group is called a primitive nth root of unity.\n\nExample 23.26. The polynomial xn − 1 is solvable by radicals over Q. The roots of this\npolynomial are 1, ω, ω2, . . . , ωn−1, where\n\nω = cos\n(\n2π\n\nn\n\n)\n+ i sin\n\n(\n2π\n\nn\n\n)\n.\n\nThe splitting field of xn − 1 over Q is Q(ω).\n\nWe shall prove that a polynomial is solvable by radicals if its Galois group is solvable.\nRecall that a subnormal series of a group G is a finite sequence of subgroups\n\nG = Hn ⊃ Hn−1 ⊃ · · · ⊃ H1 ⊃ H0 = {e},\n\n\n\n408 CHAPTER 23. GALOIS THEORY\n\nwhere Hi is normal in Hi+1. A group G is solvable if it has a subnormal series {Hi} such\nthat all of the factor groups Hi+1/Hi are abelian. For example, if we examine the series\n{id} ⊂ A3 ⊂ S3, we see that S3 is solvable. On the other hand, S5 is not solvable, by\nTheorem 10.11.\n\nLemma 23.27. Let F be a field of characteristic zero and E be the splitting field of xn − a\nover F with a ∈ F . Then G(E/F ) is a solvable group.\n\nProof. The roots of xn − a are n\n√\na, ω n\n\n√\na, . . . , ωn−1 n\n\n√\na, where ω is a primitive nth root\n\nof unity. Suppose that F contains all of its nth roots of unity. If ζ is one of the roots of\nxn − a, then distinct roots of xn − a are ζ, ωζ, . . . , ωn−1ζ, and E = F (ζ). Since G(E/F )\npermutes the roots xn − a, the elements in G(E/F ) must be determined by their action on\nthese roots. Let σ and τ be in G(E/F ) and suppose that σ(ζ) = ωiζ and τ(ζ) = ωjζ. If F\ncontains the roots of unity, then\n\nστ(ζ) = σ(ωjζ) = ωjσ(ζ) = ωi+jζ = ωiτ(ζ) = τ(ωiζ) = τσ(ζ).\n\nTherefore, στ = τσ and G(E/F ) is abelian, and G(E/F ) must be solvable.\nNow suppose that F does not contain a primitive nth root of unity. Let ω be a generator\n\nof the cyclic group of the nth roots of unity. Let α be a zero of xn − a. Since α and ωα\nare both in the splitting field of xn − a, ω = (ωα)/α is also in E. Let K = F (ω). Then\nF ⊂ K ⊂ E. Since K is the splitting field of xn − 1, K is a normal extension of F .\nTherefore, any automorphism σ in G(F (ω)/F ) is determined by σ(ω). It must be the case\nthat σ(ω) = ωi for some integer i since all of the zeros of xn−1 are powers of ω. If τ(ω) = ωj\n\nis in G(F (ω)/F ), then\n\nστ(ω) = σ(ωj) = [σ(ω)]j = ωij = [τ(ω)]i = τ(ωi) = τσ(ω).\n\nTherefore, G(F (ω)/F ) is abelian. By the Fundamental Theorem of Galois Theory the series\n\n{id} ⊂ G(E/F (ω)) ⊂ G(E/F )\n\nis a normal series. By our previous argument, G(E/F (ω)) is abelian. Since\n\nG(E/F )/G(E/F (ω)) ∼= G(F (ω)/F )\n\nis also abelian, G(E/F ) is solvable.\n\nLemma 23.28. Let F be a field of characteristic zero and let\n\nF = F0 ⊆ F1 ⊆ F2 ⊆ · · · ⊆ Fr = E\n\na radical extension of F . Then there exists a normal radical extension\n\nF = K0 ⊆ K1 ⊆ K2 ⊆ · · · ⊆ Kr = K\n\nsuch that K that contains E and Ki is a normal extension of Ki−1.\n\nProof. Since E is a radical extension of F , there exists a chain of subfields\n\nF = F0 ⊆ F1 ⊆ F2 ⊆ · · · ⊆ Fr = E\n\nsuch for i = 1, 2, . . . , r, we have Fi = Fi−1(αi) and αni\ni ∈ Fi−1 for some positive integer ni.\n\nWe will build a normal radical extension of F ,\n\nF = K0 ⊆ K1 ⊆ K2 ⊆ · · · ⊆ Kr = K\n\n\n\n23.3. APPLICATIONS 409\n\nsuch that K ⊇ E. Define K1 for be the splitting field of xn1 − αn1\n1 . The roots of this\n\npolynomial are α1, α1ω, α1ω\n2, . . . , α1ω\n\nn1−1, where ω is a primitive n1th root of unity. If F\ncontains all of its n1 roots of unity, then K1 = F (α!). On the other hand, suppose that\nF does not contain a primitive n1th root of unity. If β is a root of xn1 − αn1\n\n1 , then all of\nthe roots of xn1 − αn1\n\n1 must be β, ωβ, . . . , ωn1−1, where ω is a primitive n1th root of unity.\nIn this case, K1 = F (ωβ). Thus, K1 is a normal radical extension of F containing F1.\nContinuing in this manner, we obtain\n\nF = K0 ⊆ K1 ⊆ K2 ⊆ · · · ⊆ Kr = K\n\nsuch that Ki is a normal extension of Ki−1 and Ki ⊇ Fi for i = 1, 2, . . . , r.\n\nWe will now prove the main theorem about solvability by radicals.\n\nTheorem 23.29. Let f(x) be in F [x], where charF = 0. If f(x) is solvable by radicals,\nthen the Galois group of f(x) over F is solvable.\n\nProof. Since f(x) is solvable by radicals there exists an extension E of F by radicals\nF = F0 ⊆ F1 ⊆ · · · ⊆ Fn = E. By Lemma 23.28, we can assume that E is a splitting field\nf(x) and Fi is normal over Fi−1. By the Fundamental Theorem of Galois Theory, G(E/Fi)\nis a normal subgroup of G(E/Fi−1). Therefore, we have a subnormal series of subgroups of\nG(E/F ):\n\n{id} ⊂ G(E/Fn−1) ⊂ · · · ⊂ G(E/F1) ⊂ G(E/F ).\n\nAgain by the Fundamental Theorem of Galois Theory, we know that\n\nG(E/Fi−1)/G(E/Fi) ∼= G(Fi/Fi−1).\n\nBy Lemma 23.27, G(Fi/Fi−1) is solvable; hence, G(E/F ) is also solvable.\n\nThe converse of Theorem 23.29 is also true. For a proof, see any of the references at the\nend of this chapter.\n\nInsolvability of the Quintic\n\nWe are now in a position to find a fifth-degree polynomial that is not solvable by radicals.\nWe merely need to find a polynomial whose Galois group is S5. We begin by proving a\nlemma.\n\nLemma 23.30. If p is prime, then any subgroup of Sp that contains a transposition and a\ncycle of length p must be all of Sp.\n\nProof. Let G be a subgroup of Sp that contains a transposition σ and τ a cycle of length\np. We may assume that σ = (12). The order of τ is p and τn must be a cycle of length\np for 1 ≤ n < p. Therefore, we may assume that µ = τn = (12i3 . . . ip) for some n, where\n1 ≤ n < p (see Exercise 5.3.13 in Chapter 5). Noting that (12)(12i3 . . . ip) = (2i3 . . . ip)\nand (2i3 . . . ip)\n\nk(12)(2i3 . . . ip)\n−k = (1ik), we can obtain all the transpositions of the form\n\n(1n) for 1 ≤ n < p. However, these transpositions generate all transpositions in Sp, since\n(1j)(1i)(1j) = (ij). The transpositions generate Sp.\n\n\n\n410 CHAPTER 23. GALOIS THEORY\n\n-3 -2 -1 1 2 3\nx\n\n-60\n\n-40\n\n-20\n\n20\n\n40\n\n60\n\ny\n\nf(x) =x5 −6x3 −27x−3\n\nFigure 23.31: The graph of f(x) = x5 − 6x3 − 27x− 3\n\nExample 23.32. We will show that f(x) = x5 − 6x3 − 27x − 3 ∈ Q[x] is not solvable.\nWe claim that the Galois group of f(x) over Q is S5. By Eisenstein’s Criterion, f(x) is\nirreducible and, therefore, must be separable. The derivative of f(x) is f ′(x) = 5x4−18x2−\n27; hence, setting f ′(x) = 0 and solving, we find that the only real roots of f ′(x) are\n\nx = ±\n\n√\n6\n√\n6 + 9\n\n5\n.\n\nTherefore, f(x) can have at most one maximum and one minimum. It is easy to show that\nf(x) changes sign between −3 and −2, between −2 and 0, and once again between 0 and\n4 (Figure 23.31). Therefore, f(x) has exactly three distinct real roots. The remaining two\nroots of f(x) must be complex conjugates. Let K be the splitting field of f(x). Since f(x)\nhas five distinct roots in K and every automorphism of K fixing Q is determined by the\nway it permutes the roots of f(x), we know that G(K/Q) is a subgroup of S5. Since f\nis irreducible, there is an element in σ ∈ G(K/Q) such that σ(a) = b for two roots a and\nb of f(x). The automorphism of C that takes a + bi 7→ a − bi leaves the real roots fixed\nand interchanges the complex roots; consequently, G(K/Q) ⊂ S5. By Lemma 23.30, S5 is\ngenerated by a transposition and an element of order 5; therefore, G(K/Q) must be all of\nS5. By Theorem 10.11, S5 is not solvable. Consequently, f(x) cannot be solved by radicals.\n\n\n\n23.4. EXERCISES 411\n\nThe Fundamental Theorem of Algebra\nIt seems fitting that the last theorem that we will state and prove is the Fundamental\nTheorem of Algebra. This theorem was first proven by Gauss in his doctoral thesis. Prior\nto Gauss’s proof, mathematicians suspected that there might exist polynomials over the real\nand complex numbers having no solutions. The Fundamental Theorem of Algebra states\nthat every polynomial over the complex numbers factors into distinct linear factors.\n\nTheorem 23.33 (Fundamental Theorem of Algebra). The field of complex numbers is\nalgebraically closed; that is, every polynomial in C[x] has a root in C.\n\nProof. Suppose that E is a proper finite field extension of the complex numbers. Since\nany finite extension of a field of characteristic zero is a simple extension, there exists an\nα ∈ E such that E = C(α) with α the root of an irreducible polynomial f(x) in C[x]. The\nsplitting field L of f(x) is a finite normal separable extension of C that contains E. We\nmust show that it is impossible for L to be a proper extension of C.\n\nSuppose that L is a proper extension of C. Since L is the splitting field of f(x)(x2 + 1)\nover R, L is a finite normal separable extension of R. Let K be the fixed field of a Sylow\n2-subgroup G of G(L/R). Then L ⊃ K ⊃ R and |G(L/K)| = [L : K]. Since [L : R] = [L :\nK][K : R], we know that [K : R] must be odd. Consequently, K = R(β) with β having a\nminimal polynomial f(x) of odd degree. Therefore, K = R.\n\nWe now know that G(L/R) must be a 2-group. It follows that G(L/C) is a 2-group. We\nhave assumed that L ̸= C; therefore, |G(L/C)| ≥ 2. By the first Sylow Theorem and the\nFundamental Theorem of Galois Theory, there exists a subgroup G of G(L/C) of index 2\nand a field E fixed elementwise by G. Then [E : C] = 2 and there exists an element γ ∈ E\nwith minimal polynomial x2+ bx+ c in C[x]. This polynomial has roots (−b±\n\n√\nb2 − 4c )/2\n\nthat are in C, since b2 − 4c is in C. This is impossible; hence, L = C.\n\nAlthough our proof was strictly algebraic, we were forced to rely on results from cal-\nculus. It is necessary to assume the completeness axiom from analysis to show that every\npolynomial of odd degree has a real root and that every positive real number has a square\nroot. It seems that there is no possible way to avoid this difficulty and formulate a purely\nalgebraic argument. It is somewhat amazing that there are several elegant proofs of the\nFundamental Theorem of Algebra that use complex analysis. It is also interesting to note\nthat we can obtain a proof of such an important theorem from two very different fields of\nmathematics.\n\n23.4 Exercises\n1. Compute each of the following Galois groups. Which of these field extensions are normal\nfield extensions? If the extension is not normal, find a normal extension of Q in which the\nextension field is contained.\n\n(a) G(Q(\n√\n30 )/Q)\n\n(b) G(Q( 4\n√\n5 )/Q)\n\n(c) G(Q(\n√\n2,\n√\n3,\n√\n5 )/Q)\n\n(d) G(Q(\n√\n2, 3\n\n√\n2, i)/Q)\n\n(e) G(Q(\n√\n6, i)/Q)\n\n2. Determine the separability of each of the following polynomials.\n\n\n\n412 CHAPTER 23. GALOIS THEORY\n\n(a) x3 + 2x2 − x− 2 over Q\n(b) x4 + 2x2 + 1 over Q\n\n(c) x4 + x2 + 1 over Z3\n\n(d) x3 + x2 + 1 over Z2\n\n3. Give the order and describe a generator of the Galois group of GF(729) over GF(9).\n\n4. Determine the Galois groups of each of the following polynomials in Q[x]; hence, deter-\nmine the solvability by radicals of each of the polynomials.\n\n(a) x5 − 12x2 + 2\n\n(b) x5 − 4x4 + 2x+ 2\n\n(c) x3 − 5\n\n(d) x4 − x2 − 6\n\n(e) x5 + 1\n\n(f) (x2 − 2)(x2 + 2)\n\n(g) x8 − 1\n\n(h) x8 + 1\n\n(i) x4 − 3x2 − 10\n\n5. Find a primitive element in the splitting field of each of the following polynomials in\nQ[x].\n\n(a) x4 − 1\n\n(b) x4 − 8x2 + 15\n\n(c) x4 − 2x2 − 15\n\n(d) x3 − 2\n\n6. Prove that the Galois group of an irreducible quadratic polynomial is isomorphic to Z2.\n\n7. Prove that the Galois group of an irreducible cubic polynomial is isomorphic to S3 or\nZ3.\n\n8. Let F ⊂ K ⊂ E be fields. If E is a normal extension of F , show that E must also be a\nnormal extension of K.\n\n9. Let G be the Galois group of a polynomial of degree n. Prove that |G| divides n!.\n\n10. Let F ⊂ E. If f(x) is solvable over F , show that f(x) is also solvable over E.\n\n11. Construct a polynomial f(x) in Q[x] of degree 7 that is not solvable by radicals.\n\n12. Let p be prime. Prove that there exists a polynomial f(x) ∈ Q[x] of degree p with\nGalois group isomorphic to Sp. Conclude that for each prime p with p ≥ 5 there exists a\npolynomial of degree p that is not solvable by radicals.\n\n13. Let p be a prime and Zp(t) be the field of rational functions over Zp. Prove that\nf(x) = xp − t is an irreducible polynomial in Zp(t)[x]. Show that f(x) is not separable.\n\n14. Let E be an extension field of F . Suppose that K and L are two intermediate fields.\nIf there exists an element σ ∈ G(E/F ) such that σ(K) = L, then K and L are said to be\nconjugate fields. Prove that K and L are conjugate if and only if G(E/K) and G(E/L)\nare conjugate subgroups of G(E/F ).\n\n15. Let σ ∈ Aut(R). If a is a positive real number, show that σ(a) > 0.\n\n16. Let K be the splitting field of x3 + x2 + 1 ∈ Z2[x]. Prove or disprove that K is an\nextension by radicals.\n\n17. Let F be a field such that charF ̸= 2. Prove that the splitting field of f(x) = ax2+bx+c\nis F (\n\n√\nα ), where α = b2 − 4ac.\n\n\n\n23.5. REFERENCES AND SUGGESTED READINGS 413\n\n18. Prove or disprove: Two different subgroups of a Galois group will have different fixed\nfields.\n\n19. Let K be the splitting field of a polynomial over F . If E is a field extension of F\ncontained in K and [E : F ] = 2, then E is the splitting field of some polynomial in F [x].\n\n20. We know that the cyclotomic polynomial\n\nΦp(x) =\nxp − 1\n\nx− 1\n= xp−1 + xp−2 + · · ·+ x+ 1\n\nis irreducible over Q for every prime p. Let ω be a zero of Φp(x), and consider the field\nQ(ω).\n(a) Show that ω, ω2, . . . , ωp−1 are distinct zeros of Φp(x), and conclude that they are all\n\nthe zeros of Φp(x).\n(b) Show that G(Q(ω)/Q) is abelian of order p− 1.\n(c) Show that the fixed field of G(Q(ω)/Q) is Q.\n\n21. Let F be a finite field or a field of characteristic zero. Let E be a finite normal\nextension of F with Galois group G(E/F ). Prove that F ⊂ K ⊂ L ⊂ E if and only if\n{id} ⊂ G(E/L) ⊂ G(E/K) ⊂ G(E/F ).\n\n22. Let F be a field of characteristic zero and let f(x) ∈ F [x] be a separable polynomial\nof degree n. If E is the splitting field of f(x), let α1, . . . , αn be the roots of f(x) in E. Let\n∆ =\n\n∏\ni<j(αi − αj). We define the discriminant of f(x) to be ∆2.\n\n(a) If f(x) = x2 + bx+ c, show that ∆2 = b2 − 4c.\n(b) If f(x) = x3 + px+ q, show that ∆2 = −4p3 − 27q2.\n(c) Prove that ∆2 is in F .\n(d) If σ ∈ G(E/F ) is a transposition of two roots of f(x), show that σ(∆) = −∆.\n(e) If σ ∈ G(E/F ) is an even permutation of the roots of f(x), show that σ(∆) = ∆.\n(f) Prove that G(E/F ) is isomorphic to a subgroup of An if and only if ∆ ∈ F .\n(g) Determine the Galois groups of x3 + 2x− 4 and x3 + x− 3.\n\n23.5 References and Suggested Readings\n[1] Artin, E. Theory: Lectures Delivered at the University of Notre Dame (Notre Dame\n\nMathematical Lectures, Number 2). Dover, Mineola, NY, 1997.\n[2] Edwards, H. M. Galois Theory. Springer-Verlag, New York, 1984.\n[3] Fraleigh, J. B. A First Course in Abstract Algebra. 7th ed. Pearson, Upper Saddle\n\nRiver, NJ, 2003.\n[4] Gaal, L. Classical Galois Theory with Examples. American Mathematical Society,\n\nProvidence, 1979.\n[5] Garling, D. J. H. A Course in Galois Theory. Cambridge University Press, Cambridge,\n\n1986.\n[6] Kaplansky, I. Fields and Rings. 2nd ed. University of Chicago Press, Chicago, 1972.\n[7] Rothman, T. “The Short Life of Évariste Galois,” Scientific American, April 1982,\n\n136–49.\n\n\n\n414 CHAPTER 23. GALOIS THEORY\n\n23.6 Sage\nAgain, our competence at examining fields with Sage will allow us to study the main con-\ncepts of Galois Theory easily. We will thoroughly examine Example 7 carefully using our\ncomputational tools.\n\nGalois Groups\nWe will repeat Example 23.24 and analyze carefully the splitting field of the polynomial\np(x) = x4 − 2. We begin with an initial field extension containing at least one root.\n\nx = polygen(QQ, \' x \' )\nN.<a> = NumberField(x^4 - 2); N\n\nNumber Field in a with defining polynomial x^4 - 2\n\nThe .galois_closure() method will create an extension containing all of the roots of the\ndefining polynomial of a number field.\n\nL.<b> = N.galois_closure (); L\n\nNumber Field in b with defining polynomial x^8 + 28*x^4 + 2500\n\nL.degree ()\n\n8\n\ny = polygen(L, \' y \' )\n(y^4 - 2).factor ()\n\n(y - 1/120*b^5 - 19/60*b) *\n(y - 1/240*b^5 + 41/120*b) *\n(y + 1/240*b^5 - 41/120*b) *\n(y + 1/120*b^5 + 19/60*b)\n\nFrom the factorization, it is clear that L is the splitting field of the polynomial, even\nif the factorization is not pretty. It is easy to then obtain the Galois group of this field\nextension.\n\nG = L.galois_group (); G\n\nGalois group of Number Field in b with\ndefining polynomial x^8 + 28*x^4 + 2500\n\nWe can examine this group, and identify it. Notice that since the field is a degree\n8 extension, the group is described as a permutation group on 8 symbols. (It is just a\ncoincidence that the group has 8 elements.) With a paucity of nonabelian groups of order\n8, it is not hard to guess the nature of the group.\n\nG.is_abelian ()\n\nFalse\n\nG.order()\n\n8\n\n\n\n23.6. SAGE 415\n\nG.list()\n\n[(), (1,2,8,7)(3,4,6,5),\n(1,3)(2,5)(4,7)(6,8), (1,4)(2,3)(5,8)(6,7),\n(1,5)(2,6)(3,7)(4,8), (1,6)(2,4)(3,8)(5,7),\n(1,7,8,2)(3,5,6,4), (1,8)(2,7)(3,6)(4,5)]\n\nG.is_isomorphic(DihedralGroup (4))\n\nTrue\n\nThat’s it. But maybe not very satisfying. Let us dig deeper for more understanding.\nWe will start over and create the splitting field of p(x) = x4 − 2 again, but the primary\ndifference is that we will make the roots extremely obvious so we can work more carefully\nwith the Galois group and the fixed fields. Along the way, we will see another example of\nlinear algebra enabling certain computations. The following construction should be familiar\nby now.\n\nx = polygen(QQ, \' x \' )\np = x^4 - 2\nN.<a> = NumberField(p); N\n\nNumber Field in a with defining polynomial x^4 - 2\n\ny = polygen(N, \' y \' )\np = p.subs(x=y)\np.factor ()\n\n(y - a) * (y + a) * (y^2 + a^2)\n\nM.<b> = NumberField(y^2 + a^2); M\n\nNumber Field in b with defining polynomial y^2 + a^2 over\nits base field\n\nz = polygen(M, \' z \' )\n(z^4 - 2).factor ()\n\n(z - b) * (z - a) * (z + a) * (z + b)\n\nThe important thing to notice here is that we have arranged the splitting field so that\nthe four roots, a, -a, b, -b, are very simple functions of the generators. In more traditional\nnotation, a is 2\n\n1\n4 = 4\n\n√\n2, and b is 2\n\n1\n4 i = 4\n\n√\n2i (or their negatives).\n\nWe will find it easier to compute in the flattened tower, a now familiar construction.\nL.<c> = M.absolute_field (); L\n\nNumber Field in c with defining polynomial x^8 + 28*x^4 + 2500\n\nfromL , toL = L.structure ()\n\nWe can return to our original polynomial (over the rationals), and ask for its roots in\nthe flattened tower, custom-designed to contain these roots.\n\nroots = p.roots(ring=L, multiplicities=False); roots\n\n\n\n416 CHAPTER 23. GALOIS THEORY\n\n[1/120*c^5 + 19/60*c,\n1/240*c^5 - 41/120*c,\n\n-1/240*c^5 + 41/120*c,\n-1/120*c^5 - 19/60*c]\n\nHmmm. Do those look right? If you look back at the factorization obtained in the\nfield constructed with the .galois_closure() method, then they look right. But we can do\nbetter.\n\n[fromL(r) for r in roots]\n\n[b, a, -a, -b]\n\nYes, those are the roots.\nThe End() command will create the group of automorphisms of the field L.\nG = End(L); G\n\nAutomorphism group of Number Field in c with\ndefining polynomial x^8 + 28*x^4 + 2500\n\nWe can check that each of these automorphisms fixes the rational numbers elementwise.\nIf a field homomorphism fixes 1, then it will fix the integers, and thus fix all fractions of\nintegers.\n\n[tau(1) for tau in G]\n\n[1, 1, 1, 1, 1, 1, 1, 1]\n\nSo each element of G fixes the rationals elementwise and thus G is the Galois group of\nthe splitting field L over the rationals.\n\nProposition 23.5 is fundamental. It says every automorphism in the Galois group of a\nfield extension creates a permutation of the roots of a polynomial with coefficients in the\nbase field. We have all of those ingredients here. So we will evaluate each automorphism of\nthe Galois group at each of the four roots of our polynomial, which in each case should be\nanother root. (We use the Sequence() constructor just to get nicely-aligned output.)\n\nSequence ([[ fromL(tau(r)) for r in roots] for tau in G], cr=True)\n\n[\n[b, a, -a, -b],\n[-b, -a, a, b],\n[a, -b, b, -a],\n[b, -a, a, -b],\n[-a, -b, b, a],\n[a, b, -b, -a],\n[-b, a, -a, b],\n[-a, b, -b, a]\n]\n\nEach row of the output is a list of the roots, but permuted, and so corresponds to a\npermutation of four objects (the roots). For example, the second row shows the second\nautomorphism interchanging a with -a, and b with -b. (Notice that the first row is the\nresult of the identity automorphism, so we can mentally comine the first row with any other\nrow to imagine a “two-row” form of a permutation.) We can number the roots, 1 through\n4, and create each permutation as an element of S4. It is overkill, but we can then build\nthe permutation group by letting all of these elements generate a group.\n\n\n\n23.6. SAGE 417\n\nS4 = SymmetricGroup (4)\nelements = [S4([1, 2, 3, 4]),\n\nS4([4, 3, 2, 1]),\nS4([2, 4, 1, 3]),\nS4([1, 3, 2, 4]),\nS4([3, 4, 1, 2]),\nS4([2, 1, 4, 3]),\nS4([4, 2, 3, 1]),\nS4([3, 1, 4, 2])]\n\nelements\n\n[(), (1,4)(2,3), (1,2,4,3), (2,3), (1,3)(2,4),\n(1,2)(3,4), (1,4), (1,3,4,2)]\n\nP = S4.subgroup(elements)\nP.is_isomorphic(DihedralGroup (4))\n\nTrue\n\nNotice that we now have built an isomorphism from the Galois group to a group of\npermutations using just four symbols, rather than the eight used previously.\n\nFixed Fields\nIn a previous Sage exercise, we computed the fixed fields of single field automorphisms for\nfinite fields. This was “easy” in the sense that we could just test every element of the field\nto see if it was fixed, since the field was finite. Now we have an infinite field extension.\nHow are we going to determine which elements are fixed by individual automorphisms, or\nsubgroups of automorphisms?\n\nThe answer is to use the vector space structure of the flattened tower. As a degree 8\nextension of the rationals, the first 8 powers of the primitive element c form a basis when\nthe field is viewed as a vector space with the rationals as the scalars. It is sufficient to\nknow how each field automorphism behaves on this basis to fully specify the definition of\nthe automorphism. To wit,\n\nτ(x) = τ\n\n(\n7∑\n\ni=0\n\nqic\ni\n\n)\nqi ∈ Q\n\n=\n\n7∑\ni=0\n\nτ(qi)τ(c\ni) τ is a field automorphism\n\n=\n7∑\n\ni=0\n\nqiτ(c\ni) rationals are fixed\n\nSo we can compute the value of a field automorphism at any linear combination of powers\nof the primitive element as a linear combination of the values of the field automorphism at\njust the powers of the primitive element. This is known as the “power basis”, which we can\nobtain simply with the .power_basis() method. We will begin with an example of how we\ncan use this basis. We will illustrate with the fourth automorphism of the Galois group.\nNotice that the .vector() method is a convenience that strips a linear combination of the\npowers of c into a vector of just the coefficients. (Notice too that τ is totally defined by\nthe value of τ(c), since as a field automorphism τ(ck) = (τ(c))k. However, we still need to\nwork with the entire power basis to exploit the vector space structure.)\n\n\n\n418 CHAPTER 23. GALOIS THEORY\n\nbasis = L.power_basis (); basis\n\n[1, c, c^2, c^3, c^4, c^5, c^6, c^7]\n\ntau = G[3]\nz = 4 + 5*c+ 6*c^3-7*c^6\ntz = tau(4 + 5*c+ 6*c^3-7*c^6); tz\n\n11/250*c^7 - 98/25*c^6 + 1/12*c^5 + 779/125*c^3 +\n6006/25*c^2 - 11/6*c + 4\n\ntz.vector ()\n\n(4, -11/6, 6006/25 , 779/125 , 0, 1/12, -98/25, 11/250)\n\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\ntau_matrix\n\n[ 1 0 0 0 -28 0 0 0]\n[ 0 -11/30 0 0 0 779/15 0 0]\n[ 0 0 -14/25 0 0 0 -858/25 0]\n[ 0 0 0 779/750 0 0 0 -4031/375]\n[ 0 0 0 0 -1 0 0 0]\n[ 0 1/60 0 0 0 11/30 0 0]\n[ 0 0 -1/50 0 0 0 14/25 0]\n[ 0 0 0 11/1500 0 0 0 -779/750]\n\ntau_matrix*z.vector ()\n\n(4, -11/6, 6006/25 , 779/125 , 0, 1/12, -98/25, 11/250)\n\ntau_matrix *(z.vector ()) == (tau(z)).vector ()\n\nTrue\n\nThe last line expresses the fact that tau_matrix is a matrix representation of the field\nautomorphism, viewed as a linear transformation of the vector space structure. As a rep-\nresentation of an invertible field homomorphism, the matrix is invertible. As an order\n2 permutation of the roots, the inverse of the matrix is itself. But these facts are just\nverifications that we have the right thing, we are interested in other properties.\n\nTo construct fixed fields, we want to find elements fixed by automorphisms. Continuing\nwith tau from above, we seek elements z (written as vectors) such that tau_matrix*z=z. These\nare eigenvectors for the eigenvalue 1, or elements of the null space of (tau_matrix - I) (null\nspaces are obtained with .right_kernel() in Sage).\n\nK = (tau_matrix -identity_matrix (8)).right_kernel (); K\n\nVector space of degree 8 and dimension 4 over Rational Field\nBasis matrix:\n[ 1 0 0 0 0 0 0 0]\n[ 0 1 0 0 0 1/38 0 0]\n[ 0 0 1 0 0 0 -1/22 0]\n[ 0 0 0 1 0 0 0 1/278]\n\n\n\n23.6. SAGE 419\n\nEach row of the basis matrix is a vector representing an element of the field, specifically\n1, c + (1/38)*c^5, c^2 - (1/22)*c^6, c^3 + (1/278)*c^7. Let’s take a closer look at these\nfixed elements, in terms we recognize.\n\nfromL (1)\n\n1\n\nfromL(c + (1/38)*c^5)\n\n60/19*b\n\nfromL(c^2 - (1/22)*c^6)\n\n150/11*a^2\n\nfromL(c^3 + (1/278)*c^7)\n\n1500/139*a^2*b\n\nAny element fixed by tau will be a linear combination of these four elements. We can\nignore any rational multiples present, the first element is just saying the rationals are fixed,\nand the last element is just a product of the middle two. So fundamentally tau is fixing\nrationals, b (which is 4\n\n√\n2i) and a^2 (which is\n\n√\n2). Furthermore, b^2 = -a^2 (the check\n\nfollows), so we can create any fixed element of tau by just adjoining b= 4\n√\n2i to the rationals.\n\nSo the elements fixed by tau are Q( 4\n√\n2i).\n\na^2 + b^2\n\n0\n\nGalois Correspondence\nThe entire subfield structure of our splitting field is determined by the subgroup structure\nof the Galois group (Theorem 23.22), which is isomorphic to a group we know well. What\nare the subgroups of our Galois group, expressed as permutation groups? (For brevity, we\njust list the generators of each subgroup.)\n\nsg = P.subgroups ();\n[H.gens() for H in sg]\n\n[[()],\n[(2,3)],\n[(1,4)],\n[(1,4)(2,3)],\n[(1,2)(3,4)],\n[(1,3)(2,4)],\n[(2,3), (1,4)],\n[(1,2)(3,4), (1,4)(2,3)],\n[(1,3,4,2), (1,4)(2,3)],\n[(2,3), (1,2)(3,4), (1,4)]]\n\n[H.order () for H in sg]\n\n[1, 2, 2, 2, 2, 2, 4, 4, 4, 8]\n\n\n\n420 CHAPTER 23. GALOIS THEORY\n\ntau above is the fourth element of the automorphism group, and the fourth permutation\nin elements is the permutation (2,3), the generator (of order 2) for the second subgroup.\nSo as the only nontrivial element of this subgroup, we know that the corresponding fixed\nfield is Q( 4\n\n√\n2i).\n\nLet us analyze another subgroup of order 2, without all the explanation, and starting\nwith the subgroup. The sixth subgroup is generated by the fifth automorphism, so let us\ndetermine the elements that are fixed.\n\ntau = G[4]\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\n(tau_matrix -identity_matrix (8)).right_kernel ()\n\nVector space of degree 8 and dimension 4 over Rational Field\nBasis matrix:\n[ 1 0 0 0 0 0 0 0]\n[ 0 1 0 0 0 1/158 0 0]\n[ 0 0 1 0 0 0 1/78 0]\n[ 0 0 0 1 0 0 0 13/614]\n\nfromL(tau(1))\n\n1\n\nfromL(tau(c+(1/158)*c^5))\n\n120/79*b - 120/79*a\n\nfromL(tau(c^2+(1/78)*c^6))\n\n-200/39*a*b\n\nfromL(tau(c^3+(13/614)*c^7))\n\n3000/307*a^2*b + 3000/307*a^3\n\nThe first element indicates that the rationals are fixed (we knew that). Scaling the\nsecond element gives b - a as a fixed element. Scaling the third and fourth fixed elements,\nwe recognize that they can be obtained from powers of b - a.\n\n(b-a)^2\n\n-2*a*b\n\n(b-a)^3\n\n2*a^2*b + 2*a^3\n\nSo the fixed field of this subgroup can be formed by adjoining b - a to the rationals,\nwhich in mathematical notation is 4\n\n√\n2i− 4\n\n√\n2 = (1− i) 4\n\n√\n2, so the fixed field is Q( 4\n\n√\n2i− 4\n\n√\n2 =\n\n(1− i) 4\n√\n2).\n\nWe can create this fixed field, though as created here it is not strictly a subfield of L.\nWe will use an expression for b - a that is a linear combination of powers of c.\n\nsubinfo = L.subfield ((79/120) *(c+(1/158)*c^5)); subinfo\n\n\n\n23.6. SAGE 421\n\n(Number Field in c0 with defining polynomial x^4 + 8, Ring morphism:\nFrom: Number Field in c0 with defining polynomial x^4 + 8\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: c0 |--> 1/240*c^5 + 79/120*c)\n\nThe .subfield() method returns a pair. The first item is a new number field, isomorphic\nto a subfield of L. The second item is an injective mapping from the new number field into\nL. In this case, the image of the primitive element c0 is the element we have specified as the\ngenerator of the subfield. The primitive element of the new field will satisfy the defining\npolynomial x4+8 — you can check that (1− i) 4\n\n√\n2 is indeed a root of the polynomial x4+8.\n\nThere are five subgroups of order 2, we have found fixed fields for two of them. The other\nthree are similar, so it would be a good exercise to work through them. Our automorphism\ngroup has three subgroups of order 4, and at least one of each possible type (cyclic versus\nnon-cyclic). Fixed fields of larger subgroups require that we find elements fixed by all of the\nautomorphisms in the subgroup. (We were conveniently ignoring the identity automorphism\nabove.) This will require more computation, but will restrict the possibilities (smaller fields)\nto where it will be easier to deduce a primitive element for each field.\n\nThe seventh subgroup is generated by two elements of order 2 and is composed entirely\nof elements of order 2 (except the identity), so is isomorphic to Z2 ×Z2. The permutations\ncorrespond to automorphisms number 0, 1, 3, and 6. To determine the elements fixed by\nall four automorphisms, we will build the kernel for each one and as we go, we form the\nintersection of all four kernels. We will work via a loop over the four automorphisms.\n\nV = QQ^8\nfor tau in [G[0], G[1], G[3], G[6]]:\n\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\nK = (tau_matrix -identity_matrix (8)).right_kernel ()\nV = V.intersection(K)\n\nV\n\nVector space of degree 8 and dimension 2 over Rational Field\nBasis matrix:\n[ 1 0 0 0 0 0 0 0]\n[ 0 0 1 0 0 0 -1/22 0]\n\nOutside of the rationals, there is a single fixed element.\nfromL(tau(c^2 - (1/22)*c^6))\n\n150/11*a^2\n\nRemoving a scalar multiple, our primitive element is a^2, which mathematically is\n√\n2,\n\nso the fixed field is Q(\n√\n2). Again, we can build this fixed field, but ignore the mapping.\n\nF, mapping = L.subfield ((11/150) *(c^2 - (1/22)*c^6))\nF\n\nNumber Field in c0 with defining polynomial x^2 - 2\n\nOne more subgroup. The penultimate subgroup has a permutation of order 4 as a\ngenerator, so is a cyclic group of order 4. The individual permutations of the subgroup\ncorrespond to automorphisms 0, 1, 2, 7.\n\nV = QQ^8\nfor tau in [G[0], G[1], G[2], G[7]]:\n\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\n\n\n\n422 CHAPTER 23. GALOIS THEORY\n\nK = (tau_matrix -identity_matrix (8)).right_kernel ()\nV = V.intersection(K)\n\nV\n\nVector space of degree 8 and dimension 2 over Rational Field\nBasis matrix:\n[1 0 0 0 0 0 0 0]\n[0 0 0 0 1 0 0 0]\n\nSo we compute the primitive element.\nfromL(tau(c^4))\n\n-24*a^3*b - 14\n\nSince rationals are fixed, we can remove the −14 and the multiple and take a^3*b as\nthe primitive element. Mathematically, this is 2i, so we might as well use just i as the\nprimitive element and the fixed field is Q(i). We can then build the fixed field (and ignore\nthe mapping also returned).\n\nF, mapping = L.subfield ((c^4+14) /-48)\nF\n\nNumber Field in c0 with defining polynomial x^2 + 1\n\nThere is one more subgroup of order 4, which we will leave as an exercise to analyze.\nThere are also two trivial subgroups (the identity and the full group) which are not very\ninteresting or surprising.\n\nIf the above seems like too much work, you can always just have Sage do it all with the\n.subfields() method.\n\nL.subfields ()\n\n[\n(Number Field in c0 with defining polynomial x,\nRing morphism:\n\nFrom: Number Field in c0 with defining polynomial x\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: 0 |--> 0,\n\nNone),\n(Number Field in c1 with defining polynomial x^2 + 112*x + 40000,\nRing morphism:\n\nFrom: Number Field in c1 with defining polynomial x^2 + 112*x +\n40000\n\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n2500\n\nDefn: c1 |--> 4*c^4,\nNone),\n\n(Number Field in c2 with defining polynomial x^2 + 512,\nRing morphism:\n\nFrom: Number Field in c2 with defining polynomial x^2 + 512\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: c2 |--> 1/25*c^6 + 78/25*c^2,\n\nNone),\n(Number Field in c3 with defining polynomial x^2 - 288,\nRing morphism:\n\n\n\n23.6. SAGE 423\n\nFrom: Number Field in c3 with defining polynomial x^2 - 288\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: c3 |--> -1/25*c^6 + 22/25*c^2,\n\nNone),\n(Number Field in c4 with defining polynomial x^4 + 112*x^2 + 40000,\nRing morphism:\n\nFrom: Number Field in c4 with defining polynomial x^4 + 112*x^2 +\n40000\n\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n2500\n\nDefn: c4 |--> 2*c^2,\nNone),\n\n(Number Field in c5 with defining polynomial x^4 + 648,\nRing morphism:\n\nFrom: Number Field in c5 with defining polynomial x^4 + 648\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: c5 |--> 1/80*c^5 + 79/40*c,\n\nNone),\n(Number Field in c6 with defining polynomial x^4 + 8,\nRing morphism:\n\nFrom: Number Field in c6 with defining polynomial x^4 + 8\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: c6 |--> -1/80*c^5 + 1/40*c,\nNone),\n\n(Number Field in c7 with defining polynomial x^4 - 512,\nRing morphism:\n\nFrom: Number Field in c7 with defining polynomial x^4 - 512\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: c7 |--> -1/60*c^5 + 41/30*c,\n\nNone),\n(Number Field in c8 with defining polynomial x^4 - 32,\nRing morphism:\n\nFrom: Number Field in c8 with defining polynomial x^4 - 32\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n\n2500\nDefn: c8 |--> 1/60*c^5 + 19/30*c,\n\nNone),\n(Number Field in c9 with defining polynomial x^8 + 28*x^4 + 2500,\nRing morphism:\n\nFrom: Number Field in c9 with defining polynomial x^8 + 28*x^4 +\n2500\n\nTo: Number Field in c with defining polynomial x^8 + 28*x^4 +\n2500\n\nDefn: c9 |--> c,\nRing morphism:\n\nFrom: Number Field in c with defining polynomial x^8 + 28*x^4 +\n2500\n\nTo: Number Field in c9 with defining polynomial x^8 + 28*x^4 +\n2500\n\nDefn: c |--> c9)\n]\n\nTen subfields are described, which is what we would expect, given the 10 subgroups\n\n\n\n424 CHAPTER 23. GALOIS THEORY\n\nof the Galois group. Each begins with a new number field that is a subfield. Technically,\neach is not a subset of L, but the second item returned for each subfield is an injective\nhomomorphism, also known generally as an “embedding.” Each embedding describes how\na primitive element of the subfield translates to an element of L. Some of these primitive\nelements could be manipulated (as we have done above) to yield slightly simpler minimal\npolynomials, but the results are quite impressive nonetheless. Each item in the list has a\nthird component, which is almost always None, except when the subfield is the whole field,\nand then the third component is an injective homomorphism “in the other direction.”\n\nNormal Extensions\nConsider the third subgroup in the list above, generated by the permutation (1,4). As\na subgroup of order 2, it only has one nontrivial element, which here corresponds to the\nseventh automorphism. We determine the fixed elements as before.\n\ntau = G[6]\ntau_matrix = column_matrix ([tau(be).vector () for be in basis])\n(tau_matrix -identity_matrix (8)).right_kernel ()\n\nVector space of degree 8 and dimension 4 over Rational Field\nBasis matrix:\n[ 1 0 0 0 0 0 0 0]\n[ 0 1 0 0 0 -1/82 0 0]\n[ 0 0 1 0 0 0 -1/22 0]\n[ 0 0 0 1 0 0 0 11/58]\n\nfromL(tau(1))\n\n1\n\nfromL(tau(c+( -1/82)*c^5))\n\n-120/41*a\n\nfromL(tau(c^2+( -1/22)*c^6))\n\n150/11*a^2\n\nfromL(tau(c^3+(11/58)*c^7))\n\n3000/29*a^3\n\nAs usual, ignoring rational multiples, we see powers of a and recognize that a alone will\nbe a primitive element for the fixed field, which is thus Q( 4\n\n√\n2). Recognize that a was our\n\nfirst root of x4 − 2, and was used to create the first part of original tower, N. So N is both\nQ( 4\n\n√\n2) and the fixed field of H = ⟨(1, 4)⟩.\n\nQ( 4\n√\n2) contains at least one root of the irreducible x4 − 2, but not all of the roots\n\n(witness the factorization above) and therefore does not qualify as a normal extension. By\npart (4) of Theorem 23.22 the automorphism group of the extension is not normal in the\nfull Galois group.\n\nsg[2]. is_normal(P)\n\nFalse\n\nAs expected.\n\n\n\n23.7. SAGE EXERCISES 425\n\n23.7 Sage Exercises\n1. In the analysis of Example 23.24 with Sage, two subgroups of order 2 and one subgroup\nof order 4 were not analyzed. Determine the fixed fields of these three subgroups.\n\n2. Build the splitting field of p(x) = x3 − 6x2 + 12x − 10 and then determine the Galois\ngroup of p(x) as a concrete group of explicit permutations. Build the lattice of subgroups of\nthe Galois group, again using the same explicit permutations. Using the Fundamental The-\norem of Galois Theory, construct the subfields of the splitting field. Include your supporting\ndocumentation in your submitted Sage worksheet. Also, submit a written component of\nthis assignment containing a complete layout of the subgroups and subfields, written en-\ntirely with mathematical notation and with no Sage commands, designed to illustrate the\ncorrespondence between the two. All you need here is the graphical layout, suitably labeled\n— the Sage worksheet will substantiate your work.\n\n3. The polynomial x5−x−1 has all of the symmetric group S5 as its Galois group. Because\nS5 is not solvable, we know this polynomial to be an example of a quintic polynomial that\nis not solvable by radicals. Unfortunately, asking Sage to compute this Galois group takes\nfar too long. So this exercise will simulate that experience with a slightly smaller example.\nConsider the polynomial p(x) = x4 + x+ 1.\n(a) Build the splitting field of p(x) one root at a time. Create an extension, factor there,\n\ndiscard linear factors, use the remaining irreducible factor to extend once more. Repeat\nuntil p(x) factors completely. Be sure to do a final extension via just a linear factor.\nThis is a little silly, and Sage will seem to ignore your final generator (so you will want\nto setermine what it is equivalent to in terms of the previous gfenerators). Directions\nbelow depend on taking this extra step.\n\n(b) Factor the original polynomial over the final extension field in the tower. What is\nboring about this factorization in comparison to some other examples we have done?\n\n(c) Construct the full tower as an absolute field over Q. From the degree of this extension\nand the degree of the original polynomial, infer the Galois group of the polynomial.\n\n(d) Using the mappings that allow you to translate between the tower and the absolute\nfield (obtained from the .structure() method), choose one of the roots (any one) and\nexpress it in terms of the single generator of the absolute field. Then reverse the\nprocedure and express the single generator of the absolute field in terms of the roots\nin the tower.\n\n(e) Compute the group of automorphisms of the absolute field (but don’t display the whole\ngroup in what you submit). Take all four roots (including your silly one from the last\nstep of the tower construction) and apply each field automorphism to the four roots\n(creating the guaranteed permutations of the roots). Comment on what you see.\n\n(f) There is one nontrivial automorphism that has an especially simple form (it is the\nsecond one for me) when applied to the generator of the absolute field. What does\nthis automorphism do to the roots of p(x)?\n\n(g) Consider the extension of Q formed by adjoining just one of the roots. This is a subfield\nof the splitting field of the polynomial, so is the fixed field of a subgroup of the Galois\ngroup. Give a simple description of the corresponding subgroup using language we\ntypically only apply to permutation groups.\n\n4. Return to the splitting field of the quintic discussed in the introduction to the previous\nproblem (x5 − x − 1). Create the first two intermediate fields by adjoining two roots (one\nat a time). But instead of factoring at each step to get a new irreducible polynomial, divide\n\n\n\n426 CHAPTER 23. GALOIS THEORY\n\nby the linear factor you know is a factor. In general, the quotient might factor further, but\nin this exercise presume it does not. In other words, act as if your quotient by the linear\nfactor is irreducible. If it is not, then the NumberField() command should complain (which\nit will not).\nAfter adjoining two roots, create the extension producing a third root, and do the division.\nYou should now have a quadratic factor. Assuming the quadratic is irreducible (it is) argue\nthat you have enough evidence to establish the order of the Galois group, and hence can\ndetermine exactly which group it is.\nYou can try to use this quadratic factor to create one more step in the extensions, and you\nwill arrive at the splitting field, as can be seen with logic or division. However, this could take\na long time to complete (save your work beforehand!). You can try passing the check=False\n\nargument to the NumberField() command — this will bypass checking irreducibility.\n\n5. Create the finite field of order 36, letting Sage supply the default polynomial for its\nconstruction. The polynomial x6 + x2 + 2 ∗ x+ 1 is irreducible over this finite field. Check\nthat this polynomial splits in the finite field, and then use the .roots() method to collect\nthe roots of the polynomial. Get the group of automorphisms of the field with the End()\n\ncommand.\nYou now have all of the pieces to associate each field automorphism with a permutation of\nthe roots. From this, identify the Galois group and all of its subgroups. For each subgroup,\ndetermine the fixed field. You might find the roots easier to work with if you use the .log()\n\nmethod to identify them as powers of the field’s multiplicative generator.\nYour Galois group in this example will be abelian. So every subgroup is normal, and\nhence any extension is also normal. Can you extend this example by choosing a nontrivial\nintermediate field with a nontrivial irreducible polynomial that has all of its roots in the\nintermediate field and a nontrivial irreducible polynomial with none of its roots in the\nintermediate field?\nYour results here are “typical” in the sense that the particular field or irreducible polynomial\nmakes little difference in the qualitative nature of the results.\n\n6. The splitting field for the irreducible polynomial p(x) = x7 − 7x + 3 has degree 168\n(hence this is the order of the Galois group). This polynomial is derived from an “Elkies\ntrinomial curve,” a hyperelliptic curve (below) that produces polynomials with interesting\nGalois groups:\n\ny2 = x(81x5 + 396x4 + 738x3 + 660x2 + 269x+ 48)\n\nFor p(x) the resulting Galois group is PSL(2, 7), a simple group. If SL(2, 7) is all 2 ×\n2 matrices over Z7 with determinant 1, then PSL(2, 7) is the quotient by the subgroup\n{I2,−I2}. It is the second-smallest non-abelian simple group (after A5).\nSee how far you can get in using Sage to build this splitting field. A degree 7 extension\nwill yield one linear factor, and a subsequent degree 6 extension will yield two linear fac-\ntors, leaving a quartic factor. Here is where the computations begin to slow down. If we\nbelieve that the splitting field has degree 168, then we know that adding a root from this\ndegree 4 factor will get us to the splitting field. Creating this extension may be possible\ncomputationally, but verifying that the quartic splits into linear factors here seems to be\ninfeasible.\n\n7. Return to Example 23.24, and the complete list of subfields obtainable from the .subfields()\n\nmethod applied to the flattened tower. As mentioned, these are technically not subfields,\nbut do have embeddings into the tower. Given two subfields, their respective primitive ele-\nments are embedded into the tower, with an image that is a linear combination of powers\nof the primitive element for the tower.\n\n\n\n23.7. SAGE EXERCISES 427\n\nIf one subfield is contained in the other, then the image of the primitive element for the\nsmaller field should be a linear combination of the (appropriate) powers of the image of the\nprimitive element for the larger field. This is a linear algebra computation that should be\npossible in the tower, relative to the power basis for the whole tower.\nWrite a procedure to determine if two subfields are related by one being a subset of the\nother. Then use this procedure to create the lattice of subfields. The eventual goal would be\na graphical display of the lattice, using the existing plotting facilities available for lattices,\nsimilar to the top half of Figure 23.25. This is a “challenging” exercise, which is code for\n“it is speculative and has not been tested.”\n\n\n\nA\n\nGNU Free Documentation License\n\nVersion 1.3, 3 November 2008\nCopyright © 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. <http://www.\n\nfsf.org/>\nEveryone is permitted to copy and distribute verbatim copies of this license document,\n\nbut changing it is not allowed.\n\n0. PREAMBLE The purpose of this License is to make a manual, textbook, or other\nfunctional and useful document “free” in the sense of freedom: to assure everyone the effec-\ntive freedom to copy and redistribute it, with or without modifying it, either commercially\nor noncommercially. Secondarily, this License preserves for the author and publisher a way\nto get credit for their work, while not being considered responsible for modifications made\nby others.\n\nThis License is a kind of “copyleft”, which means that derivative works of the document\nmust themselves be free in the same sense. It complements the GNU General Public License,\nwhich is a copyleft license designed for free software.\n\nWe have designed this License in order to use it for manuals for free software, because free\nsoftware needs free documentation: a free program should come with manuals providing the\nsame freedoms that the software does. But this License is not limited to software manuals;\nit can be used for any textual work, regardless of subject matter or whether it is published\nas a printed book. We recommend this License principally for works whose purpose is\ninstruction or reference.\n\n1. APPLICABILITY AND DEFINITIONS This License applies to any manual or\nother work, in any medium, that contains a notice placed by the copyright holder saying\nit can be distributed under the terms of this License. Such a notice grants a world-wide,\nroyalty-free license, unlimited in duration, to use that work under the conditions stated\nherein. The “Document”, below, refers to any such manual or work. Any member of the\npublic is a licensee, and is addressed as “you”. You accept the license if you copy, modify\nor distribute the work in a way requiring permission under copyright law.\n\nA “Modified Version” of the Document means any work containing the Document or a\nportion of it, either copied verbatim, or with modifications and/or translated into another\nlanguage.\n\nA “Secondary Section” is a named appendix or a front-matter section of the Document\nthat deals exclusively with the relationship of the publishers or authors of the Document\nto the Document’s overall subject (or to related matters) and contains nothing that could\nfall directly within that overall subject. (Thus, if the Document is in part a textbook of\nmathematics, a Secondary Section may not explain any mathematics.) The relationship\n\n428\n\nhttp://www.fsf.org/\nhttp://www.fsf.org/\n\n\n429\n\ncould be a matter of historical connection with the subject or with related matters, or of\nlegal, commercial, philosophical, ethical or political position regarding them.\n\nThe “Invariant Sections” are certain Secondary Sections whose titles are designated, as\nbeing those of Invariant Sections, in the notice that says that the Document is released\nunder this License. If a section does not fit the above definition of Secondary then it is not\nallowed to be designated as Invariant. The Document may contain zero Invariant Sections.\nIf the Document does not identify any Invariant Sections then there are none.\n\nThe “Cover Texts” are certain short passages of text that are listed, as Front-Cover\nTexts or Back-Cover Texts, in the notice that says that the Document is released under this\nLicense. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at\nmost 25 words.\n\nA “Transparent” copy of the Document means a machine-readable copy, represented in\na format whose specification is available to the general public, that is suitable for revising\nthe document straightforwardly with generic text editors or (for images composed of pixels)\ngeneric paint programs or (for drawings) some widely available drawing editor, and that is\nsuitable for input to text formatters or for automatic translation to a variety of formats\nsuitable for input to text formatters. A copy made in an otherwise Transparent file format\nwhose markup, or absence of markup, has been arranged to thwart or discourage subsequent\nmodification by readers is not Transparent. An image format is not Transparent if used for\nany substantial amount of text. A copy that is not “Transparent” is called “Opaque”.\n\nExamples of suitable formats for Transparent copies include plain ASCII without markup,\nTexinfo input format, LaTeX input format, SGML or XML using a publicly available DTD,\nand standard-conforming simple HTML, PostScript or PDF designed for human modifica-\ntion. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats\ninclude proprietary formats that can be read and edited only by proprietary word proces-\nsors, SGML or XML for which the DTD and/or processing tools are not generally available,\nand the machine-generated HTML, PostScript or PDF produced by some word processors\nfor output purposes only.\n\nThe “Title Page” means, for a printed book, the title page itself, plus such following\npages as are needed to hold, legibly, the material this License requires to appear in the title\npage. For works in formats which do not have any title page as such, “Title Page” means\nthe text near the most prominent appearance of the work’s title, preceding the beginning\nof the body of the text.\n\nThe “publisher” means any person or entity that distributes copies of the Document to\nthe public.\n\nA section “Entitled XYZ” means a named subunit of the Document whose title either\nis precisely XYZ or contains XYZ in parentheses following text that translates XYZ in\nanother language. (Here XYZ stands for a specific section name mentioned below, such\nas “Acknowledgements”, “Dedications”, “Endorsements”, or “History”.) To “Preserve the\nTitle” of such a section when you modify the Document means that it remains a section\n“Entitled XYZ” according to this definition.\n\nThe Document may include Warranty Disclaimers next to the notice which states that\nthis License applies to the Document. These Warranty Disclaimers are considered to be\nincluded by reference in this License, but only as regards disclaiming warranties: any other\nimplication that these Warranty Disclaimers may have is void and has no effect on the\nmeaning of this License.\n\n2. VERBATIM COPYING You may copy and distribute the Document in any\nmedium, either commercially or noncommercially, provided that this License, the copyright\nnotices, and the license notice saying this License applies to the Document are reproduced\n\n\n\n430 APPENDIX A. GNU FREE DOCUMENTATION LICENSE\n\nin all copies, and that you add no other conditions whatsoever to those of this License. You\nmay not use technical measures to obstruct or control the reading or further copying of\nthe copies you make or distribute. However, you may accept compensation in exchange for\ncopies. If you distribute a large enough number of copies you must also follow the conditions\nin section 3.\n\nYou may also lend copies, under the same conditions stated above, and you may publicly\ndisplay copies.\n\n3. COPYING IN QUANTITY If you publish printed copies (or copies in media\nthat commonly have printed covers) of the Document, numbering more than 100, and the\nDocument’s license notice requires Cover Texts, you must enclose the copies in covers that\ncarry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and\nBack-Cover Texts on the back cover. Both covers must also clearly and legibly identify you\nas the publisher of these copies. The front cover must present the full title with all words\nof the title equally prominent and visible. You may add other material on the covers in\naddition. Copying with changes limited to the covers, as long as they preserve the title of\nthe Document and satisfy these conditions, can be treated as verbatim copying in other\nrespects.\n\nIf the required texts for either cover are too voluminous to fit legibly, you should put\nthe first ones listed (as many as fit reasonably) on the actual cover, and continue the rest\nonto adjacent pages.\n\nIf you publish or distribute Opaque copies of the Document numbering more than 100,\nyou must either include a machine-readable Transparent copy along with each Opaque copy,\nor state in or with each Opaque copy a computer-network location from which the general\nnetwork-using public has access to download using public-standard network protocols a\ncomplete Transparent copy of the Document, free of added material. If you use the latter\noption, you must take reasonably prudent steps, when you begin distribution of Opaque\ncopies in quantity, to ensure that this Transparent copy will remain thus accessible at the\nstated location until at least one year after the last time you distribute an Opaque copy\n(directly or through your agents or retailers) of that edition to the public.\n\nIt is requested, but not required, that you contact the authors of the Document well\nbefore redistributing any large number of copies, to give them a chance to provide you with\nan updated version of the Document.\n\n4. MODIFICATIONS You may copy and distribute a Modified Version of the Docu-\nment under the conditions of sections 2 and 3 above, provided that you release the Modified\nVersion under precisely this License, with the Modified Version filling the role of the Doc-\nument, thus licensing distribution and modification of the Modified Version to whoever\npossesses a copy of it. In addition, you must do these things in the Modified Version:\n\nA. Use in the Title Page (and on the covers, if any) a title distinct from that of the\nDocument, and from those of previous versions (which should, if there were any, be\nlisted in the History section of the Document). You may use the same title as a\nprevious version if the original publisher of that version gives permission.\n\nB. List on the Title Page, as authors, one or more persons or entities responsible for\nauthorship of the modifications in the Modified Version, together with at least five\nof the principal authors of the Document (all of its principal authors, if it has fewer\nthan five), unless they release you from this requirement.\n\nC. State on the Title page the name of the publisher of the Modified Version, as the\npublisher.\n\n\n\n431\n\nD. Preserve all the copyright notices of the Document.\n\nE. Add an appropriate copyright notice for your modifications adjacent to the other\ncopyright notices.\n\nF. Include, immediately after the copyright notices, a license notice giving the public\npermission to use the Modified Version under the terms of this License, in the form\nshown in the Addendum below.\n\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover\nTexts given in the Document’s license notice.\n\nH. Include an unaltered copy of this License.\n\nI. Preserve the section Entitled “History”, Preserve its Title, and add to it an item\nstating at least the title, year, new authors, and publisher of the Modified Version as\ngiven on the Title Page. If there is no section Entitled “History” in the Document,\ncreate one stating the title, year, authors, and publisher of the Document as given\non its Title Page, then add an item describing the Modified Version as stated in the\nprevious sentence.\n\nJ. Preserve the network location, if any, given in the Document for public access to a\nTransparent copy of the Document, and likewise the network locations given in the\nDocument for previous versions it was based on. These may be placed in the “History”\nsection. You may omit a network location for a work that was published at least four\nyears before the Document itself, or if the original publisher of the version it refers to\ngives permission.\n\nK. For any section Entitled “Acknowledgements” or “Dedications”, Preserve the Title\nof the section, and preserve in the section all the substance and tone of each of the\ncontributor acknowledgements and/or dedications given therein.\n\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in\ntheir titles. Section numbers or the equivalent are not considered part of the section\ntitles.\n\nM. Delete any section Entitled “Endorsements”. Such a section may not be included in\nthe Modified Version.\n\nN. Do not retitle any existing section to be Entitled “Endorsements” or to conflict in title\nwith any Invariant Section.\n\nO. Preserve any Warranty Disclaimers.\n\nIf the Modified Version includes new front-matter sections or appendices that qualify as\nSecondary Sections and contain no material copied from the Document, you may at your\noption designate some or all of these sections as invariant. To do this, add their titles to\nthe list of Invariant Sections in the Modified Version’s license notice. These titles must be\ndistinct from any other section titles.\n\nYou may add a section Entitled “Endorsements”, provided it contains nothing but en-\ndorsements of your Modified Version by various parties — for example, statements of peer\nreview or that the text has been approved by an organization as the authoritative definition\nof a standard.\n\nYou may add a passage of up to five words as a Front-Cover Text, and a passage of up\nto 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified\n\n\n\n432 APPENDIX A. GNU FREE DOCUMENTATION LICENSE\n\nVersion. Only one passage of Front-Cover Text and one of Back-Cover Text may be added\nby (or through arrangements made by) any one entity. If the Document already includes\na cover text for the same cover, previously added by you or by arrangement made by the\nsame entity you are acting on behalf of, you may not add another; but you may replace the\nold one, on explicit permission from the previous publisher that added the old one.\n\nThe author(s) and publisher(s) of the Document do not by this License give permission\nto use their names for publicity for or to assert or imply endorsement of any Modified\nVersion.\n\n5. COMBINING DOCUMENTS You may combine the Document with other docu-\nments released under this License, under the terms defined in section 4 above for modified\nversions, provided that you include in the combination all of the Invariant Sections of all of\nthe original documents, unmodified, and list them all as Invariant Sections of your combined\nwork in its license notice, and that you preserve all their Warranty Disclaimers.\n\nThe combined work need only contain one copy of this License, and multiple identical\nInvariant Sections may be replaced with a single copy. If there are multiple Invariant\nSections with the same name but different contents, make the title of each such section\nunique by adding at the end of it, in parentheses, the name of the original author or\npublisher of that section if known, or else a unique number. Make the same adjustment to\nthe section titles in the list of Invariant Sections in the license notice of the combined work.\n\nIn the combination, you must combine any sections Entitled “History” in the various\noriginal documents, forming one section Entitled “History”; likewise combine any sections\nEntitled “Acknowledgements”, and any sections Entitled “Dedications”. You must delete\nall sections Entitled “Endorsements”.\n\n6. COLLECTIONS OF DOCUMENTS You may make a collection consisting of\nthe Document and other documents released under this License, and replace the individual\ncopies of this License in the various documents with a single copy that is included in the\ncollection, provided that you follow the rules of this License for verbatim copying of each\nof the documents in all other respects.\n\nYou may extract a single document from such a collection, and distribute it individually\nunder this License, provided you insert a copy of this License into the extracted document,\nand follow this License in all other respects regarding verbatim copying of that document.\n\n7. AGGREGATION WITH INDEPENDENT WORKS A compilation of the\nDocument or its derivatives with other separate and independent documents or works, in or\non a volume of a storage or distribution medium, is called an “aggregate” if the copyright\nresulting from the compilation is not used to limit the legal rights of the compilation’s users\nbeyond what the individual works permit. When the Document is included in an aggregate,\nthis License does not apply to the other works in the aggregate which are not themselves\nderivative works of the Document.\n\nIf the Cover Text requirement of section 3 is applicable to these copies of the Document,\nthen if the Document is less than one half of the entire aggregate, the Document’s Cover\nTexts may be placed on covers that bracket the Document within the aggregate, or the\nelectronic equivalent of covers if the Document is in electronic form. Otherwise they must\nappear on printed covers that bracket the whole aggregate.\n\n8. TRANSLATION Translation is considered a kind of modification, so you may dis-\ntribute translations of the Document under the terms of section 4. Replacing Invariant\nSections with translations requires special permission from their copyright holders, but you\n\n\n\n433\n\nmay include translations of some or all Invariant Sections in addition to the original ver-\nsions of these Invariant Sections. You may include a translation of this License, and all\nthe license notices in the Document, and any Warranty Disclaimers, provided that you also\ninclude the original English version of this License and the original versions of those notices\nand disclaimers. In case of a disagreement between the translation and the original version\nof this License or a notice or disclaimer, the original version will prevail.\n\nIf a section in the Document is Entitled “Acknowledgements”, “Dedications”, or “His-\ntory”, the requirement (section 4) to Preserve its Title (section 1) will typically require\nchanging the actual title.\n\n9. TERMINATION You may not copy, modify, sublicense, or distribute the Document\nexcept as expressly provided under this License. Any attempt otherwise to copy, modify,\nsublicense, or distribute it is void, and will automatically terminate your rights under this\nLicense.\n\nHowever, if you cease all violation of this License, then your license from a particular\ncopyright holder is reinstated (a) provisionally, unless and until the copyright holder explic-\nitly and finally terminates your license, and (b) permanently, if the copyright holder fails to\nnotify you of the violation by some reasonable means prior to 60 days after the cessation.\n\nMoreover, your license from a particular copyright holder is reinstated permanently if\nthe copyright holder notifies you of the violation by some reasonable means, this is the first\ntime you have received notice of violation of this License (for any work) from that copyright\nholder, and you cure the violation prior to 30 days after your receipt of the notice.\n\nTermination of your rights under this section does not terminate the licenses of parties\nwho have received copies or rights from you under this License. If your rights have been\nterminated and not permanently reinstated, receipt of a copy of some or all of the same\nmaterial does not give you any rights to use it.\n\n10. FUTURE REVISIONS OF THIS LICENSE The Free Software Foundation\nmay publish new, revised versions of the GNU Free Documentation License from time to\ntime. Such new versions will be similar in spirit to the present version, but may differ in\ndetail to address new problems or concerns. See http://www.gnu.org/copyleft/.\n\nEach version of the License is given a distinguishing version number. If the Document\nspecifies that a particular numbered version of this License “or any later version” applies\nto it, you have the option of following the terms and conditions either of that specified\nversion or of any later version that has been published (not as a draft) by the Free Software\nFoundation. If the Document does not specify a version number of this License, you may\nchoose any version ever published (not as a draft) by the Free Software Foundation. If the\nDocument specifies that a proxy can decide which future versions of this License can be\nused, that proxy’s public statement of acceptance of a version permanently authorizes you\nto choose that version for the Document.\n\n11. RELICENSING “Massive Multiauthor Collaboration Site” (or “MMC Site”) means\nany World Wide Web server that publishes copyrightable works and also provides prominent\nfacilities for anybody to edit those works. A public wiki that anybody can edit is an example\nof such a server. A “Massive Multiauthor Collaboration” (or “MMC”) contained in the site\nmeans any set of copyrightable works thus published on the MMC site.\n\n“CC-BY-SA” means the Creative Commons Attribution-Share Alike 3.0 license pub-\nlished by Creative Commons Corporation, a not-for-profit corporation with a principal\nplace of business in San Francisco, California, as well as future copyleft versions of that\nlicense published by that same organization.\n\nhttp://www.gnu.org/copyleft/\n\n\n434 APPENDIX A. GNU FREE DOCUMENTATION LICENSE\n\n“Incorporate” means to publish or republish a Document, in whole or in part, as part\nof another Document.\n\nAn MMC is “eligible for relicensing” if it is licensed under this License, and if all works\nthat were first published under this License somewhere other than this MMC, and subse-\nquently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant\nsections, and (2) were thus incorporated prior to November 1, 2008.\n\nThe operator of an MMC Site may republish an MMC contained in the site under CC-\nBY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible\nfor relicensing.\n\nADDENDUM: How to use this License for your documents To use this License\nin a document you have written, include a copy of the License in the document and put the\nfollowing copyright and license notices just after the title page:\n\nCopyright (C) YEAR YOUR NAME.\nPermission is granted to copy, distribute and/or modify this document\nunder the terms of the GNU Free Documentation License, Version 1.3\nor any later version published by the Free Software Foundation;\nwith no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.\nA copy of the license is included in the section entitled "GNU\nFree Documentation License".\n\nIf you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the\n“with… Texts.” line with this:\n\nwith the Invariant Sections being LIST THEIR TITLES, with the\nFront-Cover Texts being LIST, and with the Back-Cover Texts being LIST.\n\nIf you have Invariant Sections without Cover Texts, or some other combination of the\nthree, merge those two alternatives to suit the situation.\n\nIf your document contains nontrivial examples of program code, we recommend releasing\nthese examples in parallel under your choice of free software license, such as the GNU\nGeneral Public License, to permit their use in free software.\n\n\n\nB\n\nHints and Solutions to Selected\nExercises\n\n1.3 Exercises\n\n1. (a) A ∩B = {2}; (b) B ∩ C = {5}.\n\n2. (a) A×B = {(a, 1), (a, 2), (a, 3), (b, 1), (b, 2), (b, 3), (c, 1), (c, 2), (c, 3)}; (d) A×D = ∅.\n\n6. If x ∈ A ∪ (B ∩ C), then either x ∈ A or x ∈ B ∩ C. Thus, x ∈ A ∪ B and A ∪ C.\nHence, x ∈ (A ∪ B) ∩ (A ∪ C). Therefore, A ∪ (B ∩ C) ⊂ (A ∪ B) ∩ (A ∪ C). Conversely,\nif x ∈ (A ∪ B) ∩ (A ∪ C), then x ∈ A ∪ B and A ∪ C. Thus, x ∈ A or x is in both B\nand C. So x ∈ A ∪ (B ∩ C) and therefore (A ∪ B) ∩ (A ∪ C) ⊂ A ∪ (B ∩ C). Hence,\nA ∪ (B ∩ C) = (A ∪B) ∩ (A ∪ C).\n\n10. (A∩B)∪ (A\\B)∪ (B \\A) = (A∩B)∪ (A∩B′)∪ (B∩A′) = [A∩ (B∪B′)]∪ (B∩A′) =\nA ∪ (B ∩A′) = (A ∪B) ∩ (A ∪A′) = A ∪B.\n\n14. A\\(B∪C) = A∩(B∪C)′ = (A∩A)∩(B′∩C ′) = (A∩B′)∩(A∩C ′) = (A\\B)∩(A\\C).\n\n17. (a) Not a map since f(2/3) is undefined; (b) this is a map; (c) not a map, since\nf(1/2) = 3/4 but f(2/4) = 3/8; (d) this is a map.\n\n18. (a) f is one-to-one but not onto. f(R) = {x ∈ R : x > 0}. (c) f is neither one-to-one\nnor onto. f(R) = {x : −1 ≤ x ≤ 1}.\n\n20. (a) f(n) = n+ 1.\n\n22. (a) Let x, y ∈ A. Then g(f(x)) = (g ◦f)(x) = (g ◦f)(y) = g(f(y)). Thus, f(x) = f(y)\nand x = y, so g ◦ f is one-to-one. (b) Let c ∈ C, then c = (g ◦ f)(x) = g(f(x)) for some\nx ∈ A. Since f(x) ∈ B, g is onto.\n\n23. f−1(x) = (x+ 1)/(x− 1).\n\n24. (a) Let y ∈ f(A1 ∪ A2). Then there exists an x ∈ A1 ∪ A2 such that f(x) = y.\nHence, y ∈ f(A1) or f(A2). Therefore, y ∈ f(A1) ∪ f(A2). Consequently, f(A1 ∪ A2) ⊂\nf(A1) ∪ f(A2). Conversely, if y ∈ f(A1) ∪ f(A2), then y ∈ f(A1) or f(A2). Hence, there\nexists an x in A1 or A2 such that f(x) = y. Thus, there exists an x ∈ A1 ∪ A2 such that\nf(x) = y. Therefore, f(A1) ∪ f(A2) ⊂ f(A1 ∪A2), and f(A1 ∪A2) = f(A1) ∪ f(A2).\n\n25. (a) NThe relation fails to be symmetric. (b) The relation is not reflexive, since 0 is\nnot equivalent to itself. (c) The relation is not transitive.\n\n28. Let X = N ∪ {\n√\n2 } and define x ∼ y if x+ y ∈ N.\n\n435\n\n\n\n436 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\n\n2.3 Exercises\n1. The base case, S(1) : [1(1 + 1)(2(1) + 1)]/6 = 1 = 12 is true. Assume that S(k) :\n12 + 22 + · · ·+ k2 = [k(k + 1)(2k + 1)]/6 is true. Then\n\n12 + 22 + · · ·+ k2 + (k + 1)2 = [k(k + 1)(2k + 1)]/6 + (k + 1)2\n\n= [(k + 1)((k + 1) + 1)(2(k + 1) + 1)]/6,\n\nand so S(k + 1) is true. Thus, S(n) is true for all positive integers n.\n3. The base case, S(4) : 4! = 24 > 16 = 24 is true. Assume S(k) : k! > 2k is true. Then\n(k + 1)! = k!(k + 1) > 2k · 2 = 2k+1, so S(k + 1) is true. Thus, S(n) is true for all positive\nintegers n.\n8. Follow the proof in Example 2.4.\n11. The base case, S(0) : (1+x)0−1 = 0 ≥ 0 = 0·x is true. Assume S(k) : (1+x)k−1 ≥ kx\nis true. Then\n\n(1 + x)k+1 − 1 = (1 + x)(1 + x)k − 1\n\n= (1 + x)k + x(1 + x)k − 1\n\n≥ kx+ x(1 + x)k\n\n≥ kx+ x\n\n= (k + 1)x,\n\nso S(k + 1) is true. Therefore, S(n) is true for all positive integers n.\n17. For (a) and (b) use mathematical induction. (c) Show that f1 = 1, f2 = 1, and\nfn+2 = fn+1 + fn. (d) Use part (c). (e) Use part (b) and Exercise 2.3.16.\n19. Use the Fundamental Theorem of Arithmetic.\n23. Use the Principle of Well-Ordering and the division algorithm.\n27. Since gcd(a, b) = 1, there exist integers r and s such that ar+bs = 1. Thus, acr+bcs =\nc. Since a divides both bc and itself, a must divide c.\n29. Every prime must be of the form 2, 3, 6n + 1, or 6n + 5. Suppose there are only\nfinitely many primes of the form 6k + 5.\n\n3.4 Exercises\n1. (a) 3 + 7Z = {. . . ,−4, 3, 10, . . .}; (c) 18 + 26Z; (e) 5 + 6Z.\n2. (a) Not a group; (c) a group.\n6.\n\n· 1 5 7 11\n\n1 1 5 7 11\n\n5 5 1 11 7\n\n7 7 11 1 5\n\n11 11 7 5 1\n\n8. Pick two matrices. Almost any pair will work.\n15. There is a nonabelian group containing six elements.\n16. Look at the symmetry group of an equilateral triangle or a square.\n17. The are five different groups of order 8.\n\n\n\n437\n\n18. Let\nσ =\n\n(\n1 2 · · · n\n\na1 a2 · · · an\n\n)\nbe in Sn. All of the ais must be distinct. There are n ways to choose a1, n − 1 ways to\nchoose a2, . . ., 2 ways to choose an−1, and only one way to choose an. Therefore, we can\nform σ in n(n− 1) · · · 2 · 1 = n! ways.\n25.\n\n(aba−1)n = (aba−1)(aba−1) · · · (aba−1)\n\n= ab(aa−1)b(aa−1)b · · · b(aa−1)ba−1\n\n= abna−1.\n\n31. Since abab = (ab)2 = e = a2b2 = aabb, we know that ba = ab.\n35. H1 = {id}, H2 = {id, ρ1, ρ2}, H3 = {id, µ1}, H4 = {id, µ2}, H5 = {id, µ3}, S3.\n41. The identity of G is 1 = 1+0\n\n√\n2. Since (a+b\n\n√\n2 )(c+d\n\n√\n2 ) = (ac+2bd)+(ad+bc)\n\n√\n2,\n\nG is closed under multiplication. Finally, (a+ b\n√\n2 )−1 = a/(a2 − 2b2)− b\n\n√\n2/(a2 − 2b2).\n\n46. Look at S3.\n49. ba = a4b = a3ab = ab\n\n4.4 Exercises\n1. (a) False; (c) false; (e) true.\n2. (a) 12; (c) infinite; (e) 10.\n3. (a) 7Z = {. . . ,−7, 0, 7, 14, . . .}; (b) {0, 3, 6, 9, 12, 15, 18, 21}; (c) {0}, {0, 6}, {0, 4, 8},\n{0, 3, 6, 9}, {0, 2, 4, 6, 8, 10}; (g) {1, 3, 7, 9}; (j) {1,−1, i,−i}.\n4. (a) (\n\n1 0\n\n0 1\n\n)\n,\n\n(\n−1 0\n\n0 −1\n\n)\n,\n\n(\n0 −1\n\n1 0\n\n)\n,\n\n(\n0 1\n\n−1 0\n\n)\n.\n\n(c) (\n1 0\n\n0 1\n\n)\n,\n\n(\n1 −1\n\n1 0\n\n)\n,\n\n(\n−1 1\n\n−1 0\n\n)\n,\n\n(\n0 1\n\n−1 1\n\n)\n,\n\n(\n0 −1\n\n1 −1\n\n)\n,\n\n(\n−1 0\n\n0 −1\n\n)\n.\n\n10. (a) 0; (b) 1,−1.\n11. 1, 2, 3, 4, 6, 8, 12, 24.\n15. (a) −3 + 3i; (c) 43− 18i; (e) i\n16. (a)\n\n√\n3 + i; (c) −3.\n\n17. (a)\n√\n2 cis(7π/4); (c) 2\n\n√\n2 cis(π/4); (e) 3 cis(3π/2).\n\n18. (a) (1− i)/2; (c) 16(i−\n√\n3 ); (e) −1/4.\n\n22. (a) 292; (c) 1523.\n27. |⟨g⟩ ∩ ⟨h⟩| = 1.\n31. The identity element in any group has finite order. Let g, h ∈ G have orders m and\nn, respectively. Since (g−1)m = e and (gh)mn = e, the elements of finite order in G form a\nsubgroup of G.\n37. If g is an element distinct from the identity in G, g must generate G; otherwise, ⟨g⟩\nis a nontrivial proper subgroup of G.\n\n\n\n438 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\n\n5.3 Exercises\n\n1. (a) (12453); (c) (13)(25).\n2. (a) (135)(24); (c) (14)(23); (e) (1324); (g) (134)(25); (n) (17352).\n3. (a) (16)(15)(13)(14); (c) (16)(14)(12).\n4. (a1, a2, . . . , an)\n\n−1 = (a1, an, an−1, . . . , a2)\n\n5. (a) {(13), (13)(24), (132), (134), (1324), (1342)} is not a subgroup.\n8. (12345)(678).\n11. Permutations of the form\n\n(1), (a1, a2)(a3, a4), (a1, a2, a3), (a1, a2, a3, a4, a5)\n\nare possible for A5.\n17. Calculate (123)(12) and (12)(123).\n25. Consider the cases (ab)(bc) and (ab)(cd).\n30. For (a), show that στσ−1(σ(ai)) = σ(ai+1).\n\n6.4 Exercises\n\n1. The order of g and the order h must both divide the order of G.\n2. The possible orders must divide 60.\n3. This is true for every proper nontrivial subgroup.\n4. False.\n5. (a) ⟨8⟩, 1 + ⟨8⟩, 2 + ⟨8⟩, 3 + ⟨8⟩, 4 + ⟨8⟩, 5 + ⟨8⟩, 6 + ⟨8⟩, and 7 + ⟨8⟩; (c) 3Z, 1 + 3Z,\nand 2 + 3Z.\n7. 4ϕ(15) ≡ 48 ≡ 1 (mod 15).\n12. Let g1 ∈ gH. Show that g1 ∈ Hg and thus gH ⊂ Hg.\n19. Show that g(H ∩K) = gH ∩ gK.\n22. If gcd(m,n) = 1, then ϕ(mn) = ϕ(m)ϕ(n) (Exercise 2.3.26 in Chapter 2).\n\n7.3 Exercises\n\n1. LAORYHAPDWK\n\n3. Hint: V = E, E = X (also used for spaces and punctuation), K = R.\n4. 26!− 1\n\n7. (a) 2791; (c) 112135 25032 442.\n9. (a) 31; (c) 14.\n10. (a) n = 11 · 41; (c) n = 8779 · 4327.\n\n\n\n439\n\n8.5 Exercises\n2. This cannot be a group code since (0000) /∈ C.\n3. (a) 2; (c) 2.\n4. (a) 3; (c) 4.\n6. (a) dmin = 2; (c) dmin = 1.\n7.\n\n(a) (00000), (00101), (10011), (10110)\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n0 1\n\n0 0\n\n1 0\n\n0 1\n\n1 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n(b) (000000), (010111), (101101), (111010)\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 0\n\n0 1\n\n1 0\n\n1 1\n\n0 1\n\n1 1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n9. Multiple errors occur in one of the received words.\n11. (a) A canonical parity-check matrix with standard generator matrix\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\n\n1\n\n0\n\n0\n\n1\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n(c) A canonical parity-check matrix with standard generator matrix\n\nG =\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ed\n1 0\n\n0 1\n\n1 1\n\n1 0\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n12. (a) All possible syndromes occur.\n15. (a) C, (10000)+C, (01000)+C, (00100)+C, (00010)+C, (11000)+C, (01100)+C,\n(01010)+C. A decoding table does not exist for C since this is only a single error-detecting\ncode.\n19. Let x ∈ C have odd weight and define a map from the set of odd codewords to the\nset of even codewords by y 7→ x + y. Show that this map is a bijection.\n23. For 20 information positions, at least 6 check bits are needed to ensure an error-\ncorrecting code.\n\n\n\n440 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\n\n9.3 Exercises\n\n1. Every infinite cyclic group is isomorphic to Z by Theorem 9.7.\n2. Define ϕ : C∗ → GL2(R) by\n\nϕ(a+ bi) =\n\n(\na b\n\n−b a\n\n)\n.\n\n3. False.\n6. Define a map from Zn into the nth roots of unity by k 7→ cis(2kπ/n).\n8. Assume that Q is cyclic and try to find a generator.\n11. There are two nonabelian and three abelian groups that are not isomorphic.\n16. (a) 12; (c) 5.\n19. Draw the picture.\n20. True.\n25. True.\n27. Let a be a generator for G. If ϕ : G → H is an isomorphism, show that ϕ(a) is a\ngenerator for H.\n38. Any automorphism of Z6 must send 1 to another generator of Z6.\n45. To show that ϕ is one-to-one, let g1 = h1k1 and g2 = h2k2 and consider ϕ(g1) = ϕ(g2).\n\n10.3 Exercises\n\n1. (a)\nA4 (12)A4\n\nA4 A4 (12)A4\n\n(12)A4 (12)A4 A4\n\n(c) D4 is not normal in S4.\n8. If a ∈ G is a generator for G, then aH is a generator for G/H.\n11. For any g ∈ G, show that the map ig : G → G defined by ig : x 7→ gxg−1 is an\nisomorphism of G with itself. Then consider ig(H).\n12. Suppose that ⟨g⟩ is normal in G and let y be an arbitrary element of G. If x ∈ C(g),\nwe must show that yxy−1 is also in C(g). Show that (yxy−1)g = g(yxy−1).\n14. (a) Let g ∈ G and h ∈ G′. If h = aba−1b−1, then\n\nghg−1 = gaba−1b−1g−1\n\n= (gag−1)(gbg−1)(ga−1g−1)(gb−1g−1)\n\n= (gag−1)(gbg−1)(gag−1)−1(gbg−1)−1.\n\nWe also need to show that if h = h1 · · ·hn with hi = aibia\n−1\ni b−1\n\ni , then ghg−1 is a product of\nelements of the same type. However, ghg−1 = gh1 · · ·hng−1 = (gh1g\n\n−1)(gh2g\n−1) · · · (ghng−1).\n\n\n\n441\n\n11.3 Exercises\n2. (a) is a homomorphism with kernel {1}; (c) is not a homomorphism.\n4. Since ϕ(m+ n) = 7(m+ n) = 7m+ 7n = ϕ(m) + ϕ(n), ϕ is a homomorphism.\n5. For any homomorphism ϕ : Z24 → Z18, the kernel of ϕ must be a subgroup of Z24 and\nthe image of ϕ must be a subgroup of Z18. Now use the fact that a generator must map to\na generator.\n9. Let a, b ∈ G. Then ϕ(a)ϕ(b) = ϕ(ab) = ϕ(ba) = ϕ(b)ϕ(a).\n17. Find a counterexample.\n\n12.3 Exercises\n1.\n\n1\n\n2\n\n[\n∥x + y∥2 + ∥x∥2 − ∥y∥2\n\n]\n=\n\n1\n\n2\n\n[\n⟨x+ y, x+ y⟩ − ∥x∥2 − ∥y∥2\n\n]\n=\n\n1\n\n2\n\n[\n∥x∥2 + 2⟨x, y⟩+ ∥y∥2 − ∥x∥2 − ∥y∥2\n\n]\n= ⟨x,y⟩.\n\n3. (a) is in SO(2); (c) is not in O(3).\n5. (a) ⟨x,y⟩ = ⟨y,x⟩.\n7. Use the unimodular matrix (\n\n5 2\n\n2 1\n\n)\n.\n\n10. Show that the kernel of the map det : O(n) → R∗ is SO(n).\n13. True.\n17. p6m\n\n13.3 Exercises\n1. There are three possible groups.\n4. (a) {0} ⊂ ⟨6⟩ ⊂ ⟨3⟩ ⊂ Z12; (e) {(1)} × {0} ⊂ {(1), (123), (132)} × {0} ⊂ S3 × {0} ⊂\nS3 × ⟨2⟩ ⊂ S3 × Z4.\n7. Use the Fundamental Theorem of Finitely Generated Abelian Groups.\n12. If N and G/N are solvable, then they have solvable series\n\nN = Nn ⊃ Nn−1 ⊃ · · · ⊃ N1 ⊃ N0 = {e}\nG/N = Gn/N ⊃ Gn−1/N ⊃ · · ·G1/N ⊃ G0/N = {N}.\n\n16. Use the fact that Dn has a cyclic subgroup of index 2.\n21. G/G′ is abelian.\n\n\n\n442 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\n\n14.4 Exercises\n1. Example 14.1: 0, R2 \\ {0}. Example 14.2: X = {1, 2, 3, 4}.\n2. (a) X(1) = {1, 2, 3}, X(12) = {3}, X(13) = {2}, X(23) = {1}, X(123) = X(132) = ∅.\nG1 = {(1), (23)}, G2 = {(1), (13)}, G3 = {(1), (12)}.\n3. (a) O1 = O2 = O3 = {1, 2, 3}.\n6. The conjugacy classes for S4 are\n\nO(1) = {(1)},\nO(12) = {(12), (13), (14), (23), (24), (34)},\nO(12)(34) = {(12)(34), (13)(24), (14)(23)},\n\nO(123) = {(123), (132), (124), (142), (134), (143), (234), (243)},\nO(1234) = {(1234), (1243), (1324), (1342), (1423), (1432)}.\n\nThe class equation is 1 + 3 + 6 + 6 + 8 = 24.\n8. (34 + 31 + 32 + 31 + 32 + 32 + 33 + 33)/8 = 21.\n11. The group of rigid motions of the cube can be described by the allowable permutations\nof the six faces and is isomorphic to S4. There are the identity cycle, 6 permutations with\nthe structure (abcd) that correspond to the quarter turns, 3 permutations with the structure\n(ab)(cd) that correspond to the half turns, 6 permutations with the structure (ab)(cd)(ef)\nthat correspond to rotating the cube about the centers of opposite edges, and 8 permutations\nwith the structure (abc)(def) that correspond to rotating the cube about opposite vertices.\n15. (1 · 26 + 3 · 24 + 4 · 23 + 2 · 22 + 2 · 21)/12 = 13.\n17. (1 · 28 + 3 · 26 + 2 · 24)/6 = 80.\n22. Use the fact that x ∈ gC(a)g−1 if and only if g−1xg ∈ C(a).\n\n15.3 Exercises\n1. If |G| = 18 = 2 · 32, then the order of a Sylow 2-subgroup is 2, and the order of a Sylow\n3-subgroup is 9.\n2. The four Sylow 3-subgroups of S4 are P1 = {(1), (123), (132)}, P2 = {(1), (124), (142)},\nP3 = {(1), (134), (143)}, P4 = {(1), (234), (243)}.\n5. Since |G| = 96 = 25 · 3, G has either one or three Sylow 2-subgroups by the Third\nSylow Theorem. If there is only one subgroup, we are done. If there are three Sylow 2-\nsubgroups, let H and K be two of them. Therefore, |H ∩K| ≥ 16; otherwise, HK would\nhave (32 ·32)/8 = 128 elements, which is impossible. Thus, H ∩K is normal in both H and\nK since it has index 2 in both groups.\n8. Show that G has a normal Sylow p-subgroup of order p2 and a normal Sylow q-subgroup\nof order q2.\n10. False.\n17. If G is abelian, then G is cyclic, since |G| = 3 · 5 · 17. Now look at Example 15.14.\n23. Define a mapping between the right cosets of N(H) in G and the conjugates of H in\nG by N(H)g 7→ g−1Hg. Prove that this map is a bijection.\n26. Let aG′, bG′ ∈ G/G′. Then (aG′)(bG′) = abG′ = ab(b−1a−1ba)G′ = (abb−1a−1)baG′ =\nbaG′.\n\n\n\n443\n\n16.6 Exercises\n1. (a) 7Z is a ring but not a field; (c) Q(\n\n√\n2 ) is a field; (f) R is not a ring.\n\n3. (a) {1, 3, 7, 9}; (c) {1, 2, 3, 4, 5, 6}; (e){(\n1 0\n\n0 1\n\n)\n,\n\n(\n1 1\n\n0 1\n\n)\n,\n\n(\n1 0\n\n1 1\n\n)\n,\n\n(\n0 1\n\n1 0\n\n)\n,\n\n(\n1 1\n\n1 0\n\n)\n,\n\n(\n0 1\n\n1 1\n\n)\n,\n\n}\n.\n\n4. (a) {0}, {0, 9}, {0, 6, 12}, {0, 3, 6, 9, 12, 15}, {0, 2, 4, 6, 8, 10, 12, 14, 16}; (c) there are no\nnontrivial ideals.\n7. Assume there is an isomorphism ϕ : C → R with ϕ(i) = a.\n8. False. Assume there is an isomorphism ϕ : Q(\n\n√\n2 ) → Q(\n\n√\n3 ) such that ϕ(\n\n√\n2 ) = a.\n\n13. (a) x ≡ 17 (mod 55); (c) x ≡ 214 (mod 2772).\n16. If I ̸= {0}, show that 1 ∈ I.\n18. (a) ϕ(a)ϕ(b) = ϕ(ab) = ϕ(ba) = ϕ(b)ϕ(a).\n26. Let a ∈ R with a ̸= 0. Then the principal ideal generated by a is R. Thus, there\nexists a b ∈ R such that ab = 1.\n28. Compute (a+ b)2 and (−ab)2.\n34. Let a/b, c/d ∈ Z(p). Then a/b+ c/d = (ad+ bc)/bd and (a/b) · (c/d) = (ac)/(bd) are\nboth in Z(p), since gcd(bd, p) = 1.\n38. Suppose that x2 = x and x ̸= 0. Since R is an integral domain, x = 1. To find a\nnontrivial idempotent, look in M2(R).\n\n17.4 Exercises\n2. (a) 9x2 + 2x+ 5; (b) 8x4 + 7x3 + 2x2 + 7x.\n3. (a) 5x3 + 6x2 − 3x + 4 = (5x2 + 2x + 1)(x − 2) + 6; (c) 4x5 − x3 + x2 + 4 = (4x2 +\n4)(x3 + 3) + 4x2 + 2.\n5. (a) No zeros in Z12; (c) 3, 4.\n7. Look at (2x+ 1).\n8. (a) Reducible; (c) irreducible.\n10. One factorization is x2 + x+ 8 = (x+ 2)(x+ 9).\n13. The integers Z do not form a field.\n14. False.\n16. Let ϕ : R→ S be an isomorphism. Define ϕ : R[x] → S[x] by ϕ(a0+a1x+· · ·+anxn) =\nϕ(a0) + ϕ(a1)x+ · · ·+ ϕ(an)x\n\nn.\n20. The polynomial\n\nΦn(x) =\nxn − 1\n\nx− 1\n= xn−1 + xn−2 + · · ·+ x+ 1\n\nis called the cyclotomic polynomial. Show that Φp(x) is irreducible over Q for any prime\np.\n26. Find a nontrivial proper ideal in F [x].\n\n\n\n444 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\n\n18.3 Exercises\n1. Note that z−1 = 1/(a + b\n\n√\n3 i) = (a − b\n\n√\n3 i)/(a2 + 3b2) is in Z[\n\n√\n3 i] if and only if\n\na2 + 3b2 = 1. The only integer solutions to the equation are a = ±1, b = 0.\n2. (a) 5 = −i(1 + 2i)(2 + i); (c) 6 + 8i = −i(1 + i)2(2 + i)2.\n4. True.\n9. Let z = a+ bi and w = c+ di ̸= 0 be in Z[i]. Prove that z/w ∈ Q(i).\n15. Let a = ub with u a unit. Then ν(b) ≤ ν(ub) ≤ ν(a). Similarly, ν(a) ≤ ν(b).\n16. Show that 21 can be factored in two different ways.\n\n19.4 Exercises\n2.\n\n1\n\n5\n\n10\n\n30\n\n2 3\n\n15\n\n5. False.\n6. (a) (a ∨ b ∨ a′) ∧ a\n\na′\n\nb\n\na\n\na\n\n(c) a ∨ (a ∧ b)\n\na\n\na b\n\n8. Not equivalent.\n10. (a) a′ ∧ [(a ∧ b′) ∨ b] = a ∧ (a ∨ b).\n14. Let I, J be ideals in R. We need to show that I + J = {r + s : r ∈ I and s ∈ J}\nis the smallest ideal in R containing both I and J . If r1, r2 ∈ I and s1, s2 ∈ J , then\n(r1+s1)+(r2+s2) = (r1+r2)+(s1+s2) is in I+J . For a ∈ R, a(r1+s1) = ar1+as1 ∈ I+J ;\nhence, I + J is an ideal in R.\n18. (a) No.\n\n\n\n445\n\n20. (⇒). a = b⇒ (a∧b′)∨(a′∧b) = (a∧a′)∨(a′∧a) = O∨O = O. (⇐). (a∧b′)∨(a′∧b) =\nO ⇒ a ∨ b = (a ∨ a) ∨ b = a ∨ (a ∨ b) = a ∨ [I ∧ (a ∨ b)] = a ∨ [(a ∨ a′) ∧ (a ∨ b)] =\n[a ∨ (a ∧ b′)] ∨ [a ∨ (a′ ∧ b)] = a ∨ [(a ∧ b′) ∨ (a′ ∧ b)] = a ∨ 0 = a. A symmetric argument\nshows that a ∨ b = b.\n\n20.4 Exercises\n3. Q(\n\n√\n2,\n√\n3 ) has basis {1,\n\n√\n2,\n√\n3,\n√\n6 } over Q.\n\n5. The set {1, x, x2, . . . , xn−1} is a basis for Pn.\n7. (a) Subspace of dimension 2 with basis {(1, 0,−3), (0, 1, 2)}; (d) not a subspace\n10. Since 0 = α0 = α(−v + v) = α(−v) + αv, it follows that −αv = α(−v).\n12. Let v0 = 0, v1, . . . , vn ∈ V and α0 ̸= 0, α1, . . . , αn ∈ F . Then α0v0 + · · ·+ αnvn = 0.\n15. (a) Let u, v ∈ ker(T ) and α ∈ F . Then\n\nT (u+ v) = T (u) + T (v) = 0\n\nT (αv) = αT (v) = α0 = 0.\n\nHence, u+ v, αv ∈ ker(T ), and ker(T ) is a subspace of V .\n(c) The statement that T (u) = T (v) is equivalent to T (u− v) = T (u)−T (v) = 0, which\n\nis true if and only if u− v = 0 or u = v.\n17. (a) Let u, u′ ∈ U and v, v′ ∈ V . Then\n\n(u+ v) + (u′ + v′) = (u+ u′) + (v + v′) ∈ U + V\n\nα(u+ v) = αu+ αv ∈ U + V.\n\n21.4 Exercises\n1. (a) x4 − (2/3)x2 − 62/9; (c) x4 − 2x2 + 25.\n2. (a) {1,\n\n√\n2,\n√\n3,\n√\n6 }; (c) {1, i,\n\n√\n2,\n√\n2 i}; (e) {1, 21/6, 21/3, 21/2, 22/3, 25/6}.\n\n3. (a) Q(\n√\n3,\n√\n7 ).\n\n5. Use the fact that the elements of Z2[x]/⟨x3+x+1⟩ are 0, 1, α, 1+α, α2, 1+α2, α+α2,\n1 + α+ α2 and the fact that α3 + α+ 1 = 0.\n8. False.\n14. Suppose that E is algebraic over F and K is algebraic over E. Let α ∈ K. It suffices\nto show that α is algebraic over some finite extension of F . Since α is algebraic over E,\nit must be the zero of some polynomial p(x) = β0 + β1x + · · · + βnx\n\nn in E[x]. Hence α is\nalgebraic over F (β0, . . . , βn).\n22. Since {1,\n\n√\n3,\n√\n7,\n√\n21 } is a basis for Q(\n\n√\n3,\n√\n7 ) over Q, Q(\n\n√\n3,\n√\n7 ) ⊃ Q(\n\n√\n3 +\n\n√\n7 ).\n\nSince [Q(\n√\n3,\n√\n7 ) : Q] = 4, [Q(\n\n√\n3 +\n\n√\n7 ) : Q] = 2 or 4. Since the degree of the minimal\n\npolynomial of\n√\n3 +\n\n√\n7 is 4, Q(\n\n√\n3,\n√\n7 ) = Q(\n\n√\n3 +\n\n√\n7 ).\n\n27. Let β ∈ F (α) not in F . Then β = p(α)/q(α), where p and q are polynomials in α\nwith q(α) ̸= 0 and coefficients in F . If β is algebraic over F , then there exists a polynomial\nf(x) ∈ F [x] such that f(β) = 0. Let f(x) = a0 + a1x+ · · ·+ anx\n\nn. Then\n\n0 = f(β) = f\n\n(\np(α)\n\nq(α)\n\n)\n= a0 + a1\n\n(\np(α)\n\nq(α)\n\n)\n+ · · ·+ an\n\n(\np(α)\n\nq(α)\n\n)n\n\n.\n\nNow multiply both sides by q(α)n to show that there is a polynomial in F [x] that has α as\na zero.\n\n\n\n446 APPENDIX B. HINTS AND SOLUTIONS TO SELECTED EXERCISES\n\n22.3 Exercises\n1. Make sure that you have a field extension.\n4. There are eight elements in Z2(α). Exhibit two more zeros of x3 + x2 +1 other than α\nin these eight elements.\n5. Find an irreducible polynomial p(x) in Z3[x] of degree 3 and show that Z3[x]/⟨p(x)⟩\nhas 27 elements.\n7. (a) x5− 1 = (x+1)(x4+x3+x2+x+1); (c) x9− 1 = (x+1)(x2+x+1)(x6+x3+1).\n8. True.\n11. (a) Use the fact that x7 − 1 = (x+ 1)(x3 + x+ 1)(x3 + x2 + 1).\n12. False.\n17. If p(x) ∈ F [x], then p(x) ∈ E[x].\n18. Since α is algebraic over F of degree n, we can write any element β ∈ F (α) uniquely as\nβ = a0+a1α+ · · ·+an−1α\n\nn−1 with ai ∈ F . There are qn possible n-tuples (a0, a1, . . . , an−1).\n24. Factor xp−1 − 1 over Zp.\n\n23.4 Exercises\n1. (a) Z2; (c) Z2 × Z2 × Z2.\n2. (a) Separable over Q since x3 + 2x2 − x− 2 = (x− 1)(x+ 1)(x+ 2); (c) not separable\nover Z3 since x4 + x2 + 1 = (x+ 1)2(x+ 2)2.\n3. If\n\n[GF(729) : GF(9)] = [GF(729) : GF(3)]/[GF(9) : GF(3)] = 6/2 = 3,\n\nthen G(GF(729)/GF(9)) ∼= Z3. A generator for G(GF(729)/GF(9)) is σ, where σ36(α) =\nα36 = α729 for α ∈ GF(729).\n4. (a) S5; (c) S3; (g) see Example 23.10.\n5. (a) Q(i)\n\n7. Let E be the splitting field of a cubic polynomial in F [x]. Show that [E : F ] is less\nthan or equal to 6 and is divisible by 3. Since G(E/F ) is a subgroup of S3 whose order is\ndivisible by 3, conclude that this group must be isomorphic to Z3 or S3.\n9. G is a subgroup of Sn.\n16. True.\n20.\n\n(a) Clearly ω, ω2, . . . , ωp−1 are distinct since ω ̸= 1 or 0. To show that ωi is a zero of Φp,\ncalculate Φp(ω\n\ni).\n\n(b) The conjugates of ω are ω, ω2, . . . , ωp−1. Define a map ϕi : Q(ω) → Q(ωi) by\n\nϕi(a0 + a1ω + · · ·+ ap−2ω\np−2) = a0 + a1ω\n\ni + · · ·+ cp−2(ω\ni)p−2,\n\nwhere ai ∈ Q. Prove that ϕi is an isomorphism of fields. Show that ϕ2 generates\nG(Q(ω)/Q).\n\n(c) Show that {ω, ω2, . . . , ωp−1} is a basis for Q(ω) over Q, and consider which linear\ncombinations of ω, ω2, . . . , ωp−1 are left fixed by all elements of G(Q(ω)/Q).\n\n\n\nC\n\nNotation\n\nThe following table defines the notation used in this book. Page numbers or references refer\nto the first appearance of each symbol.\n\nSymbol Description Page\n\na ∈ A a is in the set A 3\nN the natural numbers 4\nZ the integers 4\nQ the rational numbers 4\nR the real numbers 4\nC the complex numbers 4\nA ⊂ B A is a subset of B 4\n∅ the empty set 4\nA ∪B the union of sets A and B 4\nA ∩B the intersection of sets A and B 4\nA′ complement of the set A 4\nA \\B difference between sets A and B 5\nA×B Cartesian product of sets A and B 6\nAn A× · · · ×A (n times) 6\nid identity mapping 9\nf−1 inverse of the function f 9\na ≡ b (mod n) a is congruent to b modulo n 12\nn! n factorial 23(\nn\nk\n\n)\nbinomial coefficient n!/(k!(n− k)!) 23\n\na | b a divides b 25\ngcd(a, b) greatest common divisor of a and b 25\nP(X) power set of X 29\nlcm(m,n) the least common multiple of m and n 30\nZn the integers modulo n 36\nU(n) group of units in Zn 41\nMn(R) the n× n matrices with entries in R 42\ndetA the determinant of A 42\nGLn(R) the general linear group 42\nQ8 the group of quaternions 42\nC∗ the multiplicative group of complex numbers 42\n|G| the order of a group 43\n\n(Continued on next page)\n\n447\n\n\n\n448 APPENDIX C. NOTATION\n\nSymbol Description Page\n\nR∗ the multiplicative group of real numbers 45\nQ∗ the multiplicative group of rational numbers 45\nSLn(R) the special linear group 45\nZ(G) the center of a group 50\n⟨a⟩ cyclic group generated by a 59\n|a| the order of an element a 60\ncis θ cos θ + i sin θ 63\nT the circle group 64\nSn the symmetric group on n letters 80\n(a1, a2, . . . , ak) cycle of length k 82\nAn the alternating group on n letters 85\nDn the dihedral group 86\n[G : H] index of a subgroup H in a group G 102\nLH the set of left cosets of a subgroup H in a group G 103\nRH the set of right cosets of a subgroup H in a group G 103\nd(x,y) Hamming distance between x and y 130\ndmin the minimum distance of a code 130\nw(x) the weight of x 130\nMm×n(Z2) the set of m× n matrices with entries in Z2 134\nNull(H) null space of a matrix H 134\nδij Kronecker delta 138\nG ∼= H G is isomorphic to a group H 152\nAut(G) automorphism group of a group G 161\nig ig(x) = gxg−1 161\nInn(G) inner automorphism group of a group G 161\nρg right regular representation 161\nG/N factor group of G mod N 169\nG′ commutator subgroup of G 174\nkerϕ kernel of ϕ 180\n(aij) matrix 193\nO(n) orthogonal group 195\n∥x∥ length of a vector x 195\nSO(n) special orthogonal group 198\nE(n) Euclidean group 198\nOx orbit of x 221\nXg fixed point set of g 221\nGx isotropy subgroup of x 221\nN(H) normalizer of s subgroup H 240\nH the ring of quaternions 257\nZ[i] the Gaussian integers 258\ncharR characteristic of a ring R 259\nZ(p) ring of integers localized at p 272\ndeg f(x) degree of a polynomial 283\nR[x] ring of polynomials over a ring R 284\n\n(Continued on next page)\n\n\n\n449\n\nSymbol Description Page\n\nR[x1, x2, . . . , xn] ring of polynomials in n indeterminants 286\nϕα evaluation homomorphism at α 286\nQ(x) field of rational functions over Q 307\nν(a) Euclidean valuation of a 310\nF (x) field of rational functions in x 314\nF (x1, . . . , xn) field of rational functions in x1, . . . , xn 314\na ⪯ b a is less than b 320\na ∨ b join of a and b 322\na ∧ b meet of a and b 322\nI largest element in a lattice 323\nO smallest element in a lattice 323\na′ complement of a in a lattice 323\ndimV dimension of a vector space V 344\nU ⊕ V direct sum of vector spaces U and V 346\nHom(V,W ) set of all linear transformations from U into V 346\nV ∗ dual of a vector space V 346\nF (α1, . . . , αn) smallest field containing F and α1, . . . , αn 356\n[E : F ] dimension of a field extension of E over F 359\nGF(pn) Galois field of order pn 382\nF ∗ multiplicative group of a field F 383\nG(E/F ) Galois group of E over F 398\nF{σi} field fixed by the automorphism σi 401\nFG field fixed by the automorphism group G 401\n∆2 discriminant of a polynomial 413\n\n\n\n450 APPENDIX C. NOTATION\n\n\n\nIndex\n\nG-equivalent, 221\nG-set, 220\nnth root of unity, 64, 407\nrsa cryptosystem, 116\n\nAbel, Niels Henrik, 406\nAbelian group, 40\nAdleman, L., 116\nAlgebraic closure, 361\nAlgebraic extension, 356\nAlgebraic number, 357\nAlgorithm\n\ndivision, 286\nEuclidean, 27\n\nAscending chain condition, 309\nAssociate elements, 307\nAtom, 326\nAutomorphism\n\ninner, 185\n\nBasis of a lattice, 201\nBieberbach, L., 204\nBinary operation, 40\nBinary symmetric channel, 129\nBoole, George, 330\nBoolean algebra\n\natom in a, 326\ndefinition of, 324\nfinite, 326\nisomorphism, 326\n\nBoolean function, 228, 333\nBurnside’s Counting Theorem, 225\nBurnside, William, 44, 173, 230\n\nCancellation law\nfor groups, 44\nfor integral domains, 259\n\nCardano, Gerolamo, 293\nCarmichael numbers, 121\n\nCauchy’s Theorem, 239\nCauchy, Augustin-Louis, 86\nCayley table, 41\nCayley’s Theorem, 155\nCayley, Arthur, 155\nCentralizer\n\nof a subgroup, 223\nCharacteristic of a ring, 259\nChinese Remainder Theorem\n\nfor integers, 266\nCipher, 113\nCiphertext, 113\nCircuit\n\nparallel, 328\nseries, 328\nseries-parallel, 329\n\nClass equation, 223\nCode\n\nbch, 389\ncyclic, 383\ngroup, 132\nlinear, 135\nminimum distance of, 130\npolynomial, 384\n\nCommutative diagrams, 182\nCommutative rings, 255\nComposite integer, 27\nComposition series, 213\nCongruence modulo n, 12\nConjugacy classes, 223\nConjugate elements, 398\nConjugate, complex, 62\nConjugation, 221\nConstructible number, 365\nCorrespondence Theorem\n\nfor groups, 183\nfor rings, 263\n\nCoset\n\n451\n\n\n\n452 INDEX\n\nleader, 142\nleft, 101\nrepresentative, 101\nright, 101\n\nCoset decoding, 141\nCryptanalysis, 114\nCryptosystem\n\nrsa, 116\naffine, 115\ndefinition of, 113\nmonoalphabetic, 114\npolyalphabetic, 115\nprivate key, 113\npublic key, 113\nsingle key, 113\n\nCycle\ndefinition of, 82\ndisjoint, 82\n\nDe Morgan’s laws\nfor Boolean algebras, 325\nfor sets, 5\n\nDe Morgan, Augustus, 330\nDecoding table, 142\nDeligne, Pierre, 369\nDeMoivre’s Theorem, 64\nDerivative, 381\nDeterminant, Vandermonde, 387\nDickson, L. E., 173\nDiffie, W., 115\nDirect product of groups\n\nexternal, 156\ninternal, 158\n\nDiscriminant\nof the cubic equation, 297\nof the quadratic equation, 296\n\nDivision algorithm\nfor integers, 25\nfor polynomials, 286\n\nDivision ring, 255\nDomain\n\nEuclidean, 310\nprincipal ideal, 308\nunique factorization, 307\n\nDoubling the cube, 368\n\nEisenstein’s Criterion, 291\nElement\n\nassociate, 307\nidentity, 40\ninverse, 40\n\nirreducible, 307\norder of, 60\nprime, 307\nprimitive, 400\ntranscendental, 356\n\nEquivalence class, 12\nEquivalence relation, 11\nEuclidean algorithm, 27\nEuclidean domain, 310\nEuclidean group, 198\nEuclidean inner product, 195\nEuclidean valuation, 310\nEuler ϕ-function, 104\nEuler, Leonhard, 105, 369\nExtension\n\nalgebraic, 356\nfield, 354\nfinite, 359\nnormal, 403\nradical, 407\nseparable, 381, 400\nsimple, 356\n\nExternal direct product, 156\n\nFaltings, Gerd, 369\nFeit, W., 173, 230\nFermat’s factorizationalgorithm, 120\nFermat’s Little Theorem, 105\nFermat, Pierre de, 105, 368\nFerrari, Ludovico, 293\nFerro, Scipione del, 293\nField, 255\n\nalgebraically closed, 361\nbase, 354\nextension, 354\nfixed, 401\nGalois, 382\nof fractions, 306\nof quotients, 306\nsplitting, 362\n\nFinitely generated group, 208\nFior, Antonio, 293\nFirst Isomorphism Theorem\n\nfor groups, 181\nfor rings, 262\n\nFixed point set, 221\nFreshman’s Dream, 380\nFunction\n\nbijective, 7\nBoolean, 228, 333\ncomposition of, 7\n\n\n\nINDEX 453\n\ndefinition of, 6\ndomain of, 6\nidentity, 9\ninjective, 7\ninvertible, 9\none-to-one, 7\nonto, 7\nrange of, 6\nsurjective, 7\nswitching, 228, 333\n\nFundamental Theorem\nof Algebra, 362, 411\nof Arithmetic, 27\nof Finite Abelian Groups, 209\n\nFundamental Theorem of Galois Theory,\n404\n\nGalois field, 382\nGalois group, 398\nGalois, Évariste, 44, 406\nGauss’s Lemma, 312\nGauss, Karl Friedrich, 313\nGaussian integers, 258\nGenerator of a cyclic subgroup, 60\nGenerators for a group, 208\nGlide reflection, 199\nGorenstein, Daniel, 173\nGreatest common divisor\n\nof two integers, 25\nof two polynomials, 288\n\nGreatest lower bound, 321\nGreiss, R., 173\nGrothendieck, Alexander, 369\nGroup\n\np-group, 209, 239\nabelian, 40\naction, 220\nalternating, 85\ncenter of, 223\ncircle, 64\ncommutative, 40\ncyclic, 60\ndefinition of, 40\ndihedral, 86\nEuclidean, 198\nfactor, 169\nfinite, 43\nfinitely generated, 208\nGalois, 398\ngeneral linear, 42, 194\ngenerators of, 208\n\nhomomorphism of, 179\ninfinite, 43\nisomorphic, 152\nisomorphism of, 152\nnonabelian, 40\nnoncommutative, 40\nof units, 41\norder of, 43\northogonal, 195\npermutation, 81\npoint, 202\nquaternion, 42\nquotient, 169\nsimple, 170, 173\nsolvable, 215\nspace, 202\nspecial linear, 45, 194\nspecial orthogonal, 198\nsymmetric, 80\nsymmetry, 200\n\nGödel, Kurt, 330\n\nHamming distance, 130\nHamming, R., 132\nHellman, M., 115\nHilbert, David, 204, 264, 330, 369\nHomomorphic image, 179\nHomomorphism\n\ncanonical, 181, 262\nevaluation, 260, 286\nkernel of a group, 180\nkernel of a ring, 260\nnatural, 181, 262\nof groups, 179\nring, 260\n\nIdeal\ndefinition of, 261\nmaximal, 263\none-sided, 262\nprime, 264\nprincipal, 261\ntrivial, 261\ntwo-sided, 262\n\nIndeterminate, 283\nIndex of a subgroup, 102\nInduction\n\nfirst principle of, 22\nsecond principle of, 24\n\nInfimum, 321\nInner product, 133\n\n\n\n454 INDEX\n\nIntegral domain, 255\nInternal direct product, 158\nInternational standard book number, 51\nIrreducible element, 307\nIrreducible polynomial, 289\nIsometry, 198\nIsomorphism\n\nof Boolean algebras, 326\nof groups, 152\nring, 260\n\nJoin, 322\nJordan, C., 173\nJordan-Hölder Theorem, 214\n\nKernel\nof a group homomorphism, 180\nof a ring homomorphism, 260\n\nKey\ndefinition of, 113\nprivate, 113\npublic, 113\nsingle, 113\n\nKlein, Felix, 44, 192, 264\nKronecker delta, 138, 196\nKronecker, Leopold, 369\nKummer, Ernst, 369\n\nLagrange’s Theorem, 103\nLagrange, Joseph-Louis, 44, 86, 105\nLaplace, Pierre-Simon, 86\nLattice\n\ncompleted, 323\ndefinition of, 322\ndistributive, 324\n\nLattice of points, 201\nLattices, Principle of Duality for, 322\nLeast upper bound, 321\nLeft regular representation, 155\nLie, Sophus, 44, 242\nLinear combination, 342\nLinear dependence, 342\nLinear independence, 342\nLinear map, 192\nLinear transformation\n\ndefinition of, 8, 192\nLower bound, 321\n\nMapping, see Function\nMatrix\n\ndistance-preserving, 196\ngenerator, 135\n\ninner product-preserving, 196\ninvertible, 193\nlength-preserving, 196\nnonsingular, 193\nnull space of, 134\northogonal, 195\nparity-check, 135\nsimilar, 12\nunimodular, 202\n\nMatrix, Vandermonde, 387\nMaximal ideal, 263\nMaximum-likelihood decoding, 129\nMeet, 322\nMinimal generator polynomial, 386\nMinimal polynomial, 357\nMinkowski, Hermann, 369\nMonic polynomial, 283\nMordell-Weil conjecture, 369\nMultiplicity of a root, 400\n\nNoether, A. Emmy, 264\nNoether, Max, 264\nNormal extension, 403\nNormal series of a group, 212\nNormal subgroup, 168\nNormalizer, 240\nNull space\n\nof a matrix, 134\n\nOdd Order Theorem, 244\nOrbit, 221\nOrthogonal group, 195\nOrthogonal matrix, 195\nOrthonormal set, 196\n\nPartial order, 320\nPartially ordered set, 320\nPartitions, 12\nPermutation\n\ndefinition of, 9, 80\neven, 85\nodd, 85\n\nPermutation group, 81\nPlaintext, 113\nPolynomial\n\ncode, 384\ncontent of, 312\ndefinition of, 283\ndegree of, 283\nerror, 392\nerror-locator, 392\n\n\n\nINDEX 455\n\ngreatest common divisor of, 288\nin n indeterminates, 286\nirreducible, 289\nleading coefficient of, 283\nminimal, 357\nminimal generator, 386\nmonic, 283\nprimitive, 312\nroot of, 288\nseparable, 400\nzero of, 288\n\nPolynomial separable, 381\nPoset\n\ndefinition of, 320\nlargest element in, 323\nsmallest element in, 323\n\nPower set, 320\nPrime element, 307\nPrime ideal, 264\nPrime integer, 27\nPrimitive nth root of unity, 65, 407\nPrimitive element, 400\nPrimitive Element Theorem, 401\nPrimitive polynomial, 312\nPrincipal ideal, 261\nPrincipal ideal domain (pid), 308\nPrincipal series, 213\nPseudoprime, 120\n\nQuaternions, 42, 257\n\nResolvent cubic equation, 297\nRigid motion, 38, 198\nRing\n\ncharacteristic of, 259\ncommutative, 255\ndefinition of, 255\ndivision, 255\nfactor, 262\nhomomorphism, 260\nisomorphism, 260\nNoetherian, 309\nquotient, 262\nwith identity, 255\nwith unity, 255\n\nRivest, R., 116\nRuffini, P., 406\nRussell, Bertrand, 330\n\nScalar product, 340\nSecond Isomorphism Theorem\n\nfor groups, 182\nfor rings, 263\n\nShamir, A., 116\nShannon, C.., 132\nSimple extension, 356\nSimple group, 170\nSimple root, 400\nSolvability by radicals, 407\nSpanning set, 342\nSplitting field, 362\nSquaring the circle is impossible, 368\nStandard decoding, 141\nSubgroup\n\np-subgroup, 239\ncentralizer, 223\ncommutator, 243\ncyclic, 60\ndefinition of, 45\nindex of, 102\nisotropy, 221\nnormal, 168\nnormalizer of, 240\nproper, 45\nstabilizer, 221\nSylowp-subgroup, 240\ntranslation, 202\ntrivial, 45\n\nSubnormal series of a group, 212\nSubring, 257\nSupremum, 321\nSwitch\n\nclosed, 328\ndefinition of, 328\nopen, 328\n\nSwitching function, 228, 333\nSylow p-subgroup, 240\nSylow, Ludvig, 242\nSyndrome of a code, 140, 392\n\nTartaglia, 293\nThird Isomorphism Theorem\n\nfor groups, 183\nfor rings, 263\n\nThompson, J., 173, 230\nTranscendental element, 356\nTranscendental number, 357\nTransposition, 84\nTrisection of an angle, 368\n\nUnique factorization domain (ufd), 307\nUnit, 255, 307\n\n\n\n456 INDEX\n\nUniversal Product Code, 50\nUpper bound, 321\n\nVandermonde determinant, 387\nVandermonde matrix, 387\nVector space\n\nbasis of, 343\ndefinition of, 340\ndimension of, 344\nsubspace of, 341\n\nWeight of a codeword, 130\nWeil, André, 369\nWell-defined map, 7\nWell-ordered set, 24\nWhitehead, Alfred North, 330\n\nZero\nmultiplicity of, 400\nof a polynomial, 288\n\nZero divisor, 256\n\n\n\nThis book was authored and produced with MathBook XML.\n\nhttps://mathbook.pugetsound.edu\n\n\tAcknowledgements\n\tPreface\n\tPreliminaries\n\tA Short Note on Proofs\n\tSets and Equivalence Relations\n\tExercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tThe Integers\n\tMathematical Induction\n\tThe Division Algorithm\n\tExercises\n\tProgramming Exercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tGroups\n\tInteger Equivalence Classes and Symmetries\n\tDefinitions and Examples\n\tSubgroups\n\tExercises\n\tAdditional Exercises: Detecting Errors\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tCyclic Groups\n\tCyclic Subgroups\n\tMultiplicative Group of Complex Numbers\n\tThe Method of Repeated Squares\n\tExercises\n\tProgramming Exercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tPermutation Groups\n\tDefinitions and Notation\n\tDihedral Groups\n\tExercises\n\tSage\n\tSage Exercises\n\n\tCosets and Lagrange\'s Theorem\n\tCosets\n\tLagrange\'s Theorem\n\tFermat\'s and Euler\'s Theorems\n\tExercises\n\tSage\n\tSage Exercises\n\n\tIntroduction to Cryptography\n\tPrivate Key Cryptography\n\tPublic Key Cryptography\n\tExercises\n\tAdditional Exercises: Primality and Factoring\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tAlgebraic Coding Theory\n\tError-Detecting and Correcting Codes\n\tLinear Codes\n\tParity-Check and Generator Matrices\n\tEfficient Decoding\n\tExercises\n\tProgramming Exercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tIsomorphisms\n\tDefinition and Examples\n\tDirect Products\n\tExercises\n\tSage\n\tSage Exercises\n\n\tNormal Subgroups and Factor Groups\n\tFactor Groups and Normal Subgroups\n\tThe Simplicity of the Alternating Group\n\tExercises\n\tSage\n\tSage Exercises\n\n\tHomomorphisms\n\tGroup Homomorphisms\n\tThe Isomorphism Theorems\n\tExercises\n\tAdditional Exercises: Automorphisms\n\tSage\n\tSage Exercises\n\n\tMatrix Groups and Symmetry\n\tMatrix Groups\n\tSymmetry\n\tExercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tThe Structure of Groups\n\tFinite Abelian Groups\n\tSolvable Groups\n\tExercises\n\tProgramming Exercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tGroup Actions\n\tGroups Acting on Sets\n\tThe Class Equation\n\tBurnside\'s Counting Theorem\n\tExercises\n\tProgramming Exercise\n\tReferences and Suggested Reading\n\tSage\n\tSage Exercises\n\n\tThe Sylow Theorems\n\tThe Sylow Theorems\n\tExamples and Applications\n\tExercises\n\tA Project\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tRings\n\tRings\n\tIntegral Domains and Fields\n\tRing Homomorphisms and Ideals\n\tMaximal and Prime Ideals\n\tAn Application to Software Design\n\tExercises\n\tProgramming Exercise\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tPolynomials\n\tPolynomial Rings\n\tThe Division Algorithm\n\tIrreducible Polynomials\n\tExercises\n\tAdditional Exercises: Solving the Cubic and Quartic Equations\n\tSage\n\tSage Exercises\n\n\tIntegral Domains\n\tFields of Fractions\n\tFactorization in Integral Domains\n\tExercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tLattices and Boolean Algebras\n\tLattices\n\tBoolean Algebras\n\tThe Algebra of Electrical Circuits\n\tExercises\n\tProgramming Exercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tVector Spaces\n\tDefinitions and Examples\n\tSubspaces\n\tLinear Independence\n\tExercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tFields\n\tExtension Fields\n\tSplitting Fields\n\tGeometric Constructions\n\tExercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tFinite Fields\n\tStructure of a Finite Field\n\tPolynomial Codes\n\tExercises\n\tAdditional Exercises: Error Correction for BCH Codes\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tGalois Theory\n\tField Automorphisms\n\tThe Fundamental Theorem\n\tApplications\n\tExercises\n\tReferences and Suggested Readings\n\tSage\n\tSage Exercises\n\n\tGNU Free Documentation License\n\tHints and Solutions to Selected Exercises\n\tNotation\n\tIndex\n\n', 'metadata_resourceName': "b'Abstract Algebra - 2016 (aata-20160809-sage-7.3).pdf'", 'metadata_pdf:docinfo:title': 'Abstract Algebra'}}, {'_index': 'mongo_index', '_id': 'D_9p5IUB9nynXRNhQsyr', '_score': 0.90830183, '_ignored': ['content.keyword', 'log_entry.keyword'], '_source': {'log_entry': '{"_id"=>BSON::ObjectId(\'63cffa2778b994746c729ce6\'), "content"=>"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n\xa0A\xa0Brief\xa0Introduction\xa0to\xa0\\n\\nNeural\xa0Networks\xa0\\n\xa0 \xa0\\n\\n\xa0David\xa0Kriesel\xa0\\n\xa0dkriesel.com\xa0\\n\\nDownload\xa0location:\\nhttp://www.dkriesel.com/en/science/neural_networks\\n\\nNEW\xa0–\xa0for\xa0the\xa0programmers:\xa0\\nScalable\xa0and\xa0efficient\xa0NN\xa0framework,\xa0written\xa0in\xa0JAVA\xa0\\n\\nhttp://www.dkriesel.com/en/tech/snipe\\n\\n\\n\\n\\n\\ndkriesel.com\\n\\nIn remembrance of\\nDr. Peter Kemp, Notary (ret.), Bonn, Germany.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) iii\\n\\n\\n\\n\\n\\nA small preface\\n\\"Originally, this work has been prepared in the framework of a seminar of the\\nUniversity of Bonn in Germany, but it has been and will be extended (after\\n\\nbeing presented and published online under www.dkriesel.com on\\n5/27/2005). First and foremost, to provide a comprehensive overview of the\\n\\nsubject of neural networks and, second, just to acquire more and more\\nknowledge about LATEX . And who knows – maybe one day this summary will\\n\\nbecome a real preface!\\"\\n\\nAbstract of this work, end of 2005\\n\\nThe above abstract has not yet become a\\npreface but at least a little preface, ever\\nsince the extended text (then 40 pages\\nlong) has turned out to be a download\\nhit.\\n\\nAmbition and intention of this\\nmanuscript\\n\\nThe entire text is written and laid out\\nmore effectively and with more illustra-\\ntions than before. I did all the illustra-\\ntions myself, most of them directly in\\nLATEX by using XYpic. They reflect what\\nI would have liked to see when becoming\\nacquainted with the subject: Text and il-\\nlustrations should be memorable and easy\\nto understand to offer as many people as\\npossible access to the field of neural net-\\nworks.\\n\\nNevertheless, the mathematically and for-\\nmally skilled readers will be able to under-\\n\\nstand the definitions without reading the\\nrunning text, while the opposite holds for\\nreaders only interested in the subject mat-\\nter; everything is explained in both collo-\\nquial and formal language. Please let me\\nknow if you find out that I have violated\\nthis principle.\\n\\nThe sections of this text are mostly\\nindependent from each other\\n\\nThe document itself is divided into differ-\\nent parts, which are again divided into\\nchapters. Although the chapters contain\\ncross-references, they are also individually\\naccessible to readers with little previous\\nknowledge. There are larger and smaller\\nchapters: While the larger chapters should\\nprovide profound insight into a paradigm\\nof neural networks (e.g. the classic neural\\nnetwork structure: the perceptron and its\\nlearning procedures), the smaller chapters\\ngive a short overview – but this is also ex-\\n\\nv\\n\\n\\n\\ndkriesel.com\\n\\nplained in the introduction of each chapter.\\nIn addition to all the definitions and expla-\\nnations I have included some excursuses\\nto provide interesting information not di-\\nrectly related to the subject.\\n\\nUnfortunately, I was not able to find free\\nGerman sources that are multi-faceted\\nin respect of content (concerning the\\nparadigms of neural networks) and, nev-\\nertheless, written in coherent style. The\\naim of this work is (even if it could not\\nbe fulfilled at first go) to close this gap bit\\nby bit and to provide easy access to the\\nsubject.\\n\\nWant to learn not only by\\nreading, but also by coding?\\nUse SNIPE!\\n\\nSNIPE1 is a well-documented JAVA li-\\nbrary that implements a framework for\\nneural networks in a speedy, feature-rich\\nand usable way. It is available at no\\ncost for non-commercial purposes. It was\\noriginally designed for high performance\\nsimulations with lots and lots of neural\\nnetworks (even large ones) being trained\\nsimultaneously. Recently, I decided to\\ngive it away as a professional reference im-\\nplementation that covers network aspects\\nhandled within this work, while at the\\nsame time being faster and more efficient\\nthan lots of other implementations due to\\n\\n1 Scalable and Generalized Neural Information Pro-\\ncessing Engine, downloadable at http://www.\\ndkriesel.com/tech/snipe, online JavaDoc at\\nhttp://snipe.dkriesel.com\\n\\nthe original high-performance simulation\\ndesign goal. Those of you who are up for\\nlearning by doing and/or have to use a\\nfast and stable neural networks implemen-\\ntation for some reasons, should definetely\\nhave a look at Snipe.\\n\\nHowever, the aspects covered by Snipe are\\nnot entirely congruent with those covered\\nby this manuscript. Some of the kinds\\nof neural networks are not supported by\\nSnipe, while when it comes to other kinds\\nof neural networks, Snipe may have lots\\nand lots more capabilities than may ever\\nbe covered in the manuscript in the form\\nof practical hints. Anyway, in my experi-\\nence almost all of the implementation re-\\nquirements of my readers are covered well.\\nOn the Snipe download page, look for the\\nsection \\"Getting started with Snipe\\" – you\\nwill find an easy step-by-step guide con-\\ncerning Snipe and its documentation, as\\nwell as some examples.\\n\\nSNIPE: This manuscript frequently incor-\\nporates Snipe. Shaded Snipe-paragraphs\\nlike this one are scattered among large\\nparts of the manuscript, providing infor-\\nmation on how to implement their con-\\ntext in Snipe. This also implies that\\nthose who do not want to use Snipe,\\njust have to skip the shaded Snipe-\\nparagraphs! The Snipe-paragraphs as-\\nsume the reader has had a close look at\\nthe \\"Getting started with Snipe\\" section.\\nOften, class names are used. As Snipe con-\\nsists of only a few different packages, I omit-\\nted the package names within the qualified\\nclass names for the sake of readability.\\n\\nvi D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\nhttp://www.dkriesel.com/tech/snipe\\nhttp://www.dkriesel.com/tech/snipe\\nhttp://snipe.dkriesel.com\\n\\n\\ndkriesel.com\\n\\nIt’s easy to print this\\nmanuscript\\n\\nThis text is completely illustrated in\\ncolor, but it can also be printed as is in\\nmonochrome: The colors of figures, tables\\nand text are well-chosen so that in addi-\\ntion to an appealing design the colors are\\nstill easy to distinguish when printed in\\nmonochrome.\\n\\nThere are many tools directly\\nintegrated into the text\\n\\nDifferent aids are directly integrated in the\\ndocument to make reading more flexible:\\nHowever, anyone (like me) who prefers\\nreading words on paper rather than on\\nscreen can also enjoy some features.\\n\\nIn the table of contents, different\\ntypes of chapters are marked\\n\\nDifferent types of chapters are directly\\nmarked within the table of contents. Chap-\\nters, that are marked as \\"fundamental\\"\\nare definitely ones to read because almost\\nall subsequent chapters heavily depend on\\nthem. Other chapters additionally depend\\non information given in other (preceding)\\nchapters, which then is marked in the ta-\\nble of contents, too.\\n\\nSpeaking headlines throughout the\\ntext, short ones in the table of\\ncontents\\n\\nThe whole manuscript is now pervaded by\\nsuch headlines. Speaking headlines are\\nnot just title-like (\\"Reinforcement Learn-\\ning\\"), but centralize the information given\\nin the associated section to a single sen-\\ntence. In the named instance, an appro-\\npriate headline would be \\"Reinforcement\\nlearning methods provide feedback to the\\nnetwork, whether it behaves good or bad\\".\\nHowever, such long headlines would bloat\\nthe table of contents in an unacceptable\\nway. So I used short titles like the first one\\nin the table of contents, and speaking ones,\\nlike the latter, throughout the text.\\n\\nMarginal notes are a navigational\\naid\\n\\nThe entire document contains marginal\\nnotes in colloquial language (see the exam-\\n\\nHypertext\\non paper\\n:-)\\n\\nple in the margin), allowing you to \\"scan\\"\\nthe document quickly to find a certain pas-\\nsage in the text (including the titles).\\n\\nNew mathematical symbols are marked by\\nspecific marginal notes for easy finding Jx(see the example for x in the margin).\\n\\nThere are several kinds of indexing\\n\\nThis document contains different types of\\nindexing: If you have found a word in\\nthe index and opened the corresponding\\npage, you can easily find it by searching\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) vii\\n\\n\\n\\ndkriesel.com\\n\\nfor highlighted text – all indexed words\\nare highlighted like this.\\n\\nMathematical symbols appearing in sev-\\neral chapters of this document (e.g. Ω for\\nan output neuron; I tried to maintain a\\nconsistent nomenclature for regularly re-\\ncurring elements) are separately indexed\\nunder \\"Mathematical Symbols\\", so they\\ncan easily be assigned to the correspond-\\ning term.\\n\\nNames of persons written in small caps\\nare indexed in the category \\"Persons\\" and\\nordered by the last names.\\n\\nTerms of use and license\\n\\nBeginning with the epsilon edition, the\\ntext is licensed under the Creative Com-\\nmons Attribution-No Derivative Works\\n3.0 Unported License2, except for some\\nlittle portions of the work licensed under\\nmore liberal licenses as mentioned (mainly\\nsome figures from Wikimedia Commons).\\nA quick license summary:\\n\\n1. You are free to redistribute this docu-\\nment (even though it is a much better\\nidea to just distribute the URL of my\\nhomepage, for it always contains the\\nmost recent version of the text).\\n\\n2. You may not modify, transform, or\\nbuild upon the document except for\\npersonal use.\\n\\n2 http://creativecommons.org/licenses/\\nby-nd/3.0/\\n\\n3. You must maintain the author’s attri-\\nbution of the document at all times.\\n\\n4. You may not use the attribution to\\nimply that the author endorses you\\nor your document use.\\n\\nFor I’m no lawyer, the above bullet-point\\nsummary is just informational: if there is\\nany conflict in interpretation between the\\nsummary and the actual license, the actual\\nlicense always takes precedence. Note that\\nthis license does not extend to the source\\nfiles used to produce the document. Those\\nare still mine.\\n\\nHow to cite this manuscript\\n\\nThere’s no official publisher, so you need\\nto be careful with your citation. Please\\nfind more information in English and\\nGerman language on my homepage, re-\\nspectively the subpage concerning the\\nmanuscript3.\\n\\nAcknowledgement\\n\\nNow I would like to express my grati-\\ntude to all the people who contributed, in\\nwhatever manner, to the success of this\\nwork, since a work like this needs many\\nhelpers. First of all, I want to thank\\nthe proofreaders of this text, who helped\\nme and my readers very much. In al-\\nphabetical order: Wolfgang Apolinarski,\\nKathrin Gräve, Paul Imhoff, Thomas\\n\\n3 http://www.dkriesel.com/en/science/\\nneural_networks\\n\\nviii D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\nhttp://creativecommons.org/licenses/by-nd/3.0/\\nhttp://creativecommons.org/licenses/by-nd/3.0/\\nhttp://www.dkriesel.com/en/science/neural_networks\\nhttp://www.dkriesel.com/en/science/neural_networks\\n\\n\\ndkriesel.com\\n\\nKühn, Christoph Kunze, Malte Lohmeyer,\\nJoachim Nock, Daniel Plohmann, Daniel\\nRosenthal, Christian Schulz and Tobias\\nWilken.\\n\\nAdditionally, I want to thank the readers\\nDietmar Berger, Igor Buchmüller, Marie\\nChrist, Julia Damaschek, Jochen Döll,\\nMaximilian Ernestus, Hardy Falk, Anne\\nFeldmeier, Sascha Fink, Andreas Fried-\\nmann, Jan Gassen, Markus Gerhards, Se-\\nbastian Hirsch, Andreas Hochrath, Nico\\nHöft, Thomas Ihme, Boris Jentsch, Tim\\nHussein, Thilo Keller, Mario Krenn, Mirko\\nKunze, Maikel Linke, Adam Maciak,\\nBenjamin Meier, David Möller, Andreas\\nMüller, Rainer Penninger, Lena Reichel,\\nAlexander Schier, Matthias Siegmund,\\nMathias Tirtasana, Oliver Tischler, Max-\\nimilian Voit, Igor Wall, Achim Weber,\\nFrank Weinreis, Gideon Maillette de Buij\\nWenniger, Philipp Woock and many oth-\\ners for their feedback, suggestions and re-\\nmarks.\\n\\nAdditionally, I’d like to thank Sebastian\\nMerzbach, who examined this work in a\\nvery conscientious way finding inconsisten-\\ncies and errors. In particular, he cleared\\nlots and lots of language clumsiness from\\nthe English version.\\n\\nEspecially, I would like to thank Beate\\nKuhl for translating the entire text from\\nGerman to English, and for her questions\\nwhich made me think of changing the\\nphrasing of some paragraphs.\\n\\nI would particularly like to thank Prof.\\nRolf Eckmiller and Dr. Nils Goerke as\\nwell as the entire Division of Neuroinfor-\\nmatics, Department of Computer Science\\n\\nof the University of Bonn – they all made\\nsure that I always learned (and also had\\nto learn) something new about neural net-\\nworks and related subjects. Especially Dr.\\nGoerke has always been willing to respond\\nto any questions I was not able to answer\\nmyself during the writing process. Conver-\\nsations with Prof. Eckmiller made me step\\nback from the whiteboard to get a better\\noverall view on what I was doing and what\\nI should do next.\\n\\nGlobally, and not only in the context of\\nthis work, I want to thank my parents who\\nnever get tired to buy me specialized and\\ntherefore expensive books and who have\\nalways supported me in my studies.\\n\\nFor many \\"remarks\\" and the very special\\nand cordial atmosphere ;-) I want to thank\\nAndreas Huber and Tobias Treutler. Since\\nour first semester it has rarely been boring\\nwith you!\\n\\nNow I would like to think back to my\\nschool days and cordially thank some\\nteachers who (in my opinion) had im-\\nparted some scientific knowledge to me –\\nalthough my class participation had not\\nalways been wholehearted: Mr. Wilfried\\nHartmann, Mr. Hubert Peters and Mr.\\nFrank Nökel.\\n\\nFurthermore I would like to thank the\\nwhole team at the notary’s office of Dr.\\nKemp and Dr. Kolb in Bonn, where I have\\nalways felt to be in good hands and who\\nhave helped me to keep my printing costs\\nlow - in particular Christiane Flamme and\\nDr. Kemp!\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) ix\\n\\n\\n\\ndkriesel.com\\n\\nThanks go also to the Wikimedia Com-\\nmons, where I took some (few) images and\\naltered them to suit this text.\\n\\nLast but not least I want to thank two\\npeople who made outstanding contribu-\\ntions to this work who occupy, so to speak,\\na place of honor: My girlfriend Verena\\nThomas, who found many mathematical\\nand logical errors in my text and dis-\\ncussed them with me, although she has\\nlots of other things to do, and Chris-\\ntiane Schultze, who carefully reviewed the\\ntext for spelling mistakes and inconsisten-\\ncies.\\n\\nDavid Kriesel\\n\\nx D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\nContents\\n\\nA small preface v\\n\\nI From biology to formalization – motivation, philosophy, history and\\nrealization of neural models 1\\n\\n1 Introduction, motivation and history 3\\n1.1 Why neural networks? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n\\n1.1.1 The 100-step rule . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.1.2 Simple application examples . . . . . . . . . . . . . . . . . . . . . 6\\n\\n1.2 History of neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.2.1 The beginning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.2.2 Golden age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.2.3 Long silence and slow reconstruction . . . . . . . . . . . . . . . . 11\\n1.2.4 Renaissance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n\\n2 Biological neural networks 13\\n2.1 The vertebrate nervous system . . . . . . . . . . . . . . . . . . . . . . . 13\\n\\n2.1.1 Peripheral and central nervous system . . . . . . . . . . . . . . . 13\\n2.1.2 Cerebrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.1.3 Cerebellum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1.4 Diencephalon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1.5 Brainstem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n\\n2.2 The neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.2.1 Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.2.2 Electrochemical processes in the neuron . . . . . . . . . . . . . . 19\\n\\n2.3 Receptor cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.3.1 Various types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.3.2 Information processing within the nervous system . . . . . . . . 25\\n2.3.3 Light sensing organs . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n\\n2.4 The amount of neurons in living organisms . . . . . . . . . . . . . . . . 28\\n\\nxi\\n\\n\\n\\nContents dkriesel.com\\n\\n2.5 Technical neurons as caricature of biology . . . . . . . . . . . . . . . . . 30\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n\\n3 Components of artificial neural networks (fundamental) 33\\n3.1 The concept of time in neural networks . . . . . . . . . . . . . . . . . . 33\\n3.2 Components of neural networks . . . . . . . . . . . . . . . . . . . . . . . 33\\n\\n3.2.1 Connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n3.2.2 Propagation function and network input . . . . . . . . . . . . . . 34\\n3.2.3 Activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n3.2.4 Threshold value . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n3.2.5 Activation function . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n3.2.6 Common activation functions . . . . . . . . . . . . . . . . . . . . 37\\n3.2.7 Output function . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n3.2.8 Learning strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n\\n3.3 Network topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3.3.1 Feedforward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3.3.2 Recurrent networks . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n3.3.3 Completely linked networks . . . . . . . . . . . . . . . . . . . . . 42\\n\\n3.4 The bias neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.5 Representing neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n3.6 Orders of activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n\\n3.6.1 Synchronous activation . . . . . . . . . . . . . . . . . . . . . . . 45\\n3.6.2 Asynchronous activation . . . . . . . . . . . . . . . . . . . . . . . 46\\n\\n3.7 Input and output of data . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n\\n4 Fundamentals on learning and training samples (fundamental) 51\\n4.1 Paradigms of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n\\n4.1.1 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . 52\\n4.1.2 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . 53\\n4.1.3 Supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n4.1.4 Offline or online learning? . . . . . . . . . . . . . . . . . . . . . . 54\\n4.1.5 Questions in advance . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n\\n4.2 Training patterns and teaching input . . . . . . . . . . . . . . . . . . . . 54\\n4.3 Using training samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n\\n4.3.1 Division of the training set . . . . . . . . . . . . . . . . . . . . . 57\\n4.3.2 Order of pattern representation . . . . . . . . . . . . . . . . . . . 57\\n\\n4.4 Learning curve and error measurement . . . . . . . . . . . . . . . . . . . 58\\n4.4.1 When do we stop learning? . . . . . . . . . . . . . . . . . . . . . 59\\n\\nxii D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Contents\\n\\n4.5 Gradient optimization procedures . . . . . . . . . . . . . . . . . . . . . . 61\\n4.5.1 Problems of gradient procedures . . . . . . . . . . . . . . . . . . 62\\n\\n4.6 Exemplary problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n4.6.1 Boolean functions . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n4.6.2 The parity function . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n4.6.3 The 2-spiral problem . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n4.6.4 The checkerboard problem . . . . . . . . . . . . . . . . . . . . . . 65\\n4.6.5 The identity function . . . . . . . . . . . . . . . . . . . . . . . . 65\\n4.6.6 Other exemplary problems . . . . . . . . . . . . . . . . . . . . . 66\\n\\n4.7 Hebbian rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n4.7.1 Original rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n4.7.2 Generalized form . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n\\nII Supervised learning network paradigms 69\\n\\n5 The perceptron, backpropagation and its variants 71\\n5.1 The singlelayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n\\n5.1.1 Perceptron learning algorithm and convergence theorem . . . . . 75\\n5.1.2 Delta rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n\\n5.2 Linear separability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n5.3 The multilayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n5.4 Backpropagation of error . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n\\n5.4.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\\n5.4.2 Boiling backpropagation down to the delta rule . . . . . . . . . . 91\\n5.4.3 Selecting a learning rate . . . . . . . . . . . . . . . . . . . . . . . 92\\n\\n5.5 Resilient backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . 93\\n5.5.1 Adaption of weights . . . . . . . . . . . . . . . . . . . . . . . . . 94\\n5.5.2 Dynamic learning rate adjustment . . . . . . . . . . . . . . . . . 94\\n5.5.3 Rprop in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\n\\n5.6 Further variations and extensions to backpropagation . . . . . . . . . . 96\\n5.6.1 Momentum term . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\\n5.6.2 Flat spot elimination . . . . . . . . . . . . . . . . . . . . . . . . . 97\\n5.6.3 Second order backpropagation . . . . . . . . . . . . . . . . . . . 98\\n5.6.4 Weight decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\\n5.6.5 Pruning and Optimal Brain Damage . . . . . . . . . . . . . . . . 98\\n\\n5.7 Initial configuration of a multilayer perceptron . . . . . . . . . . . . . . 99\\n5.7.1 Number of layers . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n5.7.2 The number of neurons . . . . . . . . . . . . . . . . . . . . . . . 100\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) xiii\\n\\n\\n\\nContents dkriesel.com\\n\\n5.7.3 Selecting an activation function . . . . . . . . . . . . . . . . . . . 100\\n5.7.4 Initializing weights . . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n\\n5.8 The 8-3-8 encoding problem and related problems . . . . . . . . . . . . 101\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\\n\\n6 Radial basis functions 105\\n6.1 Components and structure . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n6.2 Information processing of an RBF network . . . . . . . . . . . . . . . . 106\\n\\n6.2.1 Information processing in RBF neurons . . . . . . . . . . . . . . 108\\n6.2.2 Analytical thoughts prior to the training . . . . . . . . . . . . . . 111\\n\\n6.3 Training of RBF networks . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n6.3.1 Centers and widths of RBF neurons . . . . . . . . . . . . . . . . 115\\n\\n6.4 Growing RBF networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n6.4.1 Adding neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n6.4.2 Limiting the number of neurons . . . . . . . . . . . . . . . . . . . 119\\n6.4.3 Deleting neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n\\n6.5 Comparing RBF networks and multilayer perceptrons . . . . . . . . . . 119\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n\\n7 Recurrent perceptron-like networks (depends on chapter 5) 121\\n7.1 Jordan networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n7.2 Elman networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n7.3 Training recurrent networks . . . . . . . . . . . . . . . . . . . . . . . . . 124\\n\\n7.3.1 Unfolding in time . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n7.3.2 Teacher forcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\\n7.3.3 Recurrent backpropagation . . . . . . . . . . . . . . . . . . . . . 127\\n7.3.4 Training with evolution . . . . . . . . . . . . . . . . . . . . . . . 127\\n\\n8 Hopfield networks 129\\n8.1 Inspired by magnetism . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n8.2 Structure and functionality . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n\\n8.2.1 Input and output of a Hopfield network . . . . . . . . . . . . . . 130\\n8.2.2 Significance of weights . . . . . . . . . . . . . . . . . . . . . . . . 131\\n8.2.3 Change in the state of neurons . . . . . . . . . . . . . . . . . . . 131\\n\\n8.3 Generating the weight matrix . . . . . . . . . . . . . . . . . . . . . . . . 132\\n8.4 Autoassociation and traditional application . . . . . . . . . . . . . . . . 133\\n8.5 Heteroassociation and analogies to neural data storage . . . . . . . . . . 134\\n\\n8.5.1 Generating the heteroassociative matrix . . . . . . . . . . . . . . 135\\n8.5.2 Stabilizing the heteroassociations . . . . . . . . . . . . . . . . . . 135\\n8.5.3 Biological motivation of heterassociation . . . . . . . . . . . . . . 136\\n\\nxiv D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Contents\\n\\n8.6 Continuous Hopfield networks . . . . . . . . . . . . . . . . . . . . . . . . 136\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\n\\n9 Learning vector quantization 139\\n9.1 About quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n9.2 Purpose of LVQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\\n9.3 Using codebook vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\\n9.4 Adjusting codebook vectors . . . . . . . . . . . . . . . . . . . . . . . . . 141\\n\\n9.4.1 The procedure of learning . . . . . . . . . . . . . . . . . . . . . . 141\\n9.5 Connection to neural networks . . . . . . . . . . . . . . . . . . . . . . . 143\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\\n\\nIII Unsupervised learning network paradigms 145\\n\\n10 Self-organizing feature maps 147\\n10.1 Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\n10.2 Functionality and output interpretation . . . . . . . . . . . . . . . . . . 149\\n10.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\\n\\n10.3.1 The topology function . . . . . . . . . . . . . . . . . . . . . . . . 150\\n10.3.2 Monotonically decreasing learning rate and neighborhood . . . . 152\\n\\n10.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\\n10.4.1 Topological defects . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n\\n10.5 Adjustment of resolution and position-dependent learning rate . . . . . 156\\n10.6 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n\\n10.6.1 Interaction with RBF networks . . . . . . . . . . . . . . . . . . . 161\\n10.7 Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n\\n10.7.1 Neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n10.7.2 Multi-SOMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n10.7.3 Multi-neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n10.7.4 Growing neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n\\n11 Adaptive resonance theory 165\\n11.1 Task and structure of an ART network . . . . . . . . . . . . . . . . . . . 165\\n\\n11.1.1 Resonance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n11.2 Learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n\\n11.2.1 Pattern input and top-down learning . . . . . . . . . . . . . . . . 167\\n11.2.2 Resonance and bottom-up learning . . . . . . . . . . . . . . . . . 167\\n11.2.3 Adding an output neuron . . . . . . . . . . . . . . . . . . . . . . 167\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) xv\\n\\n\\n\\nContents dkriesel.com\\n\\n11.3 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n\\nIV Excursi, appendices and registers 169\\n\\nA Excursus: Cluster analysis and regional and online learnable fields 171\\nA.1 k-means clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\nA.2 k-nearest neighboring . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\nA.3 ε-nearest neighboring . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\nA.4 The silhouette coefficient . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\nA.5 Regional and online learnable fields . . . . . . . . . . . . . . . . . . . . . 175\\n\\nA.5.1 Structure of a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . 176\\nA.5.2 Training a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\\nA.5.3 Evaluating a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . 178\\nA.5.4 Comparison with popular clustering methods . . . . . . . . . . . 179\\nA.5.5 Initializing radii, learning rates and multiplier . . . . . . . . . . . 180\\nA.5.6 Application examples . . . . . . . . . . . . . . . . . . . . . . . . 180\\n\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\\n\\nB Excursus: neural networks used for prediction 181\\nB.1 About time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nB.2 One-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 183\\nB.3 Two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n\\nB.3.1 Recursive two-step-ahead prediction . . . . . . . . . . . . . . . . 185\\nB.3.2 Direct two-step-ahead prediction . . . . . . . . . . . . . . . . . . 185\\n\\nB.4 Additional optimization approaches for prediction . . . . . . . . . . . . . 185\\nB.4.1 Changing temporal parameters . . . . . . . . . . . . . . . . . . . 185\\nB.4.2 Heterogeneous prediction . . . . . . . . . . . . . . . . . . . . . . 187\\n\\nB.5 Remarks on the prediction of share prices . . . . . . . . . . . . . . . . . 187\\n\\nC Excursus: reinforcement learning 191\\nC.1 System structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n\\nC.1.1 The gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\nC.1.2 Agent und environment . . . . . . . . . . . . . . . . . . . . . . . 193\\nC.1.3 States, situations and actions . . . . . . . . . . . . . . . . . . . . 194\\nC.1.4 Reward and return . . . . . . . . . . . . . . . . . . . . . . . . . . 195\\nC.1.5 The policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\\n\\nC.2 Learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\nC.2.1 Rewarding strategies . . . . . . . . . . . . . . . . . . . . . . . . . 198\\nC.2.2 The state-value function . . . . . . . . . . . . . . . . . . . . . . . 199\\n\\nxvi D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Contents\\n\\nC.2.3 Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . 201\\nC.2.4 Temporal difference learning . . . . . . . . . . . . . . . . . . . . 202\\nC.2.5 The action-value function . . . . . . . . . . . . . . . . . . . . . . 203\\nC.2.6 Q learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\\n\\nC.3 Example applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\nC.3.1 TD gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\nC.3.2 The car in the pit . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\nC.3.3 The pole balancer . . . . . . . . . . . . . . . . . . . . . . . . . . 206\\n\\nC.4 Reinforcement learning in connection with neural networks . . . . . . . 207\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\n\\nBibliography 209\\n\\nList of Figures 215\\n\\nIndex 219\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) xvii\\n\\n\\n\\n\\n\\nPart I\\n\\nFrom biology to formalization –\\nmotivation, philosophy, history and\\n\\nrealization of neural models\\n\\n1\\n\\n\\n\\n\\n\\nChapter 1\\n\\nIntroduction, motivation and history\\nHow to teach a computer? You can either write a fixed program – or you can\\n\\nenable the computer to learn on its own. Living beings do not have any\\nprogrammer writing a program for developing their skills, which then only has\\nto be executed. They learn by themselves – without the previous knowledge\\n\\nfrom external impressions – and thus can solve problems better than any\\ncomputer today. What qualities are needed to achieve such a behavior for\\n\\ndevices like computers? Can such cognition be adapted from biology? History,\\ndevelopment, decline and resurgence of a wide approach to solve problems.\\n\\n1.1 Why neural networks?\\n\\nThere are problem categories that cannot\\nbe formulated as an algorithm. Problems\\nthat depend on many subtle factors, for ex-\\nample the purchase price of a real estate\\nwhich our brain can (approximately) cal-\\nculate. Without an algorithm a computer\\ncannot do the same. Therefore the ques-\\ntion to be asked is: How do we learn to\\nexplore such problems?\\n\\nExactly – we learn; a capability comput-\\ners obviously do not have. Humans have\\n\\nComputers\\ncannot\\nlearn\\n\\na brain that can learn. Computers have\\nsome processing units and memory. They\\nallow the computer to perform the most\\ncomplex numerical calculations in a very\\nshort time, but they are not adaptive.\\n\\nIf we compare computer and brain1, we\\nwill note that, theoretically, the computer\\nshould be more powerful than our brain:\\nIt comprises 109 transistors with a switch-\\ning time of 10−9 seconds. The brain con-\\ntains 1011 neurons, but these only have a\\nswitching time of about 10−3 seconds.\\n\\nThe largest part of the brain is work-\\ning continuously, while the largest part of\\nthe computer is only passive data storage.\\nThus, the brain is parallel and therefore\\n\\nparallelism\\nperforming close to its theoretical maxi-\\n\\n1 Of course, this comparison is - for obvious rea-\\nsons - controversially discussed by biologists and\\ncomputer scientists, since response time and quan-\\ntity do not tell anything about quality and perfor-\\nmance of the processing units as well as neurons\\nand transistors cannot be compared directly. Nev-\\nertheless, the comparison serves its purpose and\\nindicates the advantage of parallelism by means\\nof processing time.\\n\\n3\\n\\n\\n\\nChapter 1 Introduction, motivation and history dkriesel.com\\n\\nBrain Computer\\nNo. of processing units ≈ 1011 ≈ 109\\n\\nType of processing units Neurons Transistors\\nType of calculation massively parallel usually serial\\nData storage associative address-based\\nSwitching time ≈ 10−3s ≈ 10−9s\\nPossible switching operations ≈ 1013 1\\n\\ns ≈ 1018 1\\ns\\n\\nActual switching operations ≈ 1012 1\\ns ≈ 1010 1\\n\\ns\\n\\nTable 1.1: The (flawed) comparison between brain and computer at a glance. Inspired by: [Zel94]\\n\\nmum, from which the computer is orders\\nof magnitude away (Table 1.1). Addition-\\nally, a computer is static - the brain as\\na biological neural network can reorganize\\nitself during its \\"lifespan\\" and therefore is\\nable to learn, to compensate errors and so\\nforth.\\n\\nWithin this text I want to outline how\\nwe can use the said characteristics of our\\nbrain for a computer system.\\n\\nSo the study of artificial neural networks\\nis motivated by their similarity to success-\\nfully working biological systems, which - in\\ncomparison to the overall system - consist\\nof very simple but numerous nerve cells\\n\\nsimple\\nbut many\\nprocessing\\n\\nunits\\n\\nthat work massively in parallel and (which\\nis probably one of the most significant\\naspects) have the capability to learn.\\nThere is no need to explicitly program a\\nneural network. For instance, it can learn\\nfrom training samples or by means of en-\\n\\nn. network\\ncapable\\nto learn\\n\\ncouragement - with a carrot and a stick,\\nso to speak (reinforcement learning).\\n\\nOne result from this learning procedure is\\nthe capability of neural networks to gen-\\n\\neralize and associate data: After suc-\\ncessful training a neural network can find\\nreasonable solutions for similar problems\\nof the same class that were not explicitly\\ntrained. This in turn results in a high de-\\ngree of fault tolerance against noisy in-\\nput data.\\n\\nFault tolerance is closely related to biolog-\\nical neural networks, in which this charac-\\nteristic is very distinct: As previously men-\\ntioned, a human has about 1011 neurons\\nthat continuously reorganize themselves\\nor are reorganized by external influences\\n(about 105 neurons can be destroyed while\\nin a drunken stupor, some types of food\\nor environmental influences can also de-\\nstroy brain cells). Nevertheless, our cogni-\\ntive abilities are not significantly affected.\\n\\nn. network\\nfault\\ntolerant\\n\\nThus, the brain is tolerant against internal\\nerrors – and also against external errors,\\nfor we can often read a really \\"dreadful\\nscrawl\\" although the individual letters are\\nnearly impossible to read.\\n\\nOur modern technology, however, is not\\nautomatically fault-tolerant. I have never\\nheard that someone forgot to install the\\n\\n4 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 1.1 Why neural networks?\\n\\nhard disk controller into a computer and\\ntherefore the graphics card automatically\\ntook over its tasks, i.e. removed con-\\nductors and developed communication, so\\nthat the system as a whole was affected\\nby the missing component, but not com-\\npletely destroyed.\\n\\nA disadvantage of this distributed fault-\\ntolerant storage is certainly the fact that\\nwe cannot realize at first sight what a neu-\\nral neutwork knows and performs or where\\nits faults lie. Usually, it is easier to per-\\nform such analyses for conventional algo-\\nrithms. Most often we can only trans-\\nfer knowledge into our neural network by\\nmeans of a learning procedure, which can\\ncause several errors and is not always easy\\nto manage.\\n\\nFault tolerance of data, on the other hand,\\nis already more sophisticated in state-of-\\nthe-art technology: Let us compare a\\nrecord and a CD. If there is a scratch on a\\nrecord, the audio information on this spot\\nwill be completely lost (you will hear a\\npop) and then the music goes on. On a CD\\nthe audio data are distributedly stored: A\\nscratch causes a blurry sound in its vicin-\\nity, but the data stream remains largely\\nunaffected. The listener won’t notice any-\\nthing.\\n\\nSo let us summarize the main characteris-\\ntics we try to adapt from biology:\\n\\n. Self-organization and learning capa-\\nbility,\\n\\n. Generalization capability and\\n\\n. Fault tolerance.\\n\\nWhat types of neural networks particu-\\nlarly develop what kinds of abilities and\\ncan be used for what problem classes will\\nbe discussed in the course of this work.\\n\\nIn the introductory chapter I want to\\nclarify the following: \\"The neural net-\\nwork\\" does not exist. There are differ- Important!\\nent paradigms for neural networks, how\\nthey are trained and where they are used.\\nMy goal is to introduce some of these\\nparadigms and supplement some remarks\\nfor practical application.\\n\\nWe have already mentioned that our brain\\nworks massively in parallel, in contrast to\\nthe functioning of a computer, i.e. every\\ncomponent is active at any time. If we\\nwant to state an argument for massive par-\\nallel processing, then the 100-step rule\\ncan be cited.\\n\\n1.1.1 The 100-step rule\\n\\nExperiments showed that a human can\\nrecognize the picture of a familiar object\\nor person in ≈ 0.1 seconds, which cor-\\nresponds to a neuron switching time of\\n≈ 10−3 seconds in ≈ 100 discrete time\\nsteps of parallel processing.\\n\\nparallel\\nprocessing\\n\\nA computer following the von Neumann\\narchitecture, however, can do practically\\nnothing in 100 time steps of sequential pro-\\ncessing, which are 100 assembler steps or\\ncycle steps.\\n\\nNow we want to look at a simple applica-\\ntion example for a neural network.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 5\\n\\n\\n\\nChapter 1 Introduction, motivation and history dkriesel.com\\n\\nFigure 1.1: A small robot with eight sensors\\nand two motors. The arrow indicates the driv-\\ning direction.\\n\\n1.1.2 Simple application examples\\n\\nLet us assume that we have a small robot\\nas shown in fig. 1.1. This robot has eight\\ndistance sensors from which it extracts in-\\nput data: Three sensors are placed on the\\nfront right, three on the front left, and two\\non the back. Each sensor provides a real\\nnumeric value at any time, that means we\\nare always receiving an input I ∈ R8.\\n\\nDespite its two motors (which will be\\nneeded later) the robot in our simple ex-\\nample is not capable to do much: It shall\\nonly drive on but stop when it might col-\\nlide with an obstacle. Thus, our output\\nis binary: H = 0 for \\"Everything is okay,\\ndrive on\\" and H = 1 for \\"Stop\\" (The out-\\n\\nput is called H for \\"halt signal\\"). There-\\nfore we need a mapping\\n\\nf : R8 → B1,\\n\\nthat applies the input signals to a robot\\nactivity.\\n\\n1.1.2.1 The classical way\\n\\nThere are two ways of realizing this map-\\nping. On the one hand, there is the clas-\\nsical way: We sit down and think for a\\nwhile, and finally the result is a circuit or\\na small computer program which realizes\\nthe mapping (this is easily possible, since\\nthe example is very simple). After that\\nwe refer to the technical reference of the\\nsensors, study their characteristic curve in\\norder to learn the values for the different\\nobstacle distances, and embed these values\\ninto the aforementioned set of rules. Such\\nprocedures are applied in the classic artifi-\\ncial intelligence, and if you know the exact\\nrules of a mapping algorithm, you are al-\\nways well advised to follow this scheme.\\n\\n1.1.2.2 The way of learning\\n\\nOn the other hand, more interesting and\\nmore successful for many mappings and\\nproblems that are hard to comprehend\\nstraightaway is the way of learning: We\\nshow different possible situations to the\\nrobot (fig. 1.2 on page 8), – and the robot\\nshall learn on its own what to do in the\\ncourse of its robot life.\\n\\nIn this example the robot shall simply\\nlearn when to stop. We first treat the\\n\\n6 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 1.1 Why neural networks?\\n\\nFigure 1.3: Initially, we regard the robot control\\nas a black box whose inner life is unknown. The\\nblack box receives eight real sensor values and\\nmaps these values to a binary output value.\\n\\nneural network as a kind of black box\\n(fig. 1.3). This means we do not know its\\nstructure but just regard its behavior in\\npractice.\\n\\nThe situations in form of simply mea-\\nsured sensor values (e.g. placing the robot\\nin front of an obstacle, see illustration),\\nwhich we show to the robot and for which\\nwe specify whether to drive on or to stop,\\nare called training samples. Thus, a train-\\ning sample consists of an exemplary input\\nand a corresponding desired output. Now\\nthe question is how to transfer this knowl-\\nedge, the information, into the neural net-\\nwork.\\n\\nThe samples can be taught to a neural\\nnetwork by using a simple learning pro-\\ncedure (a learning procedure is a simple\\nalgorithm or a mathematical formula. If\\nwe have done everything right and chosen\\ngood samples, the neural network will gen-\\neralize from these samples and find a uni-\\nversal rule when it has to stop.\\n\\nOur example can be optionally expanded.\\nFor the purpose of direction control it\\nwould be possible to control the motors\\nof our robot separately2, with the sensor\\nlayout being the same. In this case we are\\nlooking for a mapping\\n\\nf : R8 → R2,\\n\\nwhich gradually controls the two motors\\nby means of the sensor inputs and thus\\ncannot only, for example, stop the robot\\nbut also lets it avoid obstacles. Here it\\nis more difficult to analytically derive the\\nrules, and de facto a neural network would\\nbe more appropriate.\\n\\nOur goal is not to learn the samples by\\nheart, but to realize the principle behind\\nthem: Ideally, the robot should apply the\\nneural network in any situation and be\\nable to avoid obstacles. In particular, the\\nrobot should query the network continu-\\nously and repeatedly while driving in order\\nto continously avoid obstacles. The result\\nis a constant cycle: The robot queries the\\nnetwork. As a consequence, it will drive\\nin one direction, which changes the sen-\\nsors values. Again the robot queries the\\nnetwork and changes its position, the sen-\\nsor values are changed once again, and so\\non. It is obvious that this system can also\\nbe adapted to dynamic, i.e changing, en-\\nvironments (e.g. the moving obstacles in\\nour example).\\n\\n2 There is a robot called Khepera with more or less\\nsimilar characteristics. It is round-shaped, approx.\\n7 cm in diameter, has two motors with wheels\\nand various sensors. For more information I rec-\\nommend to refer to the internet.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 7\\n\\n\\n\\nChapter 1 Introduction, motivation and history dkriesel.com\\n\\nFigure 1.2: The robot is positioned in a landscape that provides sensor values for different situa-\\ntions. We add the desired output values H and so receive our learning samples. The directions in\\nwhich the sensors are oriented are exemplarily applied to two robots.\\n\\n1.2 A brief history of neural\\nnetworks\\n\\nThe field of neural networks has, like any\\nother field of science, a long history of\\ndevelopment with many ups and downs,\\nas we will see soon. To continue the style\\nof my work I will not represent this history\\nin text form but more compact in form of a\\ntimeline. Citations and bibliographical ref-\\nerences are added mainly for those topics\\nthat will not be further discussed in this\\ntext. Citations for keywords that will be\\nexplained later are mentioned in the corre-\\nsponding chapters.\\n\\nThe history of neural networks begins in\\nthe early 1940’s and thus nearly simulta-\\n\\nneously with the history of programmable\\nelectronic computers. The youth of this\\nfield of research, as with the field of com-\\nputer science itself, can be easily recog-\\nnized due to the fact that many of the\\ncited persons are still with us.\\n\\n1.2.1 The beginning\\n\\nAs soon as 1943 Warren McCulloch\\nand Walter Pitts introduced mod-\\nels of neurological networks, recre-\\nated threshold switches based on neu-\\nrons and showed that even simple\\nnetworks of this kind are able to\\ncalculate nearly any logic or arith-\\nmetic function [MP43]. Further-\\n\\n8 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 1.2 History of neural networks\\n\\nFigure 1.4: Some institutions of the field of neural networks. From left to right: John von Neu-\\nmann, Donald O. Hebb, Marvin Minsky, Bernard Widrow, Seymour Papert, Teuvo Kohonen, John\\nHopfield, \\"in the order of appearance\\" as far as possible.\\n\\nmore, the first computer precur-\\nsors (\\"electronic brains\\")were de-\\nveloped, among others supported by\\nKonrad Zuse, who was tired of cal-\\nculating ballistic trajectories by hand.\\n\\n1947: Walter Pitts and Warren Mc-\\nCulloch indicated a practical field\\nof application (which was not men-\\ntioned in their work from 1943),\\nnamely the recognition of spacial pat-\\nterns by neural networks [PM47].\\n\\n1949: Donald O. Hebb formulated the\\nclassical Hebbian rule [Heb49] which\\nrepresents in its more generalized\\nform the basis of nearly all neural\\nlearning procedures. The rule im-\\nplies that the connection between two\\nneurons is strengthened when both\\nneurons are active at the same time.\\nThis change in strength is propor-\\ntional to the product of the two activ-\\nities. Hebb could postulate this rule,\\nbut due to the absence of neurological\\nresearch he was not able to verify it.\\n\\n1950: The neuropsychologist Karl\\nLashley defended the thesis that\\n\\nbrain information storage is realized\\nas a distributed system. His thesis\\nwas based on experiments on rats,\\nwhere only the extent but not the\\nlocation of the destroyed nerve tissue\\ninfluences the rats’ performance to\\nfind their way out of a labyrinth.\\n\\n1.2.2 Golden age\\n\\n1951: For his dissertation Marvin Min-\\nsky developed the neurocomputer\\nSnark, which has already been capa-\\nble to adjust its weights3 automati-\\ncally. But it has never been practi-\\ncally implemented, since it is capable\\nto busily calculate, but nobody really\\nknows what it calculates.\\n\\n1956: Well-known scientists and ambi-\\ntious students met at the Dart-\\nmouth Summer Research Project\\nand discussed, to put it crudely, how\\nto simulate a brain. Differences be-\\ntween top-down and bottom-up re-\\nsearch developed. While the early\\n\\n3 We will learn soon what weights are.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 9\\n\\n\\n\\nChapter 1 Introduction, motivation and history dkriesel.com\\n\\nsupporters of artificial intelligence\\nwanted to simulate capabilities by\\nmeans of software, supporters of neu-\\nral networks wanted to achieve sys-\\ntem behavior by imitating the small-\\nest parts of the system – the neurons.\\n\\n1957-1958: At the MIT, Frank Rosen-\\nblatt, Charles Wightman and\\ntheir coworkers developed the first\\nsuccessful neurocomputer, the Mark\\nI perceptron, which was capable to\\n\\ndevelopment\\naccelerates recognize simple numerics by means\\n\\nof a 20 × 20 pixel image sensor and\\nelectromechanically worked with 512\\nmotor driven potentiometers - each\\npotentiometer representing one vari-\\nable weight.\\n\\n1959: Frank Rosenblatt described dif-\\nferent versions of the perceptron, for-\\nmulated and verified his perceptron\\nconvergence theorem. He described\\nneuron layers mimicking the retina,\\nthreshold switches, and a learning\\nrule adjusting the connecting weights.\\n\\n1960: Bernard Widrow and Mar-\\ncian E. Hoff introduced the ADA-\\nLINE (ADAptive LInear NEu-\\nron) [WH60], a fast and precise\\nadaptive learning system being the\\nfirst widely commercially used neu-\\nral network: It could be found in\\nnearly every analog telephone for real-\\ntime adaptive echo filtering and was\\ntrained by menas of the Widrow-Hoff\\n\\nfirst\\nspread\\n\\nuse\\nrule or delta rule. At that time Hoff,\\nlater co-founder of Intel Corporation,\\nwas a PhD student of Widrow, who\\nhimself is known as the inventor of\\n\\nmodern microprocessors. One advan-\\ntage the delta rule had over the origi-\\nnal perceptron learning algorithm was\\nits adaptivity: If the difference be-\\ntween the actual output and the cor-\\nrect solution was large, the connect-\\ning weights also changed in larger\\nsteps – the smaller the steps, the\\ncloser the target was. Disadvantage:\\nmissapplication led to infinitesimal\\nsmall steps close to the target. In the\\nfollowing stagnation and out of fear\\nof scientific unpopularity of the neu-\\nral networks ADALINE was renamed\\nin adaptive linear element – which\\nwas undone again later on.\\n\\n1961: Karl Steinbuch introduced tech-\\nnical realizations of associative mem-\\nory, which can be seen as predecessors\\nof today’s neural associative mem-\\nories [Ste61]. Additionally, he de-\\nscribed concepts for neural techniques\\nand analyzed their possibilities and\\nlimits.\\n\\n1965: In his book Learning Machines,\\nNils Nilsson gave an overview of\\nthe progress and works of this period\\nof neural network research. It was\\nassumed that the basic principles of\\nself-learning and therefore, generally\\nspeaking, \\"intelligent\\" systems had al-\\nready been discovered. Today this as-\\nsumption seems to be an exorbitant\\noverestimation, but at that time it\\nprovided for high popularity and suf-\\nficient research funds.\\n\\n1969: Marvin Minsky and Seymour\\nPapert published a precise mathe-\\n\\n10 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 1.2 History of neural networks\\n\\nmatical analysis of the perceptron\\n[MP69] to show that the perceptron\\nmodel was not capable of representing\\nmany important problems (keywords:\\nXOR problem and linear separability),\\nand so put an end to overestimation,\\npopularity and research funds. The\\n\\nresearch\\nfunds were\\n\\nstopped\\nimplication that more powerful mod-\\nels would show exactly the same prob-\\nlems and the forecast that the entire\\nfield would be a research dead end re-\\nsulted in a nearly complete decline in\\nresearch funds for the next 15 years\\n– no matter how incorrect these fore-\\ncasts were from today’s point of view.\\n\\n1.2.3 Long silence and slow\\nreconstruction\\n\\nThe research funds were, as previously-\\nmentioned, extremely short. Everywhere\\nresearch went on, but there were neither\\nconferences nor other events and therefore\\nonly few publications. This isolation of\\nindividual researchers provided for many\\nindependently developed neural network\\nparadigms: They researched, but there\\nwas no discourse among them.\\n\\nIn spite of the poor appreciation the field\\nreceived, the basic theories for the still\\ncontinuing renaissance were laid at that\\ntime:\\n\\n1972: Teuvo Kohonen introduced a\\nmodel of the linear associator,\\na model of an associative memory\\n[Koh72]. In the same year, such a\\nmodel was presented independently\\nand from a neurophysiologist’s point\\n\\nof view by James A. Anderson\\n[And72].\\n\\n1973: Christoph von der Malsburg\\nused a neuron model that was non-\\nlinear and biologically more moti-\\nvated [vdM73].\\n\\n1974: For his dissertation in Harvard\\nPaul Werbos developed a learning\\nprocedure called backpropagation of\\nerror [Wer74], but it was not until\\none decade later that this procedure\\nreached today’s importance.\\n\\nbackprop\\ndeveloped\\n\\n1976-1980 and thereafter: Stephen\\nGrossberg presented many papers\\n(for instance [Gro76]) in which\\nnumerous neural models are analyzed\\nmathematically. Furthermore, he\\ndedicated himself to the problem of\\nkeeping a neural network capable\\nof learning without destroying\\nalready learned associations. Under\\ncooperation of Gail Carpenter\\nthis led to models of adaptive\\nresonance theory (ART).\\n\\n1982: Teuvo Kohonen described the\\nself-organizing feature maps\\n(SOM) [Koh82, Koh98] – also\\nknown as Kohonen maps. He was\\nlooking for the mechanisms involving\\nself-organization in the brain (He\\nknew that the information about the\\ncreation of a being is stored in the\\ngenome, which has, however, not\\nenough memory for a structure like\\nthe brain. As a consequence, the\\nbrain has to organize and create\\nitself for the most part).\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 11\\n\\n\\n\\nChapter 1 Introduction, motivation and history dkriesel.com\\n\\nJohn Hopfield also invented the\\nso-called Hopfield networks [Hop82]\\nwhich are inspired by the laws of mag-\\nnetism in physics. They were not\\nwidely used in technical applications,\\nbut the field of neural networks slowly\\nregained importance.\\n\\n1983: Fukushima, Miyake and Ito in-\\ntroduced the neural model of the\\nNeocognitron which could recognize\\nhandwritten characters [FMI83] and\\nwas an extension of the Cognitron net-\\nwork already developed in 1975.\\n\\n1.2.4 Renaissance\\n\\nThrough the influence of John Hopfield,\\nwho had personally convinced many re-\\nsearchers of the importance of the field,\\nand the wide publication of backpro-\\npagation by Rumelhart, Hinton and\\nWilliams, the field of neural networks\\nslowly showed signs of upswing.\\n\\n1985: John Hopfield published an arti-\\ncle describing a way of finding accept-\\nable solutions for the Travelling Sales-\\nman problem by using Hopfield nets.\\n\\nRenaissance\\n\\n1986: The backpropagation of error learn-\\ning procedure as a generalization of\\nthe delta rule was separately devel-\\noped and widely published by the Par-\\nallel Distributed Processing Group\\n[RHW86a]: Non-linearly-separable\\nproblems could be solved by multi-\\nlayer perceptrons, and Marvin Min-\\nsky’s negative evaluations were dis-\\nproven at a single blow. At the same\\n\\ntime a certain kind of fatigue spread\\nin the field of artificial intelligence,\\ncaused by a series of failures and un-\\nfulfilled hopes.\\n\\nFrom this time on, the development of\\nthe field of research has almost been\\nexplosive. It can no longer be item-\\nized, but some of its results will be\\nseen in the following.\\n\\nExercises\\n\\nExercise 1. Give one example for each\\nof the following topics:\\n\\n. A book on neural networks or neuroin-\\nformatics,\\n\\n. A collaborative group of a university\\nworking with neural networks,\\n\\n. A software tool realizing neural net-\\nworks (\\"simulator\\"),\\n\\n. A company using neural networks,\\nand\\n\\n. A product or service being realized by\\nmeans of neural networks.\\n\\nExercise 2. Show at least four applica-\\ntions of technical neural networks: two\\nfrom the field of pattern recognition and\\ntwo from the field of function approxima-\\ntion.\\n\\nExercise 3. Briefly characterize the four\\ndevelopment phases of neural networks\\nand give expressive examples for each\\nphase.\\n\\n12 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\nChapter 2\\n\\nBiological neural networks\\nHow do biological systems solve problems? How does a system of neurons\\n\\nwork? How can we understand its functionality? What are different quantities\\nof neurons able to do? Where in the nervous system does information\\n\\nprocessing occur? A short biological overview of the complexity of simple\\nelements of neural information processing followed by some thoughts about\\n\\ntheir simplification in order to technically adapt them.\\n\\nBefore we begin to describe the technical\\nside of neural networks, it would be use-\\nful to briefly discuss the biology of neu-\\nral networks and the cognition of living\\norganisms – the reader may skip the fol-\\nlowing chapter without missing any tech-\\nnical information. On the other hand I\\nrecommend to read the said excursus if\\nyou want to learn something about the\\nunderlying neurophysiology and see that\\nour small approaches, the technical neural\\nnetworks, are only caricatures of nature\\n– and how powerful their natural counter-\\nparts must be when our small approaches\\nare already that effective. Now we want\\nto take a brief look at the nervous system\\nof vertebrates: We will start with a very\\nrough granularity and then proceed with\\nthe brain and up to the neural level. For\\nfurther reading I want to recommend the\\nbooks [CR00,KSJ00], which helped me a\\nlot during this chapter.\\n\\n2.1 The vertebrate nervous\\nsystem\\n\\nThe entire information processing system,\\ni.e. the vertebrate nervous system, con-\\nsists of the central nervous system and the\\nperipheral nervous system, which is only\\na first and simple subdivision. In real-\\nity, such a rigid subdivision does not make\\nsense, but here it is helpful to outline the\\ninformation processing in a body.\\n\\n2.1.1 Peripheral and central\\nnervous system\\n\\nThe peripheral nervous system (PNS)\\ncomprises the nerves that are situated out-\\nside of the brain or the spinal cord. These\\nnerves form a branched and very dense net-\\nwork throughout the whole body. The pe-\\n\\n13\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nripheral nervous system includes, for ex-\\nample, the spinal nerves which pass out\\nof the spinal cord (two within the level of\\neach vertebra of the spine) and supply ex-\\ntremities, neck and trunk, but also the cra-\\nnial nerves directly leading to the brain.\\n\\nThe central nervous system (CNS),\\nhowever, is the \\"main-frame\\" within the\\nvertebrate. It is the place where infor-\\nmation received by the sense organs are\\nstored and managed. Furthermore, it con-\\ntrols the inner processes in the body and,\\nlast but not least, coordinates the mo-\\ntor functions of the organism. The ver-\\ntebrate central nervous system consists of\\nthe brain and the spinal cord (Fig. 2.1).\\nHowever, we want to focus on the brain,\\nwhich can - for the purpose of simplifica-\\ntion - be divided into four areas (Fig. 2.2\\non the next page) to be discussed here.\\n\\n2.1.2 The cerebrum is responsible\\nfor abstract thinking\\nprocesses.\\n\\nThe cerebrum (telencephalon) is one of\\nthe areas of the brain that changed most\\nduring evolution. Along an axis, running\\nfrom the lateral face to the back of the\\nhead, this area is divided into two hemi-\\nspheres, which are organized in a folded\\nstructure. These cerebral hemispheres\\nare connected by one strong nerve cord\\n(\\"bar\\") and several small ones. A large\\nnumber of neurons are located in the cere-\\nbral cortex (cortex) which is approx. 2-\\n4 cm thick and divided into different cor-\\ntical fields, each having a specific task to Figure 2.1: Illustration of the central nervous\\n\\nsystem with spinal cord and brain.\\n\\n14 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.1 The vertebrate nervous system\\n\\nFigure 2.2: Illustration of the brain. The col-\\nored areas of the brain are discussed in the text.\\nThe more we turn from abstract information pro-\\ncessing to direct reflexive processing, the darker\\nthe areas of the brain are colored.\\n\\nfulfill. Primary cortical fields are re-\\nsponsible for processing qualitative infor-\\nmation, such as the management of differ-\\nent perceptions (e.g. the visual cortex\\nis responsible for the management of vi-\\nsion). Association cortical fields, how-\\never, perform more abstract association\\nand thinking processes; they also contain\\nour memory.\\n\\n2.1.3 The cerebellum controls and\\ncoordinates motor functions\\n\\nThe cerebellum is located below the cere-\\nbrum, therefore it is closer to the spinal\\ncord. Accordingly, it serves less abstract\\nfunctions with higher priority: Here, large\\nparts of motor coordination are performed,\\ni.e., balance and movements are controlled\\n\\nand errors are continually corrected. For\\nthis purpose, the cerebellum has direct\\nsensory information about muscle lengths\\nas well as acoustic and visual informa-\\ntion. Furthermore, it also receives mes-\\nsages about more abstract motor signals\\ncoming from the cerebrum.\\n\\nIn the human brain the cerebellum is con-\\nsiderably smaller than the cerebrum, but\\nthis is rather an exception. In many ver-\\ntebrates this ratio is less pronounced. If\\nwe take a look at vertebrate evolution, we\\nwill notice that the cerebellum is not \\"too\\nsmall\\" but the cerebum is \\"too large\\" (at\\nleast, it is the most highly developed struc-\\nture in the vertebrate brain). The two re-\\nmaining brain areas should also be briefly\\ndiscussed: the diencephalon and the brain-\\nstem.\\n\\n2.1.4 The diencephalon controls\\nfundamental physiological\\nprocesses\\n\\nThe interbrain (diencephalon) includes\\nparts of which only the thalamus will\\n\\nthalamus\\nfilters\\nincoming\\ndata\\n\\nbe briefly discussed: This part of the di-\\nencephalon mediates between sensory and\\nmotor signals and the cerebrum. Particu-\\nlarly, the thalamus decides which part of\\nthe information is transferred to the cere-\\nbrum, so that especially less important\\nsensory perceptions can be suppressed at\\nshort notice to avoid overloads. Another\\npart of the diencephalon is the hypotha-\\nlamus, which controls a number of pro-\\ncesses within the body. The diencephalon\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 15\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nis also heavily involved in the human cir-\\ncadian rhythm (\\"internal clock\\") and the\\nsensation of pain.\\n\\n2.1.5 The brainstem connects the\\nbrain with the spinal cord and\\ncontrols reflexes.\\n\\nIn comparison with the diencephalon the\\nbrainstem or the (truncus cerebri) re-\\nspectively is phylogenetically much older.\\nRoughly speaking, it is the \\"extended\\nspinal cord\\" and thus the connection be-\\ntween brain and spinal cord. The brain-\\nstem can also be divided into different ar-\\neas, some of which will be exemplarily in-\\ntroduced in this chapter. The functions\\nwill be discussed from abstract functions\\ntowards more fundamental ones. One im-\\nportant component is the pons (=bridge),\\na kind of transit station for many nerve sig-\\nnals from brain to body and vice versa.\\n\\nIf the pons is damaged (e.g. by a cere-\\nbral infarct), then the result could be the\\nlocked-in syndrome – a condition in\\nwhich a patient is \\"walled-in\\" within his\\nown body. He is conscious and aware\\nwith no loss of cognitive function, but can-\\nnot move or communicate by any means.\\nOnly his senses of sight, hearing, smell and\\ntaste are generally working perfectly nor-\\nmal. Locked-in patients may often be able\\nto communicate with others by blinking or\\nmoving their eyes.\\n\\nFurthermore, the brainstem is responsible\\nfor many fundamental reflexes, such as the\\nblinking reflex or coughing.\\n\\nAll parts of the nervous system have one\\nthing in common: information processing.\\nThis is accomplished by huge accumula-\\ntions of billions of very similar cells, whose\\nstructure is very simple but which com-\\nmunicate continuously. Large groups of\\nthese cells send coordinated signals and\\nthus reach the enormous information pro-\\ncessing capacity we are familiar with from\\nour brain. We will now leave the level of\\nbrain areas and continue with the cellular\\nlevel of the body - the level of neurons.\\n\\n2.2 Neurons are information\\nprocessing cells\\n\\nBefore specifying the functions and pro-\\ncesses within a neuron, we will give a\\nrough description of neuron functions: A\\nneuron is nothing more than a switch with\\ninformation input and output. The switch\\nwill be activated if there are enough stim-\\nuli of other neurons hitting the informa-\\ntion input. Then, at the information out-\\nput, a pulse is sent to, for example, other\\nneurons.\\n\\n2.2.1 Components of a neuron\\n\\nNow we want to take a look at the com-\\nponents of a neuron (Fig. 2.3 on the fac-\\ning page). In doing so, we will follow the\\nway the electrical information takes within\\nthe neuron. The dendrites of a neuron\\nreceive the information by special connec-\\ntions, the synapses.\\n\\n16 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.2 The neuron\\n\\nFigure 2.3: Illustration of a biological neuron with the components discussed in this text.\\n\\n2.2.1.1 Synapses weight the individual\\nparts of information\\n\\nIncoming signals from other neurons or\\ncells are transferred to a neuron by special\\nconnections, the synapses. Such connec-\\ntions can usually be found at the dendrites\\nof a neuron, sometimes also directly at the\\nsoma. We distinguish between electrical\\nand chemical synapses.\\n\\nThe electrical synapse is the simpler\\nelectrical\\nsynapse:\\nsimple\\n\\nvariant. An electrical signal received by\\nthe synapse, i.e. coming from the presy-\\nnaptic side, is directly transferred to the\\npostsynaptic nucleus of the cell. Thus,\\nthere is a direct, strong, unadjustable\\nconnection between the signal transmitter\\nand the signal receiver, which is, for exam-\\nple, relevant to shortening reactions that\\nmust be \\"hard coded\\" within a living or-\\nganism.\\n\\nThe chemical synapse is the more dis-\\ntinctive variant. Here, the electrical cou-\\npling of source and target does not take\\nplace, the coupling is interrupted by the\\nsynaptic cleft. This cleft electrically sep-\\narates the presynaptic side from the post-\\nsynaptic one. You might think that, never-\\ntheless, the information has to flow, so we\\nwill discuss how this happens: It is not an\\nelectrical, but a chemical process. On the\\npresynaptic side of the synaptic cleft the\\nelectrical signal is converted into a chemi-\\ncal signal, a process induced by chemical\\ncues released there (the so-called neuro-\\ntransmitters). These neurotransmitters\\ncross the synaptic cleft and transfer the\\ninformation into the nucleus of the cell\\n(this is a very simple explanation, but later\\non we will see how this exactly works),\\nwhere it is reconverted into electrical in-\\nformation. The neurotransmitters are de-\\ngraded very fast, so that it is possible to re-\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 17\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nlease very precise information pulses here,\\ntoo.\\n\\nIn spite of the more complex function-\\ncemical\\nsynapse\\nis more\\ncomplex\\nbut also\\n\\nmore\\npowerful\\n\\ning, the chemical synapse has - compared\\nwith the electrical synapse - utmost advan-\\ntages:\\n\\nOne-way connection: A chemical\\nsynapse is a one-way connection.\\nDue to the fact that there is no direct\\nelectrical connection between the\\npre- and postsynaptic area, electrical\\npulses in the postsynaptic area\\ncannot flash over to the presynaptic\\narea.\\n\\nAdjustability: There is a large number of\\ndifferent neurotransmitters that can\\nalso be released in various quantities\\nin a synaptic cleft. There are neuro-\\ntransmitters that stimulate the post-\\nsynaptic cell nucleus, and others that\\nslow down such stimulation. Some\\nsynapses transfer a strongly stimulat-\\ning signal, some only weakly stimu-\\nlating ones. The adjustability varies\\na lot, and one of the central points\\nin the examination of the learning\\nability of the brain is, that here the\\nsynapses are variable, too. That is,\\nover time they can form a stronger or\\nweaker connection.\\n\\n2.2.1.2 Dendrites collect all parts of\\ninformation\\n\\nDendrites branch like trees from the cell\\nnucleus of the neuron (which is called\\nsoma) and receive electrical signals from\\n\\nmany different sources, which are then\\ntransferred into the nucleus of the cell.\\nThe amount of branching dendrites is also\\ncalled dendrite tree.\\n\\n2.2.1.3 In the soma the weighted\\ninformation is accumulated\\n\\nAfter the cell nucleus (soma) has re-\\nceived a plenty of activating (=stimulat-\\ning) and inhibiting (=diminishing) signals\\nby synapses or dendrites, the soma accu-\\nmulates these signals. As soon as the ac-\\ncumulated signal exceeds a certain value\\n(called threshold value), the cell nucleus\\nof the neuron activates an electrical pulse\\nwhich then is transmitted to the neurons\\nconnected to the current one.\\n\\n2.2.1.4 The axon transfers outgoing\\npulses\\n\\nThe pulse is transferred to other neurons\\nby means of the axon. The axon is a\\nlong, slender extension of the soma. In\\nan extreme case, an axon can stretch up\\nto one meter (e.g. within the spinal cord).\\nThe axon is electrically isolated in order\\nto achieve a better conduction of the elec-\\ntrical signal (we will return to this point\\nlater on) and it leads to dendrites, which\\ntransfer the information to, for example,\\nother neurons. So now we are back at the\\nbeginning of our description of the neuron\\nelements. An axon can, however, transfer\\ninformation to other kinds of cells in order\\nto control them.\\n\\n18 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.2 The neuron\\n\\n2.2.2 Electrochemical processes in\\nthe neuron and its\\ncomponents\\n\\nAfter having pursued the path of an elec-\\ntrical signal from the dendrites via the\\nsynapses to the nucleus of the cell and\\nfrom there via the axon into other den-\\ndrites, we now want to take a small step\\nfrom biology towards technology. In doing\\nso, a simplified introduction of the electro-\\nchemical information processing should be\\nprovided.\\n\\n2.2.2.1 Neurons maintain electrical\\nmembrane potential\\n\\nOne fundamental aspect is the fact that\\ncompared to their environment the neu-\\nrons show a difference in electrical charge,\\na potential. In the membrane (=enve-\\nlope) of the neuron the charge is different\\nfrom the charge on the outside. This dif-\\nference in charge is a central concept that\\nis important to understand the processes\\nwithin the neuron. The difference is called\\nmembrane potential. The membrane\\npotential, i.e., the difference in charge, is\\ncreated by several kinds of charged atoms\\n(ions), whose concentration varies within\\nand outside of the neuron. If we penetrate\\nthe membrane from the inside outwards,\\nwe will find certain kinds of ions more of-\\nten or less often than on the inside. This\\ndescent or ascent of concentration is called\\na concentration gradient.\\n\\nLet us first take a look at the membrane\\npotential in the resting state of the neu-\\n\\nron, i.e., we assume that no electrical sig-\\nnals are received from the outside. In this\\ncase, the membrane potential is −70 mV.\\nSince we have learned that this potential\\ndepends on the concentration gradients of\\nvarious ions, there is of course the central\\nquestion of how to maintain these concen-\\ntration gradients: Normally, diffusion pre-\\ndominates and therefore each ion is eager\\nto decrease concentration gradients and\\nto spread out evenly. If this happens,\\nthe membrane potential will move towards\\n0 mV, so finally there would be no mem-\\nbrane potential anymore. Thus, the neu-\\nron actively maintains its membrane po-\\ntential to be able to process information.\\nHow does this work?\\n\\nThe secret is the membrane itself, which is\\npermeable to some ions, but not for others.\\nTo maintain the potential, various mecha-\\nnisms are in progress at the same time:\\n\\nConcentration gradient: As described\\nabove the ions try to be as uniformly\\ndistributed as possible. If the\\nconcentration of an ion is higher on\\nthe inside of the neuron than on\\nthe outside, it will try to diffuse\\nto the outside and vice versa.\\nThe positively charged ion K+\\n\\n(potassium) occurs very frequently\\nwithin the neuron but less frequently\\noutside of the neuron, and therefore\\nit slowly diffuses out through the\\nneuron’s membrane. But another\\ngroup of negative ions, collectively\\ncalled A−, remains within the neuron\\nsince the membrane is not permeable\\nto them. Thus, the inside of the\\nneuron becomes negatively charged.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 19\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nNegative A ions remain, positive K\\nions disappear, and so the inside of\\nthe cell becomes more negative. The\\nresult is another gradient.\\n\\nElectrical Gradient: The electrical gradi-\\nent acts contrary to the concentration\\ngradient. The intracellular charge is\\nnow very strong, therefore it attracts\\npositive ions: K+ wants to get back\\ninto the cell.\\n\\nIf these two gradients were now left alone,\\nthey would eventually balance out, reach\\na steady state, and a membrane poten-\\ntial of −85 mV would develop. But we\\nwant to achieve a resting membrane po-\\ntential of −70 mV, thus there seem to ex-\\nist some disturbances which prevent this.\\nFurthermore, there is another important\\nion, Na+ (sodium), for which the mem-\\nbrane is not very permeable but which,\\nhowever, slowly pours through the mem-\\nbrane into the cell. As a result, the sodium\\nis driven into the cell all the more: On the\\none hand, there is less sodium within the\\nneuron than outside the neuron. On the\\nother hand, sodium is positively charged\\nbut the interior of the cell has negative\\ncharge, which is a second reason for the\\nsodium wanting to get into the cell.\\n\\nDue to the low diffusion of sodium into the\\ncell the intracellular sodium concentration\\nincreases. But at the same time the inside\\nof the cell becomes less negative, so that\\nK+ pours in more slowly (we can see that\\nthis is a complex mechanism where every-\\nthing is influenced by everything). The\\nsodium shifts the intracellular equilibrium\\nfrom negative to less negative, compared\\n\\nwith its environment. But even with these\\ntwo ions a standstill with all gradients be-\\ning balanced out could still be achieved.\\nNow the last piece of the puzzle gets into\\nthe game: a \\"pump\\" (or rather, the protein\\nATP) actively transports ions against the\\ndirection they actually want to take!\\n\\nSodium is actively pumped out of the cell,\\nalthough it tries to get into the cell\\nalong the concentration gradient and\\nthe electrical gradient.\\n\\nPotassium, however, diffuses strongly out\\nof the cell, but is actively pumped\\nback into it.\\n\\nFor this reason the pump is also called\\nsodium-potassium pump. The pump\\nmaintains the concentration gradient for\\nthe sodium as well as for the potassium,\\nso that some sort of steady state equilib-\\nrium is created and finally the resting po-\\ntential is −70 mV as observed. All in all\\nthe membrane potential is maintained by\\nthe fact that the membrane is imperme-\\nable to some ions and other ions are ac-\\ntively pumped against the concentration\\nand electrical gradients. Now that we\\nknow that each neuron has a membrane\\npotential we want to observe how a neu-\\nron receives and transmits signals.\\n\\n2.2.2.2 The neuron is activated by\\nchanges in the membrane\\npotential\\n\\nAbove we have learned that sodium and\\npotassium can diffuse through the mem-\\nbrane - sodium slowly, potassium faster.\\n\\n20 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.2 The neuron\\n\\nThey move through channels within the\\nmembrane, the sodium and potassium\\nchannels. In addition to these per-\\nmanently open channels responsible for\\ndiffusion and balanced by the sodium-\\npotassium pump, there also exist channels\\nthat are not always open but which only\\nresponse \\"if required\\". Since the opening\\nof these channels changes the concentra-\\ntion of ions within and outside of the mem-\\nbrane, it also changes the membrane po-\\ntential.\\n\\nThese controllable channels are opened as\\nsoon as the accumulated received stimulus\\nexceeds a certain threshold. For example,\\nstimuli can be received from other neurons\\nor have other causes. There exist, for ex-\\nample, specialized forms of neurons, the\\nsensory cells, for which a light incidence\\ncould be such a stimulus. If the incom-\\ning amount of light exceeds the threshold,\\ncontrollable channels are opened.\\n\\nThe said threshold (the threshold poten-\\ntial) lies at about −55 mV. As soon as the\\nreceived stimuli reach this value, the neu-\\nron is activated and an electrical signal,\\nan action potential, is initiated. Then\\nthis signal is transmitted to the cells con-\\nnected to the observed neuron, i.e. the\\ncells \\"listen\\" to the neuron. Now we want\\nto take a closer look at the different stages\\nof the action potential (Fig. 2.4 on the next\\npage):\\n\\nResting state: Only the permanently\\nopen sodium and potassium channels\\nare permeable. The membrane\\npotential is at −70 mV and actively\\nkept there by the neuron.\\n\\nStimulus up to the threshold: A stimu-\\nlus opens channels so that sodium\\ncan pour in. The intracellular charge\\nbecomes more positive. As soon as\\nthe membrane potential exceeds the\\nthreshold of −55 mV, the action po-\\ntential is initiated by the opening of\\nmany sodium channels.\\n\\nDepolarization: Sodium is pouring in. Re-\\nmember: Sodium wants to pour into\\nthe cell because there is a lower in-\\ntracellular than extracellular concen-\\ntration of sodium. Additionally, the\\ncell is dominated by a negative en-\\nvironment which attracts the posi-\\ntive sodium ions. This massive in-\\nflux of sodium drastically increases\\nthe membrane potential - up to ap-\\nprox. +30 mV - which is the electrical\\npulse, i.e., the action potential.\\n\\nRepolarization: Now the sodium channels\\nare closed and the potassium channels\\nare opened. The positively charged\\nions want to leave the positive inte-\\nrior of the cell. Additionally, the intra-\\ncellular concentration is much higher\\nthan the extracellular one, which in-\\ncreases the efflux of ions even more.\\nThe interior of the cell is once again\\nmore negatively charged than the ex-\\nterior.\\n\\nHyperpolarization: Sodium as well as\\npotassium channels are closed again.\\nAt first the membrane potential is\\nslightly more negative than the rest-\\ning potential. This is due to the\\nfact that the potassium channels close\\nmore slowly. As a result, (positively\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 21\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nFigure 2.4: Initiation of action potential over time.\\n\\n22 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.2 The neuron\\n\\ncharged) potassium effuses because of\\nits lower extracellular concentration.\\nAfter a refractory period of 1 − 2\\nms the resting state is re-established\\nso that the neuron can react to newly\\napplied stimuli with an action poten-\\ntial. In simple terms, the refractory\\nperiod is a mandatory break a neu-\\nron has to take in order to regenerate.\\nThe shorter this break is, the more\\noften a neuron can fire per time.\\n\\nThen the resulting pulse is transmitted by\\nthe axon.\\n\\n2.2.2.3 In the axon a pulse is\\nconducted in a saltatory way\\n\\nWe have already learned that the axon\\nis used to transmit the action potential\\nacross long distances (remember: You will\\nfind an illustration of a neuron including\\nan axon in Fig. 2.3 on page 17). The axon\\nis a long, slender extension of the soma.\\nIn vertebrates it is normally coated by a\\nmyelin sheath that consists of Schwann\\ncells (in the PNS) or oligodendrocytes\\n(in the CNS) 1, which insulate the axon\\nvery well from electrical activity. At a dis-\\ntance of 0.1−2mm there are gaps between\\nthese cells, the so-called nodes of Ran-\\nvier. The said gaps appear where one in-\\nsulate cell ends and the next one begins.\\nIt is obvious that at such a node the axon\\nis less insulated.\\n1 Schwann cells as well as oligodendrocytes are vari-\\neties of the glial cells. There are about 50 times\\nmore glial cells than neurons: They surround the\\nneurons (glia = glue), insulate them from each\\nother, provide energy, etc.\\n\\nNow you may assume that these less in-\\nsulated nodes are a disadvantage of the\\naxon - however, they are not. At the\\nnodes, mass can be transferred between\\nthe intracellular and extracellular area, a\\ntransfer that is impossible at those parts\\nof the axon which are situated between\\ntwo nodes (internodes) and therefore in-\\nsulated by the myelin sheath. This mass\\ntransfer permits the generation of signals\\nsimilar to the generation of the action po-\\ntential within the soma. The action po-\\ntential is transferred as follows: It does\\nnot continuously travel along the axon but\\njumps from node to node. Thus, a series\\nof depolarization travels along the nodes of\\nRanvier. One action potential initiates the\\nnext one, and mostly even several nodes\\nare active at the same time here. The\\npulse \\"jumping\\" from node to node is re-\\nsponsible for the name of this pulse con-\\nductor: saltatory conductor.\\n\\nObviously, the pulse will move faster if its\\njumps are larger. Axons with large in-\\nternodes (2 mm) achieve a signal disper-\\nsion of approx. 180 meters per second.\\nHowever, the internodes cannot grow in-\\ndefinitely, since the action potential to be\\ntransferred would fade too much until it\\nreaches the next node. So the nodes have\\na task, too: to constantly amplify the sig-\\nnal. The cells receiving the action poten-\\ntial are attached to the end of the axon –\\noften connected by dendrites and synapses.\\nAs already indicated above, the action po-\\ntentials are not only generated by informa-\\ntion received by the dendrites from other\\nneurons.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 23\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\n2.3 Receptor cells are\\nmodified neurons\\n\\nAction potentials can also be generated by\\nsensory information an organism receives\\nfrom its environment through its sensory\\ncells. Specialized receptor cells are able\\nto perceive specific stimulus energies such\\nas light, temperature and sound or the ex-\\nistence of certain molecules (like, for exam-\\nple, the sense of smell). This is working\\nbecause of the fact that these sensory cells\\nare actually modified neurons. They do\\nnot receive electrical signals via dendrites\\nbut the existence of the stimulus being\\nspecific for the receptor cell ensures that\\nthe ion channels open and an action po-\\ntential is developed. This process of trans-\\nforming stimulus energy into changes in\\nthe membrane potential is called sensory\\ntransduction. Usually, the stimulus en-\\nergy itself is too weak to directly cause\\nnerve signals. Therefore, the signals are\\namplified either during transduction or by\\nmeans of the stimulus-conducting ap-\\nparatus. The resulting action potential\\ncan be processed by other neurons and is\\nthen transmitted into the thalamus, which\\nis, as we have already learned, a gateway\\nto the cerebral cortex and therefore can re-\\nject sensory impressions according to cur-\\nrent relevance and thus prevent an abun-\\ndance of information to be managed.\\n\\n2.3.1 There are different receptor\\ncells for various types of\\nperceptions\\n\\nPrimary receptors transmit their pulses\\ndirectly to the nervous system. A good\\nexample for this is the sense of pain.\\nHere, the stimulus intensity is propor-\\ntional to the amplitude of the action po-\\ntential. Technically, this is an amplitude\\nmodulation.\\n\\nSecondary receptors, however, continu-\\nously transmit pulses. These pulses con-\\ntrol the amount of the related neurotrans-\\nmitter, which is responsible for transfer-\\nring the stimulus. The stimulus in turn\\ncontrols the frequency of the action poten-\\ntial of the receiving neuron. This process\\nis a frequency modulation, an encoding of\\nthe stimulus, which allows to better per-\\nceive the increase and decrease of a stimu-\\nlus.\\n\\nThere can be individual receptor cells or\\ncells forming complex sensory organs (e.g.\\neyes or ears). They can receive stimuli\\nwithin the body (by means of the intero-\\nceptors) as well as stimuli outside of the\\nbody (by means of the exteroceptors).\\n\\nAfter having outlined how information is\\nreceived from the environment, it will be\\ninteresting to look at how the information\\nis processed.\\n\\n24 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.3 Receptor cells\\n\\n2.3.2 Information is processed on\\nevery level of the nervous\\nsystem\\n\\nThere is no reason to believe that all re-\\nceived information is transmitted to the\\nbrain and processed there, and that the\\nbrain ensures that it is \\"output\\" in the\\nform of motor pulses (the only thing an\\norganism can actually do within its envi-\\nronment is to move). The information pro-\\ncessing is entirely decentralized. In order\\nto illustrate this principle, we want to take\\na look at some examples, which leads us\\nagain from the abstract to the fundamen-\\ntal in our hierarchy of information process-\\ning.\\n\\n. It is certain that information is pro-\\ncessed in the cerebrum, which is the\\nmost developed natural information\\nprocessing structure.\\n\\n. The midbrain and the thalamus,\\nwhich serves – as we have already\\nlearned – as a gateway to the cere-\\nbral cortex, are situated much lower\\nin the hierarchy. The filtering of in-\\nformation with respect to the current\\nrelevance executed by the midbrain\\nis a very important method of infor-\\nmation processing, too. But even the\\nthalamus does not receive any prepro-\\ncessed stimuli from the outside. Now\\nlet us continue with the lowest level,\\nthe sensory cells.\\n\\n. On the lowest level, i.e. at the recep-\\ntor cells, the information is not only\\nreceived and transferred but directly\\nprocessed. One of the main aspects of\\n\\nthis subject is to prevent the transmis-\\nsion of \\"continuous stimuli\\" to the cen-\\ntral nervous system because of sen-\\nsory adaptation: Due to continu-\\nous stimulation many receptor cells\\nautomatically become insensitive to\\nstimuli. Thus, receptor cells are not\\na direct mapping of specific stimu-\\nlus energy onto action potentials but\\ndepend on the past. Other sensors\\nchange their sensitivity according to\\nthe situation: There are taste recep-\\ntors which respond more or less to the\\nsame stimulus according to the nutri-\\ntional condition of the organism.\\n\\n. Even before a stimulus reaches the\\nreceptor cells, information processing\\ncan already be executed by a preced-\\ning signal carrying apparatus, for ex-\\nample in the form of amplification:\\nThe external and the internal ear\\nhave a specific shape to amplify the\\nsound, which also allows – in asso-\\nciation with the sensory cells of the\\nsense of hearing – the sensory stim-\\nulus only to increase logarithmically\\nwith the intensity of the heard sig-\\nnal. On closer examination, this is\\nnecessary, since the sound pressure of\\nthe signals for which the ear is con-\\nstructed can vary over a wide expo-\\nnential range. Here, a logarithmic\\nmeasurement is an advantage. Firstly,\\nan overload is prevented and secondly,\\nthe fact that the intensity measure-\\nment of intensive signals will be less\\nprecise, doesn’t matter as well. If a jet\\nfighter is starting next to you, small\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 25\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nchanges in the noise level can be ig-\\nnored.\\n\\nJust to get a feeling for sensory organs\\nand information processing in the organ-\\nism, we will briefly describe \\"usual\\" light\\nsensing organs, i.e. organs often found in\\nnature. For the third light sensing organ\\ndescribed below, the single lens eye, we\\nwill discuss the information processing in\\nthe eye.\\n\\n2.3.3 An outline of common light\\nsensing organs\\n\\nFor many organisms it turned out to be ex-\\ntremely useful to be able to perceive elec-\\ntromagnetic radiation in certain regions of\\nthe spectrum. Consequently, sensory or-\\ngans have been developed which can de-\\ntect such electromagnetic radiation and\\nthe wavelength range of the radiation per-\\nceivable by the human eye is called visible\\nrange or simply light. The different wave-\\nlengths of this electromagnetic radiation\\nare perceived by the human eye as differ-\\nent colors. The visible range of the elec-\\ntromagnetic radiation is different for each\\norganism. Some organisms cannot see the\\ncolors (=wavelength ranges) we can see,\\nothers can even perceive additional wave-\\nlength ranges (e.g. in the UV range). Be-\\nfore we begin with the human being – in\\norder to get a broader knowledge of the\\nsense of sight– we briefly want to look at\\ntwo organs of sight which, from an evolu-\\ntionary point of view, exist much longer\\nthan the human.\\n\\n2.3.3.1 Compound eyes and pinhole\\neyes only provide high temporal\\nor spatial resolution\\n\\nLet us first take a look at the so-called\\ncompound eye (Fig. 2.5 on the next\\npage), which is, for example, common in\\ninsects and crustaceans. The compound\\n\\nCompound eye:\\nhigh temp.,\\nlow\\nspatial\\nresolution\\n\\neye consists of a great number of small,\\nindividual eyes. If we look at the com-\\npound eye from the outside, the individ-\\nual eyes are clearly visible and arranged\\nin a hexagonal pattern. Each individual\\neye has its own nerve fiber which is con-\\nnected to the insect brain. Since the indi-\\nvidual eyes can be distinguished, it is ob-\\nvious that the number of pixels, i.e. the\\nspatial resolution, of compound eyes must\\nbe very low and the image is blurred. But\\ncompound eyes have advantages, too, espe-\\ncially for fast-flying insects. Certain com-\\npound eyes process more than 300 images\\nper second (to the human eye, however,\\nmovies with 25 images per second appear\\nas a fluent motion).\\n\\nPinhole eyes are, for example, found in\\noctopus species and work – as you can\\nguess – similar to a pinhole camera. A\\n\\npinhole\\ncamera:\\nhigh spat.,\\nlow\\ntemporal\\nresolution\\n\\npinhole eye has a very small opening for\\nlight entry, which projects a sharp image\\nonto the sensory cells behind. Thus, the\\nspatial resolution is much higher than in\\nthe compound eye. But due to the very\\nsmall opening for light entry the resulting\\nimage is less bright.\\n\\n26 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.3 Receptor cells\\n\\nFigure 2.5: Compound eye of a robber fly\\n\\n2.3.3.2 Single lens eyes combine the\\nadvantages of the other two\\neye types, but they are more\\ncomplex\\n\\nThe light sensing organ common in verte-\\nbrates is the single lense eye. The result-\\ning image is a sharp, high-resolution image\\nof the environment at high or variable light\\nintensity. On the other hand it is more\\ncomplex. Similar to the pinhole eye the\\nlight enters through an opening (pupil)\\nand is projected onto a layer of sensory\\ncells in the eye. (retina). But in contrast\\n\\nSingle\\nlense eye:\\n\\nhigh temp.\\nand spat.\\nresolution\\n\\nto the pinhole eye, the size of the pupil can\\nbe adapted to the lighting conditions (by\\nmeans of the iris muscle, which expands\\nor contracts the pupil). These differences\\nin pupil dilation require to actively focus\\nthe image. Therefore, the single lens eye\\ncontains an additional adjustable lens.\\n\\n2.3.3.3 The retina does not only\\nreceive information but is also\\nresponsible for information\\nprocessing\\n\\nThe light signals falling on the eye are\\nreceived by the retina and directly pre-\\nprocessed by several layers of information-\\nprocessing cells. We want to briefly dis-\\ncuss the different steps of this informa-\\ntion processing and in doing so, we follow\\nthe way of the information carried by the\\nlight:\\n\\nPhotoreceptors receive the light signal\\nund cause action potentials (there\\nare different receptors for different\\ncolor components and light intensi-\\nties). These receptors are the real\\nlight-receiving part of the retina and\\nthey are sensitive to such an extent\\nthat only one single photon falling\\non the retina can cause an action po-\\ntential. Then several photoreceptors\\ntransmit their signals to one single\\n\\nbipolar cell. This means that here the in-\\nformation has already been summa-\\nrized. Finally, the now transformed\\nlight signal travels from several bipo-\\nlar cells 2 into\\n\\nganglion cells. Various bipolar cells can\\ntransmit their information to one gan-\\nglion cell. The higher the number\\nof photoreceptors that affect the gan-\\nglion cell, the larger the field of per-\\nception, the receptive field, which\\ncovers the ganglions – and the less\\n\\n2 There are different kinds of bipolar cells, as well,\\nbut to discuss all of them would go too far.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 27\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nsharp is the image in the area of this\\nganglion cell. So the information is\\nalready reduced directly in the retina\\nand the overall image is, for exam-\\nple, blurred in the peripheral field\\nof vision. So far, we have learned\\nabout the information processing in\\nthe retina only as a top-down struc-\\nture. Now we want to take a look at\\nthe\\n\\nhorizontal and amacrine cells. These\\ncells are not connected from the\\nfront backwards but laterally. They\\nallow the light signals to influence\\nthemselves laterally directly during\\nthe information processing in the\\nretina – a much more powerful\\nmethod of information processing\\nthan compressing and blurring.\\nWhen the horizontal cells are excited\\nby a photoreceptor, they are able to\\nexcite other nearby photoreceptors\\nand at the same time inhibit more\\ndistant bipolar cells and receptors.\\nThis ensures the clear perception of\\noutlines and bright points. Amacrine\\ncells can further intensify certain\\nstimuli by distributing information\\nfrom bipolar cells to several ganglion\\ncells or by inhibiting ganglions.\\n\\nThese first steps of transmitting visual in-\\nformation to the brain show that informa-\\ntion is processed from the first moment the\\ninformation is received and, on the other\\nhand, is processed in parallel within mil-\\nlions of information-processing cells. The\\nsystem’s power and resistance to errors\\nis based upon this massive division of\\nwork.\\n\\n2.4 The amount of neurons in\\nliving organisms at\\ndifferent stages of\\ndevelopment\\n\\nAn overview of different organisms and\\ntheir neural capacity (in large part from\\n[RD05]):\\n\\n302 neurons are required by the nervous\\nsystem of a nematode worm, which\\nserves as a popular model organism\\nin biology. Nematodes live in the soil\\nand feed on bacteria.\\n\\n104 neurons make an ant (To simplify\\nmatters we neglect the fact that some\\nant species also can have more or less\\nefficient nervous systems). Due to the\\nuse of different attractants and odors,\\nants are able to engage in complex\\nsocial behavior and form huge states\\nwith millions of individuals. If you re-\\ngard such an ant state as an individ-\\nual, it has a cognitive capacity similar\\nto a chimpanzee or even a human.\\n\\nWith 105 neurons the nervous system of\\na fly can be constructed. A fly can\\nevade an object in real-time in three-\\ndimensional space, it can land upon\\nthe ceiling upside down, has a consid-\\nerable sensory system because of com-\\npound eyes, vibrissae, nerves at the\\nend of its legs and much more. Thus,\\na fly has considerable differential and\\nintegral calculus in high dimensions\\nimplemented \\"in hardware\\". We all\\nknow that a fly is not easy to catch.\\nOf course, the bodily functions are\\n\\n28 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.4 The amount of neurons in living organisms\\n\\nalso controlled by neurons, but these\\nshould be ignored here.\\n\\nWith 0.8 · 106 neurons we have enough\\ncerebral matter to create a honeybee.\\nHoneybees build colonies and have\\namazing capabilities in the field of\\naerial reconnaissance and navigation.\\n\\n4 · 106 neurons result in a mouse, and\\nhere the world of vertebrates already\\nbegins.\\n\\n1.5 · 107 neurons are sufficient for a rat,\\nan animal which is denounced as be-\\ning extremely intelligent and are of-\\nten used to participate in a variety\\nof intelligence tests representative for\\nthe animal world. Rats have an ex-\\ntraordinary sense of smell and orien-\\ntation, and they also show social be-\\nhavior. The brain of a frog can be\\npositioned within the same dimension.\\nThe frog has a complex build with\\nmany functions, it can swim and has\\nevolved complex behavior. A frog\\ncan continuously target the said fly\\nby means of his eyes while jumping\\nin three-dimensional space and and\\ncatch it with its tongue with consid-\\nerable probability.\\n\\n5 · 107 neurons make a bat. The bat can\\nnavigate in total darkness through a\\nroom, exact up to several centime-\\nters, by only using their sense of hear-\\ning. It uses acoustic signals to localize\\nself-camouflaging insects (e.g. some\\nmoths have a certain wing structure\\nthat reflects less sound waves and the\\necho will be small) and also eats its\\nprey while flying.\\n\\n1.6 · 108 neurons are required by the\\nbrain of a dog, companion of man for\\nages. Now take a look at another pop-\\nular companion of man:\\n\\n3 · 108 neurons can be found in a cat,\\nwhich is about twice as much as in\\na dog. We know that cats are very\\nelegant, patient carnivores that can\\nshow a variety of behaviors. By the\\nway, an octopus can be positioned\\nwithin the same magnitude. Only\\nvery few people know that, for exam-\\nple, in labyrinth orientation the octo-\\npus is vastly superior to the rat.\\n\\nFor 6 · 109 neurons you already get a\\nchimpanzee, one of the animals being\\nvery similar to the human.\\n\\n1011 neurons make a human. Usually,\\nthe human has considerable cognitive\\ncapabilities, is able to speak, to ab-\\nstract, to remember and to use tools\\nas well as the knowledge of other hu-\\nmans to develop advanced technolo-\\ngies and manifold social structures.\\n\\nWith 2 · 1011 neurons there are nervous\\nsystems having more neurons than\\nthe human nervous system. Here we\\nshould mention elephants and certain\\nwhale species.\\n\\nOur state-of-the-art computers are not\\nable to keep up with the aforementioned\\nprocessing power of a fly. Recent research\\nresults suggest that the processes in ner-\\nvous systems might be vastly more pow-\\nerful than people thought until not long\\nago: Michaeva et al. describe a separate,\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 29\\n\\n\\n\\nChapter 2 Biological neural networks dkriesel.com\\n\\nsynapse-integrated information way of in-\\nformation processing [MBW+10]. Poster-\\nity will show if they are right.\\n\\n2.5 Transition to technical\\nneurons: neural networks\\nare a caricature of biology\\n\\nHow do we change from biological neural\\nnetworks to the technical ones? Through\\nradical simplification. I want to briefly\\nsummarize the conclusions relevant for the\\ntechnical part:\\n\\nWe have learned that the biological neu-\\nrons are linked to each other in a weighted\\nway and when stimulated they electrically\\ntransmit their signal via the axon. From\\nthe axon they are not directly transferred\\nto the succeeding neurons, but they first\\nhave to cross the synaptic cleft where the\\nsignal is changed again by variable chem-\\nical processes. In the receiving neuron\\nthe various inputs that have been post-\\nprocessed in the synaptic cleft are summa-\\nrized or accumulated to one single pulse.\\nDepending on how the neuron is stimu-\\nlated by the cumulated input, the neuron\\nitself emits a pulse or not – thus, the out-\\nput is non-linear and not proportional to\\nthe cumulated input. Our brief summary\\ncorresponds exactly with the few elements\\nof biological neural networks we want to\\ntake over into the technical approxima-\\ntion:\\n\\nVectorial input: The input of technical\\nneurons consists of many components,\\n\\ntherefore it is a vector. In nature a\\nneuron receives pulses of 103 to 104\\n\\nother neurons on average.\\n\\nScalar output: The output of a neuron is\\na scalar, which means that the neu-\\nron only consists of one component.\\nSeveral scalar outputs in turn form\\nthe vectorial input of another neuron.\\nThis particularly means that some-\\nwhere in the neuron the various input\\ncomponents have to be summarized in\\nsuch a way that only one component\\nremains.\\n\\nSynapses change input: In technical neu-\\nral networks the inputs are prepro-\\ncessed, too. They are multiplied by\\na number (the weight) – they are\\nweighted. The set of such weights rep-\\nresents the information storage of a\\nneural network – in both biological\\noriginal and technical adaptation.\\n\\nAccumulating the inputs: In biology, the\\ninputs are summarized to a pulse ac-\\ncording to the chemical change, i.e.,\\nthey are accumulated – on the techni-\\ncal side this is often realized by the\\nweighted sum, which we will get to\\nknow later on. This means that after\\naccumulation we continue with only\\none value, a scalar, instead of a vec-\\ntor.\\n\\nNon-linear characteristic: The input of\\nour technical neurons is also not pro-\\nportional to the output.\\n\\nAdjustable weights: The weights weight-\\ning the inputs are variable, similar to\\n\\n30 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 2.5 Technical neurons as caricature of biology\\n\\nthe chemical processes at the synap-\\ntic cleft. This adds a great dynamic\\nto the network because a large part of\\nthe \\"knowledge\\" of a neural network is\\nsaved in the weights and in the form\\nand power of the chemical processes\\nin a synaptic cleft.\\n\\nSo our current, only casually formulated\\nand very simple neuron model receives a\\nvectorial input\\n\\n~x,\\n\\nwith components xi. These are multiplied\\nby the appropriate weights wi and accumu-\\nlated: ∑\\n\\ni\\n\\nwixi.\\n\\nThe aforementioned term is called\\nweighted sum. Then the nonlinear\\nmapping f defines the scalar output y:\\n\\ny = f\\n\\n(∑\\ni\\n\\nwixi\\n\\n)\\n.\\n\\nAfter this transition we now want to spec-\\nify more precisely our neuron model and\\nadd some odds and ends. Afterwards we\\nwill take a look at how the weights can be\\nadjusted.\\n\\nExercises\\n\\nExercise 4. It is estimated that a hu-\\nman brain consists of approx. 1011 nerve\\ncells, each of which has about 103 to 104\\n\\nsynapses. For this exercise we assume 103\\n\\nsynapses per neuron. Let us further as-\\nsume that a single synapse could save 4\\n\\nbits of information. Naïvely calculated:\\nHow much storage capacity does the brain\\nhave? Note: The information which neu-\\nron is connected to which other neuron is\\nalso important.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 31\\n\\n\\n\\n\\n\\nChapter 3\\n\\nComponents of artificial neural networks\\nFormal definitions and colloquial explanations of the components that realize\\nthe technical adaptations of biological neural networks. Initial descriptions of\\n\\nhow to combine these components into a neural network.\\n\\nThis chapter contains the formal defini-\\ntions for most of the neural network com-\\nponents used later in the text. After this\\nchapter you will be able to read the indi-\\nvidual chapters of this work without hav-\\ning to know the preceding ones (although\\nthis would be useful).\\n\\n3.1 The concept of time in\\nneural networks\\n\\nIn some definitions of this text we use the\\nterm time or the number of cycles of the\\nneural network, respectively. Time is di-\\nvided into discrete time steps:\\n\\ndiscrete\\ntime steps\\n\\nDefinition 3.1 (The concept of time).\\nThe current time (present time) is referred\\nto as (t), the next time step as (t + 1),\\n\\n(t)I the preceding one as (t − 1). All other\\ntime steps are referred to analogously. If in\\nthe following chapters several mathemati-\\ncal variables (e.g. netj or oi) refer to a\\n\\ncertain point in time, the notation will be,\\nfor example, netj(t− 1) or oi(t).\\n\\nFrom a biological point of view this is, of\\ncourse, not very plausible (in the human\\nbrain a neuron does not wait for another\\none), but it significantly simplifies the im-\\nplementation.\\n\\n3.2 Components of neural\\nnetworks\\n\\nA technical neural network consists of sim-\\nple processing units, the neurons, and\\ndirected, weighted connections between\\nthose neurons. Here, the strength of a\\nconnection (or the connecting weight) be-\\n\\n33\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\ntween two neurons i and j is referred to as\\nwi,j\\n\\n1.\\n\\nDefinition 3.2 (Neural network). A\\nneural network is a sorted triple\\n(N,V,w) with two sets N , V and a func-\\ntion w, where N is the set of neurons and\\nV a set {(i, j)|i, j ∈ N} whose elements are\\ncalled connections between neuron i and\\nneuron j. The function w : V → R defines\\n\\nn. network\\n= neurons\\n+ weighted\\nconnection\\n\\nthe weights, where w((i, j)), the weight of\\nthe connection between neuron i and neu-\\nron j, is shortened to wi,j . Depending on\\n\\nwi,jI the point of view it is either undefined or\\n0 for connections that do not exist in the\\nnetwork.\\n\\nSNIPE: In Snipe, an instance of the class\\nNeuralNetworkDescriptor is created in\\nthe first place. The descriptor object\\nroughly outlines a class of neural networks,\\ne.g. it defines the number of neuron lay-\\ners in a neural network. In a second step,\\nthe descriptor object is used to instantiate\\nan arbitrary number of NeuralNetwork ob-\\njects. To get started with Snipe program-\\nming, the documentations of exactly these\\ntwo classes are – in that order – the right\\nthing to read. The presented layout involv-\\ning descriptor and dependent neural net-\\nworks is very reasonable from the imple-\\nmentation point of view, because it is en-\\nables to create and maintain general param-\\neters of even very large sets of similar (but\\nnot neccessarily equal) networks.\\n\\nSo the weights can be implemented in a\\nsquare weight matrix W or, optionally,\\nin a weight vector W with the row num-\\n\\nWI\\n1 Note: In some of the cited literature i and j could\\nbe interchanged in wi,j . Here, a consistent stan-\\ndard does not exist. But in this text I try to use\\nthe notation I found more frequently and in the\\nmore significant citations.\\n\\nber of the matrix indicating where the con-\\nnection begins, and the column number of\\nthe matrix indicating, which neuron is the\\ntarget. Indeed, in this case the numeric\\n0 marks a non-existing connection. This\\nmatrix representation is also called Hin-\\nton diagram2.\\n\\nThe neurons and connections comprise the\\nfollowing components and variables (I’m\\nfollowing the path of the data within a\\nneuron, which is according to fig. 3.1 on\\nthe facing page in top-down direction):\\n\\n3.2.1 Connections carry information\\nthat is processed by neurons\\n\\nData are transferred between neurons via\\nconnections with the connecting weight be-\\ning either excitatory or inhibitory. The\\ndefinition of connections has already been\\nincluded in the definition of the neural net-\\nwork.\\n\\nSNIPE: Connection weights\\ncan be set using the method\\nNeuralNetwork.setSynapse.\\n\\n3.2.2 The propagation function\\nconverts vector inputs to\\nscalar network inputs\\n\\nLooking at a neuron j, we will usually find\\na lot of neurons with a connection to j, i.e.\\nwhich transfer their output to j.\\n\\n2 Note that, here again, in some of the cited liter-\\nature axes and rows could be interchanged. The\\npublished literature is not consistent here, as well.\\n\\n34 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.2 Components of neural networks\\n\\n \\n\\nPropagierungsfunktion \\n(oft gewichtete Summe, verarbeitet \\n\\nEingaben zur Netzeingabe) \\n\\nAusgabefunktion \\n(Erzeugt aus Aktivierung die Ausgabe, \\n\\nist oft Identität) \\n\\nAktivierungsfunktion \\n(Erzeugt aus Netzeingabe und alter \\n\\nAktivierung die neue Aktivierung)\\n\\nEingaben anderer \\nNeuronen Netzeingabe\\n\\nAktivierung Ausgabe zu \\nanderen Neuronen \\n\\nPropagation function \\n(often weighted sum, transforms \\n\\noutputs of other neurons to net input) \\n\\nOutput function \\n(often identity function, transforms \\n\\nactivation to output for other neurons) \\n\\nActivation function \\n(Transforms net input and sometimes \\n\\nold activation to new activation)\\n\\nData Input of \\nother Neurons \\n\\nNetwork Input\\n\\nActivation\\n\\nData Output to \\nother Neurons \\n\\nFigure 3.1: Data processing of a neuron. The\\nactivation function of a neuron implies the\\nthreshold value.\\n\\nFor a neuron j the propagation func-\\ntion receives the outputs oi1 , . . . , oin of\\nother neurons i1, i2, . . . , in (which are con-\\nnected to j), and transforms them in con- manages\\n\\ninputssideration of the connecting weights wi,j\\ninto the network input netj that can be fur-\\nther processed by the activation function.\\nThus, the network input is the result of\\nthe propagation function.\\n\\nDefinition 3.3 (Propagation func-\\ntion and network input). Let\\nI = {i1, i2, . . . , in} be the set of neurons,\\nsuch that ∀z ∈ {1, . . . , n} : ∃wiz ,j . Then\\nthe network input of j, called netj , is\\ncalculated by the propagation function\\nfprop as follows:\\n\\nnetj = fprop(oi1 , . . . , oin , wi1,j , . . . , win,j)\\n(3.1)\\n\\nHere the weighted sum is very popular:\\nThe multiplication of the output of each\\nneuron i by wi,j , and the summation of\\nthe results:\\n\\nnetj =\\n∑\\ni∈I\\n\\n(oi · wi,j) (3.2)\\n\\nSNIPE: The propagation function in\\nSnipe was implemented using the weighted\\nsum.\\n\\n3.2.3 The activation is the\\n\\"switching status\\" of a\\nneuron\\n\\nBased on the model of nature every neuron\\nis, to a certain extent, at all times active,\\nexcited or whatever you will call it. The\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 35\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\nreactions of the neurons to the input val-\\nues depend on this activation state. The\\n\\nHow active\\nis a\\n\\nneuron?\\nactivation state indicates the extent of a\\nneuron’s activation and is often shortly re-\\nferred to as activation. Its formal defini-\\ntion is included in the following definition\\nof the activation function. But generally,\\nit can be defined as follows:\\n\\nDefinition 3.4 (Activation state / activa-\\ntion in general). Let j be a neuron. The\\nactivation state aj , in short activation, is\\nexplicitly assigned to j, indicates the ex-\\ntent of the neuron’s activity and results\\nfrom the activation function.\\n\\nSNIPE: It is possible to get and set activa-\\ntion states of neurons by using the meth-\\nods getActivation or setActivation in\\nthe class NeuralNetwork.\\n\\n3.2.4 Neurons get activated if the\\nnetwork input exceeds their\\ntreshold value\\n\\nNear the threshold value, the activation\\nfunction of a neuron reacts particularly\\nsensitive. From the biological point of\\nview the threshold value represents the\\nthreshold at which a neuron starts fir-\\ning. The threshold value is also mostly\\n\\nhighest\\npoint of\\n\\nsensation\\nincluded in the definition of the activation\\nfunction, but generally the definition is the\\nfollowing:\\n\\nDefinition 3.5 (Threshold value in gen-\\neral). Let j be a neuron. The threshold\\nvalue Θj is uniquely assigned to j and\\n\\nΘI marks the position of the maximum gradi-\\nent value of the activation function.\\n\\n3.2.5 The activation function\\ndetermines the activation of a\\nneuron dependent on network\\ninput and treshold value\\n\\nAt a certain time – as we have already\\nlearned – the activation aj of a neuron j\\ndepends on the previous3 activation state\\nof the neuron and the external input.\\n\\nDefinition 3.6 (Activation function and\\nActivation). Let j be a neuron. The ac-\\n\\ncalculates\\nactivationtivation function is defined as\\n\\naj(t) = fact(netj(t), aj(t− 1),Θj). (3.3)\\n\\nIt transforms the network input netj , Jfactas well as the previous activation state\\naj(t− 1) into a new activation state aj(t),\\nwith the threshold value Θ playing an im-\\nportant role, as already mentioned.\\n\\nUnlike the other variables within the neu-\\nral network (particularly unlike the ones\\ndefined so far) the activation function is\\noften defined globally for all neurons or\\nat least for a set of neurons and only the\\nthreshold values are different for each neu-\\nron. We should also keep in mind that\\nthe threshold values can be changed, for\\nexample by a learning procedure. So it\\ncan in particular become necessary to re-\\nlate the threshold value to the time and to\\nwrite, for instance Θj as Θj(t) (but for rea-\\nsons of clarity, I omitted this here). The\\nactivation function is also called transfer\\nfunction.\\n\\n3 The previous activation is not always relevant for\\nthe current – we will see examples for both vari-\\nants.\\n\\n36 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.2 Components of neural networks\\n\\nSNIPE: In Snipe, activation functions are\\ngeneralized to neuron behaviors. Such\\nbehaviors can represent just normal acti-\\nvation functions, or even incorporate in-\\nternal states and dynamics. Correspond-\\ning parts of Snipe can be found in the\\npackage neuronbehavior, which also con-\\ntains some of the activation functions in-\\ntroduced in the next section. The inter-\\nface NeuronBehavior allows for implemen-\\ntation of custom behaviors. Objects that\\ninherit from this interface can be passed to\\na NeuralNetworkDescriptor instance. It\\nis possible to define individual behaviors\\nper neuron layer.\\n\\n3.2.6 Common activation functions\\n\\nThe simplest activation function is the bi-\\nnary threshold function (fig. 3.2 on the\\nnext page), which can only take on two val-\\nues (also referred to as Heaviside func-\\ntion). If the input is above a certain\\nthreshold, the function changes from one\\nvalue to another, but otherwise remains\\nconstant. This implies that the function\\nis not differentiable at the threshold and\\nfor the rest the derivative is 0. Due to\\nthis fact, backpropagation learning, for ex-\\nample, is impossible (as we will see later).\\nAlso very popular is the Fermi function\\nor logistic function (fig. 3.2)\\n\\n1\\n1 + e−x , (3.4)\\n\\nwhich maps to the range of values of (0, 1)\\nand the hyperbolic tangent (fig. 3.2)\\nwhich maps to (−1, 1). Both functions are\\ndifferentiable. The Fermi function can be\\nexpanded by a temperature parameter\\nT into the form\\n\\nTI\\n\\n1\\n1 + e−xT\\n\\n. (3.5)\\n\\nThe smaller this parameter, the more does\\nit compress the function on the x axis.\\nThus, one can arbitrarily approximate the\\nHeaviside function. Incidentally, there ex-\\nist activation functions which are not ex-\\nplicitly defined but depend on the input ac-\\ncording to a random distribution (stochas-\\ntic activation function).\\n\\nA alternative to the hypberbolic tangent\\nthat is really worth mentioning was sug-\\ngested by Anguita et al. [APZ93], who\\nhave been tired of the slowness of the work-\\nstations back in 1993. Thinking about\\nhow to make neural network propagations\\nfaster, they quickly identified the approx-\\nimation of the e-function used in the hy-\\nperbolic tangent as one of the causes of\\nslowness. Consequently, they \\"engineered\\"\\nan approximation to the hyperbolic tan-\\ngent, just using two parabola pieces and\\ntwo half-lines. At the price of delivering\\na slightly smaller range of values than the\\nhyperbolic tangent ([−0.96016; 0.96016] in-\\nstead of [−1; 1]), dependent on what CPU\\none uses, it can be calculated 200 times\\nfaster because it just needs two multipli-\\ncations and one addition. What’s more,\\nit has some other advantages that will be\\nmentioned later.\\n\\nSNIPE: The activation functions intro-\\nduced here are implemented within the\\nclasses Fermi and TangensHyperbolicus,\\nboth of which are located in the package\\nneuronbehavior. The fast hyperbolic tan-\\ngent approximation is located within the\\nclass TangensHyperbolicusAnguita.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 37\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\n−1\\n\\n−0.5\\n\\n 0\\n\\n 0.5\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nf(\\nx)\\n\\nx\\n\\nHeaviside Function\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nf(\\nx)\\n\\nx\\n\\nFermi Function with Temperature Parameter\\n\\n−1\\n\\n−0.8\\n\\n−0.6\\n\\n−0.4\\n\\n−0.2\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nta\\nnh\\n\\n(x\\n)\\n\\nx\\n\\nHyperbolic Tangent\\n\\nFigure 3.2: Various popular activation func-\\ntions, from top to bottom: Heaviside or binary\\nthreshold function, Fermi function, hyperbolic\\ntangent. The Fermi function was expanded by\\na temperature parameter. The original Fermi\\nfunction is represented by dark colors, the tem-\\nperature parameters of the modified Fermi func-\\ntions are, ordered ascending by steepness, 1\\n\\n2 ,\\n1\\n5 ,1\\n\\n10 und 1\\n25 .\\n\\n3.2.7 An output function may be\\nused to process the activation\\nonce again\\n\\nThe output function of a neuron j cal-\\nculates the values which are transferred to\\nthe other neurons connected to j. More\\nformally:\\n\\nDefinition 3.7 (Output function). Let j\\ninforms\\nother\\nneurons\\n\\nbe a neuron. The output function\\n\\nfout(aj) = oj (3.6)\\n\\ncalculates the output value oj of the neu- Jfoutron j from its activation state aj .\\n\\nGenerally, the output function is defined\\nglobally, too. Often this function is the\\nidentity, i.e. the activation aj is directly\\noutput4:\\n\\nfout(aj) = aj , so oj = aj (3.7)\\n\\nUnless explicitly specified differently, we\\nwill use the identity as output function\\nwithin this text.\\n\\n3.2.8 Learning strategies adjust a\\nnetwork to fit our needs\\n\\nSince we will address this subject later in\\ndetail and at first want to get to know the\\nprinciples of neural network structures, I\\nwill only provide a brief and general defi-\\nnition here:\\n4 Other definitions of output functions may be use-\\nful if the range of values of the activation function\\nis not sufficient.\\n\\n38 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.3 Network topologies\\n\\nDefinition 3.8 (General learning rule).\\nThe learning strategy is an algorithm\\nthat can be used to change and thereby\\ntrain the neural network, so that the net-\\nwork produces a desired output for a given\\ninput.\\n\\n3.3 Network topologies\\n\\nAfter we have become acquainted with the\\ncomposition of the elements of a neural\\nnetwork, I want to give an overview of\\nthe usual topologies (= designs) of neural\\nnetworks, i.e. to construct networks con-\\nsisting of these elements. Every topology\\ndescribed in this text is illustrated by a\\nmap and its Hinton diagram so that the\\nreader can immediately see the character-\\nistics and apply them to other networks.\\n\\nIn the Hinton diagram the dotted weights\\nare represented by light grey fields, the\\nsolid ones by dark grey fields. The input\\nand output arrows, which were added for\\nreasons of clarity, cannot be found in the\\nHinton diagram. In order to clarify that\\nthe connections are between the line neu-\\nrons and the column neurons, I have in-\\nserted the small arrow � in the upper-left\\ncell.\\n\\nSNIPE: Snipe is designed for realization\\nof arbitrary network topologies. In this\\nrespect, Snipe defines different kinds of\\nsynapses depending on their source and\\ntheir target. Any kind of synapse can sep-\\narately be allowed or forbidden for a set of\\nnetworks using the setAllowed methods in\\na NeuralNetworkDescriptor instance.\\n\\n3.3.1 Feedforward networks consist\\nof layers and connections\\ntowards each following layer\\n\\nFeedforward In this text feedforward net-\\nworks (fig. 3.3 on the following page) are\\nthe networks we will first explore (even if\\nwe will use different topologies later). The\\nneurons are grouped in the following lay-\\ners: One input layer, n hidden pro-\\n\\nnetwork of\\nlayerscessing layers (invisible from the out-\\n\\nside, that’s why the neurons are also re-\\nferred to as hidden neurons) and one out-\\nput layer. In a feedforward network each\\nneuron in one layer has only directed con-\\nnections to the neurons of the next layer\\n(towards the output layer). In fig. 3.3 on\\nthe next page the connections permitted\\nfor a feedforward network are represented\\nby solid lines. We will often be confronted\\nwith feedforward networks in which every\\nneuron i is connected to all neurons of the\\nnext layer (these layers are called com-\\npletely linked). To prevent naming con-\\nflicts the output neurons are often referred\\nto as Ω.\\n\\nDefinition 3.9 (Feedforward network).\\nThe neuron layers of a feedforward net-\\nwork (fig. 3.3 on the following page) are\\nclearly separated: One input layer, one\\noutput layer and one or more processing\\nlayers which are invisible from the outside\\n(also called hidden layers). Connections\\nare only permitted to neurons of the fol-\\nlowing layer.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 39\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\n�� ��GFED@ABCi1\\n\\n~~}}}}}}}}}\\n\\n  AAAAAAAAA\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\n~~}}}}}}}}}\\n\\n  AAAAAAAAA\\n\\nGFED@ABCh1\\n\\n  AAAAAAAAA\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCh2\\n\\n~~}}}}}}}}}\\n\\n  AAAAAAAAA\\nGFED@ABCh3\\n\\n~~}}}}}}}}}\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\nGFED@ABCΩ1\\n\\n��\\n\\nGFED@ABCΩ2\\n\\n��\\n\\n� i1 i2 h1 h2 h3 Ω1 Ω2\\ni1\\ni2\\nh1\\nh2\\nh3\\nΩ1\\nΩ2\\n\\nFigure 3.3: A feedforward network with three\\nlayers: two input neurons, three hidden neurons\\nand two output neurons. Characteristic for the\\nHinton diagram of completely linked feedforward\\nnetworks is the formation of blocks above the\\ndiagonal.\\n\\n3.3.1.1 Shortcut connections skip layers\\nShortcuts\\nskip\\nlayersSome feedforward networks permit the so-\\n\\ncalled shortcut connections (fig. 3.4 on\\nthe next page): connections that skip one\\nor more levels. These connections may\\nonly be directed towards the output layer,\\ntoo.\\n\\nDefinition 3.10 (Feedforward network\\nwith shortcut connections). Similar to the\\nfeedforward network, but the connections\\nmay not only be directed towards the next\\nlayer but also towards any other subse-\\nquent layer.\\n\\n3.3.2 Recurrent networks have\\ninfluence on themselves\\n\\nRecurrence is defined as the process of a\\nneuron influencing itself by any means or\\nby any connection. Recurrent networks do\\nnot always have explicitly defined input or\\noutput neurons. Therefore in the figures\\nI omitted all markings that concern this\\nmatter and only numbered the neurons.\\n\\n3.3.2.1 Direct recurrences start and\\nend at the same neuron\\n\\nSome networks allow for neurons to be\\nconnected to themselves, which is called\\ndirect recurrence (or sometimes self-\\nrecurrence (fig. 3.5 on the facing page).\\nAs a result, neurons inhibit and therefore\\nstrengthen themselves in order to reach\\ntheir activation limits.\\n\\n40 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.3 Network topologies\\n\\n�� ��GFED@ABCi1\\n\\n��\\n++\\n\\n~~   **\\n\\nGFED@ABCi2\\n\\nss\\n��\\n\\ntt ~~   GFED@ABCh1\\n\\n  **\\n\\nGFED@ABCh2\\n\\n~~   \\n\\nGFED@ABCh3\\n\\n~~ttGFED@ABCΩ1\\n\\n��\\n\\nGFED@ABCΩ2\\n\\n��\\n\\n� i1 i2 h1 h2 h3 Ω1 Ω2\\ni1\\ni2\\nh1\\nh2\\nh3\\nΩ1\\nΩ2\\n\\nFigure 3.4: A feedforward network with short-\\ncut connections, which are represented by solid\\nlines. On the right side of the feedforward blocks\\nnew connections have been added to the Hinton\\ndiagram.\\n\\n?>=<89:;1\\nvv\\n\\n�� �� ))\\n\\n?>=<89:;2\\nvv\\n\\nuu �� ��?>=<89:;3\\nvv\\n\\n�� ))\\n\\n?>=<89:;4\\nvv\\n\\n�� ��\\n\\n?>=<89:;5\\nvv\\n\\n��uu?>=<89:;6\\nvv ?>=<89:;7\\n\\nvv\\n\\n� 1 2 3 4 5 6 7\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\nFigure 3.5: A network similar to a feedforward\\nnetwork with directly recurrent neurons. The di-\\nrect recurrences are represented by solid lines and\\nexactly correspond to the diagonal in the Hinton\\ndiagram matrix.\\n\\nDefinition 3.11 (Direct recurrence).\\nNow we expand the feedforward network neurons\\n\\ninfluence\\nthemselves\\n\\nby connecting a neuron j to itself, with the\\nweights of these connections being referred\\nto as wj,j . In other words: the diagonal\\nof the weight matrix W may be different\\nfrom 0.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 41\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\n3.3.2.2 Indirect recurrences can\\ninfluence their starting neuron\\nonly by making detours\\n\\nIf connections are allowed towards the in-\\nput layer, they will be called indirect re-\\ncurrences. Then a neuron j can use in-\\ndirect forwards connections to influence it-\\nself, for example, by influencing the neu-\\nrons of the next layer and the neurons of\\nthis next layer influencing j (fig. 3.6).\\n\\nDefinition 3.12 (Indirect recurrence).\\nAgain our network is based on a feedfor-\\nward network, now with additional connec-\\ntions between neurons and their preceding\\nlayer being allowed. Therefore, below the\\ndiagonal of W is different from 0.\\n\\n3.3.2.3 Lateral recurrences connect\\nneurons within one layer\\n\\nConnections between neurons within one\\nlayer are called lateral recurrences\\n(fig. 3.7 on the facing page). Here, each\\nneuron often inhibits the other neurons of\\nthe layer and strengthens itself. As a re-\\nsult only the strongest neuron becomes ac-\\ntive (winner-takes-all scheme).\\nDefinition 3.13 (Lateral recurrence). A\\nlaterally recurrent network permits con-\\nnections within one layer.\\n\\n3.3.3 Completely linked networks\\nallow any possible connection\\n\\nCompletely linked networks permit connec-\\ntions between all neurons, except for direct\\n\\n?>=<89:;1\\n\\n�� �� ))\\n\\n?>=<89:;2\\n\\nuu �� ��?>=<89:;3\\n\\n88 22\\n\\n�� ))\\n\\n?>=<89:;4\\n\\nXX 88\\n\\n�� ��\\n\\n?>=<89:;5\\n\\nXXgg\\n\\n��uu?>=<89:;6\\n\\nXX 88 22\\n\\n?>=<89:;7\\n\\ngg XX 88\\n\\n� 1 2 3 4 5 6 7\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\nFigure 3.6: A network similar to a feedforward\\nnetwork with indirectly recurrent neurons. The\\nindirect recurrences are represented by solid lines.\\nAs we can see, connections to the preceding lay-\\ners can exist here, too. The fields that are sym-\\nmetric to the feedforward blocks in the Hinton\\ndiagram are now occupied.\\n\\n42 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.4 The bias neuron\\n\\n?>=<89:;1 ++kk\\n\\n�� �� ))\\n\\n?>=<89:;2\\n\\nuu �� ��?>=<89:;3 ++kk **\\njj\\n\\n�� ))\\n\\n?>=<89:;4 ++kk\\n\\n�� ��\\n\\n?>=<89:;5\\n\\n��uu?>=<89:;6 ++kk ?>=<89:;7\\n\\n� 1 2 3 4 5 6 7\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\nFigure 3.7: A network similar to a feedforward\\nnetwork with laterally recurrent neurons. The\\ndirect recurrences are represented by solid lines.\\nHere, recurrences only exist within the layer.\\nIn the Hinton diagram, filled squares are con-\\ncentrated around the diagonal in the height of\\nthe feedforward blocks, but the diagonal is left\\nuncovered.\\n\\nrecurrences. Furthermore, the connections\\nmust be symmetric (fig. 3.8 on the next\\npage). A popular example are the self-\\norganizing maps, which will be introduced\\nin chapter 10.\\n\\nDefinition 3.14 (Complete interconnec-\\ntion). In this case, every neuron is always\\nallowed to be connected to every other neu-\\nron – but as a result every neuron can\\nbecome an input neuron. Therefore, di-\\nrect recurrences normally cannot be ap-\\nplied here and clearly defined layers do not\\nlonger exist. Thus, the matrix W may be\\nunequal to 0 everywhere, except along its\\ndiagonal.\\n\\n3.4 The bias neuron is a\\ntechnical trick to consider\\nthreshold values as\\nconnection weights\\n\\nBy now we know that in many network\\nparadigms neurons have a threshold value\\nthat indicates when a neuron becomes ac-\\ntive. Thus, the threshold value is an\\nactivation function parameter of a neu-\\nron. From the biological point of view\\nthis sounds most plausible, but it is com-\\nplicated to access the activation function\\nat runtime in order to train the threshold\\nvalue.\\n\\nBut threshold values Θj1 , . . . ,Θjn for neu-\\nrons j1, j2, . . . , jn can also be realized as\\nconnecting weight of a continuously fir-\\ning neuron: For this purpose an addi-\\ntional bias neuron whose output value\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 43\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\n?>=<89:;1 ii\\n\\n��\\n\\nii\\n\\n))TTTTTTTTTTTTTTTTTTTTTTT\\nOO\\n\\n��\\n\\noo //\\n^^\\n\\n��>>>>>>>>>\\n?>=<89:;255\\n\\nuujjjjjjjjjjjjjjjjjjjjjjj OO\\n\\n��\\n\\n@@\\n\\n����������� ^^\\n\\n��>>>>>>>>>\\n\\n?>=<89:;3 ii\\n\\n))TTTTTTTTTTTTTTTTTTTTTTToo //\\n��\\n\\n@@��������� ?>=<89:;4 ?>=<89:;544jj 55\\n\\nuujjjjjjjjjjjjjjjjjjjjjjj//oo\\n@@\\n\\n�����������\\n\\n?>=<89:;6\\n\\n\\n\\n\\n55\\n\\n��\\n\\n@@�����������\\n\\n^>̂>>>>>>>> ?>=<89:;7//oo\\n��\\n\\n^>̂>>>>>>>>\\n\\n� 1 2 3 4 5 6 7\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\nFigure 3.8: A completely linked network with\\nsymmetric connections and without direct recur-\\nrences. In the Hinton diagram only the diagonal\\nis left blank.\\n\\nis always 1 is integrated in the network\\nand connected to the neurons j1, j2, . . . , jn.\\nThese new connections get the weights\\n−Θj1 , . . . ,−Θjn , i.e. they get the negative\\nthreshold values.\\n\\nDefinition 3.15. A bias neuron is a\\nneuron whose output value is always 1 and\\nwhich is represented by\\n\\nGFED@ABCBIAS .\\n\\nIt is used to represent neuron biases as con-\\nnection weights, which enables any weight-\\ntraining algorithm to train the biases at\\nthe same time.\\n\\nThen the threshold value of the neurons\\nj1, j2, . . . , jn is set to 0. Now the thresh-\\nold values are implemented as connection\\nweights (fig. 3.9 on page 46) and can di-\\nrectly be trained together with the con-\\nnection weights, which considerably facil-\\nitates the learning process.\\n\\nIn other words: Instead of including the\\nthreshold value in the activation function,\\nit is now included in the propagation func-\\ntion. Or even shorter: The threshold value\\nis subtracted from the network input, i.e.\\nit is part of the network input. More for-\\nmally:\\n\\nbias neuron\\nreplaces\\nthresh. value\\nwith weights\\n\\nLet j1, j2, . . . , jn be neurons with thresh-\\nold values Θj1 , . . . ,Θjn . By inserting a\\nbias neuron whose output value is always\\n1, generating connections between the said\\nbias neuron and the neurons j1, j2, . . . , jn\\nand weighting these connections\\nwBIAS,j1 , . . . , wBIAS,jnwith −Θj1 , . . . ,−Θjn ,\\nwe can set Θj1 = . . . = Θjn = 0 and\\n\\n44 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.6 Orders of activation\\n\\nreceive an equivalent neural network\\nwhose threshold values are realized by\\nconnection weights.\\n\\nUndoubtedly, the advantage of the bias\\nneuron is the fact that it is much easier\\nto implement it in the network. One dis-\\nadvantage is that the representation of the\\nnetwork already becomes quite ugly with\\nonly a few neurons, let alone with a great\\nnumber of them. By the way, a bias neu-\\nron is often referred to as on neuron.\\n\\nFrom now on, the bias neuron is omit-\\nted for clarity in the following illustrations,\\nbut we know that it exists and that the\\nthreshold values can simply be treated as\\nweights because of it.\\n\\nSNIPE: In Snipe, a bias neuron was imple-\\nmented instead of neuron-individual biases.\\nThe neuron index of the bias neuron is 0.\\n\\n3.5 Representing neurons\\n\\nWe have already seen that we can either\\nwrite its name or its threshold value into\\na neuron. Another useful representation,\\nwhich we will use several times in the\\nfollowing, is to illustrate neurons accord-\\ning to their type of data processing. See\\nfig. 3.10 for some examples without fur-\\nther explanation – the different types of\\nneurons are explained as soon as we need\\nthem.\\n\\nWVUTPQRS||c,x||\\nGauß\\n\\nGFED@ABC� ONMLHIJKΣ\\n�\\n\\nWVUTPQRSΣ\\nL|H\\n\\nWVUTPQRSΣ\\nTanh\\n\\nWVUTPQRSΣ\\nFermi\\n\\nONMLHIJKΣ\\nfact\\n\\nGFED@ABCBIAS\\n\\nFigure 3.10: Different types of neurons that will\\nappear in the following text.\\n\\n3.6 Take care of the order in\\nwhich neuron activations\\nare calculated\\n\\nFor a neural network it is very important\\nin which order the individual neurons re-\\nceive and process the input and output the\\nresults. Here, we distinguish two model\\nclasses:\\n\\n3.6.1 Synchronous activation\\n\\nAll neurons change their values syn-\\nchronously, i.e. they simultaneously cal-\\nculate network inputs, activation and out-\\nput, and pass them on. Synchronous ac-\\ntivation corresponds closest to its biolog-\\nical counterpart, but it is – if to be im-\\nplemented in hardware – only useful on\\ncertain parallel computers and especially\\nnot for feedforward networks. This order\\nof activation is the most generic and can\\nbe used with networks of arbitrary topol-\\nogy.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 45\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\n��GFED@ABCΘ1\\n\\n  BBBBBBBBB\\n\\n~~|||||||||\\n\\nGFED@ABCΘ2\\n\\n��\\n\\nGFED@ABCΘ3\\n\\n��\\n\\n��GFED@ABCBIAS −Θ1 //\\n\\n−Θ2\\nAAAA\\n\\n  AAAA −Θ3\\nTTTTTTTTTT\\n\\n**TTTTTTTTTT\\n\\n?>=<89:;0\\n\\n����?>=<89:;0\\n\\n��\\n\\n?>=<89:;0\\n\\n��\\n\\nFigure 3.9: Two equivalent neural networks, one without bias neuron on the left, one with bias\\nneuron on the right. The neuron threshold values can be found in the neurons, the connecting\\nweights at the connections. Furthermore, I omitted the weights of the already existing connections\\n(represented by dotted lines on the right side).\\n\\nDefinition 3.16 (Synchronous activa-\\ntion). All neurons of a network calculate\\n\\nbiologically\\nplausible network inputs at the same time by means\\n\\nof the propagation function, activation by\\nmeans of the activation function and out-\\nput by means of the output function. Af-\\nter that the activation cycle is complete.\\n\\nSNIPE: When implementing in software,\\none could model this very general activa-\\ntion order by every time step calculating\\nand caching every single network input,\\nand after that calculating all activations.\\nThis is exactly how it is done in Snipe, be-\\ncause Snipe has to be able to realize arbi-\\ntrary network topologies.\\n\\n3.6.2 Asynchronous activation\\n\\nHere, the neurons do not change their val-\\nues simultaneously but at different points\\n\\nof time. For this, there exist different or-\\nders, some of which I want to introduce in\\nthe following: easier to\\n\\nimplement\\n\\n3.6.2.1 Random order\\n\\nDefinition 3.17 (Random order of acti-\\nvation). With random order of acti-\\nvation a neuron i is randomly chosen and\\nits neti, ai and oi are updated. For n neu-\\nrons a cycle is the n-fold execution of this\\nstep. Obviously, some neurons are repeat-\\nedly updated during one cycle, and others,\\nhowever, not at all.\\n\\nApparently, this order of activation is not\\nalways useful.\\n\\n46 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.6 Orders of activation\\n\\n3.6.2.2 Random permutation\\n\\nWith random permutation each neuron\\nis chosen exactly once, but in random or-\\nder, during one cycle.\\n\\nDefinition 3.18 (Random permutation).\\nInitially, a permutation of the neurons is\\ncalculated randomly and therefore defines\\nthe order of activation. Then the neurons\\nare successively processed in this order.\\n\\nThis order of activation is as well used\\nrarely because firstly, the order is gener-\\nally useless and, secondly, it is very time-\\nconsuming to compute a new permutation\\nfor every cycle. A Hopfield network (chap-\\nter 8) is a topology nominally having a\\nrandom or a randomly permuted order of\\nactivation. But note that in practice, for\\nthe previously mentioned reasons, a fixed\\norder of activation is preferred.\\n\\nFor all orders either the previous neuron\\nactivations at time t or, if already existing,\\nthe neuron activations at time t + 1, for\\nwhich we are calculating the activations,\\ncan be taken as a starting point.\\n\\n3.6.2.3 Topological order\\n\\nDefinition 3.19 (Topological activation).\\nWith topological order of activation\\n\\noften very\\nuseful the neurons are updated during one cycle\\n\\nand according to a fixed order. The order\\nis defined by the network topology.\\n\\nThis procedure can only be considered for\\nnon-cyclic, i.e. non-recurrent, networks,\\n\\nsince otherwise there is no order of activa-\\ntion. Thus, in feedforward networks (for\\nwhich the procedure is very reasonable)\\nthe input neurons would be updated first,\\nthen the inner neurons and finally the out-\\nput neurons. This may save us a lot of\\ntime: Given a synchronous activation or-\\nder, a feedforward network with n layers\\nof neurons would need n full propagation\\ncycles in order to enable input data to\\nhave influence on the output of the net-\\nwork. Given the topological activation or-\\nder, we just need one single propagation.\\nHowever, not every network topology al-\\nlows for finding a special activation order\\nthat enables saving time.\\n\\nSNIPE: Those who want to use Snipe\\nfor implementing feedforward networks\\nmay save some calculation time by us-\\ning the feature fastprop (mentioned\\nwithin the documentation of the class\\nNeuralNetworkDescriptor. Once fastprop\\nis enabled, it will cause the data propaga-\\ntion to be carried out in a slightly different\\nway. In the standard mode, all net inputs\\nare calculated first, followed by all activa-\\ntions. In the fastprop mode, for every neu-\\nron, the activation is calculated right after\\nthe net input. The neuron values are calcu-\\nlated in ascending neuron index order. The\\nneuron numbers are ascending from input\\nto output layer, which provides us with the\\nperfect topological activation order for feed-\\nforward networks.\\n\\n3.6.2.4 Fixed orders of activation\\nduring implementation\\n\\nObviously, fixed orders of activation\\ncan be defined as well. Therefore, when\\nimplementing, for instance, feedforward\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 47\\n\\n\\n\\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\\n\\nnetworks it is very popular to determine\\nthe order of activation once according to\\nthe topology and to use this order without\\nfurther verification at runtime. But this is\\nnot necessarily useful for networks that are\\ncapable to change their topology.\\n\\n3.7 Communication with the\\noutside world: input and\\noutput of data in and\\nfrom neural networks\\n\\nFinally, let us take a look at the fact that,\\nof course, many types of neural networks\\npermit the input of data. Then these data\\nare processed and can produce output.\\nLet us, for example, regard the feedfor-\\nward network shown in fig. 3.3 on page 40:\\nIt has two input neurons and two output\\nneurons, which means that it also has two\\nnumerical inputs x1, x2 and outputs y1, y2.\\nAs a simplification we summarize the in-\\nput and output components for n input\\nor output neurons within the vectors x =\\n(x1, x2, . . . , xn) and y = (y1, y2, . . . , yn).\\n\\nDefinition 3.20 (Input vector). A net-\\nxI work with n input neurons needs n inputs\\n\\nx1, x2, . . . , xn. They are considered as in-\\nput vector x = (x1, x2, . . . , xn). As a\\nconsequence, the input dimension is re-\\nferred to as n. Data is put into a neural\\n\\nnI network by using the components of the in-\\nput vector as network inputs of the input\\nneurons.\\n\\nDefinition 3.21 (Output vector). A net-\\nyI work with m output neurons provides m\\n\\noutputs y1, y2, . . . , ym. They are regarded\\nas output vector y = (y1, y2, . . . , ym).\\nThus, the output dimension is referred\\nto as m. Data is output by a neural net- Jmwork by the output neurons adopting the\\ncomponents of the output vector in their\\noutput values.\\n\\nSNIPE: In order to propagate data through\\na NeuralNetwork-instance, the propagate\\nmethod is used. It receives the input vector\\nas array of doubles, and returns the output\\nvector in the same way.\\n\\nNow we have defined and closely examined\\nthe basic components of neural networks –\\nwithout having seen a network in action.\\nBut first we will continue with theoretical\\nexplanations and generally describe how a\\nneural network could learn.\\n\\nExercises\\n\\nExercise 5. Would it be useful (from\\nyour point of view) to insert one bias neu-\\nron in each layer of a layer-based network,\\nsuch as a feedforward network? Discuss\\nthis in relation to the representation and\\nimplementation of the network. Will the\\nresult of the network change?\\n\\nExercise 6. Show for the Fermi function\\nf(x) as well as for the hyperbolic tangent\\ntanh(x), that their derivatives can be ex-\\npressed by the respective functions them-\\nselves so that the two statements\\n\\n1. f ′(x) = f(x) · (1− f(x)) and\\n\\n2. tanh′(x) = 1− tanh2(x)\\n\\n48 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 3.7 Input and output of data\\n\\nare true.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 49\\n\\n\\n\\n\\n\\nChapter 4\\n\\nFundamentals on learning and training\\nsamples\\n\\nApproaches and thoughts of how to teach machines. Should neural networks\\nbe corrected? Should they only be encouraged? Or should they even learn\\n\\nwithout any help? Thoughts about what we want to change during the\\nlearning procedure and how we will change it, about the measurement of\\n\\nerrors and when we have learned enough.\\n\\nAs written above, the most interesting\\ncharacteristic of neural networks is their\\ncapability to familiarize with problems\\nby means of training and, after sufficient\\ntraining, to be able to solve unknown prob-\\nlems of the same class. This approach is re-\\nferred to as generalization. Before intro-\\nducing specific learning procedures, I want\\nto propose some basic principles about the\\nlearning procedure in this chapter.\\n\\n4.1 There are different\\nparadigms of learning\\n\\nLearning is a comprehensive term. A\\nlearning system changes itself in order to\\nadapt to e.g. environmental changes. A\\nneural network could learn from many\\nthings but, of course, there will always be\\n\\nFrom what\\ndo we learn?\\n\\nthe question of how to implement it. In\\nprinciple, a neural network changes when\\nits components are changing, as we have\\nlearned above. Theoretically, a neural net-\\nwork could learn by\\n\\n1. developing new connections,\\n\\n2. deleting existing connections,\\n\\n3. changing connecting weights,\\n\\n4. changing the threshold values of neu-\\nrons,\\n\\n5. varying one or more of the three neu-\\nron functions (remember: activation\\nfunction, propagation function and\\noutput function),\\n\\n6. developing new neurons, or\\n\\n7. deleting existing neurons (and so, of\\ncourse, existing connections).\\n\\n51\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\nAs mentioned above, we assume the\\nchange in weight to be the most common\\nprocedure. Furthermore, deletion of con-\\nnections can be realized by additionally\\ntaking care that a connection is no longer\\ntrained when it is set to 0. Moreover, we\\ncan develop further connections by setting\\na non-existing connection (with the value\\n0 in the connection matrix) to a value dif-\\nferent from 0. As for the modification of\\nthreshold values I refer to the possibility\\nof implementing them as weights (section\\n3.4). Thus, we perform any of the first four\\nof the learning paradigms by just training\\nsynaptic weights.\\n\\nThe change of neuron functions is difficult\\nto implement, not very intuitive and not\\nexactly biologically motivated. Therefore\\nit is not very popular and I will omit this\\ntopic here. The possibilities to develop or\\ndelete neurons do not only provide well\\nadjusted weights during the training of a\\nneural network, but also optimize the net-\\nwork topology. Thus, they attract a grow-\\ning interest and are often realized by using\\nevolutionary procedures. But, since we ac-\\ncept that a large part of learning possibil-\\nities can already be covered by changes in\\nweight, they are also not the subject mat-\\nter of this text (however, it is planned to\\nextend the text towards those aspects of\\ntraining).\\n\\nSNIPE: Methods of the class\\nNeuralNetwork allow for changes in\\nconnection weights, and addition and\\nremoval of both connections and neurons.\\nMethods in NeuralNetworkDescriptor\\nenable the change of neuron behaviors,\\nrespectively activation functions per\\nlayer.\\n\\nThus, we let our neural network learn by\\nmodifying the connecting weights accord-\\ning to rules that can be formulated as al-\\n\\nLearning\\nby changes\\nin weight\\n\\ngorithms. Therefore a learning procedure\\nis always an algorithm that can easily be\\nimplemented by means of a programming\\nlanguage. Later in the text I will assume\\nthat the definition of the term desired out-\\nput which is worth learning is known (and\\nI will define formally what a training pat-\\ntern is) and that we have a training set\\nof learning samples. Let a training set be\\ndefined as follows:\\n\\nDefinition 4.1 (Training set). A train- JPing set (named P ) is a set of training\\npatterns, which we use to train our neu-\\nral net.\\n\\nI will now introduce the three essential\\nparadigms of learning by presenting the\\ndifferences between their regarding train-\\ning sets.\\n\\n4.1.1 Unsupervised learning\\nprovides input patterns to the\\nnetwork, but no learning aides\\n\\nUnsupervised learning is the biologi-\\ncally most plausible method, but is not\\nsuitable for all problems. Only the in-\\nput patterns are given; the network tries\\nto identify similar patterns and to classify\\nthem into similar categories.\\n\\nDefinition 4.2 (Unsupervised learning).\\nThe training set only consists of input\\npatterns, the network tries by itself to de-\\ntect similarities and to generate pattern\\nclasses.\\n\\n52 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.1 Paradigms of learning\\n\\nHere I want to refer again to the popu-\\nlar example of Kohonen’s self-organising\\nmaps (chapter 10).\\n\\n4.1.2 Reinforcement learning\\nmethods provide feedback to\\nthe network, whether it\\nbehaves well or bad\\n\\nIn reinforcement learning the network\\nreceives a logical or a real value after\\n\\nnetwork\\nreceives\\n\\nreward or\\npunishment\\n\\ncompletion of a sequence, which defines\\nwhether the result is right or wrong. Intu-\\nitively it is clear that this procedure should\\nbe more effective than unsupervised learn-\\ning since the network receives specific crit-\\nera for problem-solving.\\n\\nDefinition 4.3 (Reinforcement learning).\\nThe training set consists of input patterns,\\nafter completion of a sequence a value is re-\\nturned to the network indicating whether\\nthe result was right or wrong and, possibly,\\nhow right or wrong it was.\\n\\n4.1.3 Supervised learning methods\\nprovide training patterns\\ntogether with appropriate\\ndesired outputs\\n\\nIn supervised learning the training set\\nconsists of input patterns as well as their\\ncorrect results in the form of the precise ac-\\ntivation of all output neurons. Thus, for\\neach training set that is fed into the net-\\nwork the output, for instance, can directly\\n\\nnetwork\\nreceives\\ncorrect\\n\\nresults for\\nsamples\\n\\nbe compared with the correct solution and\\nand the network weights can be changed\\n\\naccording to their difference. The objec-\\ntive is to change the weights to the effect\\nthat the network cannot only associate in-\\nput and output patterns independently af-\\nter the training, but can provide plausible\\nresults to unknown, similar input patterns,\\ni.e. it generalises.\\n\\nDefinition 4.4 (Supervised learning).\\nThe training set consists of input patterns\\nwith correct results so that the network can\\nreceive a precise error vector1 can be re-\\nturned.\\n\\nThis learning procedure is not always bio-\\nlogically plausible, but it is extremely ef-\\nfective and therefore very practicable.\\n\\nAt first we want to look at the the su-\\npervised learning procedures in general,\\nwhich - in this text - are corresponding\\nto the following steps:\\n\\nEntering the input pattern (activation of\\ninput neurons),\\n\\nForward propagation of the input by the\\nnetwork, generation of the output,\\n\\nlearning\\nscheme\\n\\nComparing the output with the desired\\noutput (teaching input), provides er-\\nror vector (difference vector),\\n\\nCorrections of the network are\\ncalculated based on the error vector,\\n\\nCorrections are applied.\\n\\n1 The term error vector will be defined in section\\n4.2, where mathematical formalisation of learning\\nis discussed.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 53\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\n4.1.4 Offline or online learning?\\n\\nIt must be noted that learning can be\\noffline (a set of training samples is pre-\\nsented, then the weights are changed, the\\ntotal error is calculated by means of a error\\nfunction operation or simply accumulated -\\nsee also section 4.4) or online (after every\\nsample presented the weights are changed).\\nBoth procedures have advantages and dis-\\nadvantages, which will be discussed in the\\nlearning procedures section if necessary.\\nOffline training procedures are also called\\nbatch training procedures since a batch\\nof results is corrected all at once. Such a\\ntraining section of a whole batch of train-\\ning samples including the related change\\nin weight values is called epoch.\\n\\nDefinition 4.5 (Offline learning). Sev-\\neral training patterns are entered into the\\nnetwork at once, the errors are accumu-\\nlated and it learns for all patterns at the\\nsame time.\\n\\nDefinition 4.6 (Online learning). The\\nnetwork learns directly from the errors of\\neach training sample.\\n\\n4.1.5 Questions you should answer\\nbefore learning\\n\\nThe application of such schemes certainly\\nrequires preliminary thoughts about some\\nquestions, which I want to introduce now\\nas a check list and, if possible, answer\\nthem in the course of this text:\\n\\n. Where does the learning input come\\nfrom and in what form?\\n\\n. How must the weights be modified to\\nallow fast and reliable learning?\\n\\n. How can the success of a learning pro-\\ncess be measured in an objective way?\\n\\n. Is it possible to determine the \\"best\\"\\nlearning procedure?\\n\\n. Is it possible to predict if a learning\\nprocedure terminates, i.e. whether it\\nwill reach an optimal state after a fi-\\nnite time or if it, for example, will os-\\ncillate between different states?\\n\\n. How can the learned patterns be\\nstored in the network?\\n\\n. Is it possible to avoid that newly\\nlearned patterns destroy previously\\nlearned associations (the so-called sta-\\nbility/plasticity dilemma)?\\n\\nWe will see that all these questions cannot\\nbe generally answered but that they have JJJ\\n\\nno easy\\nanswers!\\n\\nto be discussed for each learning procedure\\nand each network topology individually.\\n\\n4.2 Training patterns and\\nteaching input\\n\\nBefore we get to know our first learning\\nrule, we need to introduce the teaching\\ninput. In (this) case of supervised learn-\\ning we assume a training set consisting\\nof training patterns and the correspond-\\ning correct output values we want to see\\n\\ndesired\\noutputat the output neurons after the training.\\n\\nWhile the network has not finished train-\\ning, i.e. as long as it is generating wrong\\noutputs, these output values are referred\\n\\n54 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.2 Training patterns and teaching input\\n\\nto as teaching input, and that for each neu-\\nron individually. Thus, for a neuron j with\\nthe incorrect output oj , tj is the teaching\\ninput, which means it is the correct or de-\\nsired output for a training pattern p.\\n\\nDefinition 4.7 (Training patterns). A\\npI training pattern is an input vector p\\n\\nwith the components p1, p2, . . . , pn whose\\ndesired output is known. By entering the\\ntraining pattern into the network we re-\\nceive an output that can be compared with\\nthe teaching input, which is the desired\\noutput. The set of training patterns is\\ncalled P . It contains a finite number of or-\\ndered pairs(p, t) of training patterns with\\ncorresponding desired output.\\n\\nTraining patterns are often simply called\\npatterns, that is why they are referred\\nto as p. In the literature as well as in\\nthis text they are called synonymously pat-\\nterns, training samples etc.\\n\\nDefinition 4.8 (Teaching input). Let j\\ntI be an output neuron. The teaching in-\\n\\nput tj is the desired and correct value j\\ndesired\\noutput should output after the input of a certain\\n\\ntraining pattern. Analogously to the vec-\\ntor p the teaching inputs t1, t2, . . . , tn of\\nthe neurons can also be combined into a\\nvector t. t always refers to a specific train-\\ning pattern p and is, as already mentioned,\\ncontained in the set P of the training pat-\\nterns.\\n\\nSNIPE: Classes that are relevant\\nfor training data are located in\\nthe package training. The class\\nTrainingSampleLesson allows for storage\\nof training patterns and teaching inputs,\\n\\nas well as simple preprocessing of the\\ntraining data.\\n\\nDefinition 4.9 (Error vector). For sev- JEperal output neurons Ω1,Ω2, . . . ,Ωn the dif-\\nference between output vector and teach-\\ning input under a training input p\\n\\nEp =\\n\\n\uf8eb\uf8ec\uf8ed t1 − y1\\n...\\n\\ntn − yn\\n\\n\uf8f6\uf8f7\uf8f8\\nis referred to as error vector, sometimes\\nit is also called difference vector. De-\\npending on whether you are learning of-\\nfline or online, the difference vector refers\\nto a specific training pattern, or to the er-\\nror of a set of training patterns which is\\nnormalized in a certain way.\\n\\nNow I want to briefly summarize the vec-\\ntors we have yet defined. There is the\\n\\ninput vector x, which can be entered into\\nthe neural network. Depending on\\nthe type of network being used the\\nneural network will output an\\n\\noutput vector y. Basically, the\\n\\ntraining sample p is nothing more than\\nan input vector. We only use it for\\ntraining purposes because we know\\nthe corresponding\\n\\nteaching input t which is nothing more\\nthan the desired output vector to the\\ntraining sample. The\\n\\nerror vector Ep is the difference between\\nthe teaching input t and the actural\\noutput y.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 55\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\nSo, what x and y are for the general net-\\nwork operation are p and t for the networkImportant!\\ntraining - and during training we try to\\nbring y as close to t as possible. One ad-\\nvice concerning notation: We referred to\\nthe output values of a neuron i as oi. Thus,\\nthe output of an output neuron Ω is called\\noΩ. But the output values of a network are\\nreferred to as yΩ. Certainly, these network\\noutputs are only neuron outputs, too, but\\nthey are outputs of output neurons. In\\nthis respect\\n\\nyΩ = oΩ\\n\\nis true.\\n\\n4.3 Using training samples\\n\\nWe have seen how we can learn in prin-\\nciple and which steps are required to do\\nso. Now we should take a look at the se-\\nlection of training data and the learning\\ncurve. After successful learning it is par-\\nticularly interesting whether the network\\nhas onlymemorized – i.e. whether it can\\nuse our training samples to quite exactly\\nproduce the right output but to provide\\nwrong answers for all other problems of\\nthe same class.\\n\\nSuppose that we want the network to train\\na mapping R2 → B1 and therefor use the\\ntraining samples from fig. 4.1: Then there\\ncould be a chance that, finally, the net-\\nwork will exactly mark the colored areas\\naround the training samples with the out-\\nput 1 (fig. 4.1, top), and otherwise will\\noutput 0 . Thus, it has sufficient storage\\ncapacity to concentrate on the six training\\n\\nFigure 4.1: Visualization of training results of\\nthe same training set on networks with a capacity\\nbeing too high (top), correct (middle) or too low\\n(bottom).\\n\\n56 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.3 Using training samples\\n\\nsamples with the output 1. This implies\\nan oversized network with too much free\\nstorage capacity.\\n\\nOn the other hand a network could have\\ninsufficient capacity (fig. 4.1, bottom) –\\nthis rough presentation of input data does\\nnot correspond to the good generalization\\nperformance we desire. Thus, we have to\\nfind the balance (fig. 4.1, middle).\\n\\n4.3.1 It is useful to divide the set of\\ntraining samples\\n\\nAn often proposed solution for these prob-\\nlems is to divide, the training set into\\n\\n. one training set really used to train ,\\n\\n. and one verification set to test our\\nprogress\\n\\n– provided that there are enough train-\\ning samples. The usual division relations\\nare, for instance, 70% for training data\\nand 30% for verification data (randomly\\nchosen). We can finish the training when\\nthe network provides good results on the\\ntraining data as well as on the verification\\ndata.\\n\\nSNIPE: The method splitLesson within\\nthe class TrainingSampleLesson allows for\\nsplitting a TrainingSampleLesson with re-\\nspect to a given ratio.\\n\\nBut note: If the verification data provide\\npoor results, do not modify the network\\nstructure until these data provide good re-\\nsults – otherwise you run the risk of tai-\\nloring the network to the verification data.\\n\\nThis means, that these data are included\\nin the training, even if they are not used\\nexplicitly for the training. The solution\\nis a third set of validation data used only\\nfor validation after a supposably success-\\nful training.\\n\\nBy training less patterns, we obviously\\nwithhold information from the network\\nand risk to worsen the learning perfor-\\nmance. But this text is not about 100%\\nexact reproduction of given samples but\\nabout successful generalization and ap-\\nproximation of a whole function – for\\nwhich it can definitely be useful to train\\nless information into the network.\\n\\n4.3.2 Order of pattern\\nrepresentation\\n\\nYou can find different strategies to choose\\nthe order of pattern presentation: If pat-\\nterns are presented in random sequence,\\nthere is no guarantee that the patterns\\nare learned equally well (however, this is\\nthe standard method). Always the same\\nsequence of patterns, on the other hand,\\nprovokes that the patterns will be memo-\\nrized when using recurrent networks (later,\\nwe will learn more about this type of net-\\nworks). A random permutation would\\nsolve both problems, but it is – as already\\nmentioned – very time-consuming to cal-\\nculate such a permutation.\\n\\nSNIPE: The method shuffleSamples lo-\\ncated in the class TrainingSampleLesson\\npermutes a lesson.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 57\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\n4.4 Learning curve and error\\nmeasurement\\n\\nThe learning curve indicates the progress\\nof the error, which can be determined innorm\\n\\nto\\ncompare\\n\\nvarious ways. The motivation to create a\\nlearning curve is that such a curve can in-\\ndicate whether the network is progressing\\nor not. For this, the error should be nor-\\nmalized, i.e. represent a distance measure\\nbetween the correct and the current out-\\nput of the network. For example, we can\\ntake the same pattern-specific, squared er-\\nror with a prefactor, which we are also go-\\ning to use to derive the backpropagation\\nof error (let Ω be output neurons and O\\nthe set of output neurons):\\n\\nErrp = 1\\n2\\n∑\\nΩ∈O\\n\\n(tΩ − yΩ)2 (4.1)\\n\\nDefinition 4.10 (Specific error). The\\nspecific error Errp is based on a single\\n\\nErrpI training sample, which means it is gener-\\nated online.\\n\\nAdditionally, the root mean square (ab-\\nbreviated: RMS) and the Euclidean\\ndistance are often used.\\n\\nThe Euclidean distance (generalization of\\nthe theorem of Pythagoras) is useful for\\nlower dimensions where we can still visual-\\nize its usefulness.\\n\\nDefinition 4.11 (Euclidean distance).\\nThe Euclidean distance between two vec-\\ntors t and y is defined as\\n\\nErrp =\\n√∑\\n\\nΩ∈O\\n(tΩ − yΩ)2. (4.2)\\n\\nGenerally, the root mean square is com-\\nmonly used since it considers extreme out-\\nliers to a greater extent.\\n\\nDefinition 4.12 (Root mean square).\\nThe root mean square of two vectors t and\\ny is defined as\\n\\nErrp =\\n√∑\\n\\nΩ∈O(tΩ − yΩ)2\\n\\n|O|\\n. (4.3)\\n\\nAs for offline learning, the total error in\\nthe course of one training epoch is inter-\\nesting and useful, too:\\n\\nErr =\\n∑\\np∈P\\n\\nErrp (4.4)\\n\\nDefinition 4.13 (Total error). The total\\nerror Err is based on all training samples, JErrthat means it is generated offline.\\n\\nAnalogously we can generate a total RMS\\nand a total Euclidean distance in the\\ncourse of a whole epoch. Of course, it is\\npossible to use other types of error mea-\\nsurement. To get used to further error\\nmeasurement methods, I suggest to have a\\nlook into the technical report of Prechelt\\n[Pre94]. In this report, both error mea-\\nsurement methods and sample problems\\nare discussed (this is why there will be a\\nsimmilar suggestion during the discussion\\nof exemplary problems).\\n\\nSNIPE: There are several static meth-\\nods representing different methods of er-\\nror measurement implemented in the class\\nErrorMeasurement.\\n\\nDepending on our method of error mea-\\nsurement our learning curve certainly\\n\\n58 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.4 Learning curve and error measurement\\n\\nchanges, too. A perfect learning curve\\nlooks like a negative exponential func-\\ntion, that means it is proportional to e−t\\n(fig. 4.2 on the following page). Thus, the\\nrepresentation of the learning curve can be\\nillustrated by means of a logarithmic scale\\n(fig. 4.2, second diagram from the bot-\\ntom) – with the said scaling combination\\na descending line implies an exponential\\ndescent of the error.\\n\\nWith the network doing a good job, the\\nproblems being not too difficult and the\\nlogarithmic representation of Err you can\\nsee - metaphorically speaking - a descend-\\ning line that often forms \\"spikes\\" at the\\nbottom – here, we reach the limit of the\\n64-bit resolution of our computer and our\\nnetwork has actually learned the optimum\\nof what it is capable of learning.\\n\\nTypical learning curves can show a few flat\\nareas as well, i.e. they can show some\\nsteps, which is no sign of a malfunctioning\\nlearning process. As we can also see in fig.\\n4.2, a well-suited representation can make\\nany slightly decreasing learning curve look\\ngood – so just be cautious when reading\\nthe literature.\\n\\n4.4.1 When do we stop learning?\\n\\nNow, the big question is: When do we\\nstop learning? Generally, the training is\\nstopped when the user in front of the learn-\\ning computer \\"thinks\\" the error was small\\nenough. Indeed, there is no easy answer\\nand thus I can once again only give you\\nsomething to think about, which, however,\\n\\ndepends on a more objective view on the\\ncomparison of several learning curves.\\n\\nConfidence in the results, for example, is\\nboosted, when the network always reaches\\n\\nobjectivity\\nnearly the same final error-rate for differ-\\nent random initializations – so repeated\\ninitialization and training will provide a\\nmore objective result.\\n\\nOn the other hand, it can be possible that\\na curve descending fast in the beginning\\ncan, after a longer time of learning, be\\novertaken by another curve: This can indi-\\ncate that either the learning rate of the\\nworse curve was too high or the worse\\ncurve itself simply got stuck in a local min-\\nimum, but was the first to find it.\\n\\nRemember: Larger error values are worse\\nthan the small ones.\\n\\nBut, in any case, note: Many people only\\ngenerate a learning curve in respect of the\\ntraining data (and then they are surprised\\nthat only a few things will work) – but for\\nreasons of objectivity and clarity it should\\nnot be forgotten to plot the verification\\ndata on a second learning curve, which\\ngenerally provides values that are slightly\\nworse and with stronger oscillation. But\\nwith good generalization the curve can de-\\ncrease, too.\\n\\nWhen the network eventually begins to\\nmemorize the samples, the shape of the\\nlearning curve can provide an indication:\\nIf the learning curve of the verification\\nsamples is suddenly and rapidly rising\\nwhile the learning curve of the verification\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 59\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\n 0\\n\\n 5e−005\\n\\n 0.0001\\n\\n 0.00015\\n\\n 0.0002\\n\\n 0.00025\\n\\n 0  100  200  300  400  500  600  700  800  900 1000\\n\\nF\\neh\\n\\nle\\nr\\n\\nEpoche\\n\\n 0\\n\\n 2e−005\\n\\n 4e−005\\n\\n 6e−005\\n\\n 8e−005\\n\\n 0.0001\\n\\n 0.00012\\n\\n 0.00014\\n\\n 0.00016\\n\\n 0.00018\\n\\n 0.0002\\n\\n 1  10  100  1000\\n\\nF\\neh\\n\\nle\\nr\\n\\nEpoche\\n\\n 1e−035\\n\\n 1e−030\\n\\n 1e−025\\n\\n 1e−020\\n\\n 1e−015\\n\\n 1e−010\\n\\n 1e−005\\n\\n 1\\n\\n 0  100  200  300  400  500  600  700  800  900 1000\\n\\nF\\neh\\n\\nle\\nr\\n\\nEpoche\\n\\n 1e−035\\n\\n 1e−030\\n\\n 1e−025\\n\\n 1e−020\\n\\n 1e−015\\n\\n 1e−010\\n\\n 1e−005\\n\\n 1\\n\\n 1  10  100  1000\\n\\nF\\neh\\n\\nle\\nr\\n\\nEpoche\\n\\nFigure 4.2: All four illustrations show the same (idealized, because very smooth) learning curve.\\nNote the alternating logarithmic and linear scalings! Also note the small \\"inaccurate spikes\\" visible\\nin the sharp bend of the curve in the first and second diagram from bottom.\\n\\n60 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.5 Gradient optimization procedures\\n\\ndata is continuously falling, this could indi-\\ncate memorizing and a generalization get-\\nting poorer and poorer. At this point it\\ncould be decided whether the network has\\nalready learned well enough at the next\\npoint of the two curves, and maybe the\\nfinal point of learning is to be applied\\nhere (this procedure is called early stop-\\nping).\\n\\nOnce again I want to remind you that they\\nare all acting as indicators and not to draw\\nIf-Then conclusions.\\n\\n4.5 Gradient optimization\\nprocedures\\n\\nIn order to establish the mathematical ba-\\nsis for some of the following learning pro-\\ncedures I want to explain briefly what is\\nmeant by gradient descent: the backpro-\\npagation of error learning procedure, for\\nexample, involves this mathematical basis\\nand thus inherits the advantages and dis-\\nadvantages of the gradient descent.\\n\\nGradient descent procedures are generally\\nused where we want to maximize or mini-\\nmize n-dimensional functions. Due to clar-\\nity the illustration (fig. 4.3 on the next\\npage) shows only two dimensions, but prin-\\ncipally there is no limit to the number of\\ndimensions.\\n\\nThe gradient is a vector g that is de-\\nfined for any differentiable point of a func-\\ntion, that points from this point exactly\\ntowards the steepest ascent and indicates\\nthe gradient in this direction by means\\n\\nof its norm |g|. Thus, the gradient is a\\ngeneralization of the derivative for multi-\\ndimensional functions. Accordingly, the\\nnegative gradient −g exactly points to-\\nwards the steepest descent. The gradient\\noperator ∇ is referred to as nabla op- J∇\\n\\ngradient is\\nmulti-dim.\\nderivative\\n\\nerator, the overall notation of the the\\ngradient g of the point (x, y) of a two-\\ndimensional function f being g(x, y) =\\n∇f(x, y).\\nDefinition 4.14 (Gradient). Let g be\\na gradient. Then g is a vector with n\\ncomponents that is defined for any point\\nof a (differential) n-dimensional function\\nf(x1, x2, . . . , xn). The gradient operator\\nnotation is defined as\\ng(x1, x2, . . . , xn) = ∇f(x1, x2, . . . , xn).\\n\\ng directs from any point of f towards\\nthe steepest ascent from this point, with\\n|g| corresponding to the degree of this as-\\ncent.\\n\\nGradient descent means to going downhill\\nin small steps from any starting point of\\nour function towards the gradient g (which\\nmeans, vividly speaking, the direction to\\nwhich a ball would roll from the starting\\npoint), with the size of the steps being pro-\\nportional to |g| (the steeper the descent,\\nthe longer the steps). Therefore, we move\\nslowly on a flat plateau, and on a steep as-\\ncent we run downhill rapidly. If we came\\ninto a valley, we would - depending on the\\nsize of our steps - jump over it or we would\\nreturn into the valley across the opposite\\nhillside in order to come closer and closer\\nto the deepest point of the valley by walk-\\ning back and forth, similar to our ball mov-\\ning within a round bowl.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 61\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\nFigure 4.3: Visualization of the gradient descent on a two-dimensional error function. We\\nmove forward in the opposite direction of g, i.e. with the steepest descent towards the lowest\\npoint, with the step width being proportional to |g| (the steeper the descent, the faster the\\nsteps). On the left the area is shown in 3D, on the right the steps over the contour lines are\\nshown in 2D. Here it is obvious how a movement is made in the opposite direction of g towards\\nthe minimum of the function and continuously slows down proportionally to |g|. Source:\\nhttp://webster.fhs-hagenberg.ac.at/staff/sdreisei/Teaching/WS2001-2002/\\nPatternClassification/graddescent.pdf\\n\\nDefinition 4.15 (Gradient descent).\\nLet f be an n-dimensional function and\\n\\nWe go\\ntowards the\\n\\ngradient\\ns = (s1, s2, . . . , sn) the given starting\\npoint. Gradient descent means going\\nfrom f(s) against the direction of g, i.e.\\ntowards −g with steps of the size of |g|\\ntowards smaller and smaller values of f .\\n\\nGradient descent procedures are not an er-\\nrorless optimization procedure at all (as\\nwe will see in the following sections) – how-\\never, they work still well on many prob-\\nlems, which makes them an optimization\\nparadigm that is frequently used. Anyway,\\nlet us have a look on their potential disad-\\nvantages so we can keep them in mind a\\nbit.\\n\\n4.5.1 Gradient procedures\\nincorporate several problems\\n\\nAs already implied in section 4.5, the gra-\\ndient descent (and therefore the backpro-\\npagation) is promising but not foolproof.\\nOne problem, is that the result does not\\nalways reveal if an error has occurred.\\n\\ngradient\\ndescent\\nwith errors\\n\\n4.5.1.1 Often, gradient descents\\nconverge against suboptimal\\nminima\\n\\nEvery gradient descent procedure can, for\\nexample, get stuck within a local mini-\\nmum (part a of fig. 4.4 on the facing page).\\n\\n62 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.5 Gradient optimization procedures\\n\\nFigure 4.4: Possible errors during a gradient descent: a) Detecting bad minima, b) Quasi-standstill\\nwith small gradient, c) Oscillation in canyons, d) Leaving good minima.\\n\\nThis problem is increasing proportionally\\nto the size of the error surface, and there\\nis no universal solution. In reality, one\\ncannot know if the optimal minimum is\\nreached and considers a training success-\\nful, if an acceptable minimum is found.\\n\\n4.5.1.2 Flat plataeus on the error\\nsurface may cause training\\nslowness\\n\\nWhen passing a flat plateau, for instance,\\nthe gradient also becomes negligibly small\\nbecause there is hardly a descent (part b\\nof fig. 4.4), which requires many further\\nsteps. A hypothetically possible gradient\\nof 0 would completely stop the descent.\\n\\n4.5.1.3 Even if good minima are\\nreached, they may be left\\nafterwards\\n\\nOn the other hand the gradient is very\\nlarge at a steep slope so that large steps\\ncan be made and a good minimum can pos-\\nsibly be missed (part d of fig. 4.4).\\n\\n4.5.1.4 Steep canyons in the error\\nsurface may cause oscillations\\n\\nA sudden alternation from one very strong\\nnegative gradient to a very strong positive\\none can even result in oscillation (part c\\nof fig. 4.4). In nature, such an error does\\nnot occur very often so that we can think\\nabout the possibilities b and d.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 63\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\n4.6 Exemplary problems allow\\nfor testing self-coded\\nlearning strategies\\n\\nWe looked at learning from the formal\\npoint of view – not much yet but a little.\\nNow it is time to look at a few exemplary\\nproblem you can later use to test imple-\\nmented networks and learning rules.\\n\\n4.6.1 Boolean functions\\n\\nA popular example is the one that did\\nnot work in the nineteen-sixties: the XOR\\nfunction (B2 → B1). We need a hidden\\nneuron layer, which we have discussed in\\ndetail. Thus, we need at least two neu-\\nrons in the inner layer. Let the activation\\nfunction in all layers (except in the input\\nlayer, of course) be the hyperbolic tangent.\\nTrivially, we now expect the outputs 1.0\\nor −1.0, depending on whether the func-\\ntion XOR outputs 1 or 0 - and exactly\\nhere is where the first beginner’s mistake\\noccurs.\\n\\nFor outputs close to 1 or -1, i.e. close to\\nthe limits of the hyperbolic tangent (or\\nin case of the Fermi function 0 or 1), we\\nneed very large network inputs. The only\\nchance to reach these network inputs are\\nlarge weights, which have to be learned:\\nThe learning process is largely extended.\\nTherefore it is wiser to enter the teaching\\ninputs 0.9 or −0.9 as desired outputs or\\nto be satisfied when the network outputs\\nthose values instead of 1 and −1.\\n\\ni1 i2 i3 Ω\\n0 0 0 1\\n0 0 1 0\\n0 1 0 0\\n0 1 1 1\\n1 0 0 0\\n1 0 1 1\\n1 1 0 1\\n1 1 1 0\\n\\nTable 4.1: Illustration of the parity function\\nwith three inputs.\\n\\nAnother favourite example for singlelayer\\nperceptrons are the boolean functions\\nAND and OR.\\n\\n4.6.2 The parity function\\n\\nThe parity function maps a set of bits to 1\\nor 0, depending on whether an even num-\\nber of input bits is set to 1 or not. Ba-\\nsically, this is the function Bn → B1. It\\nis characterized by easy learnability up to\\napprox. n = 3 (shown in table 4.1), but\\nthe learning effort rapidly increases from\\nn = 4. The reader may create a score ta-\\nble for the 2-bit parity function. What is\\nconspicuous?\\n\\n4.6.3 The 2-spiral problem\\n\\nAs a training sample for a function let\\nus take two spirals coiled into each other\\n(fig. 4.5 on the facing page) with the\\nfunction certainly representing a mapping\\n\\n64 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.6 Exemplary problems\\n\\nFigure 4.5: Illustration of the training samples\\nof the 2-spiral problem\\n\\nR2 → B1. One of the spirals is assigned\\nto the output value 1, the other spiral to\\n0. Here, memorizing does not help. The\\nnetwork has to understand the mapping it-\\nself. This example can be solved by means\\nof an MLP, too.\\n\\n4.6.4 The checkerboard problem\\n\\nWe again create a two-dimensional func-\\ntion of the form R2 → B1 and specify\\ncheckered training samples (fig. 4.6) with\\none colored field representing 1 and all the\\nrest of them representing 0. The difficulty\\nincreases proportionally to the size of the\\nfunction: While a 3×3 field is easy to learn,\\nthe larger fields are more difficult (here\\nwe eventually use methods that are more\\n\\nFigure 4.6: Illustration of training samples for\\nthe checkerboard problem\\n\\nsuitable for this kind of problems than the\\nMLP).\\n\\nThe 2-spiral problem is very similar to the\\ncheckerboard problem, only that, mathe-\\nmatically speaking, the first problem is us-\\ning polar coordinates instead of Cartesian\\ncoordinates. I just want to introduce as\\nan example one last trivial case: the iden-\\ntity.\\n\\n4.6.5 The identity function\\n\\nBy using linear activation functions the\\nidentity mapping from R1 to R1 (of course\\nonly within the parameters of the used ac-\\ntivation function) is no problem for the\\nnetwork, but we put some obstacles in its\\nway by using our sigmoid functions so that\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 65\\n\\n\\n\\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\\n\\nit would be difficult for the network to\\nlearn the identity. Just try it for the fun\\nof it.\\n\\nNow, it is time to hava a look at our first\\nmathematical learning rule.\\n\\n4.6.6 There are lots of other\\nexemplary problems\\n\\nFor lots and lots of further exemplary prob-\\nlems, I want to recommend the technical\\nreport written by prechelt [Pre94] which\\nalso has been named in the sections about\\nerror measurement procedures..\\n\\n4.7 The Hebbian learning rule\\nis the basis for most\\nother learning rules\\n\\nIn 1949, Donald O. Hebb formulated\\ntheHebbian rule [Heb49] which is the ba-\\nsis for most of the more complicated learn-\\ning rules we will discuss in this text. We\\ndistinguish between the original form and\\nthe more general form, which is a kind of\\nprinciple for other learning rules.\\n\\n4.7.1 Original rule\\n\\nDefinition 4.16 (Hebbian rule). \\"If neu-\\nron j receives an input from neuron i and\\nif both neurons are strongly active at the\\nsame time, then increase the weight wi,j\\n(i.e. the strength of the connection be-\\ntween i and j).\\" Mathematically speaking,\\nthe rule is:\\n\\nearly\\nform of\\nthe rule\\n\\n∆wi,j ∼ ηoiaj (4.5)\\n\\nwith ∆wi,j being the change in weight\\nfrom i to j , which is proportional to the J∆wi,jfollowing factors:\\n\\n. the output oi of the predecessor neu-\\nron i, as well as,\\n\\n. the activation aj of the successor neu-\\nron j,\\n\\n. a constant η, i.e. the learning rate,\\nwhich will be discussed in section\\n5.4.3.\\n\\nThe changes in weight ∆wi,j are simply\\nadded to the weight wi,j .\\n\\nWhy am I speaking twice about activation,\\nbut in the formula I am using oi and aj , i.e.\\nthe output of neuron of neuron i and the ac-\\ntivation of neuron j? Remember that the\\nidentity is often used as output function\\nand therefore ai and oi of a neuron are of-\\nten the same. Besides, Hebb postulated\\nhis rule long before the specification of\\ntechnical neurons. Considering that this\\nlearning rule was preferred in binary acti-\\nvations, it is clear that with the possible\\nactivations (1, 0) the weights will either in-\\ncrease or remain constant. Sooner or later\\n\\nweights\\ngo ad\\ninfinitum\\n\\nthey would go ad infinitum, since they can\\nonly be corrected \\"upwards\\" when an error\\noccurs. This can be compensated by using\\nthe activations (-1,1)2. Thus, the weights\\nare decreased when the activation of the\\npredecessor neuron dissents from the one\\nof the successor neuron, otherwise they are\\nincreased.\\n2 But that is no longer the \\"original version\\" of the\\nHebbian rule.\\n\\n66 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 4.7 Hebbian rule\\n\\n4.7.2 Generalized form\\n\\nMost of the learning rules discussed before\\nare a specialization of the mathematically\\nmore general form [MR86] of the Hebbian\\nrule.\\n\\nDefinition 4.17 (Hebbian rule, more gen-\\neral). The generalized form of the\\nHebbian Rule only specifies the propor-\\ntionality of the change in weight to the\\nproduct of two undefined functions, but\\nwith defined input values.\\n\\n∆wi,j = η · h(oi, wi,j) · g(aj , tj) (4.6)\\n\\nThus, the product of the functions\\n\\n. g(aj , tj) and\\n\\n. h(oi, wi,j)\\n\\n. as well as the constant learning rate\\nη\\n\\nresults in the change in weight. As you\\ncan see, h receives the output of the pre-\\ndecessor cell oi as well as the weight from\\npredecessor to successor wi,j while g ex-\\npects the actual and desired activation of\\nthe successor aj and tj (here t stands for\\nthe aforementioned teaching input). As al-\\nready mentioned g and h are not specified\\nin this general definition. Therefore, we\\nwill now return to the path of specializa-\\ntion we discussed before equation 4.6. Af-\\nter we have had a short picture of what\\na learning rule could look like and of our\\nthoughts about learning itself, we will be\\nintroduced to our first network paradigm\\nincluding the learning procedure.\\n\\nExercises\\n\\nExercise 7. Calculate the average value\\nµ and the standard deviation σ for the fol-\\nlowing data points.\\n\\np1 = (2, 2, 2)\\np2 = (3, 3, 3)\\np3 = (4, 4, 4)\\np4 = (6, 0, 0)\\np5 = (0, 6, 0)\\np6 = (0, 0, 6)\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 67\\n\\n\\n\\n\\n\\nPart II\\n\\nSupervised learning network\\nparadigms\\n\\n69\\n\\n\\n\\n\\n\\nChapter 5\\n\\nThe perceptron, backpropagation and its\\nvariants\\n\\nA classic among the neural networks. If we talk about a neural network, then\\nin the majority of cases we speak about a percepton or a variation of it.\\n\\nPerceptrons are multilayer networks without recurrence and with fixed input\\nand output layers. Description of a perceptron, its limits and extensions that\\nshould avoid the limitations. Derivation of learning procedures and discussion\\n\\nof their problems.\\n\\nAs already mentioned in the history of neu-\\nral networks, the perceptron was described\\nby Frank Rosenblatt in 1958 [Ros58].\\nInitially, Rosenblatt defined the already\\ndiscussed weighted sum and a non-linear\\nactivation function as components of the\\nperceptron.\\n\\nThere is no established definition for a per-\\nceptron, but most of the time the term\\nis used to describe a feedforward network\\nwith shortcut connections. This network\\nhas a layer of scanner neurons (retina)\\nwith statically weighted connections to\\nthe following layer and is called input\\nlayer (fig. 5.1 on the next page); but the\\nweights of all other layers are allowed to be\\nchanged. All neurons subordinate to the\\nretina are pattern detectors. Here we ini-\\ntially use a binary perceptron with every\\noutput neuron having exactly two possi-\\n\\nble output values (e.g. {0, 1} or {−1, 1}).\\nThus, a binary threshold function is used\\nas activation function, depending on the\\nthreshold value Θ of the output neuron.\\n\\nIn a way, the binary activation function\\nrepresents an IF query which can also\\nbe negated by means of negative weights.\\nThe perceptron can thus be used to ac-\\ncomplish true logical information process-\\ning.\\n\\nWhether this method is reasonable is an-\\nother matter – of course, this is not the\\neasiest way to achieve Boolean logic. I just\\nwant to illustrate that perceptrons can\\nbe used as simple logical components and\\nthat, theoretically speaking, any Boolean\\nfunction can be realized by means of per-\\nceptrons being connected in series or in-\\nterconnected in a sophisticated way. But\\n\\n71\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nKapitel 5 Das Perceptron dkriesel.com\\n\\n�� \\"\\" )) ++ ,,\\n�� ## )) ++|| �� ## )){{uu\\n\\n�� \\"\\"{{uuss\\n��||uussrrGFED@ABC�\\n\\n\'\'OOOOOOOOOOOOOOOOO GFED@ABC�\\n\\n��@@@@@@@@@\\nGFED@ABC�\\n\\n��\\n\\nGFED@ABC�\\n\\n��~~~~~~~~~\\nGFED@ABC�\\n\\nwwooooooooooooooooo\\n\\nWVUTPQRSΣ\\nL|H\\n\\n��\\n\\nGFED@ABCi1\\n\\n((PPPPPPPPPPPPPPPPPP GFED@ABCi2\\n\\n!!CCCCCCCCCC\\nGFED@ABCi3\\n\\n��\\n\\nGFED@ABCi4\\n\\n}}{{{{{{{{{{\\nGFED@ABCi5\\n\\nvvnnnnnnnnnnnnnnnnnn\\n\\n?>=<89:;Ω\\n\\n��\\n\\nAbbildung 5.1: Aufbau eines Perceptrons mit einer Schicht variabler Verbindungen in verschiede-\\nnen Ansichten. Die durchgezogene Gewichtsschicht in den unteren beiden Abbildungen ist trainier-\\nbar.\\nOben: Am Beispiel der Informationsabtastung im Auge.\\nMitte: Skizze desselben mit eingezeichneter fester Gewichtsschicht unter Verwendung der definier-\\nten funktionsbeschreibenden Designs für Neurone.\\nUnten: Ohne eingezeichnete feste Gewichtsschicht, mit Benennung der einzelnen Neuronen nach\\nunserer Konvention. Wir werden die feste Gewichtschicht im weiteren Verlauf der Arbeit nicht mehr\\nbetrachten.\\n\\n70 D. Kriesel – Ein kleiner Überblick über Neuronale Netze (EPSILON-DE)\\n\\nFigure 5.1: Architecture of a perceptron with one layer of variable connections in different views.\\nThe solid-drawn weight layer in the two illustrations on the bottom can be trained.\\nLeft side: Example of scanning information in the eye.\\nRight side, upper part: Drawing of the same example with indicated fixed-weight layer using the\\ndefined designs of the functional descriptions for neurons.\\nRight side, lower part: Without indicated fixed-weight layer, with the name of each neuron\\ncorresponding to our convention. The fixed-weight layer will no longer be taken into account in the\\ncourse of this work.\\n\\n72 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com\\n\\nwe will see that this is not possible without\\nconnecting them serially. Before providing\\nthe definition of the perceptron, I want to\\ndefine some types of neurons used in this\\nchapter.\\n\\nDefinition 5.1 (Input neuron). An in-\\nput neuron is an identity neuron. It\\nexactly forwards the information received.\\nThus, it represents the identity function,input neuron\\n\\nonly forwards\\ndata\\n\\nwhich should be indicated by the symbol\\n�. Therefore the input neuron is repre-\\nsented by the symbol GFED@ABC� .\\n\\nDefinition 5.2 (Information process-\\ning neuron). Information processing\\nneurons somehow process the input infor-\\nmation, i.e. do not represent the identity\\nfunction. A binary neuron sums up all\\ninputs by using the weighted sum as prop-\\nagation function, which we want to illus-\\ntrate by the sign Σ. Then the activation\\nfunction of the neuron is the binary thresh-\\nold function, which can be illustrated by\\nL|H. This leads us to the complete de-\\npiction of information processing neurons,\\n\\nnamely WVUTPQRSΣ\\nL|H\\n\\n. Other neurons that use\\n\\nthe weighted sum as propagation function\\nbut the activation functions hyperbolic tan-\\ngent or Fermi function, or with a sepa-\\nrately defined activation function fact, are\\nsimilarly represented by\\n\\nWVUTPQRSΣ\\nTanh\\n\\nWVUTPQRSΣ\\nFermi\\n\\nONMLHIJKΣ\\nfact\\n\\n.\\n\\nThese neurons are also referred to as\\nFermi neurons or Tanh neuron.\\n\\nNow that we know the components of a\\nperceptron we should be able to define\\nit.\\n\\nDefinition 5.3 (Perceptron). The per-\\nceptron (fig. 5.1 on the facing page) is1 a\\nfeedforward network containing a retina\\nthat is used only for data acquisition and\\nwhich has fixed-weighted connections with\\nthe first neuron layer (input layer). The\\nfixed-weight layer is followed by at least\\none trainable weight layer. One neuron\\nlayer is completely linked with the follow-\\ning layer. The first layer of the percep-\\ntron consists of the input neurons defined\\nabove.\\n\\nA feedforward network often contains\\nshortcuts which does not exactly corre-\\nspond to the original description and there-\\nfore is not included in the definition. We\\ncan see that the retina is not included in\\nthe lower part of fig. 5.1. As a matter\\nof fact the first neuron layer is often un-\\nderstood (simplified and sufficient for this\\nmethod) as input layer, because this layer retina is\\n\\nunconsideredonly forwards the input values. The retina\\nitself and the static weights behind it are\\nno longer mentioned or displayed, since\\nthey do not process information in any\\ncase. So, the depiction of a perceptron\\nstarts with the input neurons.\\n\\n1 It may confuse some readers that I claim that there\\nis no definition of a perceptron but then define the\\nperceptron in the following section. I therefore\\nsuggest keeping my definition in the back of your\\nmind and just take it for granted in the course of\\nthis work.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 73\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nSNIPE: The methods\\nsetSettingsTopologyFeedForward\\nand the variation -WithShortcuts in\\na NeuralNetworkDescriptor-Instance\\napply settings to a descriptor, which\\nare appropriate for feedforward networks\\nor feedforward networks with shortcuts.\\nThe respective kinds of connections are\\nallowed, all others are not, and fastprop is\\nactivated.\\n\\n5.1 The singlelayer\\nperceptron provides only\\none trainable weight layer\\n\\nHere, connections with trainable weights\\ngo from the input layer to an output\\nneuron Ω, which returns the information\\n\\n1 trainable\\nlayer whether the pattern entered at the input\\n\\nneurons was recognized or not. Thus, a\\nsinglelayer perception (abbreviated SLP)\\nhas only one level of trainable weights\\n(fig. 5.1 on page 72).\\n\\nDefinition 5.4 (Singlelayer perceptron).\\nA singlelayer perceptron (SLP) is a\\nperceptron having only one layer of vari-\\nable weights and one layer of output neu-\\nrons Ω. The technical view of an SLP is\\nshown in fig. 5.2.\\n\\nCertainly, the existence of several output\\nneurons Ω1,Ω2, . . . ,Ωn does not consider-\\nably change the concept of the perceptronImportant!\\n(fig. 5.3): A perceptron with several out-\\nput neurons can also be regarded as sev-\\neral different perceptrons with the same\\ninput.\\n\\nGFED@ABCBIAS\\n\\nwBIAS,Ω\\n\\n  \\n\\nGFED@ABCi1\\n\\nwi1,Ω\\n\\n��\\n\\nGFED@ABCi2\\n\\nwi2,Ω\\n����\\n\\n������\\n\\n?>=<89:;Ω\\n\\n��\\n\\nFigure 5.2: A singlelayer perceptron with two in-\\nput neurons and one output neuron. The net-\\nwork returns the output by means of the ar-\\nrow leaving the network. The trainable layer of\\nweights is situated in the center (labeled). As a\\nreminder, the bias neuron is again included here.\\nAlthough the weight wBIAS,Ω is a normal weight\\nand also treated like this, I have represented it\\nby a dotted line – which significantly increases\\nthe clarity of larger networks. In future, the bias\\nneuron will no longer be included.\\n\\nGFED@ABCi1\\n\\n  @@@@@@@@@\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUUU\\n\\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCi2\\n\\n��\\n((PPPPPPPPPPPPPPPPPP\\n\\n  AAAAAAAAA\\nGFED@ABCi3\\n\\n~~}}}}}}}}}\\n\\n  AAAAAAAAA\\n\\n��\\n\\nGFED@ABCi4\\n\\nvvnnnnnnnnnnnnnnnnnn\\n\\n��~~}}}}}}}}}\\nGFED@ABCi5\\n\\n~~~~~~~~~~~\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\nwwnnnnnnnnnnnnnnnnn\\n\\nGFED@ABCΩ1\\n\\n��\\n\\nGFED@ABCΩ2\\n\\n��\\n\\nGFED@ABCΩ3\\n\\n��\\n\\nFigure 5.3: Singlelayer perceptron with several\\noutput neurons\\n\\n74 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.1 The singlelayer perceptron\\n\\nGFED@ABC�\\n\\n1\\nAAAA\\n\\n  AAAA\\n\\nGFED@ABC�\\n\\n1}}}}\\n\\n~~}}}}\\n\\nGFED@ABC1.5\\n\\n��\\n\\nGFED@ABC�\\n\\n1\\nAAAA\\n\\n  AAAA\\n\\nGFED@ABC�\\n\\n1}}}}\\n\\n~~}}}}\\n\\nGFED@ABC0.5\\n\\n��\\n\\nFigure 5.4: Two singlelayer perceptrons for\\nBoolean functions. The upper singlelayer per-\\nceptron realizes an AND, the lower one realizes\\nan OR. The activation function of the informa-\\ntion processing neuron is the binary threshold\\nfunction. Where available, the threshold values\\nare written into the neurons.\\n\\nThe Boolean functions AND and OR shown\\nin fig. 5.4 are trivial examples that can eas-\\nily be composed.\\n\\nNow we want to know how to train a single-\\nlayer perceptron. We will therefore at first\\ntake a look at the perceptron learning al-\\ngorithm and then we will look at the delta\\nrule.\\n\\n5.1.1 Perceptron learning algorithm\\nand convergence theorem\\n\\nThe original perceptron learning algo-\\nrithm with binary neuron activation func-\\ntion is described in alg. 1. It has been\\nproven that the algorithm converges in\\nfinite time – so in finite time the per-\\nceptron can learn anything it can repre-\\nsent (perceptron convergence theorem,\\n[Ros62]). But please do not get your hopes\\nup too soon! What the perceptron is capa-\\nble to represent will be explored later.\\n\\nDuring the exploration of linear separabil-\\nity of problems we will cover the fact that\\nat least the singlelayer perceptron unfor-\\ntunately cannot represent a lot of prob-\\nlems.\\n\\n5.1.2 The delta rule as a gradient\\nbased learning strategy for\\nSLPs\\n\\nIn the following we deviate from our bi-\\nnary threshold value as activation function\\nbecause at least for backpropagation of er-\\nror we need, as you will see, a differen-\\n\\nfact now differ-\\nentiabletiable or even a semi-linear activation func-\\n\\ntion. For the now following delta rule (like\\nbackpropagation derived in [MR86]) it is\\nnot always necessary but useful. This fact,\\nhowever, will also be pointed out in the\\nappropriate part of this work. Compared\\nwith the aforementioned perceptron learn-\\ning algorithm, the delta rule has the ad-\\nvantage to be suitable for non-binary acti-\\nvation functions and, being far away from\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 75\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\n1: while ∃p ∈ P and error too large do\\n2: Input p into the network, calculate output y {P set of training patterns}\\n3: for all output neurons Ω do\\n4: if yΩ = tΩ then\\n5: Output is okay, no correction of weights\\n6: else\\n7: if yΩ = 0 then\\n8: for all input neurons i do\\n9: wi,Ω := wi,Ω + oi {...increase weight towards Ω by oi}\\n\\n10: end for\\n11: end if\\n12: if yΩ = 1 then\\n13: for all input neurons i do\\n14: wi,Ω := wi,Ω − oi {...decrease weight towards Ω by oi}\\n15: end for\\n16: end if\\n17: end if\\n18: end for\\n19: end while\\nAlgorithm 1: Perceptron learning algorithm. The perceptron learning algorithm\\nreduces the weights to output neurons that return 1 instead of 0, and in the inverse\\ncase increases weights.\\n\\n76 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.1 The singlelayer perceptron\\n\\nthe learning target, to automatically learn\\nfaster.\\n\\nSuppose that we have a singlelayer percep-\\ntron with randomly set weights which we\\nwant to teach a function by means of train-\\ning samples. The set of these training sam-\\nples is called P . It contains, as already de-\\nfined, the pairs (p, t) of the training sam-\\nples p and the associated teaching input t.\\nI also want to remind you that\\n\\n. x is the input vector and\\n\\n. y is the output vector of a neural net-\\nwork,\\n\\n. output neurons are referred to as\\nΩ1,Ω2, . . . ,Ω|O|,\\n\\n. i is the input and\\n\\n. o is the output of a neuron.\\n\\nAdditionally, we defined that\\n\\n. the error vector Ep represents the dif-\\nference (t−y) under a certain training\\nsample p.\\n\\n. Furthermore, let O be the set of out-\\nput neurons and\\n\\n. I be the set of input neurons.\\n\\nAnother naming convention shall be that,\\nfor example, for an output o and a teach-\\ning input t an additional index p may be\\nset in order to indicate that these values\\nare pattern-specific. Sometimes this will\\nconsiderably enhance clarity.\\n\\nNow our learning target will certainly be,\\nthat for all training samples the output y\\n\\nof the network is approximately the de-\\nsired output t, i.e. formally it is true\\nthat\\n\\n∀p : y ≈ t or ∀p : Ep ≈ 0.\\n\\nThis means we first have to understand the\\ntotal error Err as a function of the weights:\\nThe total error increases or decreases de-\\npending on how we change the weights.\\n\\nDefinition 5.5 (Error function). The er-\\nror function\\n\\nJErr(W )\\nErr : W → R\\n\\nregards the set2 of weights W as a vector\\nand maps the values onto the normalized error as\\n\\nfunctionoutput error (normalized because other-\\nwise not all errors can be mapped onto\\none single e ∈ R to perform a gradient de-\\nscent). It is obvious that a specific error\\nfunction can analogously be generated\\n\\nJErrp(W )for a single pattern p.\\n\\nAs already shown in section 4.5, gradient\\ndescent procedures calculate the gradient\\nof an arbitrary but finite-dimensional func-\\ntion (here: of the error function Err(W ))\\nand move down against the direction of\\nthe gradient until a minimum is reached.\\nErr(W ) is defined on the set of all weights\\nwhich we here regard as the vector W .\\nSo we try to decrease or to minimize the\\nerror by simply tweaking the weights –\\nthus one receives information about how\\nto change the weights (the change in all\\n\\n2 Following the tradition of the literature, I previ-\\nously defined W as a weight matrix. I am aware\\nof this conflict but it should not bother us here.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 77\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\n 2\\nw1\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\n 2\\n\\nw2\\n\\n 0\\n\\n 1\\n\\n 2\\n\\n 3\\n\\n 4\\n\\n 5\\n\\nFigure 5.5: Exemplary error surface of a neural\\nnetwork with two trainable connections w1 und\\nw2. Generally, neural networks have more than\\ntwo connections, but this would have made the\\nillustration too complex. And most of the time\\nthe error surface is too craggy, which complicates\\nthe search for the minimum.\\n\\nweights is referred to as ∆W ) by calcu-\\nlating the gradient ∇Err(W ) of the error\\nfunction Err(W ):\\n\\n∆W ∼ −∇Err(W ). (5.1)\\n\\nDue to this relation there is a proportional-\\nity constant η for which equality holds (η\\nwill soon get another meaning and a real\\npractical use beyond the mere meaning of\\na proportionality constant. I just ask the\\nreader to be patient for a while.):\\n\\n∆W = −η∇Err(W ). (5.2)\\n\\nTo simplify further analysis, we now\\nrewrite the gradient of the error-function\\naccording to all weights as an usual par-\\ntial derivative according to a single weight\\nwi,Ω (the only variable weights exists be-\\ntween the hidden and the output layer Ω).\\n\\nThus, we tweak every single weight and ob-\\nserve how the error function changes, i.e.\\nwe derive the error function according to\\na weight wi,Ω and obtain the value ∆wi,Ω\\nof how to change this weight.\\n\\n∆wi,Ω = −η∂Err(W )\\n∂wi,Ω\\n\\n. (5.3)\\n\\nNow the following question arises: How\\nis our error function defined exactly? It\\nis not good if many results are far away\\nfrom the desired ones; the error function\\nshould then provide large values – on the\\nother hand, it is similarly bad if many\\nresults are close to the desired ones but\\nthere exists an extremely far outlying re-\\nsult. The squared distance between the\\noutput vector y and the teaching input t\\nappears adequate to our needs. It provides\\nthe error Errp that is specific for a train-\\ning sample p over the output of all output\\nneurons Ω:\\n\\nErrp(W ) = 1\\n2\\n∑\\nΩ∈O\\n\\n(tp,Ω − yp,Ω)2. (5.4)\\n\\nThus, we calculate the squared difference\\nof the components of the vectors t and\\ny, given the pattern p, and sum up these\\nsquares. The summation of the specific er-\\nrors Errp(W ) of all patterns p then yields\\nthe definition of the error Err and there-\\n\\n78 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.1 The singlelayer perceptron\\n\\nfore the definition of the error function\\nErr(W ):\\n\\nErr(W ) =\\n∑\\np∈P\\n\\nErrp(W ) (5.5)\\n\\n= 1\\n2\\n\\nsum over all p︷ ︸︸ ︷∑\\np∈P\\n\\n\uf8eb\uf8ed∑\\nΩ∈O\\n\\n(tp,Ω − yp,Ω)2\\n\\n\uf8f6\uf8f8\\n︸ ︷︷ ︸\\n\\nsum over all Ω\\n\\n.\\n\\n(5.6)\\n\\nThe observant reader will certainly wonder\\nwhere the factor 1\\n\\n2 in equation 5.4 on the\\npreceding page suddenly came from and\\nwhy there is no root in the equation, as\\nthis formula looks very similar to the Eu-\\nclidean distance. Both facts result from\\nsimple pragmatics: Our intention is to\\nminimize the error. Because the root func-\\ntion decreases with its argument, we can\\nsimply omit it for reasons of calculation\\nand implementation efforts, since we do\\nnot need it for minimization. Similarly, it\\ndoes not matter if the term to be mini-\\nmized is divided by 2: Therefore I am al-\\nlowed to multiply by 1\\n\\n2 . This is just done\\nso that it cancels with a 2 in the course of\\nour calculation.\\n\\nNow we want to continue deriving the\\ndelta rule for linear activation functions.\\nWe have already discussed that we tweak\\nthe individual weights wi,Ω a bit and see\\nhow the error Err(W ) is changing – which\\ncorresponds to the derivative of the er-\\nror function Err(W ) according to the very\\nsame weight wi,Ω. This derivative cor-\\nresponds to the sum of the derivatives\\nof all specific errors Errp according to\\nthis weight (since the total error Err(W )\\n\\nresults from the sum of the specific er-\\nrors):\\n\\n∆wi,Ω = −η∂Err(W )\\n∂wi,Ω\\n\\n(5.7)\\n\\n=\\n∑\\np∈P\\n−η∂Errp(W )\\n\\n∂wi,Ω\\n. (5.8)\\n\\nOnce again I want to think about the ques-\\ntion of how a neural network processes\\ndata. Basically, the data is only trans-\\nferred through a function, the result of the\\nfunction is sent through another one, and\\nso on. If we ignore the output function,\\nthe path of the neuron outputs oi1 and oi2 ,\\nwhich the neurons i1 and i2 entered into a\\nneuron Ω, initially is the propagation func-\\ntion (here weighted sum), from which the\\nnetwork input is going to be received. This\\nis then sent through the activation func-\\ntion of the neuron Ω so that we receive\\nthe output of this neuron which is at the\\nsame time a component of the output vec-\\ntor y:\\n\\nnetΩ → fact\\n\\n= fact(netΩ)\\n= oΩ\\n\\n= yΩ.\\n\\nAs we can see, this output results from\\nmany nested functions:\\n\\noΩ = fact(netΩ) (5.9)\\n\\n= fact(oi1 · wi1,Ω + oi2 · wi2,Ω). (5.10)\\n\\nIt is clear that we could break down the\\noutput into the single input neurons (this\\nis unnecessary here, since they do not\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 79\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nprocess information in an SLP). Thus,\\nwe want to calculate the derivatives of\\nequation 5.8 on the preceding page and\\ndue to the nested functions we can apply\\nthe chain rule to factorize the derivative\\n∂Errp(W )\\n∂wi,Ω\\n\\nin equation 5.8 on the previous\\npage.\\n\\n∂Errp(W )\\n∂wi,Ω\\n\\n= ∂Errp(W )\\n∂op,Ω\\n\\n· ∂op,Ω\\n∂wi,Ω\\n\\n. (5.11)\\n\\nLet us take a look at the first multiplica-\\ntive factor of the above equation 5.11\\nwhich represents the derivative of the spe-\\ncific error Errp(W ) according to the out-\\nput, i.e. the change of the error Errp\\nwith an output op,Ω: The examination\\nof Errp (equation 5.4 on page 78) clearly\\nshows that this change is exactly the dif-\\nference between teaching input and out-\\nput (tp,Ω− op,Ω) (remember: Since Ω is an\\noutput neuron, op,Ω = yp,Ω). The closer\\nthe output is to the teaching input, the\\nsmaller is the specific error. Thus we can\\nreplace one by the other. This difference\\nis also called δp,Ω (which is the reason for\\nthe name delta rule):\\n\\n∂Errp(W )\\n∂wi,Ω\\n\\n= −(tp,Ω − op,Ω) · ∂op,Ω\\n∂wi,Ω\\n\\n(5.12)\\n\\n= −δp,Ω ·\\n∂op,Ω\\n∂wi,Ω\\n\\n(5.13)\\n\\nThe second multiplicative factor of equa-\\ntion 5.11 and of the following one is the\\nderivative of the output specific to the pat-\\ntern p of the neuron Ω according to the\\nweight wi,Ω. So how does op,Ω change\\nwhen the weight from i to Ω is changed?\\n\\nDue to the requirement at the beginning of\\nthe derivation, we only have a linear acti-\\nvation function fact, therefore we can just\\nas well look at the change of the network\\ninput when wi,Ω is changing:\\n\\n∂Errp(W )\\n∂wi,Ω\\n\\n= −δp,Ω ·\\n∂\\n∑\\ni∈I(op,iwi,Ω)\\n∂wi,Ω\\n\\n.\\n\\n(5.14)\\n\\nThe resulting derivative ∂\\n∑\\n\\ni∈I(op,iwi,Ω)\\n∂wi,Ω\\n\\ncan now be simplified: The function∑\\ni∈I(op,iwi,Ω) to be derived consists of\\n\\nmany summands, and only the sum-\\nmand op,iwi,Ω contains the variable wi,Ω,\\naccording to which we derive. Thus,\\n∂\\n∑\\n\\ni∈I(op,iwi,Ω)\\n∂wi,Ω\\n\\n= op,i and therefore:\\n\\n∂Errp(W )\\n∂wi,Ω\\n\\n= −δp,Ω · op,i (5.15)\\n\\n= −op,i · δp,Ω. (5.16)\\n\\nWe insert this in equation 5.8 on the previ-\\nous page, which results in our modification\\nrule for a weight wi,Ω:\\n\\n∆wi,Ω = η ·\\n∑\\np∈P\\n\\nop,i · δp,Ω. (5.17)\\n\\nHowever: From the very beginning the\\nderivation has been intended as an offline\\nrule by means of the question of how to\\nadd the errors of all patterns and how to\\nlearn them after all patterns have been\\nrepresented. Although this approach is\\nmathematically correct, the implementa-\\ntion is far more time-consuming and, as\\nwe will see later in this chapter, partially\\n\\n80 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.2 Linear separability\\n\\nneeds a lot of compuational effort during\\ntraining.\\n\\nThe \\"online-learning version\\" of the delta\\nrule simply omits the summation and\\nlearning is realized immediately after the\\npresentation of each pattern, this also sim-\\nplifies the notation (which is no longer nec-\\nessarily related to a pattern p):\\n\\n∆wi,Ω = η · oi · δΩ. (5.18)\\n\\nThis version of the delta rule shall be used\\nfor the following definition:\\n\\nDefinition 5.6 (Delta rule). If we deter-\\nmine, analogously to the aforementioned\\nderivation, that the function h of the Heb-\\nbian theory (equation 4.6 on page 67) only\\nprovides the output oi of the predecessor\\nneuron i and if the function g is the differ-\\nence between the desired activation tΩ and\\nthe actual activation aΩ, we will receive\\nthe delta rule, also known as Widrow-\\nHoff rule:\\n\\n∆wi,Ω = η · oi · (tΩ − aΩ) = ηoiδΩ (5.19)\\n\\nIf we use the desired output (instead of the\\nactivation) as teaching input, and there-\\nfore the output function of the output neu-\\nrons does not represent an identity, we ob-\\ntain\\n\\n∆wi,Ω = η · oi · (tΩ − oΩ) = ηoiδΩ (5.20)\\n\\nand δΩ then corresponds to the difference\\nbetween tΩ and oΩ.\\n\\nIn the case of the delta rule, the change\\nof all weights to an output neuron Ω is\\nproportional\\n\\nIn. 1 In. 2 Output\\n0 0 0\\n0 1 1\\n1 0 1\\n1 1 0\\n\\nTable 5.1: Definition of the logical XOR. The\\ninput values are shown of the left, the output\\nvalues on the right.\\n\\n. to the difference between the current\\nactivation or output aΩ or oΩ and the\\ncorresponding teaching input tΩ. We\\nwant to refer to this factor as δΩ , Jδwhich is also referred to as \\"Delta\\".\\n\\nApparently the delta rule only applies for\\nSLPs, since the formula is always related\\nto the teaching input, and there is no\\n\\ndelta rule\\nonly for SLPteaching input for the inner processing lay-\\n\\ners of neurons.\\n\\n5.2 A SLP is only capable of\\nrepresenting linearly\\nseparable data\\n\\nLet f be the XOR function which expects\\ntwo binary inputs and generates a binary\\noutput (for the precise definition see ta-\\nble 5.1).\\n\\nLet us try to represent the XOR func-\\ntion by means of an SLP with two input\\nneurons i1, i2 and one output neuron Ω\\n(fig. 5.6 on the following page).\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 81\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\n�� ��GFED@ABCi1\\n\\nwi1,Ω\\nBBBB\\n\\n  BBBB\\n\\nGFED@ABCi2\\n\\nwi2,Ω\\n||||\\n\\n~~||||\\n\\n?>=<89:;Ω\\n\\n��\\nXOR?\\n\\nFigure 5.6: Sketch of a singlelayer perceptron\\nthat shall represent the XOR function - which is\\nimpossible.\\n\\nHere we use the weighted sum as propaga-\\ntion function, a binary activation function\\nwith the threshold value Θ and the iden-\\ntity as output function. Depending on i1\\nand i2, Ω has to output the value 1 if the\\nfollowing holds:\\n\\nnetΩ = oi1wi1,Ω + oi2wi2,Ω ≥ ΘΩ (5.21)\\n\\nWe assume a positive weight wi2,Ω, the in-\\nequality 5.21 is then equivalent to\\n\\noi1 ≥\\n1\\n\\nwi1,Ω\\n(ΘΩ − oi2wi2,Ω) (5.22)\\n\\nWith a constant threshold value ΘΩ, the\\nright part of inequation 5.22 is a straight\\nline through a coordinate system defined\\nby the possible outputs oi1 und oi2 of the\\ninput neurons i1 and i2 (fig. 5.7).\\n\\nFor a (as required for inequation 5.22) pos-\\nitive wi2,Ω the output neuron Ω fires for\\n\\nFigure 5.7: Linear separation of n = 2 inputs of\\nthe input neurons i1 and i2 by a 1-dimensional\\nstraight line. A and B show the corners belong-\\ning to the sets of the XOR function that are to\\nbe separated.\\n\\n82 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.2 Linear separability\\n\\nn number of\\nbinary\\nfunctions\\n\\nlin.\\nseparable\\nones\\n\\nshare\\n\\n1 4 4 100%\\n2 16 14 87.5%\\n3 256 104 40.6%\\n4 65, 536 1, 772 2.7%\\n5 4.3 · 109 94, 572 0.002%\\n6 1.8 · 1019 5, 028, 134 ≈ 0%\\n\\nTable 5.2: Number of functions concerning n bi-\\nnary inputs, and number and proportion of the\\nfunctions thereof which can be linearly separated.\\nIn accordance with [Zel94,Wid89,Was89].\\n\\ninput combinations lying above the gener-\\nated straight line. For a negative wi2,Ω it\\nwould fire for all input combinations lying\\nbelow the straight line. Note that only the\\nfour corners of the unit square are possi-\\nble inputs because the XOR function only\\nknows binary inputs.\\n\\nIn order to solve the XOR problem, we\\nhave to turn and move the straight line so\\nthat input set A = {(0, 0), (1, 1)} is sepa-\\nrated from input set B = {(0, 1), (1, 0)} –\\nthis is, obviously, impossible.\\n\\nGenerally, the input parameters of n many\\ninput neurons can be represented in an n-\\ndimensional cube which is separated by an\\n\\nSLP cannot\\ndo everything SLP through an (n−1)-dimensional hyper-\\n\\nplane (fig. 5.8). Only sets that can be sep-\\narated by such a hyperplane, i.e. which\\nare linearly separable, can be classified\\nby an SLP.\\n\\nFigure 5.8: Linear separation of n = 3 inputs\\nfrom input neurons i1, i2 and i3 by 2-dimensional\\nplane.\\n\\nUnfortunately, it seems that the percent-\\nage of the linearly separable problems\\nrapidly decreases with increasing n (see\\ntable 5.2), which limits the functionality\\n\\nfew tasks\\nare linearly\\nseparable\\n\\nof the SLP. Additionally, tests for linear\\nseparability are difficult. Thus, for more\\ndifficult tasks with more inputs we need\\nsomething more powerful than SLP. The\\nXOR problem itself is one of these tasks,\\nsince a perceptron that is supposed to rep-\\nresent the XOR function already needs a\\nhidden layer (fig. 5.9 on the next page).\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 83\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nGFED@ABC�\\n\\n1\\nAAAA\\n\\n  AAAA\\n\\n1\\n11111111\\n\\n��1\\n1111111\\n\\nGFED@ABC�\\n\\n1}}}}\\n\\n~~}}}}\\n\\n1\\r\\r\\r\\r\\r\\r\\r\\r\\n\\n��\\r\\r\\r\\r\\r\\r\\r\\r\\nGFED@ABC1.5\\n\\n−2\\n��GFED@ABC0.5\\n\\n��\\nXOR\\n\\nFigure 5.9: Neural network realizing the XOR\\nfunction. Threshold values (as far as they are\\nexisting) are located within the neurons.\\n\\n5.3 A multilayer perceptron\\ncontains more trainable\\nweight layers\\n\\nA perceptron with two or more trainable\\nweight layers (called multilayer perceptron\\nor MLP) is more powerful than an SLP. As\\nwe know, a singlelayer perceptron can di-\\nvide the input space by means of a hyper-\\nplane (in a two-dimensional input space\\nby means of a straight line). A two-\\nstage perceptron (two trainable weight lay-\\n\\nmore planes\\ners, three neuron layers) can classify con-\\nvex polygons by further processing these\\nstraight lines, e.g. in the form \\"recognize\\npatterns lying above straight line 1, be-\\nlow straight line 2 and below straight line\\n3\\". Thus, we – metaphorically speaking\\n- took an SLP with several output neu-\\nrons and \\"attached\\" another SLP (upper\\n\\npart of fig. 5.10 on the facing page). A\\nmultilayer perceptron represents an uni-\\nversal function approximator, which\\nis proven by the Theorem of Cybenko\\n[Cyb89].\\n\\nAnother trainable weight layer proceeds\\nanalogously, now with the convex poly-\\ngons. Those can be added, subtracted or\\nsomehow processed with other operations\\n(lower part of fig. 5.10 on the next page).\\n\\nGenerally, it can be mathematically\\nproven that even a multilayer perceptron\\nwith one layer of hidden neurons can ar-\\nbitrarily precisely approximate functions\\nwith only finitely many discontinuities as\\nwell as their first derivatives. Unfortu-\\nnately, this proof is not constructive and\\ntherefore it is left to us to find the correct\\nnumber of neurons and weights.\\n\\nIn the following we want to use a\\nwidespread abbreviated form for different\\nmultilayer perceptrons: We denote a two-\\nstage perceptron with 5 neurons in the in-\\nput layer, 3 neurons in the hidden layer\\nand 4 neurons in the output layer as a 5-\\n3-4-MLP.\\n\\nDefinition 5.7 (Multilayer perceptron).\\nPerceptrons with more than one layer of\\nvariably weighted connections are referred\\nto as multilayer perceptrons (MLP).\\nAn n-layer or n-stage perceptron has\\nthereby exactly n variable weight layers\\nand n + 1 neuron layers (the retina is dis-\\nregarded here) with neuron layer 1 being\\nthe input layer.\\n\\nSince three-stage perceptrons can classify\\nsets of any form by combining and sepa- 3-stage\\n\\nMLP is\\nsufficient\\n\\n84 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.3 The multilayer perceptron\\n\\nGFED@ABCi1\\n\\n�����������\\n\\n��@@@@@@@@@\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\\n\\n�����������\\n\\n��@@@@@@@@@\\n\\nttjjjjjjjjjjjjjjjjjjjjjjjjj\\n\\nGFED@ABCh1\\n\\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCh2\\n\\n��\\n\\nGFED@ABCh3\\n\\nwwooooooooooooooooo\\n\\n?>=<89:;Ω\\n\\n��\\n\\nGFED@ABCi1\\n\\n~~~~~~~~~~~\\n\\n��   @@@@@@@@@\\n\\n\'\' )) **\\n\\nGFED@ABCi2\\n\\ntt uu ww ~~~~~~~~~~~\\n\\n��   @@@@@@@@@\\n\\nGFED@ABCh1\\n\\n\'\'PPPPPPPPPPPPPPPPP\\n\\n--\\n\\nGFED@ABCh2\\n\\n  @@@@@@@@@\\n\\n,,\\n\\nGFED@ABCh3\\n\\n��\\n**\\n\\nGFED@ABCh4\\n\\n��\\ntt\\n\\nGFED@ABCh5\\n\\n~~~~~~~~~~~\\n\\nrr\\n\\nGFED@ABCh6\\n\\nwwnnnnnnnnnnnnnnnnn\\n\\nqqGFED@ABCh7\\n\\n��@@@@@@@@@\\nGFED@ABCh8\\n\\n��~~~~~~~~~\\n\\n?>=<89:;Ω\\n\\n��\\n\\nFigure 5.10: We know that an SLP represents a straight line. With 2 trainable weight layers,\\nseveral straight lines can be combined to form convex polygons (above). By using 3 trainable\\nweight layers several polygons can be formed into arbitrary sets (below).\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 85\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nn classifiable sets\\n1 hyperplane\\n2 convex polygon\\n3 any set\\n4 any set as well, i.e. no\\n\\nadvantage\\n\\nTable 5.3: Representation of which perceptron\\ncan classify which types of sets with n being the\\nnumber of trainable weight layers.\\n\\nrating arbitrarily many convex polygons,\\nanother step will not be advantageous\\nwith respect to function representations.\\nBe cautious when reading the literature:\\nThere are many different definitions of\\nwhat is counted as a layer. Some sources\\ncount the neuron layers, some count the\\nweight layers. Some sources include the\\nretina, some the trainable weight layers.\\nSome exclude (for some reason) the out-\\nput neuron layer. In this work, I chose\\nthe definition that provides, in my opinion,\\nthe most information about the learning\\ncapabilities – and I will use it cosistently.\\nRemember: An n-stage perceptron has ex-\\nactly n trainable weight layers. You can\\nfind a summary of which perceptrons can\\nclassify which types of sets in table 5.3.\\nWe now want to face the challenge of train-\\ning perceptrons with more than one weight\\nlayer.\\n\\n5.4 Backpropagation of error\\ngeneralizes the delta rule\\nto allow for MLP training\\n\\nNext, I want to derive and explain the\\nbackpropagation of error learning rule\\n(abbreviated: backpropagation, backprop\\nor BP), which can be used to train multi-\\nstage perceptrons with semi-linear3 activa-\\ntion functions. Binary threshold functions\\nand other non-differentiable functions are\\nno longer supported, but that doesn’t mat-\\nter: We have seen that the Fermi func-\\ntion or the hyperbolic tangent can arbi-\\ntrarily approximate the binary threshold\\nfunction by means of a temperature pa-\\nrameter T . To a large extent I will fol-\\nlow the derivation according to [Zel94] and\\n[MR86]. Once again I want to point out\\nthat this procedure had previously been\\npublished by Paul Werbos in [Wer74]\\nbut had consideraby less readers than in\\n[MR86].\\n\\nBackpropagation is a gradient descent pro-\\ncedure (including all strengths and weak-\\nnesses of the gradient descent) with the\\nerror function Err(W ) receiving all n\\nweights as arguments (fig. 5.5 on page 78)\\nand assigning them to the output error, i.e.\\nbeing n-dimensional. On Err(W ) a point\\nof small error or even a point of the small-\\nest error is sought by means of the gradi-\\nent descent. Thus, in analogy to the delta\\nrule, backpropagation trains the weights\\nof the neural network. And it is exactly\\n\\n3 Semilinear functions are monotonous and differen-\\ntiable – but generally they are not linear.\\n\\n86 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.4 Backpropagation of error\\n\\nthe delta rule or its variable δi for a neu-\\nron i which is expanded from one trainable\\nweight layer to several ones by backpropa-\\ngation.\\n\\n5.4.1 The derivation is similar to\\nthe one of the delta rule, but\\nwith a generalized delta\\n\\nLet us define in advance that the network\\ninput of the individual neurons i results\\nfrom the weighted sum. Furthermore, as\\nwith the derivation of the delta rule, let\\nop,i, netp,i etc. be defined as the already\\nfamiliar oi, neti, etc. under the input pat-\\ntern p we used for the training. Let the\\noutput function be the identity again, thus\\noi = fact(netp,i) holds for any neuron i.\\nSince this is a generalization of the delta\\nrule, we use the same formula framework\\nas with the delta rule (equation 5.20 on\\n\\ngeneral-\\nization\\n\\nof δ\\npage 81). As already indicated, we have\\nto generalize the variable δ for every neu-\\nron.\\n\\nFirst of all: Where is the neuron for which\\nwe want to calculate δ? It is obvious to\\nselect an arbitrary inner neuron h having\\na set K of predecessor neurons k as well\\nas a set of L successor neurons l, which\\nare also inner neurons (see fig. 5.11). It\\nis therefore irrelevant whether the prede-\\ncessor neurons are already the input neu-\\nrons.\\n\\nNow we perform the same derivation as\\nfor the delta rule and split functions by\\nmeans the chain rule. I will not discuss\\nthis derivation in great detail, but the prin-\\ncipal is similar to that of the delta rule (the\\n\\n/.-,()*+\\n\\n&&LLLLLLLLLLLLLLL /.-,()*+\\n\\n��========== /.-,()*+\\n\\n��\\n\\n. . . ?>=<89:;k\\nwk,h\\n\\npppppppp\\n\\nwwppppppp\\n\\nK\\n\\nONMLHIJKΣ\\nfact\\n\\nxxrrrrrrrrrrrrrrr\\n\\n������������\\n\\n��\\n\\nwh,l\\nNNNNNNN\\n\\n\'\'NNNNNNNN\\n\\nh H\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ . . . ?>=<89:;l L\\n\\nFigure 5.11: Illustration of the position of our\\nneuron h within the neural network. It is lying in\\nlayerH, the preceding layer isK, the subsequent\\nlayer is L.\\n\\ndifferences are, as already mentioned, in\\nthe generalized δ). We initially derive the\\nerror function Err according to a weight\\nwk,h.\\n\\n∂Err(wk,h)\\n∂wk,h\\n\\n= ∂Err\\n∂neth︸ ︷︷ ︸\\n=−δh\\n\\n·∂neth\\n∂wk,h\\n\\n(5.23)\\n\\nThe first factor of equation 5.23 is −δh,\\nwhich we will deal with later in this text.\\nThe numerator of the second factor of the\\nequation includes the network input, i.e.\\nthe weighted sum is included in the numer-\\nator so that we can immediately derive it.\\nAgain, all summands of the sum drop out\\napart from the summand containing wk,h.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 87\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nThis summand is referred to as wk,h ·ok. If\\nwe calculate the derivative, the output of\\nneuron k becomes:\\n\\n∂neth\\n∂wk,h\\n\\n= ∂\\n∑\\nk∈K wk,hok\\n∂wk,h\\n\\n(5.24)\\n\\n= ok (5.25)\\n\\nAs promised, we will now discuss the −δh\\nof equation 5.23 on the previous page,\\nwhich is split up again according of the\\nchain rule:\\n\\nδh = − ∂Err\\n∂neth\\n\\n(5.26)\\n\\n= −∂Err\\n∂oh\\n\\n· ∂oh\\n∂neth\\n\\n(5.27)\\n\\nThe derivation of the output according to\\nthe network input (the second factor in\\nequation 5.27) clearly equals the deriva-\\ntion of the activation function according\\nto the network input:\\n\\n∂oh\\n∂neth\\n\\n= ∂fact(neth)\\n∂neth\\n\\n(5.28)\\n\\n= fact\\n′(neth) (5.29)\\n\\nConsider this an important passage! We\\nnow analogously derive the first factor in\\nequation 5.27. Therefore, we have to point\\nout that the derivation of the error func-\\ntion according to the output of an inner\\nneuron layer depends on the vector of all\\nnetwork inputs of the next following layer.\\nThis is reflected in equation 5.30:\\n\\n−∂Err\\n∂oh\\n\\n= −\\n∂Err(netl1 , . . . ,netl|L|)\\n\\n∂oh\\n(5.30)\\n\\nAccording to the definition of the multi-\\ndimensional chain rule, we immediately ob-\\ntain equation 5.31:\\n\\n−∂Err\\n∂oh\\n\\n=\\n∑\\nl∈L\\n\\n(\\n− ∂Err\\n∂netl\\n\\n· ∂netl\\n∂oh\\n\\n)\\n(5.31)\\n\\nThe sum in equation 5.31 contains two fac-\\ntors. Now we want to discuss these factors\\nbeing added over the subsequent layer L.\\nWe simply calculate the second factor in\\nthe following equation 5.33:\\n\\n∂netl\\n∂oh\\n\\n= ∂\\n∑\\nh∈H wh,l · oh\\n∂oh\\n\\n(5.32)\\n\\n= wh,l (5.33)\\n\\nThe same applies for the first factor accord-\\ning to the definition of our δ:\\n\\n− ∂Err\\n∂netl\\n\\n= δl (5.34)\\n\\nNow we insert:\\n\\n⇒ −∂Err\\n∂oh\\n\\n=\\n∑\\nl∈L\\n\\nδlwh,l (5.35)\\n\\nYou can find a graphic version of the δ\\ngeneralization including all splittings in\\nfig. 5.12 on the facing page.\\n\\nThe reader might already have noticed\\nthat some intermediate results were shown\\nin frames. Exactly those intermediate re-\\nsults were highlighted in that way, which\\nare a factor in the change in weight of\\nwk,h. If the aforementioned equations are\\n\\n88 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.4 Backpropagation of error\\n\\nδh\\n\\n− ∂Err\\n∂neth\\n\\n�� ��\\n∂oh\\n∂neth −∂Err\\n\\n∂oh\\n\\n\\r\\r ��\\nf ′act(neth) − ∂Err\\n\\n∂netl\\n∑\\nl∈L\\n\\n∂netl\\n∂oh\\n\\nδl\\n∂\\n∑\\n\\nh∈H wh,l·oh\\n∂oh\\n\\nwh,l\\n\\nFigure 5.12: Graphical representation of the equations (by equal signs) and chain rule splittings\\n(by arrows) in the framework of the backpropagation derivation. The leaves of the tree reflect the\\nfinal results from the generalization of δ, which are framed in the derivation.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 89\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\ncombined with the highlighted intermedi-\\nate results, the outcome of this will be the\\nwanted change in weight ∆wk,h to\\n\\n∆wk,h = ηokδh with (5.36)\\n\\nδh = f ′act(neth) ·\\n∑\\nl∈L\\n\\n(δlwh,l)\\n\\n– of course only in case of h being an inner\\nneuron (otherweise there would not be a\\nsubsequent layer L).\\n\\nThe case of h being an output neuron has\\nalready been discussed during the deriva-\\ntion of the delta rule. All in all, the re-\\nsult is the generalization of the delta rule,\\ncalled backpropagation of error :\\n\\n∆wk,h = ηokδh with\\n\\nδh =\\n{\\nf ′act(neth) · (th − yh) (h outside)\\nf ′act(neth) ·∑l∈L(δlwh,l) (h inside)\\n\\n(5.37)\\n\\nIn contrast to the delta rule, δ is treated\\ndifferently depending on whether h is an\\noutput or an inner (i.e. hidden) neuron:\\n\\n1. If h is an output neuron, then\\n\\nδp,h = f ′act(netp,h) · (tp,h − yp,h)\\n(5.38)\\n\\nThus, under our training pattern p\\nthe weight wk,h from k to h is changed\\nproportionally according to\\n\\n. the learning rate η,\\n\\n. the output op,k of the predeces-\\nsor neuron k,\\n\\n. the gradient of the activation\\nfunction at the position of the\\nnetwork input of the successor\\nneuron f ′act(netp,h) and\\n\\n. the difference between teaching\\ninput tp,h and output yp,h of the\\nsuccessor neuron h.\\n\\nTeach. Input\\nchanged for\\nthe outer\\nweight layer\\n\\nIn this case, backpropagation is work-\\ning on two neuron layers, the output\\nlayer with the successor neuron h and\\nthe preceding layer with the predeces-\\nsor neuron k.\\n\\n2. If h is an inner, hidden neuron, then\\n\\nδp,h = f ′act(netp,h) ·\\n∑\\nl∈L\\n\\n(δp,l · wh,l)\\n\\n(5.39)\\n\\nholds. I want to explicitly mention\\nback-\\npropagation\\nfor inner\\nlayers\\n\\nthat backpropagation is now working\\non three layers. Here, neuron k is\\nthe predecessor of the connection to\\nbe changed with the weight wk,h, the\\nneuron h is the successor of the con-\\nnection to be changed and the neu-\\nrons l are lying in the layer follow-\\ning the successor neuron. Thus, ac-\\ncording to our training pattern p, the\\nweight wk,h from k to h is proportion-\\nally changed according to\\n\\n. the learning rate η,\\n\\n. the output of the predecessor\\nneuron op,k,\\n\\n. the gradient of the activation\\nfunction at the position of the\\nnetwork input of the successor\\nneuron f ′act(netp,h),\\n\\n. as well as, and this is the\\ndifference, according to the\\nweighted sum of the changes in\\nweight to all neurons following h,∑\\nl∈L(δp,l · wh,l).\\n\\n90 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.4 Backpropagation of error\\n\\nDefinition 5.8 (Backpropagation). If we\\nsummarize formulas 5.38 on the preceding\\npage and 5.39 on the facing page, we re-\\nceive the following final formula for back-\\npropagation (the identifiers p are om-\\nmited for reasons of clarity):\\n\\n∆wk,h = ηokδh with\\n\\nδh =\\n{\\nf ′act(neth) · (th − yh) (h outside)\\nf ′act(neth) ·∑l∈L(δlwh,l) (h inside)\\n\\n(5.40)\\n\\nSNIPE: An online variant of backpro-\\npagation is implemented in the method\\ntrainBackpropagationOfError within the\\nclass NeuralNetwork.\\n\\nIt is obvious that backpropagation ini-\\ntially processes the last weight layer di-\\nrectly by means of the teaching input and\\nthen works backwards from layer to layer\\nwhile considering each preceding change in\\nweights. Thus, the teaching input leaves\\ntraces in all weight layers. Here I describe\\nthe first (delta rule) and the second part\\nof backpropagation (generalized delta rule\\non more layers) in one go, which may meet\\nthe requirements of the matter but not\\nof the research. The first part is obvious,\\nwhich you will soon see in the framework\\nof a mathematical gimmick. Decades of\\ndevelopment time and work lie between the\\nfirst and the second, recursive part. Like\\nmany groundbreaking inventions, it was\\nnot until its development that it was recog-\\nnized how plausible this invention was.\\n\\n5.4.2 Heading back: Boiling\\nbackpropagation down to\\ndelta rule\\n\\nAs explained above, the delta rule is a\\nspecial case of backpropagation for one-\\nstage perceptrons and linear activation\\nfunctions – I want to briefly explain this\\n\\nbackprop\\nexpands\\ndelta rule\\n\\ncircumstance and develop the delta rule\\nout of backpropagation in order to aug-\\nment the understanding of both rules. We\\nhave seen that backpropagation is defined\\nby\\n\\n∆wk,h = ηokδh with\\n\\nδh =\\n{\\nf ′act(neth) · (th − yh) (h outside)\\nf ′act(neth) ·∑l∈L(δlwh,l) (h inside)\\n\\n(5.41)\\n\\nSince we only use it for one-stage percep-\\ntrons, the second part of backpropagation\\n(light-colored) is omitted without substitu-\\ntion. The result is:\\n\\n∆wk,h = ηokδh with\\nδh = f ′act(neth) · (th − oh) (5.42)\\n\\nFurthermore, we only want to use linear\\nactivation functions so that f ′act (light-\\ncolored) is constant. As is generally\\nknown, constants can be combined, and\\ntherefore we directly merge the constant\\nderivative f ′act and (being constant for at\\nleast one lerning cycle) the learning rate η\\n(also light-colored) in η. Thus, the result\\nis:\\n\\n∆wk,h = ηokδh = ηok · (th − oh) (5.43)\\n\\nThis exactly corresponds to the delta rule\\ndefinition.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 91\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\n5.4.3 The selection of the learning\\nrate has heavy influence on\\nthe learning process\\n\\nIn the meantime we have often seen that\\nthe change in weight is, in any case, pro-\\nportional to the learning rate η. Thus, the\\nselection of η is crucial for the behaviour\\nof backpropagation and for learning proce-\\ndures in general.\\n\\nhow fast\\nwill be\\n\\nlearned? Definition 5.9 (Learning rate). Speed\\nand accuracy of a learning procedure can\\nalways be controlled by and are always pro-\\nportional to a learning rate which is writ-\\nten as η.\\n\\nηI\\n\\nIf the value of the chosen η is too large,\\nthe jumps on the error surface are also\\ntoo large and, for example, narrow valleys\\ncould simply be jumped over. Addition-\\nally, the movements across the error sur-\\nface would be very uncontrolled. Thus, a\\nsmall η is the desired input, which, how-\\never, can cost a huge, often unacceptable\\namount of time. Experience shows that\\ngood learning rate values are in the range\\nof\\n\\n0.01 ≤ η ≤ 0.9.\\n\\nThe selection of η significantly depends on\\nthe problem, the network and the training\\ndata, so that it is barely possible to give\\npractical advise. But for instance it is pop-\\nular to start with a relatively large η, e.g.\\n0.9, and to slowly decrease it down to 0.1.\\nFor simpler problems η can often be kept\\nconstant.\\n\\n5.4.3.1 Variation of the learning rate\\nover time\\n\\nDuring training, another stylistic device\\ncan be a variable learning rate: In the\\nbeginning, a large learning rate leads to\\ngood results, but later it results in inac-\\ncurate learning. A smaller learning rate\\nis more time-consuming, but the result is\\nmore precise. Thus, during the learning\\nprocess the learning rate needs to be de-\\ncreased by one order of magnitude once or\\nrepeatedly.\\n\\nA common error (which also seems to be a\\nvery neat solution at first glance) is to con-\\ntinually decrease the learning rate. Here\\nit quickly happens that the descent of the\\nlearning rate is larger than the ascent of\\na hill of the error function we are climb-\\ning. The result is that we simply get stuck\\nat this ascent. Solution: Rather reduce\\nthe learning rate gradually as mentioned\\nabove.\\n\\n5.4.3.2 Different layers – Different\\nlearning rates\\n\\nThe farer we move away from the out-\\nput layer during the learning process, the\\nslower backpropagation is learning. Thus,\\nit is a good idea to select a larger learning\\nrate for the weight layers close to the in-\\nput layer than for the weight layers close\\nto the output layer.\\n\\n92 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.5 Resilient backpropagation\\n\\n5.5 Resilient backpropagation\\nis an extension to\\nbackpropagation of error\\n\\nWe have just raised two backpropagation-\\nspecific properties that can occasionally be\\na problem (in addition to those which are\\nalready caused by gradient descent itself):\\nOn the one hand, users of backpropaga-\\ntion can choose a bad learning rate. On\\nthe other hand, the further the weights are\\nfrom the output layer, the slower backpro-\\npagation learns. For this reason, Mar-\\ntin Riedmiller et al. enhanced back-\\npropagation and called their version re-\\nsilient backpropagation (short Rprop)\\n[RB93, Rie94]. I want to compare back-\\npropagation and Rprop, without explic-\\nitly declaring one version superior to the\\nother. Before actually dealing with formu-\\nlas, let us informally compare the two pri-\\nmary ideas behind Rprop (and their con-\\nsequences) to the already familiar backpro-\\npagation.\\n\\nLearning rates: Backpropagation uses by\\ndefault a learning rate η, which is se-\\nlected by the user, and applies to the\\nentire network. It remains static un-\\ntil it is manually changed. We have\\nalready explored the disadvantages of\\nthis approach. Here, Rprop pursues a\\ncompletely different approach: there\\nis no global learning rate. First, each\\nweight wi,j has its own learning rate\\n\\nOne learning-\\nrate per\\nweight\\n\\nηi,jI\\n\\nηi,j , and second, these learning rates\\nare not chosen by the user, but are au-\\ntomatically set by Rprop itself. Third,\\n\\nautomatic\\nlearning rate\\nadjustment\\n\\nthe weight changes are not static but\\n\\nare adapted for each time step of\\nRprop. To account for the temporal\\nchange, we have to correctly call it\\nηi,j(t). This not only enables more\\nfocused learning, also the problem of\\nan increasingly slowed down learning\\nthroughout the layers is solved in an\\nelegant way.\\n\\nWeight change: When using backpropa-\\ngation, weights are changed propor-\\ntionally to the gradient of the error\\nfunction. At first glance, this is really\\nintuitive. However, we incorporate ev-\\nery jagged feature of the error surface\\ninto the weight changes. It is at least\\nquestionable, whether this is always\\nuseful. Here, Rprop takes other ways\\nas well: the amount of weight change\\n∆wi,j simply directly corresponds to\\nthe automatically adjusted learning\\nrate ηi,j . Thus the change in weight is\\nnot proportional to the gradient, it is\\nonly influenced by the sign of the gra-\\ndient. Until now we still do not know\\nhow exactly the ηi,j are adapted at\\nrun time, but let me anticipate that\\nthe resulting process looks consider-\\n\\nMuch\\nsmoother learningably less rugged than an error func-\\n\\ntion.\\n\\nIn contrast to backprop the weight update\\nstep is replaced and an additional step\\nfor the adjustment of the learning rate is\\nadded. Now how exactly are these ideas\\nbeing implemented?\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 93\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\n5.5.1 Weight changes are not\\nproportional to the gradient\\n\\nLet us first consider the change in weight.\\nWe have already noticed that the weight-\\nspecific learning rates directly serve as ab-\\nsolute values for the changes of the re-\\nspective weights. There remains the ques-\\ntion of where the sign comes from – this\\nis a point at which the gradient comes\\ninto play. As with the derivation of back-\\npropagation, we derive the error function\\nErr(W ) by the individual weights wi,j and\\nobtain gradients ∂Err(W )\\n\\n∂wi,j\\n. Now, the big\\n\\ndifference: rather than multiplicatively\\nincorporating the absolute value of the\\ngradient into the weight change, we con-\\nsider only the sign of the gradient. The\\ngradient hence no longer determines the\\n\\ngradient\\ndetermines only\\ndirection of the\\n\\nupdates\\n\\nstrength, but only the direction of the\\nweight change.\\n\\nIf the sign of the gradient ∂Err(W )\\n∂wi,j\\n\\nis pos-\\nitive, we must decrease the weight wi,j .\\nSo the weight is reduced by ηi,j . If the\\nsign of the gradient is negative, the weight\\nneeds to be increased. So ηi,j is added to\\nit. If the gradient is exactly 0, nothing\\nhappens at all. Let us now create a for-\\nmula from this colloquial description. The\\ncorresponding terms are affixed with a (t)\\nto show that everything happens at the\\nsame time step. This might decrease clar-\\nity at first glance, but is nevertheless im-\\nportant because we will soon look at an-\\nother formula that operates on different\\ntime steps. Instead, we shorten the gra-\\ndient to: g = ∂Err(W )\\n\\n∂wi,j\\n.\\n\\nDefinition 5.10 (Weight change in\\nRprop).\\n\\n∆wi,j(t) =\\n\\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\\n−ηi,j(t), if g(t) > 0\\n+ηi,j(t), if g(t) < 0\\n0 otherwise.\\n\\n(5.44)\\n\\nWe now know how the weights are changed\\n– now remains the question how the learn-\\ning rates are adjusted. Finally, once we\\nhave understood the overall system, we\\nwill deal with the remaining details like ini-\\ntialization and some specific constants.\\n\\n5.5.2 Many dynamically adjusted\\nlearning rates instead of one\\nstatic\\n\\nTo adjust the learning rate ηi,j , we again\\nhave to consider the associated gradients\\ng of two time steps: the gradient that has\\njust passed (t − 1) and the current one\\n(t). Again, only the sign of the gradient\\nmatters, and we now must ask ourselves:\\nWhat can happen to the sign over two time\\nsteps? It can stay the same, and it can\\nflip.\\n\\nIf the sign changes from g(t − 1) to g(t),\\nwe have skipped a local minimum in the\\ngradient. Hence, the last update was too\\nlarge and ηi,j(t) has to be reduced as com-\\npared to the previous ηi,j(t− 1). One can\\nsay, that the search needs to be more accu-\\nrate. In mathematical terms, we obtain a\\nnew ηi,j(t) by multiplying the old ηi,j(t−1)\\nwith a constant η↓, which is between 1 and\\n\\nJη↓0. In this case we know that in the last\\ntime step (t − 1) something went wrong –\\n\\n94 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.5 Resilient backpropagation\\n\\nhence we additionally reset the weight up-\\ndate for the weight wi,j at time step (t) to\\n0, so that it not applied at all (not shown\\nin the following formula).\\n\\nHowever, if the sign remains the same, one\\ncan perform a (careful!) increase of ηi,j to\\nget past shallow areas of the error function.\\nHere we obtain our new ηi,j(t) by multiply-\\ning the old ηi,j(t − 1) with a constant η↑\\n\\nη↑I which is greater than 1.\\n\\nDefinition 5.11 (Adaptation of learning\\nrates in Rprop).\\n\\nηi,j(t) =\\n\\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\\nη↑ηi,j(t− 1), g(t− 1)g(t) > 0\\nη↓ηi,j(t− 1), g(t− 1)g(t) < 0\\nηi,j(t− 1) otherwise.\\n\\n(5.45)\\n\\nCaution: This also implies that Rprop is\\nRprop only\\n\\nlearns\\noffline\\n\\nexclusively designed for offline. If the gra-\\ndients do not have a certain continuity, the\\nlearning process slows down to the lowest\\nrates (and remains there). When learning\\nonline, one changes – loosely speaking –\\nthe error function with each new epoch,\\nsince it is based on only one training pat-\\ntern. This may be often well applicable\\nin backpropagation and it is very often\\neven faster than the offline version, which\\nis why it is used there frequently. It lacks,\\nhowever, a clear mathematical motivation,\\nand that is exactly what we need here.\\n\\n5.5.3 We are still missing a few\\ndetails to use Rprop in\\npractice\\n\\nA few minor issues remain unanswered,\\nnamely\\n\\n1. How large are η↑ and η↓ (i.e. how\\nmuch are learning rates reinforced or\\nweakened)?\\n\\n2. How to choose ηi,j(0) (i.e. how are\\nthe weight-specific learning rates ini-\\ntialized)?4\\n\\n3. What are the upper and lower bounds\\nηmin and ηmax for ηi,j set? Jηmin\\n\\nJηmaxWe now answer these questions with a\\nquick motivation. The initial value for the\\nlearning rates should be somewhere in the\\norder of the initialization of the weights.\\nηi,j(0) = 0.1 has proven to be a good\\nchoice. The authors of the Rprop paper\\nexplain in an obvious way that this value\\n– as long as it is positive and without an ex-\\norbitantly high absolute value – does not\\nneed to be dealt with very critically, as\\nit will be quickly overridden by the auto-\\nmatic adaptation anyway.\\n\\nEqually uncritical is ηmax, for which they\\nrecommend, without further mathemati-\\ncal justification, a value of 50 which is used\\nthroughout most of the literature. One\\ncan set this parameter to lower values in\\norder to allow only very cautious updates.\\nSmall update steps should be allowed in\\nany case, so we set ηmin = 10−6.\\n\\n4 Protipp: since the ηi,j can be changed only by\\nmultiplication, 0 would be a rather suboptimal ini-\\ntialization :-)\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 95\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nNow we have left only the parameters η↑\\nand η↓. Let us start with η↓: If this value\\nis used, we have skipped a minimum, from\\nwhich we do not know where exactly it lies\\non the skipped track. Analogous to the\\nprocedure of binary search, where the tar-\\nget object is often skipped as well, we as-\\nsume it was in the middle of the skipped\\ntrack. So we need to halve the learning\\nrate, which is why the canonical choice\\nη↓ = 0.5 is being selected. If the value\\nof η↑ is used, learning rates shall be in-\\ncreased with caution. Here we cannot gen-\\neralize the principle of binary search and\\nsimply use the value 2.0, otherwise the\\nlearning rate update will end up consist-\\ning almost exclusively of changes in direc-\\ntion. Independent of the particular prob-\\nlems, a value of η↑ = 1.2 has proven to\\nbe promising. Slight changes of this value\\nhave not significantly affected the rate of\\nconvergence. This fact allowed for setting\\nthis value as a constant as well.\\n\\nWith advancing computational capabili-\\nties of computers one can observe a more\\nand more widespread distribution of net-\\nworks that consist of a big number of lay-\\ners, i.e. deep networks. For such net-\\n\\nRprop is very\\ngood for\\n\\ndeep networks\\nworks it is crucial to prefer Rprop over the\\noriginal backpropagation, because back-\\nprop, as already indicated, learns very\\nslowly at weights wich are far from the\\noutput layer. For problems with a smaller\\nnumber of layers, I would recommend test-\\ning the more widespread backpropagation\\n(with both offline and online learning) and\\nthe less common Rprop equivalently.\\n\\nSNIPE: In Snipe resilient backpropa-\\ngation is supported via the method\\ntrainResilientBackpropagation of the\\nclass NeuralNetwork. Furthermore, you\\ncan also use an additional improvement\\nto resilient propagation, which is, however,\\nnot dealt with in this work. There are get-\\nters and setters for the different parameters\\nof Rprop.\\n\\n5.6 Backpropagation has\\noften been extended and\\naltered besides Rprop\\n\\nBackpropagation has often been extended.\\nMany of these extensions can simply be im-\\nplemented as optional features of backpro-\\npagation in order to have a larger scope for\\ntesting. In the following I want to briefly\\ndescribe some of them.\\n\\n5.6.1 Adding momentum to\\nlearning\\n\\nLet us assume to descent a steep slope\\non skis - what prevents us from immedi-\\nately stopping at the edge of the slope\\nto the plateau? Exactly - our momen-\\ntum. With backpropagation the momen-\\ntum term [RHW86b] is responsible for the\\nfact that a kind of moment of inertia\\n(momentum) is added to every step size\\n(fig. 5.13 on the next page), by always\\nadding a fraction of the previous change\\nto every new change in weight:\\n\\n(∆pwi,j)now = ηop,iδp,j+α·(∆pwi,j)previous.\\n\\n96 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.6 Further variations and extensions to backpropagation\\n\\nOf course, this notation is only used for\\na better understanding. Generally, as al-\\nready defined by the concept of time, when\\nreferring to the current cycle as (t), then\\nthe previous cycle is identified by (t − 1),\\nwhich is continued successively. And now\\nwe come to the formal definition of the mo-\\nmentum term:\\n\\nDefinition 5.12 (Momentum term). The\\nmoment of\\n\\ninertia variation of backpropagation by means of\\nthe momentum term is defined as fol-\\nlows:\\n\\n∆wi,j(t) = ηoiδj + α ·∆wi,j(t− 1) (5.46)\\n\\nWe accelerate on plateaus (avoiding quasi-\\nstandstill on plateaus) and slow down on\\ncraggy surfaces (preventing oscillations).\\nMoreover, the effect of inertia can be var-\\nied via the prefactor α, common val-\\n\\nαI ues are between 0.6 und 0.9. Addition-\\nally, the momentum enables the positive\\neffect that our skier swings back and\\nforth several times in a minimum, and fi-\\nnally lands in the minimum. Despite its\\nnice one-dimensional appearance, the oth-\\nerwise very rare error of leaving good min-\\nima unfortunately occurs more frequently\\nbecause of the momentum term – which\\nmeans that this is again no optimal solu-\\ntion (but we are by now accustomed to\\nthis condition).\\n\\n5.6.2 Flat spot elimination prevents\\nneurons from getting stuck\\n\\nIt must be pointed out that with the hy-\\nperbolic tangent as well as with the Fermi\\n\\nFigure 5.13: We want to execute the gradient\\ndescent like a skier crossing a slope, who would\\nhardly stop immediately at the edge to the\\nplateau.\\n\\nfunction the derivative outside of the close\\nproximity of Θ is nearly 0. This results\\nin the fact that it becomes very difficult\\nto move neurons away from the limits of\\nthe activation (flat spots), which could ex- neurons\\n\\nget stucktremely extend the learning time. This\\nproblem can be dealt with by modifying\\nthe derivative, for example by adding a\\nconstant (e.g. 0.1), which is called flat\\nspot elimination or – more colloquial –\\nfudging.\\n\\nIt is an interesting observation, that suc-\\ncess has also been achieved by using deriva-\\ntives defined as constants [Fah88]. A nice\\nexample making use of this effect is the\\nfast hyperbolic tangent approximation by\\nAnguita et al. introduced in section 3.2.6\\non page 37. In the outer regions of it’s (as\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 97\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nwell approximated and accelerated) deriva-\\ntive, it makes use of a small constant.\\n\\n5.6.3 The second derivative can be\\nused, too\\n\\nAccording to David Parker [Par87],\\nSecond order backpropagation also us-\\nese the second gradient, i.e. the second\\nmulti-dimensional derivative of the error\\nfunction, to obtain more precise estimates\\nof the correct ∆wi,j . Even higher deriva-\\ntives only rarely improve the estimations.\\nThus, less training cycles are needed but\\nthose require much more computational ef-\\nfort.\\n\\nIn general, we use further derivatives (i.e.\\nHessian matrices, since the functions are\\nmultidimensional) for higher order meth-\\nods. As expected, the procedures reduce\\nthe number of learning epochs, but signifi-\\ncantly increase the computational effort of\\nthe individual epochs. So in the end these\\nprocedures often need more learning time\\nthan backpropagation.\\n\\nThe quickpropagation learning proce-\\ndure [Fah88] uses the second derivative of\\nthe error propagation and locally under-\\nstands the error function to be a parabola.\\nWe analytically determine the vertex (i.e.\\nthe lowest point) of the said parabola and\\ndirectly jump to this point. Thus, this\\nlearning procedure is a second-order proce-\\ndure. Of course, this does not work with\\nerror surfaces that cannot locally be ap-\\nproximated by a parabola (certainly it is\\nnot always possible to directly say whether\\nthis is the case).\\n\\n5.6.4 Weight decay: Punishment of\\nlarge weights\\n\\nThe weight decay according to Paul\\nWerbos [Wer88] is a modification that ex-\\ntends the error by a term punishing large\\nweights. So the error under weight de-\\ncay\\n\\nErrWD\\n\\ndoes not only increase proportionally to JErrWDthe actual error but also proportionally to\\nthe square of the weights. As a result the\\nnetwork is keeping the weights small dur-\\ning learning.\\n\\nErrWD = Err + β · 1\\n2\\n∑\\nw∈W\\n\\n(w)2\\n\\n︸ ︷︷ ︸\\npunishment\\n\\n(5.47)\\n\\nThis approach is inspired by nature where\\nsynaptic weights cannot become infinitely\\nstrong as well. Additionally, due to these\\n\\nkeep weights\\nsmallsmall weights, the error function often\\n\\nshows weaker fluctuations, allowing easier\\nand more controlled learning.\\n\\nThe prefactor 1\\n2 again resulted from sim-\\n\\nple pragmatics. The factor β controls the Jβstrength of punishment: Values from 0.001\\nto 0.02 are often used here.\\n\\n5.6.5 Cutting networks down:\\nPruning and Optimal Brain\\nDamage\\n\\nIf we have executed the weight decay long\\nenough and notice that for a neuron in\\nthe input layer all successor weights are\\n\\nprune the\\nnetwork0 or close to 0, we can remove the neuron,\\n\\n98 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.7 Initial configuration of a multilayer perceptron\\n\\nhence losing this neuron and some weights\\nand thereby reduce the possibility that the\\nnetwork will memorize. This procedure is\\ncalled pruning.\\n\\nSuch a method to detect and delete un-\\nnecessary weights and neurons is referred\\nto as optimal brain damage [lCDS90].\\nI only want to describe it briefly: The\\nmean error per output neuron is composed\\nof two competing terms. While one term,\\nas usual, considers the difference between\\noutput and teaching input, the other one\\ntries to \\"press\\" a weight towards 0. If a\\nweight is strongly needed to minimize the\\nerror, the first term will win. If this is not\\nthe case, the second term will win. Neu-\\nrons which only have zero weights can be\\npruned again in the end.\\n\\nThere are many other variations of back-\\nprop and whole books only about this\\nsubject, but since my aim is to offer an\\noverview of neural networks, I just want\\nto mention the variations above as a moti-\\nvation to read on.\\n\\nFor some of these extensions it is obvi-\\nous that they cannot only be applied to\\nfeedforward networks with backpropaga-\\ntion learning procedures.\\n\\nWe have gotten to know backpropagation\\nand feedforward topology – now we have\\nto learn how to build a neural network. It\\nis of course impossible to fully communi-\\ncate this experience in the framework of\\nthis work. To obtain at least some of\\nthis knowledge, I now advise you to deal\\nwith some of the exemplary problems from\\n4.6.\\n\\n5.7 Getting started – Initial\\nconfiguration of a\\nmultilayer perceptron\\n\\nAfter having discussed the backpropaga-\\ntion of error learning procedure and know-\\ning how to train an existing network, it\\nwould be useful to consider how to imple-\\nment such a network.\\n\\n5.7.1 Number of layers: Two or\\nthree may often do the job,\\nbut more are also used\\n\\nLet us begin with the trivial circumstance\\nthat a network should have one layer of in-\\nput neurons and one layer of output neu-\\nrons, which results in at least two layers.\\n\\nAdditionally, we need – as we have already\\nlearned during the examination of linear\\nseparability – at least one hidden layer of\\nneurons, if our problem is not linearly sep-\\narable (which is, as we have seen, very\\nlikely).\\n\\nIt is possible, as already mentioned, to\\nmathematically prove that this MLP with\\none hidden neuron layer is already capable\\nof approximating arbitrary functions with\\nany accuracy 5 – but it is necessary not\\nonly to discuss the representability of a\\nproblem by means of a perceptron but also\\nthe learnability. Representability means\\nthat a perceptron can, in principle, realize\\n\\n5 Note: We have not indicated the number of neu-\\nrons in the hidden layer, we only mentioned the\\nhypothetical possibility.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 99\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\na mapping - but learnability means that\\nwe are also able to teach it.\\n\\nIn this respect, experience shows that two\\nhidden neuron layers (or three trainable\\nweight layers) can be very useful to solve\\na problem, since many problems can be\\nrepresented by a hidden layer but are very\\ndifficult to learn.\\n\\nOne should keep in mind that any ad-\\nditional layer generates additional sub-\\nminima of the error function in which we\\ncan get stuck. All these things consid-\\nered, a promising way is to try it with\\none hidden layer at first and if that fails,\\nretry with two layers. Only if that fails,\\none should consider more layers. However,\\ngiven the increasing calculation power of\\ncurrent computers, deep networks with\\na lot of layers are also used with success.\\n\\n5.7.2 The number of neurons has\\nto be tested\\n\\nThe number of neurons (apart from input\\nand output layer, where the number of in-\\nput and output neurons is already defined\\nby the problem statement) principally cor-\\nresponds to the number of free parameters\\nof the problem to be represented.\\n\\nSince we have already discussed the net-\\nwork capacity with respect to memorizing\\nor a too imprecise problem representation,\\nit is clear that our goal is to have as few\\nfree parameters as possible but as many as\\nnecessary.\\n\\nBut we also know that there is no stan-\\ndard solution for the question of how many\\n\\nneurons should be used. Thus, the most\\nuseful approach is to initially train with\\nonly a few neurons and to repeatedly train\\nnew networks with more neurons until the\\nresult significantly improves and, particu-\\nlarly, the generalization performance is not\\naffected (bottom-up approach).\\n\\n5.7.3 Selecting an activation\\nfunction\\n\\nAnother very important parameter for the\\nway of information processing of a neural\\nnetwork is the selection of an activa-\\ntion function. The activation function\\nfor input neurons is fixed to the identity\\nfunction, since they do not process infor-\\nmation.\\n\\nThe first question to be asked is whether\\nwe actually want to use the same acti-\\nvation function in the hidden layer and\\nin the ouput layer – no one prevents us\\nfrom choosing different functions. Gener-\\nally, the activation function is the same for\\nall hidden neurons as well as for the output\\nneurons respectively.\\n\\nFor tasks of function approximation it\\nhas been found reasonable to use the hy-\\nperbolic tangent (left part of fig. 5.14 on\\npage 102) as activation function of the hid-\\nden neurons, while a linear activation func-\\ntion is used in the output. The latter is\\nabsolutely necessary so that we do not gen-\\nerate a limited output intervall. Contrary\\nto the input layer which uses linear acti-\\nvation functions as well, the output layer\\nstill processes information, because it has\\n\\n100 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.8 The 8-3-8 encoding problem and related problems\\n\\nthreshold values. However, linear activa-\\ntion functions in the output can also cause\\nhuge learning steps and jumping over good\\nminima in the error surface. This can be\\navoided by setting the learning rate to very\\nsmall values in the output layer.\\n\\nAn unlimited output interval is not essen-\\ntial for pattern recognition tasks6. If\\nthe hyperbolic tangent is used in any case,\\nthe output interval will be a bit larger. Un-\\nlike with the hyperbolic tangent, with the\\nFermi function (right part of fig. 5.14 on\\nthe following page) it is difficult to learn\\nsomething far from the threshold value\\n(where its result is close to 0). However,\\nhere a lot of freedom is given for selecting\\nan activation function. But generally, the\\ndisadvantage of sigmoid functions is the\\nfact that they hardly learn something for\\nvalues far from thei threshold value, unless\\nthe network is modified.\\n\\n5.7.4 Weights should be initialized\\nwith small, randomly chosen\\nvalues\\n\\nThe initialization of weights is not as triv-\\nial as one might think. If they are simply\\ninitialized with 0, there will be no change\\nin weights at all. If they are all initialized\\nby the same value, they will all change\\nequally during training. The simple so-\\nlution of this problem is called symme-\\ntry breaking, which is the initialization\\nof weights with small random values. The\\n\\nrandom\\ninitial\\n\\nweights 6 Generally, pattern recognition is understood as a\\nspecial case of function approximation with a few\\ndiscrete output possibilities.\\n\\nrange of random values could be the in-\\nterval [−0.5; 0.5] not including 0 or values\\nvery close to 0. This random initialization\\nhas a nice side effect: Chances are that\\nthe average of network inputs is close to 0,\\na value that hits (in most activation func-\\ntions) the region of the greatest derivative,\\nallowing for strong learning impulses right\\nfrom the start of learning.\\n\\nSNIPE: In Snipe, weights are initial-\\nized randomly (if a synapse initial-\\nization is wanted). The maximum\\nabsolute weight value of a synapse\\ninitialized at random can be set in\\na NeuralNetworkDescriptor using the\\nmethod setSynapseInitialRange.\\n\\n5.8 The 8-3-8 encoding\\nproblem and related\\nproblems\\n\\nThe 8-3-8 encoding problem is a clas-\\nsic among the multilayer perceptron test\\ntraining problems. In our MLP we\\nhave an input layer with eight neurons\\ni1, i2, . . . , i8, an output layer with eight\\nneurons Ω1,Ω2, . . . ,Ω8 and one hidden\\nlayer with three neurons. Thus, this net-\\nwork represents a function B8 → B8. Now\\nthe training task is that an input of a value\\n1 into the neuron ij should lead to an out-\\nput of a value 1 from the neuron Ωj (only\\none neuron should be activated, which re-\\nsults in 8 training samples.\\n\\nDuring the analysis of the trained network\\nwe will see that the network with the 3\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 101\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\n−1\\n\\n−0.8\\n\\n−0.6\\n\\n−0.4\\n\\n−0.2\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nta\\nnh\\n\\n(x\\n)\\n\\nx\\n\\nHyperbolic Tangent\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nf(\\nx)\\n\\nx\\n\\nFermi Function with Temperature Parameter\\n\\nFigure 5.14: As a reminder the illustration of the hyperbolic tangent (left) and the Fermi function\\n(right). The Fermi function was expanded by a temperature parameter. The original Fermi function\\nis thereby represented by dark colors, the temperature parameter of the modified Fermi functions\\nare, ordered ascending by steepness, 1\\n\\n2 ,\\n1\\n5 ,\\n\\n1\\n10 and 1\\n\\n25 .\\n\\nhidden neurons represents some kind of bi-\\nnary encoding and that the above map-\\nping is possible (assumed training time:\\n≈ 104 epochs). Thus, our network is a ma-\\nchine in which the input is first encoded\\nand afterwards decoded again.\\n\\nAnalogously, we can train a 1024-10-1024\\nencoding problem. But is it possible to\\nimprove the efficiency of this procedure?\\nCould there be, for example, a 1024-9-\\n1024- or an 8-2-8-encoding network?\\n\\nYes, even that is possible, since the net-\\nwork does not depend on binary encodings:\\nThus, an 8-2-8 network is sufficient for our\\nproblem. But the encoding of the network\\nis far more difficult to understand (fig. 5.15\\non the next page) and the training of the\\nnetworks requires a lot more time.\\n\\nSNIPE: The static method\\ngetEncoderSampleLesson in the class\\nTrainingSampleLesson allows for creating\\nsimple training sample lessons of arbitrary\\n\\ndimensionality for encoder problems like\\nthe above.\\n\\nAn 8-1-8 network, however, does not work,\\nsince the possibility that the output of one\\nneuron is compensated by another one is\\nessential, and if there is only one hidden\\nneuron, there is certainly no compensatory\\nneuron.\\n\\nExercises\\n\\nExercise 8. Fig. 5.4 on page 75 shows\\na small network for the boolean functions\\nAND and OR. Write tables with all computa-\\ntional parameters of neural networks (e.g.\\nnetwork input, activation etc.). Perform\\nthe calculations for the four possible in-\\nputs of the networks and write down the\\nvalues of these variables for each input. Do\\nthe same for the XOR network (fig. 5.9 on\\npage 84).\\n\\n102 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 5.8 The 8-3-8 encoding problem and related problems\\n\\nFigure 5.15: Illustration of the functionality of\\n8-2-8 network encoding. The marked points rep-\\nresent the vectors of the inner neuron activation\\nassociated to the samples. As you can see, it\\nis possible to find inner activation formations so\\nthat each point can be separated from the rest\\nof the points by a straight line. The illustration\\nshows an exemplary separation of one point.\\n\\nExercise 9.\\n\\n1. List all boolean functions B3 → B1,\\nthat are linearly separable and char-\\nacterize them exactly.\\n\\n2. List those that are not linearly sepa-\\nrable and characterize them exactly,\\ntoo.\\n\\nExercise 10. A simple 2-1 network shall\\nbe trained with one single pattern by\\nmeans of backpropagation of error and\\nη = 0.1. Verify if the error\\n\\nErr = Errp = 1\\n2(t− y)2\\n\\nconverges and if so, at what value. How\\ndoes the error curve look like? Let the\\npattern (p, t) be defined by p = (p1, p2) =\\n(0.3, 0.7) and tΩ = 0.4. Randomly initalize\\nthe weights in the interval [1;−1].\\n\\nExercise 11. A one-stage perceptron\\nwith two input neurons, bias neuron\\nand binary threshold function as activa-\\ntion function divides the two-dimensional\\nspace into two regions by means of a\\nstraight line g. Analytically calculate a\\nset of weight values for such a perceptron\\nso that the following set P of the 6 pat-\\nterns of the form (p1, p2, tΩ) with ε� 1 is\\ncorrectly classified.\\n\\nP ={(0, 0,−1);\\n(2,−1, 1);\\n(7 + ε, 3− ε, 1);\\n(7− ε, 3 + ε,−1);\\n(0,−2− ε, 1);\\n(0− ε,−2,−1)}\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 103\\n\\n\\n\\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\\n\\nExercise 12. Calculate in a comprehen-\\nsible way one vector ∆W of all changes in\\nweight by means of the backpropagation of\\nerror procedure with η = 1. Let a 2-2-1\\nMLP with bias neuron be given and let the\\npattern be defined by\\n\\np = (p1, p2, tΩ) = (2, 0, 0.1).\\n\\nFor all weights with the target Ω the ini-\\ntial value of the weights should be 1. For\\nall other weights the initial value should\\nbe 0.5. What is conspicuous about the\\nchanges?\\n\\n104 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\nChapter 6\\n\\nRadial basis functions\\nRBF networks approximate functions by stretching and compressing Gaussian\\nbells and then summing them spatially shifted. Description of their functions\\n\\nand their learning process. Comparison with multilayer perceptrons.\\n\\nAccording to Poggio and Girosi [PG89]\\nradial basis function networks (RBF net-\\nworks) are a paradigm of neural networks,\\nwhich was developed considerably later\\nthan that of perceptrons. Like percep-\\ntrons, the RBF networks are built in layers.\\nBut in this case, they have exactly three\\nlayers, i.e. only one single layer of hidden\\nneurons.\\n\\nLike perceptrons, the networks have a\\nfeedforward structure and their layers are\\ncompletely linked. Here, the input layer\\nagain does not participate in information\\nprocessing. The RBF networks are -\\nlike MLPs - universal function approxima-\\ntors.\\n\\nDespite all things in common: What is the\\ndifference between RBF networks and per-\\nceptrons? The difference lies in the infor-\\nmation processing itself and in the compu-\\ntational rules within the neurons outside\\nof the input layer. So, in a moment we\\nwill define a so far unknown type of neu-\\nrons.\\n\\n6.1 Components and\\nstructure of an RBF\\nnetwork\\n\\nInitially, we want to discuss colloquially\\nand then define some concepts concerning\\nRBF networks.\\n\\nOutput neurons: In an RBF network the\\noutput neurons only contain the iden-\\ntity as activation function and one\\nweighted sum as propagation func-\\ntion. Thus, they do little more than\\nadding all input values and returning\\nthe sum.\\n\\nHidden neurons are also called RBF neu-\\nrons (as well as the layer in which\\nthey are located is referred to as RBF\\nlayer). As propagation function, each\\nhidden neuron calculates a norm that\\nrepresents the distance between the\\ninput to the network and the so-called\\nposition of the neuron (center). This\\nis inserted into a radial activation\\n\\n105\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\nfunction which calculates and outputs\\nthe activation of the neuron.\\n\\nDefinition 6.1 (RBF input neuron). Def-\\ninition and representation is identical toinput\\n\\nis linear\\nagain\\n\\nthe definition 5.1 on page 73 of the input\\nneuron.\\n\\nDefinition 6.2 (Center of an RBF neu-\\nron). The center ch of an RBF neuron\\n\\ncI\\nh is the point in the input space where\\nthe RBF neuron is located . In general,\\n\\nPosition\\nin the input\\n\\nspace\\nthe closer the input vector is to the center\\nvector of an RBF neuron, the higher is its\\nactivation.\\n\\nDefinition 6.3 (RBF neuron). The so-\\ncalled RBF neurons h have a propaga-\\ntion function fprop that determines the dis-\\ntance between the center ch of a neuronImportant!\\nand the input vector y. This distance rep-\\nresents the network input. Then the net-\\nwork input is sent through a radial basis\\nfunction fact which returns the activation\\nor the output of the neuron. RBF neurons\\n\\nare represented by the symbol WVUTPQRS||c,x||\\nGauß\\n\\n.\\n\\nDefinition 6.4 (RBF output neuron).\\nRBF output neurons Ω use the\\n\\nweighted sum as propagation function\\nfprop, and the identity as activation func-\\n\\nonly sums\\nup tion fact. They are represented by the sym-\\n\\nbol ONMLHIJKΣ\\n� .\\n\\nDefinition 6.5 (RBF network). An\\nRBF network has exactly three layers in\\nthe following order: The input layer con-\\nsisting of input neurons, the hidden layer\\n(also called RBF layer) consisting of RBF\\nneurons and the output layer consisting of\\n\\nRBF output neurons. Each layer is com-\\n3 layers,\\nfeedforwardpletely linked with the following one, short-\\n\\ncuts do not exist (fig. 6.1 on the next page)\\n– it is a feedforward topology. The connec-\\ntions between input layer and RBF layer\\nare unweighted, i.e. they only transmit\\nthe input. The connections between RBF\\nlayer and output layer are weighted. The\\noriginal definition of an RBF network only\\nreferred to an output neuron, but – in anal-\\nogy to the perceptrons – it is apparent that\\nsuch a definition can be generalized. A\\nbias neuron is not used in RBF networks.\\nThe set of input neurons shall be repre-\\nsented by I, the set of hidden neurons by JI,H,O\\nH and the set of output neurons by O.\\n\\nTherefore, the inner neurons are called ra-\\ndial basis neurons because from their def-\\ninition follows directly that all input vec-\\ntors with the same distance from the cen-\\nter of a neuron also produce the same out-\\nput value (fig. 6.2 on page 108).\\n\\n6.2 Information processing of\\nan RBF network\\n\\nNow the question is, what can be realized\\nby such a network and what is its purpose.\\nLet us go over the RBF network from top\\nto bottom: An RBF network receives the\\ninput by means of the unweighted con-\\nnections. Then the input vector is sent\\nthrough a norm so that the result is a\\nscalar. This scalar (which, by the way, can\\nonly be positive due to the norm) is pro-\\ncessed by a radial basis function, for exam-\\n\\n106 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 6.2 Information processing of an RBF network\\n\\n�� ��GFED@ABC�\\n\\n||yyyyyyyyyy\\n\\n�� \\"\\"EEEEEEEEEE\\n\\n((RRRRRRRRRRRRRRRRRRRR\\n\\n++VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV GFED@ABC�\\n\\n\\"\\"EEEEEEEEEE\\n\\n��||yyyyyyyyyy\\n\\nvvllllllllllllllllllll\\n\\nsshhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh i1, i2, . . . , i|I|\\n\\nWVUTPQRS||c,x||\\nGauß\\n\\n!!CCCCCCCCCC\\n\\n((QQQQQQQQQQQQQQQQQQQQ\\n\\n**VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV WVUTPQRS||c,x||\\nGauß\\n\\n�� !!CCCCCCCCCC\\n\\n((QQQQQQQQQQQQQQQQQQQQ\\nWVUTPQRS||c,x||\\n\\nGauß\\n\\n}}{{{{{{{{{{\\n\\n�� !!CCCCCCCCCC\\nWVUTPQRS||c,x||\\n\\nGauß\\n\\nvvmmmmmmmmmmmmmmmmmmmm\\n\\n}}{{{{{{{{{{\\n\\n��\\n\\nWVUTPQRS||c,x||\\nGauß\\n\\ntthhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\\n\\nvvmmmmmmmmmmmmmmmmmmmm\\n\\n}}{{{{{{{{{{\\nh1, h2, . . . , h|H|\\n\\nONMLHIJKΣ\\n�\\n\\n��\\n\\nONMLHIJKΣ\\n�\\n\\n��\\n\\nONMLHIJKΣ\\n�\\n\\n��\\n\\nΩ1,Ω2, . . . ,Ω|O|\\n\\nFigure 6.1: An exemplary RBF network with two input neurons, five hidden neurons and three\\noutput neurons. The connections to the hidden neurons are not weighted, they only transmit the\\ninput. Right of the illustration you can find the names of the neurons, which coincide with the\\nnames of the MLP neurons: Input neurons are called i, hidden neurons are called h and output\\nneurons are called Ω. The associated sets are referred to as I, H and O.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 107\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\nFigure 6.2: Let ch be the center of an RBF neu-\\nron h. Then the activation function facth is ra-\\ndially symmetric around ch.\\n\\nple by a Gaussian bell (fig. 6.3 on the next\\npage) .input\\n\\n→ distance\\n→ Gaussian bell\\n\\n→ sum\\n→ output\\n\\nThe output values of the different neurons\\nof the RBF layer or of the different Gaus-\\nsian bells are added within the third layer:\\nbasically, in relation to the whole input\\nspace, Gaussian bells are added here.\\n\\nSuppose that we have a second, a third\\nand a fourth RBF neuron and therefore\\nfour differently located centers. Each of\\nthese neurons now measures another dis-\\ntance from the input to its own center\\nand de facto provides different values, even\\nif the Gaussian bell is the same. Since\\nthese values are finally simply accumu-\\nlated in the output layer, one can easily\\nsee that any surface can be shaped by drag-\\n\\nging, compressing and removing Gaussian\\nbells and subsequently accumulating them.\\nHere, the parameters for the superposition\\nof the Gaussian bells are in the weights\\nof the connections between the RBF layer\\nand the output layer.\\n\\nFurthermore, the network architecture of-\\nfers the possibility to freely define or train\\nheight and width of the Gaussian bells –\\ndue to which the network paradigm be-\\ncomes even more versatile. We will get\\nto know methods and approches for this\\nlater.\\n\\n6.2.1 Information processing in\\nRBF neurons\\n\\nRBF neurons process information by using\\nnorms and radial basis functions\\n\\nAt first, let us take as an example a sim-\\nple 1-4-1 RBF network. It is apparent\\nthat we will receive a one-dimensional out-\\nput which can be represented as a func-\\ntion (fig. 6.4 on the facing page). Ad-\\nditionally, the network includes the cen-\\nters c1, c2, . . . , c4 of the four inner neurons\\nh1, h2, . . . , h4, and therefore it has Gaus-\\nsian bells which are finally added within\\nthe output neuron Ω. The network also\\npossesses four values σ1, σ2, . . . , σ4 which\\ninfluence the width of the Gaussian bells.\\nOn the contrary, the height of the Gaus-\\nsian bell is influenced by the subsequent\\nweights, since the individual output val-\\nues of the bells are multiplied by those\\nweights.\\n\\n108 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 6.2 Information processing of an RBF network\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−2 −1.5 −1 −0.5  0  0.5  1  1.5  2\\n\\nh(\\nr)\\n\\nr\\n\\nGaussian in 1D Gaussian in 2D\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\nx\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\n 2\\n\\ny\\n\\n 0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n\\n 1\\n\\nh(r)\\n\\nFigure 6.3: Two individual one- or two-dimensional Gaussian bells. In both cases σ = 0.4 holds\\nand the centers of the Gaussian bells lie in the coordinate origin. The distance r to the center (0, 0)\\nis simply calculated according to the Pythagorean theorem: r =\\n\\n√\\nx2 + y2.\\n\\n−0.6\\n\\n−0.4\\n\\n−0.2\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n 1.2\\n\\n 1.4\\n\\n−2  0  2  4  6  8\\n\\ny\\n\\nx\\n\\nFigure 6.4: Four different Gaussian bells in one-dimensional space generated by means of RBF\\nneurons are added by an output neuron of the RBF network. The Gaussian bells have different\\nheights, widths and positions. Their centers c1, c2, . . . , c4 are located at 0, 1, 3, 4, the widths\\nσ1, σ2, . . . , σ4 at 0.4, 1, 0.2, 0.8. You can see a two-dimensional example in fig. 6.5 on the following\\npage.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 109\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\nGaussian 1\\n\\n−2\\n−1\\n\\n 0\\n 1x\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\n 2\\n\\ny\\n\\n−1\\n−0.5\\n\\n 0\\n 0.5\\n\\n 1\\n 1.5\\n\\n 2\\n\\nh(r)\\nGaussian 2\\n\\n−2\\n−1\\n\\n 0\\n 1x\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\n 2\\n\\ny\\n\\n−1\\n−0.5\\n\\n 0\\n 0.5\\n\\n 1\\n 1.5\\n\\n 2\\n\\nh(r)\\n\\nGaussian 3\\n\\n−2\\n−1\\n\\n 0\\n 1x\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\n 2\\n\\ny\\n\\n−1\\n−0.5\\n\\n 0\\n 0.5\\n\\n 1\\n 1.5\\n\\n 2\\n\\nh(r)\\nGaussian 4\\n\\n−2\\n−1\\n\\n 0\\n 1x\\n\\n−2\\n−1\\n\\n 0\\n 1\\n\\n 2\\n\\ny\\n\\n−1\\n−0.5\\n\\n 0\\n 0.5\\n\\n 1\\n 1.5\\n\\n 2\\n\\nh(r)\\n\\nWVUTPQRS||c,x||\\nGauß\\n\\n((QQQQQQQQQQQQQQQQQQQQ\\nWVUTPQRS||c,x||\\n\\nGauß\\n\\n  AAAAAAAAAA\\nWVUTPQRS||c,x||\\n\\nGauß\\n\\n~~}}}}}}}}}}\\n\\nWVUTPQRS||c,x||\\nGauß\\n\\nvvmmmmmmmmmmmmmmmmmmm\\n\\nONMLHIJKΣ\\n�\\n\\n��\\n\\nSum of the 4 Gaussians\\n\\n−2\\n−1.5\\n\\n−1\\n−0.5\\n\\n 0\\n 0.5\\n\\n 1\\n 1.5\\n\\n 2\\n\\nx\\n\\n−2\\n−1.5\\n\\n−1\\n−0.5\\n\\n 0\\n 0.5\\n\\n 1\\n 1.5\\n\\n 2\\n\\ny\\n\\n−1\\n−0.75\\n\\n−0.5\\n−0.25\\n\\n 0\\n 0.25\\n\\n 0.5\\n 0.75\\n\\n 1\\n 1.25\\n\\n 1.5\\n 1.75\\n\\n 2\\n\\nFigure 6.5: Four different Gaussian bells in two-dimensional space generated by means of RBF\\nneurons are added by an output neuron of the RBF network. Once again r =\\n\\n√\\nx2 + y2 applies for\\n\\nthe distance. The heights w, widths σ and centers c = (x, y) are: w1 = 1, σ1 = 0.4, c1 = (0.5, 0.5),\\nw2 = −1, σ2 = 0.6, c2 = (1.15,−1.15), w3 = 1.5, σ3 = 0.2, c3 = (−0.5,−1), w4 = 0.8, σ4 =\\n1.4, c4 = (−2, 0).\\n\\n110 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 6.2 Information processing of an RBF network\\n\\nSince we use a norm to calculate the dis-\\ntance between the input vector and the\\ncenter of a neuron h, we have different\\nchoices: Often the Euclidian norm is cho-\\nsen to calculate the distance:\\n\\nrh = ||x− ch|| (6.1)\\n\\n=\\n√∑\\ni∈I\\n\\n(xi − ch,i)2 (6.2)\\n\\nRemember: The input vector was referred\\nto as x. Here, the index i runs through\\nthe input neurons and thereby through the\\ninput vector components and the neuron\\ncenter components. As we can see, the\\nEuclidean distance generates the squared\\ndifferences of all vector components, adds\\nthem and extracts the root of the sum.\\nIn two-dimensional space this corresponds\\nto the Pythagorean theorem. From the\\ndefinition of a norm directly follows that\\nthe distance can only be positive. Strictly\\nspeaking, we hence only use the positive\\npart of the activation function. By the\\nway, activation functions other than the\\nGaussian bell are possible. Normally, func-\\ntions that are monotonically decreasing\\nover the interval [0;∞] are chosen.\\n\\nNow that we know the distance rh be-\\nrhI tween the input vector x and the center\\n\\nch of the RBF neuron h, this distance has\\nto be passed through the activation func-\\ntion. Here we use, as already mentioned,\\na Gaussian bell:\\n\\nfact(rh) = e\\n\\n(\\n−r2\\nh\\n\\n2σ2\\nh\\n\\n)\\n(6.3)\\n\\nIt is obvious that both the center ch and\\nthe width σh can be seen as part of the\\n\\nactivation function fact, and hence the ac-\\ntivation functions should not be referred\\nto as fact simultaneously. One solution\\nwould be to number the activation func-\\ntions like fact1, fact2, . . . , fact|H| withH be-\\ning the set of hidden neurons. But as a\\nresult the explanation would be very con-\\nfusing. So I simply use the name fact for\\nall activation functions and regard σ and\\nc as variables that are defined for individ-\\nual neurons but no directly included in the\\nactivation function.\\n\\nThe reader will certainly notice that in the\\nliterature the Gaussian bell is often nor-\\nmalized by a multiplicative factor. We\\ncan, however, avoid this factor because\\nwe are multiplying anyway with the subse-\\nquent weights and consecutive multiplica-\\ntions, first by a normalization factor and\\nthen by the connections’ weights, would\\nonly yield different factors there. We do\\nnot need this factor (especially because for\\nour purpose the integral of the Gaussian\\nbell must not always be 1) and therefore\\nsimply leave it out.\\n\\n6.2.2 Some analytical thoughts\\nprior to the training\\n\\nThe output yΩ of an RBF output neuron\\nΩ results from combining the functions of\\nan RBF neuron to\\n\\nyΩ =\\n∑\\nh∈H\\n\\nwh,Ω · fact (||x− ch||) . (6.4)\\n\\nSuppose that similar to the multilayer per-\\nceptron we have a set P , that contains |P |\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 111\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\ntraining samples (p, t). Then we obtain\\n|P | functions of the form\\n\\nyΩ =\\n∑\\nh∈H\\n\\nwh,Ω · fact (||p− ch||) , (6.5)\\n\\ni.e. one function for each training sam-\\nple.\\n\\nOf course, with this effort we are aiming\\nat letting the output y for all training\\npatterns p converge to the corresponding\\nteaching input t.\\n\\n6.2.2.1 Weights can simply be\\ncomputed as solution of a\\nsystem of equations\\n\\nThus, we have |P | equations. Now let us\\nassume that the widths σ1, σ2, . . . , σk, the\\ncenters c1, c2, . . . , ck and the training sam-\\nples p including the teaching input t are\\ngiven. We are looking for the weights wh,Ω\\nwith |H| weights for one output neuron\\nΩ. Thus, our problem can be seen as a\\nsystem of equations since the only thing\\nwe want to change at the moment are the\\nweights.\\n\\nThis demands a distinction of cases con-\\ncerning the number of training samples |P |\\nand the number of RBF neurons |H|:\\n\\n|P | = |H|: If the number of RBF neurons\\nequals the number of patterns, i.e.\\n|P | = |H|, the equation can be re-\\nduced to a matrix multiplication\\n\\nsimply\\ncalculate\\nweights\\n\\nT = M ·G (6.6)\\n\\n⇔ M−1 · T = M−1 ·M ·G (6.7)\\n\\n⇔ M−1 · T = E ·G (6.8)\\n\\n⇔ M−1 · T = G, (6.9)\\n\\nwhere\\n\\n. T is the vector of the teaching JTinputs for all training samples,\\n\\n. M is the |P | × |H| matrix of JMthe outputs of all |H| RBF neu-\\nrons to |P | samples (remember:\\n|P | = |H|, the matrix is squared\\nand we can therefore attempt to\\ninvert it),\\n\\n. G is the vector of the desired JGweights and\\n\\n. E is a unit matrix with the same JEsize as G.\\n\\nMathematically speaking, we can sim-\\nply calculate the weights: In the case\\nof |P | = |H| there is exactly one RBF\\nneuron available per training sample.\\nThis means, that the network exactly\\nmeets the |P | existing nodes after hav-\\ning calculated the weights, i.e. it per-\\nforms a precise interpolation. To\\ncalculate such an equation we cer-\\ntainly do not need an RBF network,\\nand therefore we can proceed to the\\nnext case.\\n\\nExact interpolation must not be mis-\\ntaken for the memorizing ability men-\\ntioned with the MLPs: First, we are\\nnot talking about the training of RBF\\n\\n112 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 6.2 Information processing of an RBF network\\n\\nnetworks at the moment. Second,\\nit could be advantageous for us and\\nmight in fact be intended if the net-\\nwork exactly interpolates between the\\nnodes.\\n\\n|P | < |H|: The system of equations is\\nunder-determined, there are more\\nRBF neurons than training samples,\\ni.e. |P | < |H|. Certainly, this case\\nnormally does not occur very often.\\nIn this case, there is a huge variety\\nof solutions which we do not need in\\nsuch detail. We can select one set of\\nweights out of many obviously possi-\\nble ones.\\n\\n|P | > |H|: But most interesting for fur-\\nther discussion is the case if there\\nare significantly more training sam-\\nples than RBF neurons, that means\\n|P | > |H|. Thus, we again want\\nto use the generalization capability of\\nthe neural network.\\n\\nIf we have more training samples than\\nRBF neurons, we cannot assume that\\nevery training sample is exactly hit.\\nSo, if we cannot exactly hit the points\\nand therefore cannot just interpolate\\nas in the aforementioned ideal case\\nwith |P | = |H|, we must try to find\\na function that approximates our\\ntraining set P as closely as possible:\\nAs with the MLP we try to reduce\\nthe sum of the squared error to a min-\\nimum.\\n\\nHow do we continue the calculation\\nin the case of |P | > |H|? As above,\\nto solve the system of equations, we\\n\\nhave to find the solution M of a ma-\\ntrix multiplication\\n\\nT = M ·G. (6.10)\\n\\nThe problem is that this time we can-\\nnot invert the |P | × |H| matrix M be-\\ncause it is not a square matrix (here,\\n|P | 6= |H| is true). Here, we have\\nto use the Moore-Penrose pseudo\\ninverse M+ which is defined by\\n\\nJM+\\n\\nM+ = (MT ·M)−1 ·MT (6.11)\\n\\nAlthough the Moore-Penrose pseudo\\ninverse is not the inverse of a matrix,\\nit can be used similarly in this case1.\\nWe get equations that are very similar\\nto those in the case of |P | = |H|:\\n\\nT = M ·G (6.12)\\n\\n⇔ M+ · T = M+ ·M ·G (6.13)\\n\\n⇔ M+ · T = E ·G (6.14)\\n\\n⇔ M+ · T = G (6.15)\\n\\nAnother reason for the use of the\\nMoore-Penrose pseudo inverse is the\\nfact that it minimizes the squared\\nerror (which is our goal): The esti-\\nmate of the vector G in equation 6.15\\ncorresponds to the Gauss-Markov\\nmodel known from statistics, which\\nis used to minimize the squared error.\\nIn the aforementioned equations 6.11\\nand the following ones please do not\\nmistake the T in MT (of the trans-\\npose of the matrix M) for the T of\\nthe vector of all teaching inputs.\\n\\n1 Particularly, M+ = M−1 is true if M is invertible.\\nI do not want to go into detail of the reasons for\\nthese circumstances and applications ofM+ - they\\ncan easily be found in literature for linear algebra.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 113\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\n6.2.2.2 The generalization on several\\noutputs is trivial and not quite\\ncomputationally expensive\\n\\nWe have found a mathematically exact\\nway to directly calculate the weights.\\nWhat will happen if there are several out-\\nput neurons, i.e. |O| > 1, with O being, as\\nusual, the set of the output neurons Ω? In\\nthis case, as we have already indicated, it\\ndoes not change much: The additional out-\\nput neurons have their own set of weights\\nwhile we do not change the σ and c of the\\nRBF layer. Thus, in an RBF network it is\\neasy for given σ and c to realize a lot of\\noutput neurons since we only have to cal-\\nculate the individual vector of weights\\n\\nGΩ = M+ · TΩ (6.16)\\n\\nfor every new output neuron Ω, whereas\\nthe matrix M+, which generally requires\\na lot of computational effort, always stays\\nthe same: So it is quite inexpensive – atinexpensive\\n\\noutput\\ndimension\\n\\nleast concerning the computational com-\\nplexity – to add more output neurons.\\n\\n6.2.2.3 Computational effort and\\naccuracy\\n\\nFor realistic problems it normally applies\\nthat there are considerably more training\\nsamples than RBF neurons, i.e. |P | �\\n|H|: You can, without any difficulty, use\\n106 training samples, if you like. Theoreti-\\ncally, we could find the terms for the math-\\nematically correct solution on the black-\\nboard (after a very long time), but such\\ncalculations often seem to be imprecise\\n\\nand very time-consuming (matrix inver-\\nsions require a lot of computational ef-\\nfort).\\n\\nFurthermore, our Moore-Penrose pseudo-\\ninverse is, in spite of numeric stabil-\\nity, no guarantee that the output vector\\n\\nM+ complex\\nand imprecisecorresponds to the teaching vector, be-\\n\\ncause such extensive computations can be\\nprone to many inaccuracies, even though\\nthe calculation is mathematically correct:\\nOur computers can only provide us with\\n(nonetheless very good) approximations of\\nthe pseudo-inverse matrices. This means\\nthat we also get only approximations of\\nthe correct weights (maybe with a lot of\\naccumulated numerical errors) and there-\\nfore only an approximation (maybe very\\nrough or even unrecognizable) of the de-\\nsired output.\\n\\nIf we have enough computing power to an-\\nalytically determine a weight vector, we\\nshould use it nevertheless only as an initial\\nvalue for our learning process, which leads\\nus to the real training methods – but oth-\\nerwise it would be boring, wouldn’t it?\\n\\n6.3 Combinations of equation\\nsystem and gradient\\nstrategies are useful for\\ntraining\\n\\nAnalogous to the MLP we perform a gra-\\ndient descent to find the suitable weights\\nby means of the already well known delta retraining\\n\\ndelta rulerule. Here, backpropagation is unneces-\\nsary since we only have to train one single\\n\\n114 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 6.3 Training of RBF networks\\n\\nweight layer – which requires less comput-\\ning time.\\n\\nWe know that the delta rule is\\n\\n∆wh,Ω = η · δΩ · oh, (6.17)\\n\\nin which we now insert as follows:\\n\\n∆wh,Ω = η · (tΩ − yΩ) · fact(||p− ch||)\\n(6.18)\\n\\nHere again I explicitly want to mention\\nthat it is very popular to divide the train-\\ning into two phases by analytically com-\\nputing a set of weights and then refining\\nit by training with the delta rule.\\n\\nThere is still the question whether to learn\\noffline or online. Here, the answer is sim-\\nilar to the answer for the multilayer per-\\nceptron: Initially, one often trains onlinetraining\\n\\nin phases (faster movement across the error surface).\\nThen, after having approximated the so-\\nlution, the errors are once again accumu-\\nlated and, for a more precise approxima-\\ntion, one trains offline in a third learn-\\ning phase. However, similar to the MLPs,\\nyou can be successful by using many meth-\\nods.\\n\\nAs already indicated, in an RBF network\\nnot only the weights between the hidden\\nand the output layer can be optimized. So\\nlet us now take a look at the possibility to\\nvary σ and c.\\n\\n6.3.1 It is not always trivial to\\ndetermine centers and widths\\nof RBF neurons\\n\\nIt is obvious that the approximation accu-\\nracy of RBF networks can be increased by\\nadapting the widths and positions of the\\nGaussian bells in the input space to the\\nproblem that needs to be approximated.\\nThere are several methods to deal with the\\ncenters c and the widths σ of the Gaussian vary\\n\\nσ and cbells:\\n\\nFixed selection: The centers and widths\\ncan be selected in a fixed manner and\\nregardless of the training samples –\\nthis is what we have assumed until\\nnow.\\n\\nConditional, fixed selection: Again cen-\\nters and widths are selected fixedly,\\nbut we have previous knowledge\\nabout the functions to be approxi-\\nmated and comply with it.\\n\\nAdaptive to the learning process: This\\nis definitely the most elegant variant,\\nbut certainly the most challenging\\none, too. A realization of this\\napproach will not be discussed in\\nthis chapter but it can be found in\\nconnection with another network\\ntopology (section 10.6.1).\\n\\n6.3.1.1 Fixed selection\\n\\nIn any case, the goal is to cover the in-\\nput space as evenly as possible. Here,\\nwidths of 2\\n\\n3 of the distance between the\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 115\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\nFigure 6.6: Example for an even coverage of a\\ntwo-dimensional input space by applying radial\\nbasis functions.\\n\\ncenters can be selected so that the Gaus-\\nsian bells overlap by approx. \\"one third\\"2\\n(fig. 6.6). The closer the bells are set the\\nmore precise but the more time-consuming\\nthe whole thing becomes.\\n\\nThis may seem to be very inelegant, but\\nin the field of function approximation we\\ncannot avoid even coverage. Here it is\\nuseless if the function to be approximated\\nis precisely represented at some positions\\nbut at other positions the return value is\\nonly 0. However, the high input dimen-\\nsion requires a great many RBF neurons,\\nwhich increases the computational effortinput\\n\\ndimension\\nvery expensive\\n\\nexponentially with the dimension – and is\\n\\n2 It is apparent that a Gaussian bell is mathemati-\\ncally infinitely wide, therefore I ask the reader to\\napologize this sloppy formulation.\\n\\nresponsible for the fact that six- to ten-\\ndimensional problems in RBF networks\\nare already called \\"high-dimensional\\" (an\\nMLP, for example, does not cause any\\nproblems here).\\n\\n6.3.1.2 Conditional, fixed selection\\n\\nSuppose that our training samples are not\\nevenly distributed across the input space.\\nIt then seems obvious to arrange the cen-\\nters and sigmas of the RBF neurons by\\nmeans of the pattern distribution. So the\\ntraining patterns can be analyzed by statis-\\ntical techniques such as a cluster analysis,\\nand so it can be determined whether there\\nare statistical factors according to which\\nwe should distribute the centers and sig-\\nmas (fig. 6.7 on the facing page).\\n\\nA more trivial alternative would be to\\nset |H| centers on positions randomly se-\\nlected from the set of patterns. So this\\nmethod would allow for every training pat-\\ntern p to be directly in the center of a neu-\\nron (fig. 6.8 on the next page). This is\\nnot yet very elegant but a good solution\\nwhen time is an issue. Generally, for this\\nmethod the widths are fixedly selected.\\n\\nIf we have reason to believe that the set\\nof training samples is clustered, we can\\nuse clustering methods to determine them.\\nThere are different methods to determine\\nclusters in an arbitrarily dimensional set\\nof points. We will be introduced to some\\nof them in excursus A. One neural cluster-\\ning method are the so-called ROLFs (sec-\\ntion A.5), and self-organizing maps are\\n\\n116 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 6.3 Training of RBF networks\\n\\nFigure 6.7: Example of an uneven coverage of\\na two-dimensional input space, of which we\\nhave previous knowledge, by applying radial ba-\\nsis functions.\\n\\nalso useful in connection with determin-\\ning the position of RBF neurons (section\\n10.6.1). Using ROLFs, one can also receive\\nindicators for useful radii of the RBF neu-\\nrons. Learning vector quantisation (chap-\\nter 9) has also provided good results. All\\nthese methods have nothing to do with\\nthe RBF networks themselves but are only\\nused to generate some previous knowledge.\\nTherefore we will not discuss them in this\\nchapter but independently in the indicated\\nchapters.\\n\\nAnother approach is to use the approved\\nmethods: We could slightly move the po-\\nsitions of the centers and observe how our\\nerror function Err is changing – a gradient\\ndescent, as already known from the MLPs.\\n\\nFigure 6.8: Example of an uneven coverage of\\na two-dimensional input space by applying radial\\nbasis functions. The widths were fixedly selected,\\nthe centers of the neurons were randomly dis-\\ntributed throughout the training patterns. This\\ndistribution can certainly lead to slightly unrepre-\\nsentative results, which can be seen at the single\\ndata point down to the left.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 117\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\nIn a similar manner we could look how the\\nerror depends on the values σ. Analogous\\nto the derivation of backpropagation we\\nderive\\n\\n∂Err(σhch)\\n∂σh\\n\\nand ∂Err(σhch)\\n∂ch\\n\\n.\\n\\nSince the derivation of these terms corre-\\nsponds to the derivation of backpropaga-\\ntion we do not want to discuss it here.\\n\\nBut experience shows that no convincing\\nresults are obtained by regarding how the\\nerror behaves depending on the centers\\nand sigmas. Even if mathematics claim\\nthat such methods are promising, the gra-\\ndient descent, as we already know, leads\\nto problems with very craggy error sur-\\nfaces.\\n\\nAnd that is the crucial point: Naturally,\\nRBF networks generate very craggy er-\\nror surfaces because, if we considerably\\nchange a c or a σ, we will significantly\\nchange the appearance of the error func-\\ntion.\\n\\n6.4 Growing RBF networks\\nautomatically adjust the\\nneuron density\\n\\nIn growing RBF networks, the number\\n|H| of RBF neurons is not constant. A\\ncertain number |H| of neurons as well as\\ntheir centers ch and widths σh are previ-\\nously selected (e.g. by means of a cluster-\\ning method) and then extended or reduced.\\n\\nIn the following text, only simple mecha-\\nnisms are sketched. For more information,\\nI refer to [Fri94].\\n\\n6.4.1 Neurons are added to places\\nwith large error values\\n\\nAfter generating this initial configuration\\nthe vector of the weights G is analytically\\ncalculated. Then all specific errors Errp\\nconcerning the set P of the training sam-\\nples are calculated and the maximum spe-\\ncific error\\n\\nmax\\nP\\n\\n(Errp)\\n\\nis sought.\\n\\nThe extension of the network is simple:\\nWe replace this maximum error with a new\\n\\nreplace\\nerror with\\nneuron\\n\\nRBF neuron. Of course, we have to exer-\\ncise care in doing this: IF the σ are small,\\nthe neurons will only influence each other\\nif the distance between them is short. But\\nif the σ are large, the already exisiting\\nneurons are considerably influenced by the\\nnew neuron because of the overlapping of\\nthe Gaussian bells.\\n\\nSo it is obvious that we will adjust the al-\\nready existing RBF neurons when adding\\nthe new neuron.\\n\\nTo put it simply, this adjustment is made\\nby moving the centers c of the other neu-\\nrons away from the new neuron and re-\\nducing their width σ a bit. Then the\\ncurrent output vector y of the network is\\ncompared to the teaching input t and the\\nweight vector G is improved by means of\\ntraining. Subsequently, a new neuron can\\nbe inserted if necessary. This method is\\n\\n118 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 6.5 Comparing RBF networks and multilayer perceptrons\\n\\nparticularly suited for function approxima-\\ntions.\\n\\n6.4.2 Limiting the number of\\nneurons\\n\\nHere it is mandatory to see that the net-\\nwork will not grow ad infinitum, which can\\nhappen very fast. Thus, it is very useful\\nto previously define a maximum number\\nfor neurons |H|max.\\n\\n6.4.3 Less important neurons are\\ndeleted\\n\\nWhich leads to the question whether it\\nis possible to continue learning when this\\nlimit |H|max is reached. The answer is:\\nthis would not stop learning. We only have\\nto look for the \\"most unimportant\\" neuron\\nand delete it. A neuron is, for example,\\nunimportant for the network if there is an-\\nother neuron that has a similar function:\\nIt often occurs that two Gaussian bells ex-\\nactly overlap and at such a position, for\\n\\ndelete\\nunimportant\\n\\nneurons\\ninstance, one single neuron with a higher\\nGaussian bell would be appropriate.\\n\\nBut to develop automated procedures in\\norder to find less relevant neurons is highly\\nproblem dependent and we want to leave\\nthis to the programmer.\\n\\nWith RBF networks and multilayer per-\\nceptrons we have already become ac-\\nquainted with and extensivley discussed\\ntwo network paradigms for similar prob-\\nlems. Therefore we want to compare these\\n\\ntwo paradigms and look at their advan-\\ntages and disadvantages.\\n\\n6.5 Comparing RBF networks\\nand multilayer\\nperceptrons\\n\\nWe will compare multilayer perceptrons\\nand RBF networks with respect to differ-\\nent aspects.\\n\\nInput dimension: We must be careful\\nwith RBF networks in high-\\ndimensional functional spaces since\\nthe network could very quickly\\nrequire huge memory storage and\\ncomputational effort. Here, a\\nmultilayer perceptron would cause\\nless problems because its number of\\nneuons does not grow exponentially\\nwith the input dimension.\\n\\nCenter selection: However, selecting the\\ncenters c for RBF networks is (despite\\nthe introduced approaches) still a ma-\\njor problem. Please use any previous\\nknowledge you have when applying\\nthem. Such problems do not occur\\nwith the MLP.\\n\\nOutput dimension: The advantage of\\nRBF networks is that the training is\\nnot much influenced when the output\\ndimension of the network is high.\\nFor an MLP, a learning procedure\\nsuch as backpropagation thereby will\\nbe very time-consuming.\\n\\nExtrapolation: Advantage as well as dis-\\nadvantage of RBF networks is the lack\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 119\\n\\n\\n\\nChapter 6 Radial basis functions dkriesel.com\\n\\nof extrapolation capability: An RBF\\nnetwork returns the result 0 far away\\nfrom the centers of the RBF layer. On\\nthe one hand it does not extrapolate,\\nunlike the MLP it cannot be used\\nfor extrapolation (whereby we could\\nnever know if the extrapolated values\\nof the MLP are reasonable, but expe-\\nrience shows that MLPs are suitable\\nfor that matter). On the other hand,\\nunlike the MLP the network is capa-Important!\\nble to use this 0 to tell us \\"I don’t\\nknow\\", which could be an advantage.\\n\\nLesion tolerance: For the output of an\\nMLP, it is no so important if a weight\\nor a neuron is missing. It will only\\nworsen a little in total. If a weight\\nor a neuron is missing in an RBF net-\\nwork then large parts of the output\\nremain practically uninfluenced. But\\none part of the output is heavily af-\\nfected because a Gaussian bell is di-\\nrectly missing. Thus, we can choose\\nbetween a strong local error for lesion\\nand a weak but global error.\\n\\nSpread: Here the MLP is \\"advantaged\\"\\nsince RBF networks are used consid-\\nerably less often – which is not always\\nunderstood by professionals (at least\\nas far as low-dimensional input spaces\\nare concerned). The MLPs seem to\\nhave a considerably longer tradition\\nand they are working too good to take\\nthe effort to read some pages of this\\nwork about RBF networks) :-).\\n\\nExercises\\n\\nExercise 13. An |I|-|H|-|O| RBF net-\\nwork with fixed widths and centers of the\\nneurons should approximate a target func-\\ntion u. For this, |P | training samples of\\nthe form (p, t) of the function u are given.\\nLet |P | > |H| be true. The weights should\\nbe analytically determined by means of\\nthe Moore-Penrose pseudo inverse. Indi-\\ncate the running time behavior regarding\\n|P | and |O| as precisely as possible.\\n\\nNote: There are methods for matrix mul-\\ntiplications and matrix inversions that are\\nmore efficient than the canonical methods.\\nFor better estimations, I recommend to\\nlook for such methods (and their complex-\\nity). In addition to your complexity calcu-\\nlations, please indicate the used methods\\ntogether with their complexity.\\n\\n120 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\nChapter 7\\n\\nRecurrent perceptron-like networks\\nSome thoughts about networks with internal states.\\n\\nGenerally, recurrent networks are net-\\nworks that are capable of influencing them-\\nselves by means of recurrences, e.g. by\\nincluding the network output in the follow-\\ning computation steps. There are many\\ntypes of recurrent networks of nearly arbi-\\ntrary form, and nearly all of them are re-\\nferred to as recurrent neural networks.\\nAs a result, for the few paradigms in-\\ntroduced here I use the name recurrent\\nmultilayer perceptrons.\\n\\nApparently, such a recurrent network is ca-\\npable to compute more than the ordinary\\nMLP: If the recurrent weights are set to 0,\\n\\nmore capable\\nthan MLP the recurrent network will be reduced to\\n\\nan ordinary MLP. Additionally, the recur-\\nrence generates different network-internal\\nstates so that different inputs can produce\\ndifferent outputs in the context of the net-\\nwork state.\\n\\nRecurrent networks in themselves have a\\ngreat dynamic that is mathematically dif-\\nficult to conceive and has to be discussed\\nextensively. The aim of this chapter is\\nonly to briefly discuss how recurrences can\\n\\nbe structured and how network-internal\\nstates can be generated. Thus, I will\\nbriefly introduce two paradigms of recur-\\nrent networks and afterwards roughly out-\\nline their training.\\n\\nWith a recurrent network an input x that\\nis constant over time may lead to differ-\\nent results: On the one hand, the network state\\n\\ndynamicscould converge, i.e. it could transform it-\\nself into a fixed state and at some time re-\\nturn a fixed output value y. On the other\\nhand, it could never converge, or at least\\nnot until a long time later, so that it can\\nno longer be recognized, and as a conse-\\nquence, y constantly changes.\\n\\nIf the network does not converge, it is, for\\nexample, possible to check if periodicals\\nor attractors (fig. 7.1 on the following\\npage) are returned. Here, we can expect\\nthe complete variety of dynamical sys-\\ntems. That is the reason why I particu-\\nlarly want to refer to the literature con-\\ncerning dynamical systems.\\n\\n121\\n\\n\\n\\nChapter 7 Recurrent perceptron-like networks (depends on chapter 5) dkriesel.com\\n\\nFigure 7.1: The Roessler attractor\\n\\nFurther discussions could reveal what will\\nhappen if the input of recurrent networks\\nis changed.\\n\\nIn this chapter the related paradigms of\\nrecurrent networks according to Jordan\\nand Elman will be introduced.\\n\\n7.1 Jordan networks\\n\\nA Jordan network [Jor86] is a multi-\\nlayer perceptron with a set K of so-called\\ncontext neurons k1, k2, . . . , k|K|. There\\nis one context neuron per output neuron\\n(fig. 7.2 on the next page). In principle, a\\ncontext neuron just memorizes an output\\nuntil it can be processed in the next time output\\n\\nneurons\\nare buffered\\n\\nstep. Therefore, there are weighted con-\\nnections between each output neuron and\\none context neuron. The stored values are\\nreturned to the actual network by means\\nof complete links between the context neu-\\nrons and the input layer.\\n\\nIn the originial definition of a Jordan net-\\nwork the context neurons are also recur-\\nrent to themselves via a connecting weight\\nλ. But most applications omit this recur-\\nrence since the Jordan network is already\\nvery dynamic and difficult to analyze, even\\nwithout these additional recurrences.\\n\\nDefinition 7.1 (Context neuron). A con-\\ntext neuron k receives the output value of\\nanother neuron i at a time t and then reen-\\nters it into the network at a time (t+ 1).\\n\\nDefinition 7.2 (Jordan network). A Jor-\\ndan network is a multilayer perceptron\\n\\n122 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 7.2 Elman networks\\n\\n�� ��GFED@ABCi1\\n\\n~~}}}}}}}}}\\n\\n  AAAAAAAAA\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\n~~}}}}}}}}}\\n\\n  AAAAAAAAA\\nGFED@ABCk2\\n\\n����xx\\n\\nGFED@ABCk1\\n\\n��{{vvGFED@ABCh1\\n\\n  AAAAAAAAA\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCh2\\n\\n~~}}}}}}}}}\\n\\n  AAAAAAAAA\\nGFED@ABCh3\\n\\n~~}}}}}}}}}\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\nGFED@ABCΩ1\\n\\n��\\n\\n@A BC\\n\\nOO\\n\\nGFED@ABCΩ2\\n\\n��\\n\\n�� ��\\n\\nOO\\n\\nFigure 7.2: Illustration of a Jordan network. The network output is buffered in the context neurons\\nand with the next time step it is entered into the network together with the new input.\\n\\nwith one context neuron per output neu-\\nron. The set of context neurons is called\\nK. The context neurons are completely\\nlinked toward the input layer of the net-\\nwork.\\n\\n7.2 Elman networks\\n\\nThe Elman networks (a variation of\\nthe Jordan networks) [Elm90] have con-\\ntext neurons, too, but one layer of context\\nneurons per information processing neu-\\nron layer (fig. 7.3 on the following page).\\nThus, the outputs of each hidden neuron\\n\\nnearly every-\\nthing is\\nbuffered\\n\\nor output neuron are led into the associ-\\nated context layer (again exactly one con-\\ntext neuron per neuron) and from there it\\nis reentered into the complete neuron layer\\n\\nduring the next time step (i.e. again a com-\\nplete link on the way back). So the com-\\nplete information processing part1 of the\\nMLP exists a second time as a \\"context\\nversion\\" – which once again considerably\\nincreases dynamics and state variety.\\n\\nCompared with Jordan networks the El-\\nman networks often have the advantage to\\nact more purposeful since every layer can\\naccess its own context.\\n\\nDefinition 7.3 (Elman network). An El-\\nman network is an MLP with one con-\\ntext neuron per information processing\\nneuron. The set of context neurons is\\ncalledK. This means that there exists one\\ncontext layer per information processing\\n\\n1 Remember: The input layer does not process in-\\nformation.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 123\\n\\n\\n\\nChapter 7 Recurrent perceptron-like networks (depends on chapter 5) dkriesel.com\\n\\n�� ��GFED@ABCi1\\n\\n~~~~~~~~~~~~\\n\\n  @@@@@@@@@@\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\n~~~~~~~~~~~~\\n\\n  @@@@@@@@@@\\n\\nGFED@ABCh1\\n\\n��@@@@@@@@@\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUUU 44\\nGFED@ABCh2\\n\\n��~~~~~~~~~\\n\\n��@@@@@@@@@ 55\\nGFED@ABCh3\\n\\n��~~~~~~~~~\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii 55\\nONMLHIJKkh1\\n\\nuu zzvv ONMLHIJKkh2\\n\\nwwuutt ONMLHIJKkh3\\n\\nvvuutt\\n\\nGFED@ABCΩ1\\n\\n��\\n\\n55\\nGFED@ABCΩ2 55\\n\\n��\\n\\nONMLHIJKkΩ1\\n\\nuu ww ONMLHIJKkΩ2\\n\\nuu vv\\n\\nFigure 7.3: Illustration of an Elman network. The entire information processing part of the network\\nexists, in a way, twice. The output of each neuron (except for the output of the input neurons)\\nis buffered and reentered into the associated layer. For the reason of clarity I named the context\\nneurons on the basis of their models in the actual network, but it is not mandatory to do so.\\n\\nneuron layer with exactly the same num-\\nber of context neurons. Every neuron has\\na weighted connection to exactly one con-\\ntext neuron while the context layer is com-\\npletely linked towards its original layer.\\n\\nNow it is interesting to take a look at the\\ntraining of recurrent networks since, for in-\\nstance, ordinary backpropagation of error\\ncannot work on recurrent networks. Once\\nagain, the style of the following part is\\nrather informal, which means that I will\\nnot use any formal definitions.\\n\\n7.3 Training recurrent\\nnetworks\\n\\nIn order to explain the training as compre-\\nhensible as possible, we have to agree on\\nsome simplifications that do not affect the\\nlearning principle itself.\\n\\nSo for the training let us assume that in\\nthe beginning the context neurons are ini-\\ntiated with an input, since otherwise they\\nwould have an undefined input (this is no\\nsimplification but reality).\\n\\nFurthermore, we use a Jordan network\\nwithout a hidden neuron layer for our\\ntraining attempts so that the output neu-\\n\\n124 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 7.3 Training recurrent networks\\n\\nrons can directly provide input. This ap-\\nproach is a strong simplification because\\ngenerally more complicated networks are\\nused. But this does not change the learn-\\ning principle.\\n\\n7.3.1 Unfolding in time\\n\\nRemember our actual learning procedure\\nfor MLPs, the backpropagation of error,\\nwhich backpropagates the delta values.\\nSo, in case of recurrent networks the\\ndelta values would backpropagate cycli-\\ncally through the network again and again,\\nwhich makes the training more difficult.\\nOn the one hand we cannot know which\\nof the many generated delta values for a\\nweight should be selected for training, i.e.\\nwhich values are useful. On the other hand\\nwe cannot definitely know when learning\\nshould be stopped. The advantage of re-\\ncurrent networks are great state dynamics\\nwithin the network; the disadvantage of\\nrecurrent networks is that these dynamics\\nare also granted to the training and there-\\nfore make it difficult.\\n\\nOne learning approach would be the at-\\ntempt to unfold the temporal states of\\nthe network (fig. 7.4 on the next page):\\nRecursions are deleted by putting a sim-\\nilar network above the context neurons,\\ni.e. the context neurons are, as a man-\\nner of speaking, the output neurons of\\nthe attached network. More generally spo-\\nken, we have to backtrack the recurrences\\nand place \\"‘earlier\\"’ instances of neurons\\nin the network – thus creating a larger,\\n\\nbut forward-oriented network without re-\\ncurrences. This enables training a recur-\\nrent network with any training strategy\\ndeveloped for non-recurrent ones. Here,\\n\\nattach\\nthe same\\nnetwork\\nto each\\ncontext\\nlayer\\n\\nthe input is entered as teaching input into\\nevery \\"copy\\" of the input neurons. This\\ncan be done for a discrete number of time\\nsteps. These training paradigms are called\\nunfolding in time [MP69]. After the un-\\nfolding a training by means of backpropa-\\ngation of error is possible.\\n\\nBut obviously, for one weight wi,j sev-\\neral changing values ∆wi,j are received,\\nwhich can be treated differently: accumu-\\nlation, averaging etc. A simple accumu-\\nlation could possibly result in enormous\\nchanges per weight if all changes have the\\nsame sign. Hence, also the average is not\\nto be underestimated. We could also intro-\\nduce a discounting factor, which weakens\\nthe influence of ∆wi,j of the past.\\n\\nUnfolding in time is particularly useful if\\nwe receive the impression that the closer\\npast is more important for the network\\nthan the one being further away. The\\nreason for this is that backpropagation\\nhas only little influence in the layers far-\\nther away from the output (remember:\\nthe farther we are from the output layer,\\nthe smaller the influence of backpropaga-\\ntion).\\n\\nDisadvantages: the training of such an un-\\nfolded network will take a long time since\\na large number of layers could possibly be\\nproduced. A problem that is no longer\\nnegligible is the limited computational ac-\\ncuracy of ordinary computers, which is\\nexhausted very fast because of so many\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 125\\n\\n\\n\\nChapter 7 Recurrent perceptron-like networks (depends on chapter 5) dkriesel.com\\n\\n�� �� ��GFED@ABCi1\\n\\n\'\'OOOOOOOOOOOOOOOO\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\\n\\n��@@@@@@@@@\\n\\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCi3\\n\\n��   AAAAAAAAA\\nGFED@ABCk1\\n\\nwwnnnnnnnnnnnnnnnnn\\n\\n~~}}}}}}}}}\\nGFED@ABCk2\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\nwwnnnnnnnnnnnnnnnnn\\n\\nGFED@ABCΩ1@A BC\\n\\nOO\\n\\n��\\n\\nGFED@ABCΩ2�� ��\\n\\nOO\\n\\n��\\n\\n��\\n\\n...\\n\\n��\\n\\n...\\n\\n��\\n\\n...\\n...\\n\\n...\\n\\n�� �� ��\\n\\n/.-,()*+\\n\\n((RRRRRRRRRRRRRRRRR\\n\\n**VVVVVVVVVVVVVVVVVVVVVVVV /.-,()*+\\n\\n!!CCCCCCCCC\\n\\n((PPPPPPPPPPPPPPP /.-,()*+\\n�� ��???????? /.-,()*+\\n\\nwwoooooooooooooo\\n\\n����������\\n/.-,()*+\\n\\nttjjjjjjjjjjjjjjjjjjjjj\\n\\nwwoooooooooooooo\\n\\n�� �� ��\\n\\n/.-,()*+\\n\\n((RRRRRRRRRRRRRRRRRR\\n\\n**VVVVVVVVVVVVVVVVVVVVVVVVVVV /.-,()*+\\n\\n!!DDDDDDDDDD\\n\\n((QQQQQQQQQQQQQQQQQQ /.-,()*+\\n�� !!CCCCCCCCC /.-,()*+\\n\\nvvnnnnnnnnnnnnnnnn\\n\\n�����������\\n/.-,()*+\\n\\nttjjjjjjjjjjjjjjjjjjjjjjj\\n\\nwwppppppppppppppp\\n\\nGFED@ABCi1\\n\\n\'\'OOOOOOOOOOOOOOOO\\n\\n**UUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\\n\\n��@@@@@@@@@\\n\\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCi3\\n\\n��   AAAAAAAAA\\nGFED@ABCk1\\n\\nwwnnnnnnnnnnnnnnnnn\\n\\n~~}}}}}}}}}\\nGFED@ABCk2\\n\\nttiiiiiiiiiiiiiiiiiiiiiiiiii\\n\\nwwnnnnnnnnnnnnnnnnn\\n\\nGFED@ABCΩ1\\n\\n��\\n\\nGFED@ABCΩ2\\n\\n��\\n\\nFigure 7.4: Illustration of the unfolding in time with a small exemplary recurrent MLP. Top: The\\nrecurrent MLP. Bottom: The unfolded network. For reasons of clarity, I only added names to\\nthe lowest part of the unfolded network. Dotted arrows leading into the network mark the inputs.\\nDotted arrows leading out of the network mark the outputs. Each \\"network copy\\" represents a time\\nstep of the network with the most recent time step being at the bottom.\\n\\n126 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 7.3 Training recurrent networks\\n\\nnested computations (the farther we are\\nfrom the output layer, the smaller the in-\\nfluence of backpropagation, so that this\\nlimit is reached). Furthermore, with sev-\\neral levels of context neurons this proce-\\ndure could produce very large networks to\\nbe trained.\\n\\n7.3.2 Teacher forcing\\n\\nOther procedures are the equivalent\\nteacher forcing and open loop learn-\\ning. They detach the recurrence during\\nthe learning process: We simply pretend\\n\\nteaching\\ninput\\n\\napplied at\\ncontext\\nneurons\\n\\nthat the recurrence does not exist and ap-\\nply the teaching input to the context neu-\\nrons during the training. So, backpropaga-\\ntion becomes possible, too. Disadvantage:\\nwith Elman networks a teaching input for\\nnon-output-neurons is not given.\\n\\n7.3.3 Recurrent backpropagation\\n\\nAnother popular procedure without lim-\\nited time horizon is the recurrent back-\\npropagation using methods of differ-\\nential calculus to solve the problem\\n[Pin87].\\n\\n7.3.4 Training with evolution\\n\\nDue to the already long lasting train-\\ning time, evolutionary algorithms have\\nproved to be of value, especially with recur-\\nrent networks. One reason for this is that\\nthey are not only unrestricted with respect\\nto recurrences but they also have other ad-\\nvantages when the mutation mechanisms\\n\\nare chosen suitably: So, for example, neu-\\nrons and weights can be adjusted and\\nthe network topology can be optimized\\n(of course the result of learning is not\\nnecessarily a Jordan or Elman network).\\nWith ordinary MLPs, however, evolution-\\nary strategies are less popular since they\\ncertainly need a lot more time than a di-\\nrected learning procedure such as backpro-\\npagation.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 127\\n\\n\\n\\n\\n\\nChapter 8\\n\\nHopfield networks\\nIn a magnetic field, each particle applies a force to any other particle so that\\nall particles adjust their movements in the energetically most favorable way.\\nThis natural mechanism is copied to adjust noisy inputs in order to match\\n\\ntheir real models.\\n\\nAnother supervised learning example of\\nthe wide range of neural networks was\\ndeveloped by John Hopfield: the so-\\ncalled Hopfield networks [Hop82]. Hop-\\nfield and his physically motivated net-\\nworks have contributed a lot to the renais-\\nsance of neural networks.\\n\\n8.1 Hopfield networks are\\ninspired by particles in a\\nmagnetic field\\n\\nThe idea for the Hopfield networks origi-\\nnated from the behavior of particles in a\\nmagnetic field: Every particle \\"communi-\\ncates\\" (by means of magnetic forces) with\\nevery other particle (completely linked)\\nwith each particle trying to reach an ener-\\ngetically favorable state (i.e. a minimum\\nof the energy function). As for the neurons\\nthis state is known as activation. Thus,\\nall particles or neurons rotate and thereby\\n\\nencourage each other to continue this rota-\\ntion. As a manner of speaking, our neural\\nnetwork is a cloud of particles\\n\\nBased on the fact that the particles auto-\\nmatically detect the minima of the energy\\nfunction, Hopfield had the idea to use the\\n\\"spin\\" of the particles to process informa-\\ntion: Why not letting the particles search\\nminima on arbitrary functions? Even if we\\nonly use two of those spins, i.e. a binary\\nactivation, we will recognize that the devel-\\noped Hopfield network shows considerable\\ndynamics.\\n\\n8.2 In a hopfield network, all\\nneurons influence each\\nother symmetrically\\n\\nBriefly speaking, a Hopfield network con-\\nsists of a set K of completely linked neu- JKrons with binary activation (since we only\\n\\n129\\n\\n\\n\\nChapter 8 Hopfield networks dkriesel.com\\n\\n?>=<89:;↑ ii\\n\\n��\\n\\nii\\n\\n))SSSSSSSSSSSSSSSSSSSSSSSS\\nOO\\n\\n��\\n\\noo //\\n^^\\n\\n��<<<<<<<<<\\n?>=<89:;↓55\\n\\nuukkkkkkkkkkkkkkkkkkkkkkkk\\nOO\\n\\n��\\n\\n@@\\n\\n����������� ^^\\n\\n��<<<<<<<<<\\n\\n?>=<89:;↑ ii\\n\\n))SSSSSSSSSSSSSSSSSSSSSSSSoo //\\n��\\n\\n@@��������� ?>=<89:;↓ ?>=<89:;↑44jj 55\\n\\nuukkkkkkkkkkkkkkkkkkkkkkkk//oo\\n@@\\n\\n�����������\\n\\n?>=<89:;↓\\n\\n\\n\\n\\n66\\n\\n��\\n\\n@@�����������\\n\\n^^<<<<<<<<< ?>=<89:;↑//oo\\n��\\n\\n^^<<<<<<<<<\\n\\nFigure 8.1: Illustration of an exemplary Hop-\\nfield network. The arrows ↑ and ↓ mark the\\nbinary \\"spin\\". Due to the completely linked neu-\\nrons the layers cannot be separated, which means\\nthat a Hopfield network simply includes a set of\\nneurons.\\n\\nuse two spins), with the weights being\\nsymmetric between the individual neurons\\n\\ncompletely\\nlinked\\nset of\\n\\nneurons\\n\\nand without any neuron being directly con-\\nnected to itself (fig. 8.1). Thus, the state\\nof |K| neurons with two possible states\\n∈ {−1, 1} can be described by a string\\nx ∈ {−1, 1}|K|.\\n\\nThe complete link provides a full square\\nmatrix of weights between the neurons.\\nThe meaning of the weights will be dis-\\ncussed in the following. Furthermore, we\\nwill soon recognize according to which\\nrules the neurons are spinning, i.e. are\\nchanging their state.\\n\\nAdditionally, the complete link leads to\\nthe fact that we do not know any input,\\noutput or hidden neurons. Thus, we have\\nto think about how we can input some-\\nthing into the |K| neurons.\\n\\nDefinition 8.1 (Hopfield network). A\\nHopfield network consists of a set K of\\ncompletely linked neurons without direct\\nrecurrences. The activation function of\\nthe neurons is the binary threshold func-\\ntion with outputs ∈ {1,−1}.\\n\\nDefinition 8.2 (State of a Hopfield net-\\nwork). The state of the network con-\\nsists of the activation states of all neu-\\nrons. Thus, the state of the network can\\nbe understood as a binary string z ∈\\n{−1, 1}|K|.\\n\\n8.2.1 Input and output of a\\nHopfield network are\\nrepresented by neuron states\\n\\nWe have learned that a network, i.e. a\\nset of |K| particles, that is in a state\\nis automatically looking for a minimum.\\nAn input pattern of a Hopfield network\\nis exactly such a state: A binary string\\nx ∈ {−1, 1}|K| that initializes the neurons.\\nThen the network is looking for the min-\\nimum to be taken (which we have previ-\\nously defined by the input of training sam-\\nples) on its energy surface.\\n\\nBut when do we know that the minimum\\nhas been found? This is simple, too: when\\n\\ninput and\\noutput =\\nnetwork\\nstates\\n\\nthe network stops. It can be proven that a\\nHopfield network with a symmetric weight\\nmatrix that has zeros on its diagonal al-\\nways converges [CG88], i.e. at some point\\n\\nalways\\nconvergesit will stand still. Then the output is a\\n\\nbinary string y ∈ {−1, 1}|K|, namely the\\nstate string of the network that has found\\na minimum.\\n\\n130 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 8.2 Structure and functionality\\n\\nNow let us take a closer look at the con-\\ntents of the weight matrix and the rules\\nfor the state change of the neurons.\\nDefinition 8.3 (Input and output of\\na Hopfield network). The input of a\\nHopfield network is binary string x ∈\\n{−1, 1}|K| that initializes the state of the\\nnetwork. After the convergence of the\\nnetwork, the output is the binary string\\ny ∈ {−1, 1}|K| generated from the new net-\\nwork state.\\n\\n8.2.2 Significance of weights\\n\\nWe have already said that the neurons\\nchange their states, i.e. their direction,\\nfrom −1 to 1 or vice versa. These spins oc-\\ncur dependent on the current states of the\\nother neurons and the associated weights.\\nThus, the weights are capable to control\\nthe complete change of the network. The\\nweights can be positive, negative, or 0.\\nColloquially speaking, for a weight wi,j be-\\ntween two neurons i and j the following\\nholds:\\n\\nIf wi,j is positive, it will try to force the\\ntwo neurons to become equal – the\\nlarger they are, the harder the net-\\nwork will try. If the neuron i is in\\nstate 1 and the neuron j is in state\\n−1, a high positive weight will advise\\nthe two neurons that it is energeti-\\ncally more favorable to be equal.\\n\\nIf wi,j is negative, its behavior will be\\nanaloguous only that i and j are\\nurged to be different. A neuron i in\\nstate −1 would try to urge a neuron\\nj into state 1.\\n\\nZero weights lead to the two involved\\nneurons not influencing each other.\\n\\nThe weights as a whole apparently take\\nthe way from the current state of the net-\\nwork towards the next minimum of the en-\\nergy function. We now want to discuss\\nhow the neurons follow this way.\\n\\n8.2.3 A neuron changes its state\\naccording to the influence of\\nthe other neurons\\n\\nOnce a network has been trained and\\ninitialized with some starting state, the\\nchange of state xk of the individual neu-\\nrons k occurs according to the scheme\\n\\nxk(t) = fact\\n\\n\uf8eb\uf8ed∑\\nj∈K\\n\\nwj,k · xj(t− 1)\\n\\n\uf8f6\uf8f8 (8.1)\\n\\nin each time step, where the function fact\\ngenerally is the binary threshold function\\n(fig. 8.2 on the next page) with threshold\\n0. Colloquially speaking: a neuron k cal-\\nculates the sum of wj,k · xj(t − 1), which\\nindicates how strong and into which direc-\\ntion the neuron k is forced by the other\\nneurons j. Thus, the new state of the net-\\nwork (time t) results from the state of the\\nnetwork at the previous time t − 1. This\\nsum is the direction into which the neuron\\nk is pushed. Depending on the sign of the\\nsum the neuron takes state 1 or −1.\\n\\nAnother difference between Hopfield net-\\nworks and other already known network\\ntopologies is the asynchronous update: A\\nneuron k is randomly chosen every time,\\nwhich then recalculates the activation.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 131\\n\\n\\n\\nChapter 8 Hopfield networks dkriesel.com\\n\\n−1\\n\\n−0.5\\n\\n 0\\n\\n 0.5\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nf(\\nx)\\n\\nx\\n\\nHeaviside Function\\n\\nFigure 8.2: Illustration of the binary threshold\\nfunction.\\n\\nThus, the new activation of the previously\\nchanged neurons immediately influences\\nthe network, i.e. one time step indicates\\nthe change of a single neuron.\\n\\nRegardless of the aforementioned random\\nselection of the neuron, a Hopfield net-\\nwork is often much easier to implement:\\nThe neurons are simply processed one af-\\nter the other and their activations are re-\\ncalculated until no more changes occur.\\n\\nrandom\\nneuron\\n\\ncalculates\\nnew\\n\\nactivation\\n\\nDefinition 8.4 (Change in the state of\\na Hopfield network). The change of state\\nof the neurons occurs asynchronously with\\nthe neuron to be updated being randomly\\nchosen and the new state being generated\\nby means of this rule:\\n\\nxk(t) = fact\\n\\n\uf8eb\uf8ed∑\\nj∈J\\n\\nwj,k · xj(t− 1)\\n\\n\uf8f6\uf8f8 .\\nNow that we know how the weights influ-\\nence the changes in the states of the neu-\\nrons and force the entire network towards\\n\\na minimum, then there is the question of\\nhow to teach the weights to force the net-\\nwork towards a certain minimum.\\n\\n8.3 The weight matrix is\\ngenerated directly out of\\nthe training patterns\\n\\nThe aim is to generate minima on the\\nmentioned energy surface, so that at an\\ninput the network can converge to them.\\nAs with many other network paradigms,\\nwe use a set P of training patterns p ∈\\n{1,−1}|K|, representing the minima of our\\nenergy surface.\\n\\nUnlike many other network paradigms, we\\ndo not look for the minima of an unknown\\nerror function but define minima on such a\\nfunction. The purpose is that the network\\nshall automatically take the closest min-\\nimum when the input is presented. For\\nnow this seems unusual, but we will un-\\nderstand the whole purpose later.\\n\\nRoughly speaking, the training of a Hop-\\nfield network is done by training each train-\\ning pattern exactly once using the rule\\ndescribed in the following (Single Shot\\nLearning), where pi and pj are the states\\nof the neurons i and j under p ∈ P :\\n\\nwi,j =\\n∑\\np∈P\\n\\npi · pj (8.2)\\n\\nThis results in the weight matrix W . Col-\\nloquially speaking: We initialize the net-\\nwork by means of a training pattern and\\nthen process weights wi,j one after another.\\n\\n132 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 8.4 Autoassociation and traditional application\\n\\nFor each of these weights we verify: Are\\nthe neurons i, j n the same state or do the\\nstates vary? In the first case we add 1\\nto the weight, in the second case we add\\n−1.\\n\\nThis we repeat for each training pattern\\np ∈ P . Finally, the values of the weights\\nwi,j are high when i and j corresponded\\nwith many training patterns. Colloquially\\nspeaking, this high value tells the neurons:\\n\\"Often, it is energetically favorable to hold\\nthe same state\\". The same applies to neg-\\native weights.\\n\\nDue to this training we can store a certain\\nfixed number of patterns p in the weight\\nmatrix. At an input x the network will\\nconverge to the stored pattern that is clos-\\nest to the input p.\\n\\nUnfortunately, the number of the maxi-\\nmum storable and reconstructible patterns\\np is limited to\\n\\n|P |MAX ≈ 0.139 · |K|, (8.3)\\n\\nwhich in turn only applies to orthogo-\\nnal patterns. This was shown by precise\\n(and time-consuming) mathematical anal-\\nyses, which we do not want to specify\\nnow. If more patterns are entered, already\\nstored information will be destroyed.\\n\\nDefinition 8.5 (Learning rule for Hop-\\nfield networks). The individual elements\\nof the weight matrix W are defined by a\\nsingle processing of the learning rule\\n\\nwi,j =\\n∑\\np∈P\\n\\npi · pj ,\\n\\nwhere the diagonal of the matrix is covered\\nwith zeros. Here, no more than |P |MAX ≈\\n\\n0.139 · |K| training samples can be trained\\nand at the same time maintain their func-\\ntion.\\n\\nNow we know the functionality of Hopfield\\nnetworks but nothing about their practical\\nuse.\\n\\n8.4 Autoassociation and\\ntraditional application\\n\\nHopfield networks, like those mentioned\\nabove, are called autoassociators. An\\nautoassociator a exactly shows the afore- Jamentioned behavior: Firstly, when a\\nknown pattern p is entered, exactly this\\nknown pattern is returned. Thus,\\n\\na(p) = p,\\n\\nwith a being the associative mapping. Sec-\\nondly, and that is the practical use, this\\nalso works with inputs that are close to a\\npattern:\\n\\na(p+ ε) = p.\\n\\nAfterwards, the autoassociator is, in any\\ncase, in a stable state, namely in the state\\np.\\n\\nIf the set of patterns P consists of, for ex-\\nnetwork\\nrestores\\ndamaged\\ninputs\\n\\nample, letters or other characters in the\\nform of pixels, the network will be able to\\ncorrectly recognize deformed or noisy let-\\nters with high probability (fig. 8.3 on the\\nfollowing page).\\n\\nThe primary fields of application of Hop-\\nfield networks are pattern recognition\\nand pattern completion, such as the zip\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 133\\n\\n\\n\\nChapter 8 Hopfield networks dkriesel.com\\n\\nFigure 8.3: Illustration of the convergence of an\\nexemplary Hopfield network. Each of the pic-\\ntures has 10 × 12 = 120 binary pixels. In the\\nHopfield network each pixel corresponds to one\\nneuron. The upper illustration shows the train-\\ning samples, the lower shows the convergence of\\na heavily noisy 3 to the corresponding training\\nsample.\\n\\ncode recognition on letters in the eighties.\\nBut soon the Hopfield networks were re-\\nplaced by other systems in most of their\\nfields of application, for example by OCR\\nsystems in the field of letter recognition.\\nToday Hopfield networks are virtually no\\nlonger used, they have not become estab-\\nlished in practice.\\n\\n8.5 Heteroassociation and\\nanalogies to neural data\\nstorage\\n\\nSo far we have been introduced to Hopfield\\nnetworks that converge from an arbitrary\\ninput into the closest minimum of a static\\nenergy surface.\\n\\nAnother variant is a dynamic energy sur-\\nface: Here, the appearance of the energy\\nsurface depends on the current state and\\nwe receive a heteroassociator instead of\\nan autoassociator. For a heteroassocia-\\ntor\\n\\na(p+ ε) = p\\n\\nis no longer true, but rather\\n\\nh(p+ ε) = q,\\n\\nwhich means that a pattern is mapped\\nonto another one. h is the heteroasso- Jhciative mapping. Such heteroassociations\\nare achieved by means of an asymmetric\\nweight matrix V .\\n\\n134 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 8.5 Heteroassociation and analogies to neural data storage\\n\\nHeteroassociations connected in series of\\nthe form\\n\\nh(p+ ε) = q\\n\\nh(q + ε) = r\\n\\nh(r + ε) = s\\n\\n...\\nh(z + ε) = p\\n\\ncan provoke a fast cycle of states\\n\\np→ q → r → s→ . . .→ z → p,\\n\\nwhereby a single pattern is never com-\\npletely accepted: Before a pattern is en-\\ntirely completed, the heteroassociation al-\\nready tries to generate the successor of this\\npattern. Additionally, the network would\\nnever stop, since after having reached the\\nlast state z, it would proceed to the first\\nstate p again.\\n\\n8.5.1 Generating the\\nheteroassociative matrix\\n\\nWe generate the matrix V by means of el-\\nVI ements v very similar to the autoassocia-\\nvI tive matrix with p being (per transition)\\n\\nthe training sample before the transition\\nand q being the training sample to be gen-\\n\\nqI erated from p:\\n\\nvi,j =\\n∑\\n\\np,q∈P,p6=q\\npiqj (8.4)\\n\\nThe diagonal of the matrix is again filled\\nwith zeros. The neuron states are, as al-\\n\\nnetword\\nis instable\\n\\nwhile\\nchanging\\n\\nstates\\n\\nways, adapted during operation. Several\\ntransitions can be introduced into the ma-\\ntrix by a simple addition, whereby the said\\nlimitation exists here, too.\\n\\nDefinition 8.6 (Learning rule for the het-\\neroassociative matrix). For two training\\nsamples p being predecessor and q being\\nsuccessor of a heteroassociative transition\\nthe weights of the heteroassociative matrix\\nV result from the learning rule\\n\\nvi,j =\\n∑\\n\\np,q∈P,p6=q\\npiqj ,\\n\\nwith several heteroassociations being intro-\\nduced into the network by a simple addi-\\ntion.\\n\\n8.5.2 Stabilizing the\\nheteroassociations\\n\\nWe have already mentioned the problem\\nthat the patterns are not completely gen-\\nerated but that the next pattern is already\\nbeginning before the generation of the pre-\\nvious pattern is finished.\\n\\nThis problem can be avoided by not only\\ninfluencing the network by means of the\\nheteroassociative matrix V but also by\\nthe already known autoassociative matrix\\nW .\\n\\nAdditionally, the neuron adaptation rule\\nis changed so that competing terms are\\ngenerated: One term autoassociating an\\nexisting pattern and one term trying to\\nconvert the very same pattern into its suc-\\ncessor. The associative rule provokes that\\nthe network stabilizes a pattern, remains\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 135\\n\\n\\n\\nChapter 8 Hopfield networks dkriesel.com\\n\\nthere for a while, goes on to the next pat-\\ntern, and so on.\\n\\nxi(t+ 1) = (8.5)\\n\\nfact\\n\\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\\n∑\\nj∈K\\n\\nwi,jxj(t)︸ ︷︷ ︸\\nautoassociation\\n\\n+\\n∑\\nk∈K\\n\\nvi,kxk(t−∆t)︸ ︷︷ ︸\\nheteroassociation\\n\\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\\nHere, the value ∆t causes, descriptively\\n\\n∆tI\\nstable change\\n\\nin states\\n\\nspeaking, the influence of the matrix V\\nto be delayed, since it only refers to a\\nnetwork being ∆t versions behind. The\\nresult is a change in state, during which\\nthe individual states are stable for a short\\nwhile. If ∆t is set to, for example, twenty\\nsteps, then the asymmetric weight matrix\\nwill realize any change in the network only\\ntwenty steps later so that it initially works\\nwith the autoassociative matrix (since it\\nstill perceives the predecessor pattern of\\nthe current one), and only after that it will\\nwork against it.\\n\\n8.5.3 Biological motivation of\\nheterassociation\\n\\nFrom a biological point of view the transi-\\ntion of stable states into other stable states\\nis highly motivated: At least in the begin-\\nning of the nineties it was assumed that\\nthe Hopfield modell will achieve an ap-\\nproximation of the state dynamics in the\\nbrain, which realizes much by means of\\nstate chains: When I would ask you, dear\\nreader, to recite the alphabet, you gener-\\nally will manage this better than (please\\ntry it immediately) to answer the follow-\\ning question:\\n\\nWhich letter in the alphabet follows the\\nletter P?\\n\\nAnother example is the phenomenon that\\none cannot remember a situation, but the\\nplace at which one memorized it the last\\ntime is perfectly known. If one returns\\nto this place, the forgotten situation often\\ncomes back to mind.\\n\\n8.6 Continuous Hopfield\\nnetworks\\n\\nSo far, we only have discussed Hopfield net-\\nworks with binary activations. But Hop-\\nfield also described a version of his net-\\nworks with continuous activations [Hop84],\\nwhich we want to cover at least briefly:\\ncontinuous Hopfield networks. Here,\\nthe activation is no longer calculated by\\nthe binary threshold function but by the\\nFermi function with temperature parame-\\nters (fig. 8.4 on the next page).\\n\\nHere, the network is stable for symmetric\\nweight matrices with zeros on the diagonal,\\ntoo.\\n\\nHopfield also stated, that continuous Hop-\\nfield networks can be applied to find ac-\\nceptable solutions for the NP-hard trav-\\nelling salesman problem [HT85]. Accord-\\ning to some verification trials [Zel94] this\\nstatement can’t be kept up any more. But\\ntoday there are faster algorithms for han-\\ndling this problem and therefore the Hop-\\nfield network is no longer used here.\\n\\n136 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 8.6 Continuous Hopfield networks\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nf(\\nx)\\n\\nx\\n\\nFermi Function with Temperature Parameter\\n\\nFigure 8.4: The already known Fermi function\\nwith different temperature parameter variations.\\n\\nExercises\\n\\nExercise 14. Indicate the storage re-\\nquirements for a Hopfield network with\\n|K| = 1000 neurons when the weights wi,j\\nshall be stored as integers. Is it possible\\nto limit the value range of the weights in\\norder to save storage space?\\n\\nExercise 15. Compute the weights wi,j\\nfor a Hopfield network using the training\\nset\\n\\nP ={(−1,−1,−1,−1,−1, 1);\\n(−1, 1, 1,−1,−1,−1);\\n(1,−1,−1, 1,−1, 1)}.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 137\\n\\n\\n\\n\\n\\nChapter 9\\n\\nLearning vector quantization\\nLearning Vector Quantization is a learning procedure with the aim to represent\\n\\nthe vector training sets divided into predefined classes as well as possible by\\nusing a few representative vectors. If this has been managed, vectors which\\n\\nwere unkown until then could easily be assigned to one of these classes.\\n\\nSlowly, part II of this text is nearing its\\nend – and therefore I want to write a last\\nchapter for this part that will be a smooth\\ntransition into the next one: A chapter\\nabout the learning vector quantization\\n(abbreviated LVQ) [Koh89] described by\\nTeuvo Kohonen, which can be charac-\\nterized as being related to the self orga-\\nnizing feature maps. These SOMs are de-\\nscribed in the next chapter that already\\nbelongs to part III of this text, since SOMs\\nlearn unsupervised. Thus, after the explo-\\nration of LVQ I want to bid farewell to\\nsupervised learning.\\n\\nPreviously, I want to announce that there\\nare different variations of LVQ, which will\\nbe mentioned but not exactly represented.\\nThe goal of this chapter is rather to ana-\\nlyze the underlying principle.\\n\\n9.1 About quantization\\n\\nIn order to explore the learning vec-\\ntor quantization we should at first get\\na clearer picture of what quantization\\n(which can also be referred to as dis-\\ncretization) is.\\n\\nEverybody knows the sequence of discrete\\nnumbers\\n\\nN = {1, 2, 3, . . .},\\n\\nwhich contains the natural numbers. Dis-\\ncrete means, that this sequence consists of\\n\\ndiscrete\\n= separatedseparated elements that are not intercon-\\n\\nnected. The elements of our example are\\nexactly such numbers, because the natural\\nnumbers do not include, for example, num-\\nbers between 1 and 2. On the other hand,\\nthe sequence of real numbers R, for in-\\nstance, is continuous: It does not matter\\nhow close two selected numbers are, there\\nwill always be a number between them.\\n\\n139\\n\\n\\n\\nChapter 9 Learning vector quantization dkriesel.com\\n\\nQuantization means that a continuous\\nspace is divided into discrete sections: By\\ndeleting, for example, all decimal places\\nof the real number 2.71828, it could be\\nassigned to the natural number 2. Here\\nit is obvious that any other number hav-\\ning a 2 in front of the comma would also\\nbe assigned to the natural number 2, i.e.\\n2 would be some kind of representative\\nfor all real numbers within the interval\\n[2; 3).\\n\\nIt must be noted that a sequence can be ir-\\nregularly quantized, too: For instance, the\\ntimeline for a week could be quantized into\\nworking days and weekend.\\n\\nA special case of quantization is digiti-\\nzation: In case of digitization we always\\ntalk about regular quantization of a con-\\ntinuous space into a number system with\\nrespect to a certain basis. If we enter, for\\nexample, some numbers into the computer,\\nthese numbers will be digitized into the bi-\\nnary system (basis 2).\\n\\nDefinition 9.1 (Quantization). Separa-\\ntion of a continuous space into discrete sec-\\ntions.\\n\\nDefinition 9.2 (Digitization). Regular\\nquantization.\\n\\n9.2 LVQ divides the input\\nspace into separate areas\\n\\nNow it is almost possible to describe by\\nmeans of its name what LVQ should en-\\nable us to do: A set of representatives\\nshould be used to divide an input space\\n\\ninto classes that reflect the input space\\nas well as possible (fig. 9.1 on the facing input space\\n\\nreduced to\\nvector repre-\\nsentatives\\n\\npage). Thus, each element of the input\\nspace should be assigned to a vector as a\\nrepresentative, i.e. to a class, where the\\nset of these representatives should repre-\\nsent the entire input space as precisely as\\npossible. Such a vector is called codebook\\nvector. A codebook vector is the represen-\\ntative of exactly those input space vectors\\nlying closest to it, which divides the input\\nspace into the said discrete areas.\\n\\nIt is to be emphasized that we have to\\nknow in advance how many classes we\\nhave and which training sample belongs\\nto which class. Furthermore, it is impor-\\ntant that the classes must not be disjoint,\\nwhich means they may overlap.\\n\\nSuch separation of data into classes is in-\\nteresting for many problems for which it\\nis useful to explore only some characteris-\\ntic representatives instead of the possibly\\nhuge set of all vectors – be it because it is\\nless time-consuming or because it is suffi-\\nciently precise.\\n\\n9.3 Using codebook vectors:\\nthe nearest one is the\\nwinner\\n\\nThe use of a prepared set of codebook vec-\\ntors is very simple: For an input vector y\\nthe class association is easily decided by\\n\\nclosest\\nvector\\nwins\\n\\nconsidering which codebook vector is the\\nclosest – so, the codebook vectors build a\\nvoronoi diagram out of the set. Since\\n\\n140 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 9.4 Adjusting codebook vectors\\n\\nFigure 9.1: BExamples for quantization of a two-dimensional input space. DThe lines represent\\nthe class limit, the × mark the codebook vectors.\\n\\neach codebook vector can clearly be asso-\\nciated to a class, each input vector is asso-\\nciated to a class, too.\\n\\n9.4 Adjusting codebook\\nvectors\\n\\nAs we have already indicated, the LVQ is\\na supervised learning procedure. Thus, we\\nhave a teaching input that tells the learn-\\ning procedure whether the classification of\\nthe input pattern is right or wrong: In\\nother words, we have to know in advance\\nthe number of classes to be represented or\\nthe number of codebook vectors.\\n\\nRoughly speaking, it is the aim of the\\nlearning procedure that training samples\\n\\nare used to cause a previously defined num-\\nber of randomly initialized codebook vec-\\ntors to reflect the training data as precisely\\nas possible.\\n\\n9.4.1 The procedure of learning\\n\\nLearning works according to a simple\\nscheme. We have (since learning is su-\\npervised) a set P of |P | training samples.\\nAdditionally, we already know that classes\\nare predefined, too, i.e. we also have a set\\nof classes C. A codebook vector is clearly\\nassigned to each class. Thus, we can say\\nthat the set of classes |C| contains many\\ncodebook vectors C1, C2, . . . , C|C|.\\n\\nThis leads to the structure of the training\\nsamples: They are of the form (p, c) and\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 141\\n\\n\\n\\nChapter 9 Learning vector quantization dkriesel.com\\n\\ntherefore contain the training input vector\\np and its class affiliation c. For the class\\naffiliation\\n\\nc ∈ {1, 2, . . . , |C|}\\n\\nholds, which means that it clearly assigns\\nthe training sample to a class or a code-\\nbook vector.\\n\\nIntuitively, we could say about learning:\\n\\"Why a learning procedure? We calculate\\nthe average of all class members and place\\ntheir codebook vectors there – and that’s\\nit.\\" But we will see soon that our learning\\nprocedure can do a lot more.\\n\\nI only want to briefly discuss the steps\\nof the fundamental LVQ learning proce-\\ndure:\\n\\nInitialization: We place our set of code-\\nbook vectors on random positions in\\nthe input space.\\n\\nTraining sample: A training sample p of\\nour training set P is selected and pre-\\nsented.\\n\\nDistance measurement: We measure the\\ndistance ||p − C|| between all code-\\nbook vectors C1, C2, . . . , C|C| and our\\ninput p.\\n\\nWinner: The closest codebook vector\\nwins, i.e. the one with\\n\\nmin\\nCi∈C\\n\\n||p− Ci||.\\n\\nLearning process: The learning process\\ntakes place according to the rule\\n\\n∆Ci = η(t) · h(p, Ci) · (p− Ci)\\n(9.1)\\n\\nCi(t+ 1) = Ci(t) + ∆Ci, (9.2)\\n\\nwhich we now want to break down.\\n\\n. We have already seen that the first\\nfactor η(t) is a time-dependent learn-\\ning rate allowing us to differentiate\\nbetween large learning steps and fine\\ntuning.\\n\\n. The last factor (p − Ci) is obviously\\nthe direction toward which the code-\\nbook vector is moved.\\n\\n. But the function h(p, Ci) is the core of\\nthe rule: It implements a distinction\\nof cases.\\n\\nAssignment is correct: The winner\\nvector is the codebook vector of\\nthe class that includes p. In this Important!\\ncase, the function provides posi-\\ntive values and the codebook vec-\\ntor moves towards p.\\n\\nAssignment is wrong: The winner\\nvector does not represent the\\nclass that includes p. Therefore\\nit moves away from p.\\n\\nWe can see that our definition of the func-\\ntion h was not precise enough. With good\\nreason: From here on, the LVQ is divided\\ninto different nuances, dependent of how\\nexactly h and the learning rate should\\nbe defined (called LVQ1, LVQ2, LVQ3,\\n\\n142 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 9.5 Connection to neural networks\\n\\nOLVQ, etc). The differences are, for in-\\nstance, in the strength of the codebook vec-\\ntor movements. They are not all based on\\nthe same principle described here, and as\\nannounced I don’t want to discuss them\\nany further. Therefore I don’t give any\\nformal definition regarding the aforemen-\\ntioned learning rule and LVQ.\\n\\n9.5 Connection to neural\\nnetworks\\n\\nUntil now, in spite of the learning process,\\nthe question was what LVQ has to do with\\nneural networks. The codebook vectors\\ncan be understood as neurons with a fixed\\nposition within the input space, similar to\\nRBF networks. Additionally, in nature itvectors\\n\\n= neurons? often occurs that in a group one neuron\\nmay fire (a winner neuron, here: a code-\\nbook vector) and, in return, inhibits all\\nother neurons.\\n\\nI decided to place this brief chapter about\\nlearning vector quantization here so that\\nthis approach can be continued in the fol-\\nlowing chapter about self-organizing maps:\\nWe will classify further inputs by means of\\nneurons distributed throughout the input\\nspace, only that this time, we do not know\\nwhich input belongs to which class.\\n\\nNow let us take a look at the unsupervised\\nlearning networks!\\n\\nExercises\\n\\nExercise 16. Indicate a quantization\\nwhich equally distributes all vectors H ∈\\nH in the five-dimensional unit cube H into\\none of 1024 classes.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 143\\n\\n\\n\\n\\n\\nPart III\\n\\nUnsupervised learning network\\nparadigms\\n\\n145\\n\\n\\n\\n\\n\\nChapter 10\\n\\nSelf-organizing feature maps\\nA paradigm of unsupervised learning neural networks, which maps an input\\n\\nspace by its fixed topology and thus independently looks for simililarities.\\nFunction, learning procedure, variations and neural gas.\\n\\nIf you take a look at the concepts of biologi-\\ncal neural networks mentioned in the intro-\\nduction, one question will arise: How does\\nour brain store and recall the impressions\\nit receives every day. Let me point out\\nthat the brain does not have any training\\n\\nHow are\\ndata stored\\n\\nin the\\nbrain?\\n\\nsamples and therefore no \\"desired output\\".\\nAnd while already considering this subject\\nwe realize that there is no output in this\\nsense at all, too. Our brain responds to\\nexternal input by changes in state. These\\nare, so to speak, its output.\\n\\nBased on this principle and exploring\\nthe question of how biological neural net-\\nworks organize themselves, Teuvo Ko-\\nhonen developed in the Eighties his self-\\norganizing feature maps [Koh82, Koh98],\\nshortly referred to as self-organizing\\nmaps or SOMs. A paradigm of neural\\nnetworks where the output is the state of\\nthe network, which learns completely un-\\nsupervised, i.e. without a teacher.\\n\\nUnlike the other network paradigms we\\nhave already got to know, for SOMs it is\\nunnecessary to ask what the neurons calcu-\\nlate. We only ask which neuron is active at\\nthe moment. Biologically, this is very mo- no output,\\n\\nbut active\\nneuron\\n\\ntivated: If in biology the neurons are con-\\nnected to certain muscles, it will be less\\ninteresting to know how strong a certain\\nmuscle is contracted but which muscle is\\nactivated. In other words: We are not in-\\nterested in the exact output of the neuron\\nbut in knowing which neuron provides out-\\nput. Thus, SOMs are considerably more\\nrelated to biology than, for example, the\\nfeedforward networks, which are increas-\\ningly used for calculations.\\n\\n10.1 Structure of a\\nself-organizing map\\n\\nTypically, SOMs have – like our brain –\\nthe task to map a high-dimensional in-\\nput (N dimensions) onto areas in a low-\\n\\n147\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\ndimensional grid of cells (G dimensions)\\nto draw a map of the high-dimensional\\n\\nhigh-dim.\\ninput\\n↓\\n\\nlow-dim.\\nmap\\n\\nspace, so to speak. To generate this map,\\nthe SOM simply obtains arbitrary many\\npoints of the input space. During the in-\\nput of the points the SOM will try to cover\\nas good as possible the positions on which\\nthe points appear by its neurons. This par-\\nticularly means, that every neuron can be\\nassigned to a certain position in the input\\nspace.\\n\\nAt first, these facts seem to be a bit con-\\nfusing, and it is recommended to briefly\\nreflect about them. There are two spaces\\nin which SOMs are working:\\n\\n. The N -dimensional input space and\\n\\n. the G-dimensional grid on which the\\nneurons are lying and which indi-input space\\n\\nand topology cates the neighborhood relationships\\nbetween the neurons and therefore\\nthe network topology.\\n\\nIn a one-dimensional grid, the neurons\\ncould be, for instance, like pearls on a\\nstring. Every neuron would have exactly\\ntwo neighbors (except for the two end neu-\\nrons). A two-dimensional grid could be a\\nsquare array of neurons (fig. 10.1). An-\\nother possible array in two-dimensional\\nspace would be some kind of honeycomb\\nshape. Irregular topologies are possible,\\ntoo, but not very often. Topolgies with\\nmore dimensions and considerably more\\nneighborhood relationships would also be\\npossible, but due to their lack of visualiza-\\ntion capability they are not employed very\\noften.Important!\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\\n\\nFigure 10.1: Example topologies of a self-\\norganizing map. Above we can see a one-\\ndimensional topology, below a two-dimensional\\none.\\n\\nEven if N = G is true, the two spaces are\\nnot equal and have to be distinguished. In\\nthis special case they only have the same\\ndimension.\\n\\nInitially, we will briefly and formally re-\\ngard the functionality of a self-organizing\\nmap and then make it clear by means of\\nsome examples.\\n\\nDefinition 10.1 (SOM neuron). Similar\\nto the neurons in an RBF network a SOM\\nneuron k does not occupy a fixed position\\nck (a center) in the input space. Jc\\nDefinition 10.2 (Self-organizing map).\\nA self-organizing map is a set K of SOM\\nneurons. If an input vector is entered, ex- JKactly that neuron k ∈ K is activated which\\n\\n148 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.3 Training\\n\\nis closest to the input pattern in the input\\nspace. The dimension of the input space\\nis referred to as N .\\n\\nNI\\nDefinition 10.3 (Topology). The neu-\\nrons are interconnected by neighborhood\\nrelationships. These neighborhood rela-\\ntionships are called topology. The train-\\ning of a SOM is highly influenced by the\\ntopology. It is defined by the topology\\nfunction h(i, k, t), where i is the winner\\n\\niI neuron1 ist, k the neuron to be adapted\\nkI (which will be discussed later) and t the\\n\\ntimestep. The dimension of the topology\\nis referred to as G.\\n\\nGI\\n\\n10.2 SOMs always activate\\nthe neuron with the\\nleast distance to an\\ninput pattern\\n\\nLike many other neural networks, the\\nSOM has to be trained before it can be\\nused. But let us regard the very simple\\nfunctionality of a complete self-organizing\\nmap before training, since there are many\\nanalogies to the training. Functionality\\nconsists of the following steps:\\n\\nInput of an arbitrary value p of the input\\nspace RN .\\n\\nCalculation of the distance between ev-\\nery neuron k and p by means of a\\nnorm, i.e. calculation of ||p− ck||.\\n\\nOne neuron becomes active, namely\\nsuch neuron i with the shortest\\n\\n1 We will learn soon what a winner neuron is.\\n\\ncalculated distance to the input. All\\nother neurons remain inactive.This\\nparadigm of activity is also called input\\n\\n↓\\nwinner\\n\\nwinner-takes-all scheme. The output\\nwe expect due to the input of a SOM\\nshows which neuron becomes active.\\n\\nIn many literature citations, the descrip-\\ntion of SOMs is more formal: Often an\\ninput layer is described that is completely\\nlinked towards an SOM layer. Then the in-\\nput layer (N neurons) forwards all inputs\\nto the SOM layer. The SOM layer is later-\\nally linked in itself so that a winner neuron\\ncan be established and inhibit the other\\nneurons. I think that this explanation of\\na SOM is not very descriptive and there-\\nfore I tried to provide a clearer description\\nof the network structure.\\n\\nNow the question is which neuron is ac-\\ntivated by which input – and the answer\\nis given by the network itself during train-\\ning.\\n\\n10.3 Training\\n\\n[Training makes the SOM topology cover\\nthe input space] The training of a SOM\\nis nearly as straightforward as the func-\\ntionality described above. Basically, it is\\nstructured into five steps, which partially\\ncorrespond to those of functionality.\\n\\nInitialization: The network starts with\\nrandom neuron centers ck ∈ RN from\\nthe input space.\\n\\nCreating an input pattern: A stimulus,\\ni.e. a point p, is selected from the\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 149\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\ninput space RN . Now this stimulus istraining:\\ninput,\\n\\n→ winner i,\\nchange in\\nposition\\ni and\\n\\nneighbors\\n\\nentered into the network.\\n\\nDistance measurement: Then the dis-\\ntance ||p−ck|| is determined for every\\nneuron k in the network.\\n\\nWinner takes all: The winner neuron i\\nis determined, which has the smallest\\ndistance to p, i.e. which fulfills the\\ncondition\\n\\n||p− ci|| ≤ ||p− ck|| ∀ k 6= i\\n\\n. You can see that from several win-\\nner neurons one can be selected at\\nwill.\\n\\nAdapting the centers: The neuron cen-\\nters are moved within the input space\\naccording to the rule2\\n\\n∆ck = η(t) · h(i, k, t) · (p− ck),\\n\\nwhere the values ∆ck are simply\\nadded to the existing centers. The\\nlast factor shows that the change in\\nposition of the neurons k is propor-\\ntional to the distance to the input\\npattern p and, as usual, to a time-\\ndependent learning rate η(t). The\\nabove-mentioned network topology ex-\\nerts its influence by means of the func-\\ntion h(i, k, t), which will be discussed\\nin the following.\\n\\n2 Note: In many sources this rule is written ηh(p−\\nck), which wrongly leads the reader to believe that\\nh is a constant. This problem can easily be solved\\nby not omitting the multiplication dots ·.\\n\\nDefinition 10.4 (SOM learning rule). A\\nSOM is trained by presenting an input pat-\\ntern and determining the associated win-\\nner neuron. The winner neuron and its\\nneighbor neurons, which are defined by the\\ntopology function, then adapt their cen-\\nters according to the rule\\n\\n∆ck = η(t) · h(i, k, t) · (p− ck),\\n(10.1)\\n\\nck(t+ 1) = ck(t) + ∆ck(t). (10.2)\\n\\n10.3.1 The topology function\\ndefines, how a learning\\nneuron influences its\\nneighbors\\n\\nThe topology function h is not defined\\non the input space but on the grid and rep-\\nresents the neighborhood relationships be-\\ntween the neurons, i.e. the topology of the\\nnetwork. It can be time-dependent (which\\nit often is) – which explains the parameter\\n\\ndefined on\\nthe gridt. The parameter k is the index running\\n\\nthrough all neurons, and the parameter i\\nis the index of the winner neuron.\\n\\nIn principle, the function shall take a large\\nvalue if k is the neighbor of the winner neu-\\nron or even the winner neuron itself, and\\nsmall values if not. SMore precise defini-\\ntion: The topology function must be uni-\\nmodal, i.e. it must have exactly one maxi-\\nmum. This maximum must be next to the\\nwinner neuron i, for which the distance to\\nitself certainly is 0.\\n\\nonly 1 maximum\\nfor the winnerAdditionally, the time-dependence enables\\n\\nus, for example, to reduce the neighbor-\\nhood in the course of time.\\n\\n150 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.3 Training\\n\\nIn order to be able to output large values\\nfor the neighbors of i and small values for\\nnon-neighbors, the function h needs some\\nkind of distance notion on the grid because\\nfrom somewhere it has to know how far i\\nand k are apart from each other on the\\ngrid. There are different methods to cal-\\nculate this distance.\\n\\nOn a two-dimensional grid we could apply,\\nfor instance, the Euclidean distance (lower\\npart of fig. 10.2) or on a one-dimensional\\ngrid we could simply use the number of the\\nconnections between the neurons i and k\\n(upper part of the same figure).\\n\\nDefinition 10.5 (Topology function).\\nThe topology function h(i, k, t) describes\\nthe neighborhood relationships in the\\ntopology. It can be any unimodal func-\\ntion that reaches its maximum when i = k\\ngilt. Time-dependence is optional, but of-\\nten used.\\n\\n10.3.1.1 Introduction of common\\ndistance and topology\\nfunctions\\n\\nA common distance function would be, for\\nexample, the already known Gaussian\\nbell (see fig. 10.3 on page 153). It is uni-\\nmodal with a maximum close to 0. Addi-\\ntionally, its width can be changed by ap-\\nplying its parameter σ , which can be used\\n\\nσI to realize the neighborhood being reduced\\nin the course of time: We simply relate the\\ntime-dependence to the σ and the result is\\n\\n/.-,()*+ ?>=<89:;i oo 1 // ?>=<89:;k /.-,()*+ /.-,()*+\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ ?>=<89:;k\\nOO\\n\\n��\\n\\n/.-,()*+\\n\\n/.-,()*+ ?>=<89:;i xx\\n2.23qqqqqqq\\n\\n88qqqqqq\\n\\noo ///.-,()*+oo ///.-,()*+ /.-,()*+\\n\\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\\n\\nFigure 10.2: Example distances of a one-\\ndimensional SOM topology (above) and a two-\\ndimensional SOM topology (below) between two\\nneurons i and k. In the lower case the Euclidean\\ndistance is determined (in two-dimensional space\\nequivalent to the Pythagoream theorem). In the\\nupper case we simply count the discrete path\\nlength between i and k. To simplify matters I\\nrequired a fixed grid edge length of 1 in both\\ncases.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 151\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\na monotonically decreasing σ(t). Then our\\ntopology function could look like this:\\n\\nh(i, k, t) = e\\n(\\n− ||gi−ck||\\n\\n2\\n\\n2·σ(t)2\\n\\n)\\n, (10.3)\\n\\nwhere gi and gk represent the neuron po-\\nsitions on the grid, not the neuron posi-\\ntions in the input space, which would be\\nreferred to as ci and ck.\\n\\nOther functions that can be used in-\\nstead of the Gaussian function are, for\\ninstance, the cone function, the cylin-\\nder function or theMexican hat func-\\ntion (fig. 10.3 on the facing page). Here,\\nthe Mexican hat function offers a particu-\\nlar biological motivation: Due to its neg-\\native digits it rejects some neurons close\\nto the winner neuron, a behavior that has\\nalready been observed in nature. This can\\ncause sharply separated map areas – and\\nthat is exactly why the Mexican hat func-\\ntion has been suggested by Teuvo Koho-\\nnen himself. But this adjustment charac-\\nteristic is not necessary for the functional-\\nity of the map, it could even be possible\\nthat the map would diverge, i.e. it could\\nvirtually explode.\\n\\n10.3.2 Learning rates and\\nneighborhoods can decrease\\nmonotonically over time\\n\\nTo avoid that the later training phases\\nforcefully pull the entire map towards\\na new pattern, the SOMs often work\\nwith temporally monotonically decreasing\\nlearning rates and neighborhood sizes. At\\nfirst, let us talk about the learning rate:\\n\\nTypical sizes of the target value of a learn-\\ning rate are two sizes smaller than the ini-\\ntial value, e.g\\n\\n0.01 < η < 0.6\\n\\ncould be true. But this size must also de-\\npend on the network topology or the size\\nof the neighborhood.\\n\\nAs we have already seen, a decreasing\\nneighborhood size can be realized, for ex-\\nample, by means of a time-dependent,\\nmonotonically decreasing σ with the\\nGaussin bell being used in the topology\\nfunction.\\n\\nThe advantage of a decreasing neighbor-\\nhood size is that in the beginning a moving\\nneuron \\"pulls along\\" many neurons in its\\nvicinity, i.e. the randomly initialized net-\\nwork can unfold fast and properly in the\\nbeginning. In the end of the learning pro-\\ncess, only a few neurons are influenced at\\nthe same time which stiffens the network\\nas a whole but enables a good \\"fine tuning\\"\\nof the individual neurons.\\n\\nIt must be noted that\\n\\nh · η ≤ 1\\n\\nmust always be true, since otherwise the\\nneurons would constantly miss the current\\ntraining sample.\\n\\nBut enough of theory – let us take a look\\nat a SOM in action!\\n\\n152 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.3 Training\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−2 −1.5 −1 −0.5  0  0.5  1  1.5  2\\n\\nh(\\nr)\\n\\nr\\n\\nGaussian in 1D\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nf(\\nx)\\n\\nx\\n\\nCone Function\\n\\n 0\\n\\n 0.2\\n\\n 0.4\\n\\n 0.6\\n\\n 0.8\\n\\n 1\\n\\n−4 −2  0  2  4\\n\\nf(\\nx)\\n\\nx\\n\\nCylinder Funktion\\n\\n−1.5\\n\\n−1\\n\\n−0.5\\n\\n 0\\n\\n 0.5\\n\\n 1\\n\\n 1.5\\n\\n 2\\n\\n 2.5\\n\\n 3\\n\\n 3.5\\n\\n−3 −2 −1  0  1  2  3\\n\\nf(\\nx)\\n\\nx\\n\\nMexican Hat Function\\n\\nFigure 10.3: Gaussian bell, cone function, cylinder function and the Mexican hat function sug-\\ngested by Kohonen as examples for topology functions of a SOM..\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 153\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\n?>=<89:;1 ?>=<89:;2\\n\\n����������������������������\\n\\n?>=<89:;7\\n\\n?>=<89:;4\\n\\n�� ��>>>>>>>>\\n?>=<89:;6\\n\\n?>=<89:;3 // // p ?>=<89:;5\\n\\n?>=<89:;1\\n\\n?>=<89:;2\\n\\n?>=<89:;3\\n\\n?>=<89:;4\\n\\n?>=<89:;5\\n\\n?>=<89:;6\\n\\n?>=<89:;7\\n\\nFigure 10.4: Illustration of the two-dimensional input space (left) and the one-dimensional topolgy\\nspace (right) of a self-organizing map. Neuron 3 is the winner neuron since it is closest to p. In\\nthe topology, the neurons 2 and 4 are the neighbors of 3. The arrows mark the movement of the\\nwinner neuron and its neighbors towards the training sample p.\\n\\nTo illustrate the one-dimensional topology of the network, it is plotted into the input space by the\\ndotted line. The arrows mark the movement of the winner neuron and its neighbors towards the\\npattern.\\n\\n154 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.4 Examples\\n\\n10.4 Examples for the\\nfunctionality of SOMs\\n\\nLet us begin with a simple, mentally com-\\nprehensible example.\\n\\nIn this example, we use a two-dimensional\\ninput space, i.e. N = 2 is true. Let the\\ngrid structure be one-dimensional (G = 1).\\nFurthermore, our example SOM should\\nconsist of 7 neurons and the learning rate\\nshould be η = 0.5.\\n\\nThe neighborhood function is also kept\\nsimple so that we will be able to mentally\\ncomprehend the network:\\n\\nh(i, k, t) =\\n\\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\\n1 k direct neighbor of i,\\n1 k = i,\\n\\n0 otherw.\\n(10.4)\\n\\nNow let us take a look at the above-\\nmentioned network with random initializa-\\ntion of the centers (fig. 10.4 on the preced-\\ning page) and enter a training sample p.\\nObviously, in our example the input pat-\\ntern is closest to neuron 3, i.e. this is the\\nwinning neuron.\\n\\nWe remember the learning rule for\\nSOMs\\n\\n∆ck = η(t) · h(i, k, t) · (p− ck)\\n\\nand process the three factors from the\\nback:\\n\\nLearning direction: Remember that the\\nneuron centers ck are vectors in the\\ninput space, as well as the pattern p.\\n\\nThus, the factor (p−ck) indicates the\\nvector of the neuron k to the pattern\\np. This is now multiplied by different\\nscalars:\\n\\nOur topology function h indicates that\\nonly the winner neuron and its two\\nclosest neighbors (here: 2 and 4) are\\nallowed to learn by returning 0 for\\nall other neurons. A time-dependence\\nis not specified. Thus, our vector\\n(p − ck) is multiplied by either 1 or\\n0.\\n\\nThe learning rate indicates, as always,\\nthe strength of learning. As already\\nmentioned, η = 0.5, i. e. all in all, the\\nresult is that the winner neuron and\\nits neighbors (here: 2, 3 and 4) ap-\\nproximate the pattern p half the way\\n(in the figure marked by arrows).\\n\\nAlthough the center of neuron 7 – seen\\nfrom the input space – is considerably\\ncloser to the input pattern p than neuron\\n2, neuron 2 is learning and neuron 7 is\\nnot. I want to remind that the network\\ntopology specifies which neuron is allowed\\n\\ntopology\\nspecifies,\\nwho will learn\\n\\nto learn and not its position in the input\\nspace. This is exactly the mechanism by\\nwhich a topology can significantly cover an\\ninput space without having to be related\\nto it by any sort.\\n\\nAfter the adaptation of the neurons 2, 3\\nand 4 the next pattern is applied, and so\\non. Another example of how such a one-\\ndimensional SOM can develop in a two-\\ndimensional input space with uniformly\\ndistributed input patterns in the course of\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 155\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\ntime can be seen in figure 10.5 on the fac-\\ning page.\\n\\nEnd states of one- and two-dimensional\\nSOMs with differently shaped input spaces\\ncan be seen in figure 10.6 on page 158.\\nAs we can see, not every input space can\\nbe neatly covered by every network topol-\\nogy. There are so called exposed neurons\\n– neurons which are located in an area\\nwhere no input pattern has ever been oc-\\ncurred. A one-dimensional topology gen-\\nerally produces less exposed neurons than\\na two-dimensional one: For instance, dur-\\ning training on circularly arranged input\\npatterns it is nearly impossible with a two-\\ndimensional squared topology to avoid the\\nexposed neurons in the center of the cir-\\ncle. These are pulled in every direction\\nduring the training so that they finally\\nremain in the center. But this does not\\nmake the one-dimensional topology an op-\\ntimal topology since it can only find less\\ncomplex neighborhood relationships than\\na multi-dimensional one.\\n\\n10.4.1 Topological defects are\\nfailures in SOM unfolding\\n\\nDuring the unfolding of a SOM it\\ncould happen that a topological defect\\n(fig. 10.7) occurs, i.e. the SOM does not\\n\\n\\"knot\\"\\nin map unfold correctly. A topological defect can\\n\\nbe described at best by means of the word\\n\\"knotting\\".\\n\\nA remedy for topological defects could\\nbe to increase the initial values for the\\n\\nFigure 10.7: A topological defect in a two-\\ndimensional SOM.\\n\\nneighborhood size, because the more com-\\nplex the topology is (or the more neigh-\\nbors each neuron has, respectively, since a\\nthree-dimensional or a honeycombed two-\\ndimensional topology could also be gener-\\nated) the more difficult it is for a randomly\\ninitialized map to unfold.\\n\\n10.5 It is possible to adjust\\nthe resolution of certain\\nareas in a SOM\\n\\nWe have seen that a SOM is trained by\\nentering input patterns of the input space\\n\\n156 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.5 Adjustment of resolution and position-dependent learning rate\\n\\nFigure 10.5: Behavior of a SOM with one-dimensional topology (G = 1) after the input of 0, 100,\\n300, 500, 5000, 50000, 70000 and 80000 randomly distributed input patterns p ∈ R2. During the\\ntraining η decreased from 1.0 to 0.1, the σ parameter of the Gauss function decreased from 10.0\\nto 0.2.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 157\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\nFigure 10.6: End states of one-dimensional (left column) and two-dimensional (right column)\\nSOMs on different input spaces. 200 neurons were used for the one-dimensional topology, 10× 10\\nneurons for the two-dimensionsal topology and 80.000 input patterns for all maps.\\n\\n158 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.6 Application\\n\\nRN one after another, again and again so\\nthat the SOM will be aligned with these\\npatterns and map them. It could happen\\nthat we want a certain subset U of the in-\\nput space to be mapped more precise than\\nthe other ones.\\n\\nThis problem can easily be solved by\\nmeans of SOMs: During the training dis-\\nproportionally many input patterns of the\\narea U are presented to the SOM. If the\\nnumber of training patterns of U ⊂ RN\\npresented to the SOM exceeds the number\\nof those patterns of the remaining RN \\\\U ,\\nthen more neurons will group there while\\nthe remaining neurons are sparsely dis-\\ntributed on RN \\\\ U (fig. 10.8 on the next\\npage).more\\n\\npatterns\\n↓\\n\\nhigher\\nresolution\\n\\nAs you can see in the illustration, the edge\\nof the SOM could be deformed. This can\\nbe compensated by assigning to the edge\\nof the input space a slightly higher proba-\\nbility of being hit by training patterns (an\\noften applied approach for reaching every\\ncorner with the SOMs).\\n\\nAlso, a higher learning rate is often used\\nfor edge and corner neurons, since they are\\nonly pulled into the center by the topol-\\nogy. This also results in a significantly im-\\nproved corner coverage.\\n\\n10.6 Application of SOMs\\n\\nRegarding the biologically inspired asso-\\nciative data storage, there are many\\nfields of application for self-organizing\\nmaps and their variations.\\n\\nFor example, the different phonemes of\\nthe finnish language have successfully been\\nmapped onto a SOM with a two dimen-\\nsional discrete grid topology and therefore\\nneighborhoods have been found (a SOM\\ndoes nothing else than finding neighbor-\\nhood relationships). So one tries once\\nmore to break down a high-dimensional\\nspace into a low-dimensional space (the\\ntopology), looks if some structures have\\nbeen developed – et voilà: clearly defined\\nareas for the individual phenomenons are\\nformed.\\n\\nTeuvo Kohonen himself made the ef-\\nfort to search many papers mentioning his\\nSOMs in their keywords. In this large in-\\nput space the individual papers now indi-\\nvidual positions, depending on the occur-\\nrence of keywords. Then Kohonen created\\na SOM with G = 2 and used it to map the\\nhigh-dimensional \\"paper space\\" developed\\nby him.\\n\\nThus, it is possible to enter any paper\\ninto the completely trained SOM and look\\nwhich neuron in the SOM is activated. It\\nwill be likely to discover that the neigh-\\nbored papers in the topology are interest-\\ning, too. This type of brain-like context-\\nbased search also works with many other\\ninput spaces.\\n\\nSOM finds\\nsimilarities\\n\\nIt is to be noted that the system itself\\ndefines what is neighbored, i.e. similar,\\nwithin the topology – and that’s why it\\nis so interesting.\\n\\nThis example shows that the position c of\\nthe neurons in the input space is not signif-\\nicant. It is rather interesting to see which\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 159\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\nFigure 10.8: Training of a SOM with G = 2 on a two-dimensional input space. On the left side,\\nthe chance to become a training pattern was equal for each coordinate of the input space. On the\\nright side, for the central circle in the input space, this chance is more than ten times larger than\\nfor the remaining input space (visible in the larger pattern density in the background). In this circle\\nthe neurons are obviously more crowded and the remaining area is covered less dense but in both\\ncases the neurons are still evenly distributed. The two SOMS were trained by means of 80.000\\ntraining samples and decreasing η (1→ 0.2) as well as decreasing σ (5→ 0.5).\\n\\n160 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.7 Variations\\n\\nneuron is activated when an unknown in-\\nput pattern is entered. Next, we can look\\nat which of the previous inputs this neu-\\nron was also activated – and will imme-\\ndiately discover a group of very similar\\ninputs. The more the inputs within the\\ntopology are diverging, the less things they\\nhave in common. Virtually, the topology\\ngenerates a map of the input characteris-\\ntics – reduced to descriptively few dimen-\\nsions in relation to the input dimension.\\n\\nTherefore, the topology of a SOM often\\nis two-dimensional so that it can be easily\\nvisualized, while the input space can be\\nvery high-dimensional.\\n\\n10.6.1 SOMs can be used to\\ndetermine centers for RBF\\nneurons\\n\\nSOMs arrange themselves exactly towards\\nthe positions of the outgoing inputs. As a\\nresult they are used, for example, to select\\nthe centers of an RBF network. We have\\nalready been introduced to the paradigm\\nof the RBF network in chapter 6.\\n\\nAs we have already seen, it is possible\\nto control which areas of the input space\\nshould be covered with higher resolution\\n- or, in connection with RBF networks,\\non which areas of our function should the\\nRBF network work with more neurons, i.e.\\nwork more exactly. As a further useful fea-\\nture of the combination of RBF networks\\nwith SOMs one can use the topology ob-\\ntained through the SOM: During the final\\ntraining of a RBF neuron it can be used\\n\\nto influence neighboring RBF neurons in\\ndifferent ways.\\n\\nFor this, many neural network simulators\\noffer an additional so-called SOM layer\\nin connection with the simulation of RBF\\nnetworks.\\n\\n10.7 Variations of SOMs\\n\\nThere are different variations of SOMs\\nfor different variations of representation\\ntasks:\\n\\n10.7.1 A neural gas is a SOM\\nwithout a static topology\\n\\nThe neural gas is a variation of the self-\\norganizing maps of Thomas Martinetz\\n[MBS93], which has been developed from\\nthe difficulty of mapping complex input\\ninformation that partially only occur in\\nthe subspaces of the input space or even\\nchange the subspaces (fig. 10.9 on the fol-\\nlowing page).\\n\\nThe idea of a neural gas is, roughly speak-\\ning, to realize a SOM without a grid struc-\\nture. Due to the fact that they are de-\\nrived from the SOMs the learning steps\\nare very similar to the SOM learning steps,\\nbut they include an additional intermedi-\\nate step:\\n\\n. again, random initialization of ck ∈\\nRn\\n\\n. selection and presentation of a pat-\\ntern of the input space p ∈ Rn\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 161\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\nFigure 10.9: A figure filling different subspaces of the actual input space of different positions\\ntherefore can hardly be filled by a SOM.\\n\\n. neuron distance measurement\\n\\n. identification of the winner neuron i\\n\\n. Intermediate step: generation of a list\\nL of neurons sorted in ascending order\\nby their distance to the winner neu-\\nron. Thus, the first neuron in the list\\nL is the neuron that is closest to the\\nwinner neuron.\\n\\n. changing the centers by means of the\\nknown rule but with the slightly mod-\\nified topology function\\n\\nhL(i, k, t).\\n\\nThe function hL(i, k, t), which is slightly\\nmodified compared with the original func-\\ntion h(i, k, t), now regards the first el-\\nements of the list as the neighborhood\\n\\nof the winner neuron i. The direct re-\\nsult is that – similar to the free-floating\\n\\ndynamic\\nneighborhoodmolecules in a gas – the neighborhood rela-\\n\\ntionships between the neurons can change\\nanytime, and the number of neighbors is\\nalmost arbitrary, too. The distance within\\nthe neighborhood is now represented by\\nthe distance within the input space.\\n\\nThe bulk of neurons can become as stiff-\\nened as a SOM by means of a constantly\\ndecreasing neighborhood size. It does not\\nhave a fixed dimension but it can take the\\ndimension that is locally needed at the mo-\\nment, which can be very advantageous.\\n\\nA disadvantage could be that there is\\nno fixed grid forcing the input space to\\nbecome regularly covered, and therefore\\nwholes can occur in the cover or neurons\\ncan be isolated.\\n\\n162 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 10.7 Variations\\n\\nIn spite of all practical hints, it is as al-\\nways the user’s responsibility not to un-\\nderstand this text as a catalog for easy an-\\nswers but to explore all advantages and\\ndisadvantages himself.\\n\\nUnlike a SOM, the neighborhood of a neu-\\nral gas must initially refer to all neurons\\nsince otherwise some outliers of the ran-\\ndom initialization may never reach the re-\\nmaining group. To forget this is a popular\\nerror during the implementation of a neu-\\nral gas.\\n\\nWith a neural gas it is possible to learn a\\nkind of complex input such as in fig. 10.9\\n\\ncan classify\\ncomplex\\n\\nfigure\\non the preceding page since we are not\\nbound to a fixed-dimensional grid. But\\nsome computational effort could be neces-\\nsary for the permanent sorting of the list\\n(here, it could be effective to store the list\\nin an ordered data structure right from the\\nstart).\\n\\nDefinition 10.6 (Neural gas). A neural\\ngas differs from a SOM by a completely dy-\\nnamic neighborhood function. With every\\nlearning cycle it is decided anew which neu-\\nrons are the neigborhood neurons of the\\nwinner neuron. Generally, the criterion\\nfor this decision is the distance between\\nthe neurosn and the winner neuron in the\\ninput space.\\n\\n10.7.2 A Multi-SOM consists of\\nseveral separate SOMs\\n\\nIn order to present another variant of the\\nSOMs, I want to formulate an extended\\n\\nproblem: What do we do with input pat-\\nterns from which we know that they are\\nconfined in different (maybe disjoint) ar-\\neas?\\n\\nseveral SOMs\\n\\nHere, the idea is to use not only one\\nSOM but several ones: A multi-self-\\norganizing map, shortly referred to as\\nM-SOM [GKE01b,GKE01a,GS06]. It is\\nunnecessary that the SOMs have the same\\ntopology or size, an M-SOM is just a com-\\nbination of M SOMs.\\n\\nThis learning process is analog to that of\\nthe SOMs. However, only the neurons be-\\nlonging to the winner SOM of each train-\\ning step are adapted. Thus, it is easy to\\nrepresent two disjoint clusters of data by\\nmeans of two SOMs, even if one of the\\nclusters is not represented in every dimen-\\nsion of the input space RN . Actually, the\\nindividual SOMs exactly reflect these clus-\\nters.\\n\\nDefinition 10.7 (Multi-SOM). A multi-\\nSOM is nothing more than the simultane-\\nous use of M SOMs.\\n\\n10.7.3 A multi-neural gas consists\\nof several separate neural\\ngases\\n\\nAnalogous to the multi-SOM, we also have\\na set of M neural gases: a multi-neural\\ngas [GS06, SG06]. This construct be-\\n\\nseveral gases\\nhaves analogous to neural gas and M-SOM:\\nAgain, only the neurons of the winner gas\\nare adapted.\\n\\nThe reader certainly wonders what advan-\\ntage is there to use a multi-neural gas since\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 163\\n\\n\\n\\nChapter 10 Self-organizing feature maps dkriesel.com\\n\\nan individual neural gas is already capa-\\nble to divide into clusters and to work on\\ncomplex input patterns with changing di-\\nmensions. Basically, this is correct, but\\na multi-neural gas has two serious advan-\\ntages over a simple neural gas.\\n\\n1. With several gases, we can directly\\ntell which neuron belongs to which\\ngas. This is particularly important\\nfor clustering tasks, for which multi-\\nneural gases have been used recently.\\nSimple neural gases can also find and\\ncover clusters, but now we cannot rec-\\nognize which neuron belongs to which\\ncluster.\\n\\nless computa-\\ntional effort 2. A lot of computational effort is saved\\n\\nwhen large original gases are divided\\ninto several smaller ones since (as al-\\nready mentioned) the sorting of the\\nlist L could use a lot of computa-\\ntional effort while the sorting of sev-\\neral smaller lists L1, L2, . . . , LM is less\\ntime-consuming – even if these lists in\\ntotal contain the same number of neu-\\nrons.\\n\\nAs a result we will only obtain local in-\\nstead of global sortings, but in most cases\\nthese local sortings are sufficient.\\n\\nNow we can choose between two extreme\\ncases of multi-neural gases: One extreme\\ncase is the ordinary neural gas M = 1, i.e.\\nwe only use one single neural gas. Interest-\\ning enough, the other extreme case (very\\nlargeM , a few or only one neuron per gas)\\nbehaves analogously to the K-means clus-\\ntering (for more information on clustering\\nprocedures see excursus A).\\n\\nDefinition 10.8 (Multi-neural gas). A\\nmulti-neural gas is nothing more than the\\nsimultaneous use of M neural gases.\\n\\n10.7.4 Growing neural gases can\\nadd neurons to themselves\\n\\nA growing neural gas is a variation of\\nthe aforementioned neural gas to which\\nmore and more neurons are added accord-\\ning to certain rules. Thus, this is an at-\\ntempt to work against the isolation of neu-\\nrons or the generation of larger wholes in\\nthe cover.\\n\\nHere, this subject should only be men-\\ntioned but not discussed.\\n\\nTo build a growing SOM is more difficult\\nbecause new neurons have to be integrated\\nin the neighborhood.\\n\\nExercises\\n\\nExercise 17. A regular, two-dimensional\\ngrid shall cover a two-dimensional surface\\nas \\"well\\" as possible.\\n\\n1. Which grid structure would suit best\\nfor this purpose?\\n\\n2. Which criteria did you use for \\"well\\"\\nand \\"best\\"?\\n\\nThe very imprecise formulation of this ex-\\nercise is intentional.\\n\\n164 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\nChapter 11\\n\\nAdaptive resonance theory\\nAn ART network in its original form shall classify binary input vectors, i.e. to\\n\\nassign them to a 1-out-of-n output. Simultaneously, the so far unclassified\\npatterns shall be recognized and assigned to a new class.\\n\\nAs in the other smaller chapters, we want\\nto try to figure out the basic idea of\\nthe adaptive resonance theory (abbre-\\nviated: ART) without discussing its the-\\nory profoundly.\\n\\nIn several sections we have already men-\\ntioned that it is difficult to use neural\\nnetworks for the learning of new informa-\\ntion in addition to but without destroying\\nthe already existing information. This cir-\\ncumstance is called stability / plasticity\\ndilemma.\\n\\nIn 1987, Stephen Grossberg and Gail\\nCarpenter published the first version of\\ntheir ART network [Gro76] in order to al-\\nleviate this problem. This was followed\\nby a whole family of ART improvements\\n(which we want to discuss briefly, too).\\n\\nIt is the idea of unsupervised learning,\\nwhose aim is the (initially binary) pattern\\nrecognition, or more precisely the catego-\\nrization of patterns into classes. But addi-\\n\\ntionally an ART network shall be capable\\nto find new classes.\\n\\n11.1 Task and structure of an\\nART network\\n\\nAn ART network comprises exactly two\\nlayers: the input layer I and the recog-\\nnition layer O with the input layer be-\\ning completely linked towards the recog-\\nnition layer. This complete link induces\\na top-down weight matrix W that con-\\ntains the weight values of the connections\\nbetween each neuron in the input layer\\nand each neuron in the recognition layer\\n(fig. 11.1 on the following page).\\n\\nSimple binary patterns are entered into\\nthe input layer and transferred to the pattern\\n\\nrecognitionrecognition layer while the recognition\\nlayer shall return a 1-out-of-|O| encoding,\\ni.e. it should follow the winner-takes-all\\n\\n165\\n\\n\\n\\nChapter 11 Adaptive resonance theory dkriesel.com\\n\\n�� �� �� ��GFED@ABCi1\\n\\n��\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n�� ��4\\n4444444444444\\n\\n##FFFFFFFFFFFFFFFFFFFFF\\n\\n\'\'OOOOOOOOOOOOOOOOOOOOOOOOOOOOO\\n\\n))SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS GFED@ABCi2\\n\\n{{xxxxxxxxxxxxxxxxxxxxx\\n\\n��\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n�� ��4\\n4444444444444\\n\\n##FFFFFFFFFFFFFFFFFFFFF\\n\\n\'\'OOOOOOOOOOOOOOOOOOOOOOOOOOOOO GFED@ABCi3\\n\\nwwooooooooooooooooooooooooooooo\\n\\n{{xxxxxxxxxxxxxxxxxxxxx\\n\\n��\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n�� ��4\\n4444444444444\\n\\n##FFFFFFFFFFFFFFFFFFFFF GFED@ABCi4\\n\\nuukkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\\n\\nwwooooooooooooooooooooooooooooo\\n\\n{{xxxxxxxxxxxxxxxxxxxxx\\n\\n��\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n�� ��4\\n4444444444444\\n\\nGFED@ABCΩ1\\n\\nEE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n;;xxxxxxxxxxxxxxxxxxxxx\\n\\n77ooooooooooooooooooooooooooooo\\n\\n55kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\\n\\n��\\n\\nGFED@ABCΩ2\\n\\nOO EE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n;;xxxxxxxxxxxxxxxxxxxxx\\n\\n77ooooooooooooooooooooooooooooo\\n\\n��\\n\\nGFED@ABCΩ3\\n\\nYY44444444444444\\n\\nOO EE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n;;xxxxxxxxxxxxxxxxxxxxx\\n\\n��\\n\\nGFED@ABCΩ4\\n\\nccFFFFFFFFFFFFFFFFFFFFF\\n\\nYY44444444444444\\n\\nOO EE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n��\\n\\nGFED@ABCΩ5\\n\\nggOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\\n\\nccFFFFFFFFFFFFFFFFFFFFF\\n\\nYY44444444444444\\n\\nOO\\n\\n��\\n\\nGFED@ABCΩ6\\n\\niiSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\\n\\nggOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\\n\\nccFFFFFFFFFFFFFFFFFFFFF\\n\\nYY44444444444444\\n\\n��\\n\\nFigure 11.1: Simplified illustration of the ART network structure. Top: the input layer, bottom:\\nthe recognition layer. In this illustration the lateral inhibition of the recognition layer and the control\\nneurons are omitted.\\n\\nscheme. For instance, to realize this 1-\\nout-of-|O| encoding the principle of lateral\\ninhibition can be used – or in the imple-\\nmentation the most activated neuron can\\nbe searched. For practical reasons an IF\\nquery would suit this task best.\\n\\n11.1.1 Resonance takes place by\\nactivities being tossed and\\nturned\\n\\nBut there also exists a bottom-up weight\\nmatrix V , which propagates the activi-\\n\\nVI ties within the recognition layer back into\\nthe input layer. Now it is obvious that\\nthese activities are bounced forth and back\\nagain and again, a fact that leads us to\\nresonance. Every activity within the in-\\n\\nput layer causes an activity within the\\nlayers\\nactivate\\none\\nanother\\n\\nrecognition layer while in turn in the recog-\\nnition layer every activity causes an activ-\\nity within the input layer.\\n\\nIn addition to the two mentioned layers,\\nin an ART network also exist a few neu-\\nrons that exercise control functions such as\\nsignal enhancement. But we do not want\\nto discuss this theory further since here\\nonly the basic principle of the ART net-\\nwork should become explicit. I have only\\nmentioned it to explain that in spite of the\\nrecurrences, the ART network will achieve\\na stable state after an input.\\n\\n166 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com 11.3 Extensions\\n\\n11.2 The learning process of\\nan ART network is\\ndivided to top-down and\\nbottom-up learning\\n\\nThe trick of adaptive resonance theory is\\nnot only the configuration of the ART net-\\nwork but also the two-piece learning pro-\\ncedure of the theory: On the one hand\\nwe train the top-down matrix W , on the\\nother hand we train the bottom-up matrix\\nV (fig. 11.2 on the next page).\\n\\n11.2.1 Pattern input and top-down\\nlearning\\n\\nWhen a pattern is entered into the net-\\nwork it causes - as already mentioned - an\\nactivation at the output neurons and thewinner\\n\\nneuron\\nis\\n\\namplified\\n\\nstrongest neuron wins. Then the weights\\nof the matrix W going towards the output\\nneuron are changed such that the output\\nof the strongest neuron Ω is still enhanced,\\ni.e. the class affiliation of the input vector\\nto the class of the output neuron Ω be-\\ncomes enhanced.\\n\\n11.2.2 Resonance and bottom-up\\nlearning\\n\\nThe training of the backward weights ofinput is\\nteach. inp.\\n\\nfor backward\\nweights\\n\\nthe matrix V is a bit tricky: Only the\\nweights of the respective winner neuron\\nare trained towards the input layer and\\nour current input pattern is used as teach-\\ning input. Thus, the network is trained to\\nenhance input vectors.\\n\\n11.2.3 Adding an output neuron\\n\\nOf course, it could happen that the neu-\\nrons are nearly equally activated or that\\nseveral neurons are activated, i.e. that the\\nnetwork is indecisive. In this case, the\\nmechanisms of the control neurons acti-\\nvate a signal that adds a new output neu-\\nron. Then the current pattern is assigned\\nto this output neuron and the weight sets\\nof the new neuron are trained as usual.\\n\\nThus, the advantage of this system is not\\nonly to divide inputs into classes and to\\nfind new classes, it can also tell us after\\nthe activation of an output neuron what a\\ntypical representative of a class looks like\\n- which is a significant feature.\\n\\nOften, however, the system can only mod-\\nerately distinguish the patterns. The ques-\\ntion is when a new neuron is permitted to\\nbecome active and when it should learn.\\nIn an ART network there are different ad-\\nditional control neurons which answer this\\nquestion according to different mathemat-\\nical rules and which are responsible for in-\\ntercepting special cases.\\n\\nAt the same time, one of the largest ob-\\njections to an ART is the fact that an\\nART network uses a special distinction of\\ncases, similar to an IF query, that has been\\nforced into the mechanism of a neural net-\\nwork.\\n\\n11.3 Extensions\\n\\nAs already mentioned above, the ART net-\\nworks have often been extended.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 167\\n\\n\\n\\nChapter 11 Adaptive resonance theory dkriesel.comKapitel 11 Adaptive Resonance Theory dkriesel.com\\n\\n�� �� �� ��GFED@ABCi1\\n\\n�� \\"\\"\\n\\nGFED@ABCi2\\n\\n�� ��\\n\\nGFED@ABCi3\\n\\n�� ��\\n\\nGFED@ABCi4\\n\\n|| ��GFED@ABCΩ1\\n\\nYY OO EE <<\\n\\n��\\n\\nGFED@ABCΩ2\\n\\nbb YY OO EE\\n\\n��\\n0 1\\n\\n�� �� �� ��GFED@ABCi1\\n\\n�� \\"\\"FFFFFFFFFFFFFFFFFFFF GFED@ABCi2\\n\\n�� ��4\\n4444444444444\\nGFED@ABCi3\\n\\n�� ��\\n\\nGFED@ABCi4\\n\\n|| ����������������\\n\\nGFED@ABCΩ1\\n\\nYY OO EE <<\\n\\n��\\n\\nGFED@ABCΩ2\\n\\nbb YY OO EE\\n\\n��\\n0 1\\n\\n�� �� �� ��GFED@ABCi1\\n\\n�� \\"\\"\\n\\nGFED@ABCi2\\n\\n�� ��\\n\\nGFED@ABCi3\\n\\n�� ��\\n\\nGFED@ABCi4\\n\\n|| ��GFED@ABCΩ1\\n\\nYY OO EE <<\\n\\n��\\n\\nGFED@ABCΩ2\\n\\nbbFFFFFFFFFFFFFFFFFFFF\\n\\nYY44444444444444\\n\\nOO EE��������������\\n\\n��\\n0 1\\n\\nAbbildung 11.2: Vereinfachte Darstellung des\\nzweigeteilten Trainings eines ART-Netzes: Die\\njeweils trainierten Gewichte sind durchgezogen\\ndargestellt. Nehmen wir an, ein Muster wurde in\\ndas Netz eingegeben und die Zahlen markieren\\nAusgaben. Oben: Wir wir sehen, ist Ω2 das Ge-\\nwinnerneuron. Mitte: Also werden die Gewichte\\nzum Gewinnerneuron hin trainiert und (unten)\\ndie Gewichte vom Gewinnerneuron zur Eingangs-\\nschicht trainiert.\\n\\neiner IF-Abfrage, die man in den Mecha-\\nnismus eines Neuronalen Netzes gepresst\\nhat.\\n\\n11.3 Erweiterungen\\n\\nWie schon eingangs erwähnt, wurden die\\nART-Netze vielfach erweitert.\\n\\nART-2 [CG87] ist eine Erweiterung\\nauf kontinuierliche Eingaben und bietet\\nzusätzlich (in einer ART-2A genannten\\nErweiterung) Verbesserungen der Lernge-\\nschwindigkeit, was zusätzliche Kontroll-\\nneurone und Schichten zur Folge hat.\\n\\nART-3 [CG90] verbessert die Lernfähig-\\nkeit von ART-2, indem zusätzliche biolo-\\ngische Vorgänge wie z.B. die chemischen\\nVorgänge innerhalb der Synapsen adap-\\ntiert werden1.\\n\\nZusätzlich zu den beschriebenen Erweite-\\nrungen existieren noch viele mehr.\\n\\n1 Durch die häufigen Erweiterungen der Adaptive\\nResonance Theory sprechen böse Zungen bereits\\nvon ”ART-n-Netzen“.\\n\\n168 D. Kriesel – Ein kleiner Überblick über Neuronale Netze (EPSILON-DE)\\n\\nFigure 11.2: Simplified illustration of the two-\\npiece training of an ART network: The trained\\nweights are represented by solid lines. Let us as-\\nsume that a pattern has been entered into the\\nnetwork and that the numbers mark the outputs.\\nTop: We can see that Ω2 is the winner neu-\\nron. Middle: So the weights are trained towards\\nthe winner neuron and (below) the weights of\\nthe winner neuron are trained towards the input\\nlayer.\\n\\nART-2 [CG87] is extended to continuous\\ninputs and additionally offers (in an ex-\\ntension called ART-2A) enhancements of\\nthe learning speed which results in addi-\\ntional control neurons and layers.\\n\\nART-3 [CG90] 3 improves the learning\\nability of ART-2 by adapting additional\\nbiological processes such as the chemical\\nprocesses within the synapses1.\\n\\nApart from the described ones there exist\\nmany other extensions.\\n\\n1 Because of the frequent extensions of the adap-\\ntive resonance theory wagging tongues already call\\nthem \\"ART-n networks\\".\\n\\n168 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\nPart IV\\n\\nExcursi, appendices and registers\\n\\n169\\n\\n\\n\\n\\n\\nAppendix A\\n\\nExcursus: Cluster analysis and regional and\\nonline learnable fields\\n\\nIn Grimm’s dictionary the extinct German word \\"Kluster\\" is described by \\"was\\ndicht und dick zusammensitzet (a thick and dense group of sth.)\\". In static\\n\\ncluster analysis, the formation of groups within point clouds is explored.\\nIntroduction of some procedures, comparison of their advantages and\\n\\ndisadvantages. Discussion of an adaptive clustering method based on neural\\nnetworks. A regional and online learnable field models from a point cloud,\\npossibly with a lot of points, a comparatively small set of neurons being\\n\\nrepresentative for the point cloud.\\n\\nAs already mentioned, many problems can\\nbe traced back to problems in cluster\\nanalysis. Therefore, it is necessary to re-\\nsearch procedures that examine whether\\ngroups (so-called clusters) exist within\\npoint clouds.\\n\\nSince cluster analysis procedures need a\\nnotion of distance between two points, a\\nmetric must be defined on the space\\nwhere these points are situated.\\n\\nWe briefly want to specify what a metric\\nis.\\n\\nDefinition A.1 (Metric). A relation\\ndist(x1, x2) defined for two objects x1, x2\\nis referred to as metric if each of the fol-\\nlowing criteria applies:\\n\\n1. dist(x1, x2) = 0 if and only if x1 = x2,\\n\\n2. dist(x1, x2) = dist(x2, x1), i.e. sym-\\nmetry,\\n\\n3. dist(x1, x3) ≤ dist(x1, x2) +\\ndist(x2, x3), i.e. the triangle\\ninequality holds.\\n\\nColloquially speaking, a metric is a tool\\nfor determining distances between points\\nin any space. Here, the distances have\\nto be symmetrical, and the distance be-\\ntween to points may only be 0 if the two\\npoints are equal. Additionally, the trian-\\ngle inequality must apply.\\n\\nMetrics are provided by, for example, the\\nsquared distance and the Euclidean\\ndistance, which have already been intro-\\nduced. Based on such metrics we can de-\\n\\n171\\n\\n\\n\\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\\n\\nfine a clustering procedure that uses a met-\\nric as distance measure.\\n\\nNow we want to introduce and briefly dis-\\ncuss different clustering procedures.\\n\\nA.1 k-means clustering\\nallocates data to a\\npredefined number of\\nclusters\\n\\nk-means clustering according to J.\\nMacQueen [Mac67] is an algorithm that\\nis often used because of its low computa-\\ntion and storage complexity and which is\\nregarded as \\"inexpensive and good\\". The\\noperation sequence of the k-means cluster-\\ning algorithm is the following:\\n\\n1. Provide data to be examined.\\n\\n2. Define k, which is the number of clus-\\nter centers.\\n\\n3. Select k random vectors for the clus-\\nter centers (also referred to as code-\\nbook vectors).\\n\\n4. Assign each data point to the next\\ncodebook vector1\\n\\n5. Compute cluster centers for all clus-\\nters.\\n\\n6. Set codebook vectors to new cluster\\ncenters.\\n\\n1 The name codebook vector was created because\\nthe often used name cluster vector was too un-\\nclear.\\n\\n7. Continue with 4 until the assignments\\nare no longer changed.\\n\\nnumber of\\ncluster\\nmust be\\nknown\\npreviously\\n\\nStep 2 already shows one of the great ques-\\ntions of the k-means algorithm: The num-\\nber k of the cluster centers has to be de-\\ntermined in advance. This cannot be done\\nby the algorithm. The problem is that it\\nis not necessarily known in advance how k\\ncan be determined best. Another problem\\nis that the procedure can become quite in-\\nstable if the codebook vectors are badly\\ninitialized. But since this is random, it\\nis often useful to restart the procedure.\\nThis has the advantage of not requiring\\nmuch computational effort. If you are fully\\naware of those weaknesses, you will receive\\nquite good results.\\n\\nHowever, complex structures such as \\"clus-\\nters in clusters\\" cannot be recognized. If k\\nis high, the outer ring of the construction\\nin the following illustration will be recog-\\nnized as many single clusters. If k is low,\\nthe ring with the small inner clusters will\\nbe recognized as one cluster.\\n\\nFor an illustration see the upper right part\\nof fig. A.1 on page 174.\\n\\nA.2 k-nearest neighboring\\nlooks for the k nearest\\nneighbors of each data\\npoint\\n\\nThe k-nearest neighboring procedure\\n[CH67] connects each data point to the k\\nclosest neighbors, which often results in a\\ndivision of the groups. Then such a group\\n\\n172 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com A.4 The silhouette coefficient\\n\\nbuilds a cluster. The advantage is that\\nthe number of clusters occurs all by it-\\nself. The disadvantage is that a large stor-\\nage and computational effort is required to\\nfind the next neighbor (the distances be-\\ntween all data points must be computed\\nand stored).\\n\\nclustering\\nnext\\n\\npoints There are some special cases in which the\\nprocedure combines data points belonging\\nto different clusters, if k is too high. (see\\nthe two small clusters in the upper right\\nof the illustration). Clusters consisting of\\nonly one single data point are basically\\nconncted to another cluster, which is not\\nalways intentional.\\n\\nFurthermore, it is not mandatory that the\\nlinks between the points are symmetric.\\n\\nBut this procedure allows a recognition of\\nrings and therefore of \\"clusters in clusters\\",\\nwhich is a clear advantage. Another ad-\\nvantage is that the procedure adaptively\\nresponds to the distances in and between\\nthe clusters.\\n\\nFor an illustration see the lower left part\\nof fig. A.1.\\n\\nA.3 ε-nearest neighboring\\nlooks for neighbors within\\nthe radius ε for each\\ndata point\\n\\nAnother approach of neighboring: here,\\nthe neighborhood detection does not use a\\nfixed number k of neighbors but a radius ε,\\n\\nwhich is the reason for the name epsilon-\\nnearest neighboring. Points are neig-\\nbors if they are at most ε apart from each\\nother. Here, the storage and computa-\\ntional effort is obviously very high, which\\nis a disadvantage.\\n\\nclustering\\nradii around\\npointsBut note that there are some special cases:\\n\\nTwo separate clusters can easily be con-\\nnected due to the unfavorable situation of\\na single data point. This can also happen\\nwith k-nearest neighboring, but it would\\nbe more difficult since in this case the num-\\nber of neighbors per point is limited.\\n\\nAn advantage is the symmetric nature of\\nthe neighborhood relationships. Another\\nadvantage is that the combination of min-\\nimal clusters due to a fixed number of\\nneighbors is avoided.\\n\\nOn the other hand, it is necessary to skill-\\nfully initialize ε in order to be successful,\\ni.e. smaller than half the smallest distance\\nbetween two clusters. With variable clus-\\nter and point distances within clusters this\\ncan possibly be a problem.\\n\\nFor an illustration see the lower right part\\nof fig. A.1.\\n\\nA.4 The silhouette coefficient\\ndetermines how accurate\\na given clustering is\\n\\nAs we can see above, there is no easy an-\\nswer for clustering problems. Each proce-\\ndure described has very specific disadvan-\\ntages. In this respect it is useful to have\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 173\\n\\n\\n\\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\\n\\nFigure A.1: Top left: our set of points. We will use this set to explore the different clustering\\nmethods. Top right: k-means clustering. Using this procedure we chose k = 6. As we can\\nsee, the procedure is not capable to recognize \\"clusters in clusters\\" (bottom left of the illustration).\\nLong \\"lines\\" of points are a problem, too: They would be recognized as many small clusters (if k\\nis sufficiently large). Bottom left: k-nearest neighboring. If k is selected too high (higher than\\nthe number of points in the smallest cluster), this will result in cluster combinations shown in the\\nupper right of the illustration. Bottom right: ε-nearest neighboring. This procedure will cause\\ndifficulties if ε is selected larger than the minimum distance between two clusters (see upper left of\\nthe illustration), which will then be combined.\\n\\n174 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com A.5 Regional and online learnable fields\\n\\na criterion to decide how good our clus-\\nter division is. This possibility is offered\\nby the silhouette coefficient according\\nto [Kau90]. This coefficient measures how\\nwell the clusters are delimited from each\\nother and indicates if points may be as-\\nsigned to the wrong clusters.\\n\\nclustering\\nquality is\\n\\nmeasureable Let P be a point cloud and p a point in\\nP . Let c ⊆ P be a cluster within the\\npoint cloud and p be part of this cluster,\\ni.e. p ∈ c. The set of clusters is called C.\\nSummary:\\n\\np ∈ c ⊆ P\\n\\napplies.\\n\\nTo calculate the silhouette coefficient, we\\ninitially need the average distance between\\npoint p and all its cluster neighbors. This\\nvariable is referred to as a(p) and defined\\nas follows:\\n\\na(p) = 1\\n|c| − 1\\n\\n∑\\nq∈c,q 6=p\\n\\ndist(p, q) (A.1)\\n\\nFurthermore, let b(p) be the average dis-\\ntance between our point p and all points\\nof the next cluster (g represents all clusters\\nexcept for c):\\n\\nb(p) = min\\ng∈C,g 6=c\\n\\n1\\n|g|\\n∑\\nq∈g\\n\\ndist(p, q) (A.2)\\n\\nThe point p is classified well if the distance\\nto the center of the own cluster is minimal\\nand the distance to the centers of the other\\nclusters is maximal. In this case, the fol-\\nlowing term provides a value close to 1:\\n\\ns(p) = b(p)− a(p)\\nmax{a(p), b(p)} (A.3)\\n\\nApparently, the whole term s(p) can only\\nbe within the interval [−1; 1]. A value\\nclose to -1 indicates a bad classification of\\np.\\n\\nThe silhouette coefficient S(P ) results\\nfrom the average of all values s(p):\\n\\nS(P ) = 1\\n|P |\\n\\n∑\\np∈P\\n\\ns(p). (A.4)\\n\\nAs above the total quality of the clus-\\nter division is expressed by the interval\\n[−1; 1].\\n\\nAs different clustering strategies with dif-\\nferent characteristics have been presented\\nnow (lots of further material is presented\\nin [DHS01]), as well as a measure to in-\\ndicate the quality of an existing arrange-\\nment of given data into clusters, I want\\nto introduce a clustering method based\\non an unsupervised learning neural net-\\nwork [SGE05] which was published in 2005.\\nLike all the other methods this one may\\nnot be perfect but it eliminates large stan-\\ndard weaknesses of the known clustering\\nmethods\\n\\nA.5 Regional and online\\nlearnable fields are a\\nneural clustering strategy\\n\\nThe paradigm of neural networks, which I\\nwant to introduce now, are the regional\\nand online learnable fields, shortly re-\\nferred to as ROLFs.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 175\\n\\n\\n\\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\\n\\nA.5.1 ROLFs try to cover data with\\nneurons\\n\\nRoughly speaking, the regional and online\\nlearnable fields are a set K of neurons\\n\\nKI which try to cover a set of points as well\\nas possible by means of their distribution\\nin the input space. For this, neurons are\\nadded, moved or changed in their size dur-\\n\\nnetwork\\ncovers\\n\\npoint cloud\\ning training if necessary. The parameters\\nof the individual neurons will be discussed\\nlater.\\n\\nDefinition A.2 (Regional and online\\nlearnable field). A regional and on-\\nline learnable field (abbreviated ROLF or\\nROLF network) is a set K of neurons that\\nare trained to cover a certain set in the\\ninput space as well as possible.\\n\\nA.5.1.1 ROLF neurons feature a\\nposition and a radius in the\\ninput space\\n\\nHere, a ROLF neuron k ∈ K has two\\nparameters: Similar to the RBF networks,\\nit has a center ck, i.e. a position in the\\n\\ncI input space.\\n\\nBut it has yet another parameter: The ra-\\ndius σ, which defines the radius of the per-\\n\\nσI ceptive surface surrounding the neuron2.\\nA neuron covers the part of the input space\\nthat is situated within this radius.\\n\\nck and σk are locally defined for each neu-neuron\\nrepresents\\n\\nsurface 2 I write \\"defines\\" and not \\"is\\" because the actual\\nradius is specified by σ · ρ.\\n\\nFigure A.2: Structure of a ROLF neuron.\\n\\nron. This particularly means that the neu-\\nrons are capable to cover surfaces of differ-\\nent sizes.\\n\\nThe radius of the perceptive surface is\\nspecified by r = ρ · σ (fig. A.2) with\\nthe multiplier ρ being globally defined and\\npreviously specified for all neurons. Intu-\\nitively, the reader will wonder what this\\nmultiplicator is used for. Its significance\\nwill be discussed later. Furthermore, the\\nfollowing has to be observed: It is not nec-\\nessary for the perceptive surface of the dif-\\nferent neurons to be of the same size.\\n\\nDefinition A.3 (ROLF neuron). The pa-\\nrameters of a ROLF neuron k are a center\\nck and a radius σk.\\n\\nDefinition A.4 (Perceptive surface).\\nThe perceptive surface of a ROLF neuron\\n\\n176 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com A.5 Regional and online learnable fields\\n\\nk consists of all points within the radius\\nρ · σ in the input space.\\n\\nA.5.2 A ROLF learns unsupervised\\nby presenting training\\nsamples online\\n\\nLike many other paradigms of neural net-\\nworks our ROLF network learns by receiv-\\ning many training samples p of a training\\nset P . The learning is unsupervised. For\\neach training sample p entered into the net-\\nwork two cases can occur:\\n\\n1. There is one accepting neuron k for p\\nor\\n\\n2. there is no accepting neuron at all.\\n\\nIf in the first case several neurons are suit-\\nable, then there will be exactly one ac-\\ncepting neuron insofar as the closest neu-\\nron is the accepting one. For the accepting\\nneuron k ck and σk are adapted.\\n\\nDefinition A.5 (Accepting neuron). The\\ncriterion for a ROLF neuron k to be an\\naccepting neuron of a point p is that the\\npoint p must be located within the percep-\\ntive surface of k. If p is located in the per-\\nceptive surfaces of several neurons, then\\nthe closest neuron will be the accepting\\none. If there are several closest neurons,\\none can be chosen randomly.\\n\\nA.5.2.1 Both positions and radii are\\nadapted throughout learning\\n\\nAdapting\\nexisting\\nneurons\\n\\nLet us assume that we entered a training\\nsample p into the network and that there\\n\\nis an accepting neuron k. Then the radius\\nmoves towards ||p − ck|| (i.e. towards the\\ndistance between p and ck) and the center\\nck towards p. Additionally, let us define\\nthe two learning rates ησ and ηc for radii Jησ, ηcand centers.\\n\\nck(t+ 1) = ck(t) + ηc(p− ck(t))\\nσk(t+ 1) = σk(t) + ησ(||p− ck(t)|| − σk(t))\\n\\nNote that here σk is a scalar while ck is a\\nvector in the input space.\\n\\nDefinition A.6 (Adapting a ROLF neu-\\nron). A neuron k accepted by a point p is\\nadapted according to the following rules:\\n\\nck(t+ 1) = ck(t) + ηc(p− ck(t)) (A.5)\\n\\nσk(t+ 1) = σk(t) + ησ(||p− ck(t)|| − σk(t))\\n(A.6)\\n\\nA.5.2.2 The radius multiplier allows\\nneurons to be able not only to\\nshrink\\n\\nNow we can understand the function of the\\nmultiplier ρ: Due to this multiplier the per- Jρceptive surface of a neuron includes more\\nthan only all points surrounding the neu-\\nron in the radius σ. This means that due\\nto the aforementioned learning rule σ can-\\nnot only decrease but also increase.\\n\\nso the\\nneurons\\ncan growDefinition A.7 (Radius multiplier). The\\n\\nradius multiplier ρ > 1 is globally defined\\nand expands the perceptive surface of a\\nneuron k to a multiple of σk. So it is en-\\nsured that the radius σk cannot only de-\\ncrease but also increase.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 177\\n\\n\\n\\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\\n\\nGenerally, the radius multiplier is set to\\nvalues in the lower one-digit range, such\\nas 2 or 3.\\n\\nSo far we only have discussed the case in\\nthe ROLF training that there is an accept-\\ning neuron for the training sample p.\\n\\nA.5.2.3 As required, new neurons are\\ngenerated\\n\\nThis suggests to discuss the approach for\\nthe case that there is no accepting neu-\\nron.\\n\\nIn this case a new accepting neuron k is\\ngenerated for our training sample. The re-\\nsult is of course that ck and σk have to be\\ninitialized.\\n\\nThe initialization of ck can be understood\\nintuitively: The center of the new neuron\\nis simply set on the training sample, i.e.\\n\\nck = p.\\n\\nWe generate a new neuron because there\\nis no neuron close to p – for logical reasons,\\nwe place the neuron exactly on p.\\n\\nBut how to set a σ when a new neuron\\nis generated? For this purpose there exist\\ndifferent options:\\n\\nInit-σ: We always select a predefined\\nstatic σ.\\n\\nMinimum σ: We take a look at the σ of\\neach neuron and select the minimum.\\n\\nMaximum σ: We take a look at the σ of\\neach neuron and select the maximum.\\n\\nMean σ: We select the mean σ of all neu-\\nrons.\\n\\nCurrently, the mean-σ variant is the fa-\\nvorite one although the learning procedure\\nalso works with the other ones. In the\\nminimum-σ variant the neurons tend to\\ncover less of the surface, in the maximum-\\nσ variant they tend to cover more of the\\nsurface.\\n\\nDefinition A.8 (Generating a ROLF neu-\\nron). If a new ROLF neuron k is gener-\\nated by entering a training sample p, then\\n\\ninitialization\\nof a\\nneurons\\n\\nck is intialized with p and σk according to\\none of the aforementioned strategies (init-\\nσ, minimum-σ, maximum-σ, mean-σ).\\n\\nThe training is complete when after re-\\npeated randomly permuted pattern presen-\\ntation no new neuron has been generated\\nin an epoch and the positions of the neu-\\nrons barely change.\\n\\nA.5.3 Evaluating a ROLF\\n\\nThe result of the training algorithm is that\\nthe training set is gradually covered well\\nand precisely by the ROLF neurons and\\nthat a high concentration of points on a\\nspot of the input space does not automati-\\ncally generate more neurons. Thus, a pos-\\nsibly very large point cloud is reduced to\\nvery few representatives (based on the in-\\nput set).\\n\\nThen it is very easy to define the num-\\ncluster =\\nconnected\\nneurons\\n\\nber of clusters: Two neurons are (accord-\\ning to the definition of the ROLF) con-\\nnected when their perceptive surfaces over-\\n\\n178 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com A.5 Regional and online learnable fields\\n\\nlap (i.e. some kind of nearest neighbor-\\ning is executed with the variable percep-\\ntive surfaces). A cluster is a group of\\nconnected neurons or a group of points of\\nthe input space covered by these neurons\\n(fig. A.3).\\n\\nOf course, the complete ROLF network\\ncan be evaluated by means of other clus-\\ntering methods, i.e. the neurons can be\\nsearched for clusters. Particularly with\\nclustering methods whose storage effort\\ngrows quadratic to |P | the storage effort\\ncan be reduced dramatically since gener-\\nally there are considerably less ROLF neu-\\nrons than original data points, but the\\nneurons represent the data points quite\\nwell.\\n\\nA.5.4 Comparison with popular\\nclustering methods\\n\\nIt is obvious, that storing the neurons\\nrather than storing the input points takes\\nthe biggest part of the storage effort of the\\nROLFs. This is a great advantage for huge\\n\\nless\\nstorage\\neffort!\\n\\npoint clouds with a lot of points.\\n\\nSince it is unnecessary to store the en-\\ntire point cloud, our ROLF, as a neural\\nclustering method, has the capability to\\nlearn online, which is definitely a great ad-\\nvantage. Furthermore, it can (similar to\\nε nearest neighboring or k nearest neigh-\\nboring) distinguish clusters from enclosed\\nclusters – but due to the online presenta-recognize\\n\\n\\"cluster in\\nclusters\\"\\n\\ntion of the data without a quadratically\\ngrowing storage effort, which is by far the\\ngreatest disadvantage of the two neighbor-\\ning methods.\\n\\nFigure A.3: The clustering process. Top: the\\ninput set, middle: the input space covered by\\nROLF neurons, bottom: the input space only\\ncovered by the neurons (representatives).\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 179\\n\\n\\n\\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\\n\\nAdditionally, the issue of the size of the in-\\ndividual clusters proportional to their dis-\\ntance from each other is addressed by us-\\ning variable perceptive surfaces - which is\\nalso not always the case for the two men-\\ntioned methods.\\n\\nThe ROLF compares favorably with k-\\nmeans clustering, as well: Firstly, it is un-\\nnecessary to previously know the number\\nof clusters and, secondly, k-means cluster-\\ning recognizes clusters enclosed by other\\nclusters as separate clusters.\\n\\nA.5.5 Initializing radii, learning\\nrates and multiplier is not\\ntrivial\\n\\nCertainly, the disadvantages of the ROLF\\nshall not be concealed: It is not always\\neasy to select the appropriate initial value\\nfor σ and ρ. The previous knowledge\\nabout the data set can so to say be in-\\ncluded in ρ and the initial value of σ of the\\nROLF: Fine-grained data clusters should\\nuse a small ρ and a small σ initial value.\\nBut the smaller the ρ the smaller, the\\nchance that the neurons will grow if neces-\\nsary. Here again, there is no easy answer,\\njust like for the learning rates ηc and ησ.\\n\\nFor ρ the multipliers in the lower single-\\ndigit range such as 2 or 3 are very popu-\\nlar. ηc and ησ successfully work with val-\\nues about 0.005 to 0.1, variations during\\nrun-time are also imaginable for this type\\nof network. Initial values for σ generally\\ndepend on the cluster and data distribu-\\ntion (i.e. they often have to be tested).\\nBut compared to wrong initializations –\\n\\nat least with the mean-σ strategy – they\\nare relatively robust after some training\\ntime.\\n\\nAs a whole, the ROLF is on a par with\\nthe other clustering methods and is par-\\nticularly very interesting for systems with\\nlow storage capacity or huge data sets.\\n\\nA.5.6 Application examples\\n\\nA first application example could be find-\\ning color clusters in RGB images. Another\\nfield of application directly described in\\nthe ROLF publication is the recognition of\\nwords transferred into a 720-dimensional\\nfeature space. Thus, we can see that\\nROLFs are relatively robust against higher\\ndimensions. Further applications can be\\nfound in the field of analysis of attacks on\\nnetwork systems and their classification.\\n\\nExercises\\n\\nExercise 18. Determine at least four\\nadaptation steps for one single ROLF neu-\\nron k if the four patterns stated below\\nare presented one after another in the in-\\ndicated order. Let the initial values for\\nthe ROLF neuron be ck = (0.1, 0.1) and\\nσk = 1. Furthermore, let ηc = 0.5 and\\nησ = 0. Let ρ = 3.\\n\\nP = {(0.1, 0.1);\\n= (0.9, 0.1);\\n= (0.1, 0.9);\\n= (0.9, 0.9)}.\\n\\n180 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\nAppendix B\\n\\nExcursus: neural networks used for\\nprediction\\n\\nDiscussion of an application of neural networks: a look ahead into the future\\nof time series.\\n\\nAfter discussing the different paradigms of\\nneural networks it is now useful to take a\\nlook at an application of neural networks\\nwhich is brought up often and (as we will\\nsee) is also used for fraud: The applica-\\ntion of time series prediction. This ex-\\ncursus is structured into the description of\\ntime series and estimations about the re-\\nquirements that are actually needed to pre-\\ndict the values of a time series. Finally, I\\nwill say something about the range of soft-\\nware which should predict share prices or\\nother economic characteristics by means of\\nneural networks or other procedures.\\n\\nThis chapter should not be a detailed\\ndescription but rather indicate some ap-\\nproaches for time series prediction. In this\\nrespect I will again try to avoid formal def-\\ninitions.\\n\\nB.1 About time series\\n\\nA time series is a series of values dis-\\ncretized in time. For example, daily mea-\\nsured temperature values or other meteo-\\nrological data of a specific site could be\\nrepresented by a time series. Share price\\nvalues also represent a time series. Often\\nthe measurement of time series is timely\\nequidistant, and in many time series the\\nfuture development of their values is very\\ninteresting, e.g. the daily weather fore-\\ncast. time\\n\\nseries of\\nvaluesTime series can also be values of an actu-\\n\\nally continuous function read in a certain\\ndistance of time ∆t (fig. B.1 on the next J∆tpage).\\n\\nIf we want to predict a time series, we will\\nlook for a neural network that maps the\\nprevious series values to future develop-\\nments of the time series, i.e. if we know\\nlonger sections of the time series, we will\\n\\n181\\n\\n\\n\\nAppendix B Excursus: neural networks used for prediction dkriesel.com\\n\\nFigure B.1: A function x that depends on the\\ntime is sampled at discrete time steps (time dis-\\ncretized), this means that the result is a time\\nseries. The sampled values are entered into a\\nneural network (in this example an SLP) which\\nshall learn to predict the future values of the time\\nseries.\\n\\nhave enough training samples. Of course,\\nthese are not examples for the future to be\\npredicted but it is tried to generalize and\\nto extrapolate the past by means of the\\nsaid samples.\\n\\nBut before we begin to predict a time\\nseries we have to answer some questions\\nabout this time series we are dealing with\\nand ensure that it fulfills some require-\\nments.\\n\\n1. Do we have any evidence which sug-\\ngests that future values depend in any\\nway on the past values of the time se-\\nries? Does the past of a time series\\ninclude information about its future?\\n\\n2. Do we have enough past values of the\\ntime series that can be used as train-\\ning patterns?\\n\\n3. In case of a prediction of a continuous\\nfunction: What must a useful ∆t look\\nlike?\\n\\nNow these questions shall be explored in\\ndetail.\\n\\nHow much information about the future\\nis included in the past values of a time se-\\nries? This is the most important question\\nto be answered for any time series that\\nshould be mapped into the future. If the\\nfuture values of a time series, for instance,\\ndo not depend on the past values, then a\\ntime series prediction based on them will\\nbe impossible.\\n\\nIn this chapter, we assume systems whose\\nfuture values can be deduced from their\\nstates – the deterministic systems. This\\n\\n182 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com B.2 One-step-ahead prediction\\n\\nleads us to the question of what a system\\nstate is.\\n\\nA system state completely describes a sys-\\ntem for a certain point of time. The future\\nof a deterministic system would be clearly\\ndefined by means of the complete descrip-\\ntion of its current state.\\n\\nThe problem in the real world is that such\\na state concept includes all things that in-\\nfluence our system by any means.\\n\\nIn case of our weather forecast for a spe-\\ncific site we could definitely determine\\nthe temperature, the atmospheric pres-\\nsure and the cloud density as the mete-\\norological state of the place at a time t.\\nBut the whole state would include signifi-\\ncantly more information. Here, the world-\\nwide phenomena that control the weather\\nwould be interesting as well as small local\\npheonomena such as the cooling system of\\nthe local power plant.\\n\\nSo we shall note that the system state is de-\\nsirable for prediction but not always possi-\\nble to obtain. Often only fragments of the\\ncurrent states can be acquired, e.g. for a\\nweather forecast these fragments are the\\nsaid weather data.\\n\\nHowever, we can partially overcome these\\nweaknesses by using not only one single\\nstate (the last one) for the prediction, but\\nby using several past states. From this\\nwe want to derive our first prediction sys-\\ntem:\\n\\nB.2 One-step-ahead\\nprediction\\n\\nThe first attempt to predict the next fu-\\nture value of a time series out of past val-\\nues is called one-step-ahead prediction\\n(fig. B.2 on the following page).\\n\\npredict\\nthe next\\nvalueSuch a predictor system receives the last\\n\\nn observed state parts of the system as\\ninput and outputs the prediction for the\\nnext state (or state part). The idea of\\na state space with predictable states is\\ncalled state space forecasting.\\n\\nThe aim of the predictor is to realize a\\nfunction\\n\\nf(xt−n+1, . . . , xt−1, xt) = x̃t+1, (B.1)\\n\\nwhich receives exactly n past values in or-\\nder to predict the future value. Predicted\\nvalues shall be headed by a tilde (e.g. x̃) Jx̃to distinguish them from the actual future\\nvalues.\\n\\nThe most intuitive and simplest approach\\nwould be to find a linear combination\\n\\nx̃i+1 = a0xi + a1xi−1 + . . .+ ajxi−j\\n(B.2)\\n\\nthat approximately fulfills our condi-\\ntions.\\n\\nSuch a construction is called digital fil-\\nter. Here we use the fact that time series\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 183\\n\\n\\n\\nAppendix B Excursus: neural networks used for prediction dkriesel.com\\n\\nxt−3\\n\\n..\\n\\nxt−2\\n\\n..\\n\\nxt−1\\n\\n--\\n\\nxt\\n\\n++\\n\\nx̃t+1\\n\\npredictor\\n\\nKK\\n\\nFigure B.2: Representation of the one-step-ahead prediction. It is tried to calculate the future\\nvalue from a series of past values. The predicting element (in this case a neural network) is referred\\nto as predictor.\\n\\nusually have a lot of past values so that we\\ncan set up a series of equations1:\\n\\nxt = a0xt−1 + . . .+ ajxt−1−(n−1)\\n\\nxt−1 = a0xt−2 + . . .+ ajxt−2−(n−1)\\n... (B.3)\\n\\nxt−n = a0xt−n + . . .+ ajxt−n−(n−1)\\n\\nThus, n equations could be found for n un-\\nknown coefficients and solve them (if pos-\\nsible). Or another, better approach: we\\ncould usem > n equations for n unknowns\\nin such a way that the sum of the mean\\nsquared errors of the already known pre-\\ndiction is minimized. This is called mov-\\ning average procedure.\\n\\nBut this linear structure corresponds to a\\nsinglelayer perceptron with a linear activa-\\ntion function which has been trained by\\nmeans of data from the past (The experi-\\nmental setup would comply with fig. B.1\\non page 182). In fact, the training by\\n\\n1 Without going into detail, I want to remark that\\nthe prediction becomes easier the more past values\\nof the time series are available. I would like to\\nask the reader to read up on the Nyquist-Shannon\\nsampling theorem\\n\\nmeans of the delta rule provides results\\nvery close to the analytical solution.\\n\\nEven if this approach often provides satis-\\nfying results, we have seen that many prob-\\nlems cannot be solved by using a single-\\nlayer perceptron. Additional layers with\\nlinear activation function are useless, as\\nwell, since a multilayer perceptron with\\nonly linear activation functions can be re-\\nduced to a singlelayer perceptron. Such\\nconsiderations lead to a non-linear ap-\\nproach.\\n\\nThe multilayer perceptron and non-linear\\nactivation functions provide a universal\\nnon-linear function approximator, i.e. we\\ncan use an n-|H|-1-MLP for n n inputs out\\nof the past. An RBF network could also be\\nused. But remember that here the number\\nn has to remain low since in RBF networks\\nhigh input dimensions are very complex to\\nrealize. So if we want to include many past\\nvalues, a multilayer perceptron will require\\nconsiderably less computational effort.\\n\\n184 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com B.4 Additional optimization approaches for prediction\\n\\nB.3 Two-step-ahead\\nprediction\\n\\nWhat approaches can we use to to see far-\\nther into the future?\\n\\nB.3.1 Recursive two-step-ahead\\nprediction\\n\\npredict\\nfuture\\nvalues In order to extend the prediction to, for in-\\n\\nstance, two time steps into the future, we\\ncould perform two one-step-ahead predic-\\ntions in a row (fig. B.3 on the following\\npage), i.e. a recursive two-step-ahead\\nprediction. Unfortunately, the value de-\\ntermined by means of a one-step-ahead\\nprediction is generally imprecise so that\\nerrors can be built up, and the more pre-\\ndictions are performed in a row the more\\nimprecise becomes the result.\\n\\nB.3.2 Direct two-step-ahead\\nprediction\\n\\nWe have already guessed that there exists\\na better approach: Just like the system\\ncan be trained to predict the next value,\\nwe can certainly train it to predict the\\n\\ndirect\\nprediction\\nis better\\n\\nnext but one value. This means we di-\\nrectly train, for example, a neural network\\nto look two time steps ahead into the fu-\\nture, which is referred to as direct two-\\nstep-ahead prediction (fig. B.4 on the\\nnext page). Obviously, the direct two-step-\\nahead prediction is technically identical to\\nthe one-step-ahead prediction. The only\\ndifference is the training.\\n\\nB.4 Additional optimization\\napproaches for prediction\\n\\nThe possibility to predict values far away\\nin the future is not only important because\\nwe try to look farther ahead into the fu-\\nture. There can also be periodic time se-\\nries where other approaches are hardly pos-\\nsible: If a lecture begins at 9 a.m. every\\nThursday, it is not very useful to know how\\nmany people sat in the lecture room on\\nMonday to predict the number of lecture\\nparticipants. The same applies, for ex-\\nample, to periodically occurring commuter\\njams.\\n\\nB.4.1 Changing temporal\\nparameters\\n\\nThus, it can be useful to intentionally leave\\ngaps in the future values as well as in the\\npast values of the time series, i.e. to in-\\ntroduce the parameter ∆t which indicates\\nwhich past value is used for prediction.\\nTechnically speaking, we still use a one- extent\\n\\ninput\\nperiod\\n\\nstep-ahead prediction only that we extend\\nthe input space or train the system to pre-\\ndict values lying farther away.\\n\\nIt is also possible to combine different ∆t:\\nIn case of the traffic jam prediction for a\\nMonday the values of the last few days\\ncould be used as data input in addition to\\nthe values of the previous Mondays. Thus,\\nwe use the last values of several periods,\\nin this case the values of a weekly and a\\ndaily period. We could also include an an-\\nnual period in the form of the beginning of\\nthe holidays (for sure, everyone of us has\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 185\\n\\n\\n\\nAppendix B Excursus: neural networks used for prediction dkriesel.com\\n\\npredictor\\n\\n��\\nxt−3\\n\\n..\\n\\nxt−2\\n\\n00\\n\\n..\\n\\nxt−1\\n\\n00\\n\\n--\\n\\nxt\\n\\n++\\n\\n00\\n\\nx̃t+1\\n\\nOO\\n\\nx̃t+2\\n\\npredictor\\n\\nJJ\\n\\nFigure B.3: Representation of the two-step-ahead prediction. Attempt to predict the second future\\nvalue out of a past value series by means of a second predictor and the involvement of an already\\npredicted value.\\n\\nxt−3\\n\\n..\\n\\nxt−2\\n\\n..\\n\\nxt−1\\n\\n--\\n\\nxt\\n\\n++\\n\\nx̃t+1 x̃t+2\\n\\npredictor\\n\\nEE\\n\\nFigure B.4: Representation of the direct two-step-ahead prediction. Here, the second time step is\\npredicted directly, the first one is omitted. Technically, it does not differ from a one-step-ahead\\nprediction.\\n\\n186 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com B.5 Remarks on the prediction of share prices\\n\\nalready spent a lot of time on the highway\\nbecause he forgot the beginning of the hol-\\nidays).\\n\\nB.4.2 Heterogeneous prediction\\n\\nAnother prediction approach would be to\\npredict the future values of a single time\\nseries out of several time series, if it is\\nassumed that the additional time seriesuse\\n\\ninformation\\noutside of\\ntime series\\n\\nis related to the future of the first one\\n(heterogeneous one-step-ahead pre-\\ndiction, fig. B.5 on the following page).\\n\\nIf we want to predict two outputs of two\\nrelated time series, it is certainly possible\\nto perform two parallel one-step-ahead pre-\\ndictions (analytically this is done very of-\\nten because otherwise the equations would\\nbecome very confusing); or in case of\\nthe neural networks an additional output\\nneuron is attached and the knowledge of\\nboth time series is used for both outputs\\n(fig. B.6 on the next page).\\n\\nYou’ll find more and more general material\\non time series in [WG94].\\n\\nB.5 Remarks on the\\nprediction of share prices\\n\\nMany people observe the changes of a\\nshare price in the past and try to con-\\nclude the future from those values in or-\\nder to benefit from this knowledge. Share\\nprices are discontinuous and therefore they\\nare principally difficult functions. Further-\\nmore, the functions can only be used for\\n\\ndiscrete values – often, for example, in a\\ndaily rhythm (including the maximum and\\nminimum values per day, if we are lucky)\\nwith the daily variations certainly being\\neliminated. But this makes the whole\\nthing even more difficult.\\n\\nThere are chartists, i.e. people who look\\nat many diagrams and decide by means\\nof a lot of background knowledge and\\ndecade-long experience whether the equi-\\nties should be bought or not (and often\\nthey are very successful).\\n\\nApart from the share prices it is very in-\\nteresting to predict the exchange rates of\\ncurrencies: If we exchange 100 Euros into\\nDollars, the Dollars into Pounds and the\\nPounds back into Euros it could be pos-\\nsible that we will finally receive 110 Eu-\\nros. But once found out, we would do this\\nmore often and thus we would change the\\nexchange rates into a state in which such\\nan increasing circulation would no longer\\nbe possible (otherwise we could produce\\nmoney by generating, so to speak, a finan-\\ncial perpetual motion machine.\\n\\nAt the stock exchange, successful stock\\nand currency brokers raise or lower their\\nthumbs – and thereby indicate whether in\\ntheir opinion a share price or an exchange\\nrate will increase or decrease. Mathemat-\\nically speaking, they indicate the first bit\\n(sign) of the first derivative of the ex-\\nchange rate. In that way excellent world-\\nclass brokers obtain success rates of about\\n70%.\\n\\nIn Great Britain, the heterogeneous one-\\nstep-ahead prediction was successfully\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 187\\n\\n\\n\\nAppendix B Excursus: neural networks used for prediction dkriesel.com\\n\\nxt−3\\n\\n..\\n\\nxt−2\\n\\n..\\n\\nxt−1\\n\\n--\\n\\nxt\\n\\n++\\n\\nx̃t+1\\n\\npredictor\\n\\nKK\\n\\nyt−3\\n\\n00\\n\\nyt−2\\n\\n00\\n\\nyt−1\\n\\n11\\n\\nyt\\n\\n33\\n\\nFigure B.5: Representation of the heterogeneous one-step-ahead prediction. Prediction of a time\\nseries under consideration of a second one.\\n\\nxt−3\\n\\n..\\n\\nxt−2\\n\\n..\\n\\nxt−1\\n\\n--\\n\\nxt\\n\\n++\\n\\nx̃t+1\\n\\npredictor\\n\\nKK\\n\\n��\\nyt−3\\n\\n00\\n\\nyt−2\\n\\n00\\n\\nyt−1\\n\\n11\\n\\nyt\\n\\n33\\n\\nỹt+1\\n\\nFigure B.6: Heterogeneous one-step-ahead prediction of two time series at the same time.\\n\\n188 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com B.5 Remarks on the prediction of share prices\\n\\nused to increase the accuracy of such pre-\\ndictions to 76%: In addition to the time\\nseries of the values indicators such as the\\noil price in Rotterdam or the US national\\ndebt were included.\\n\\nThis is just an example to show the mag-\\nnitude of the accuracy of stock-exchange\\nevaluations, since we are still talking only\\nabout the first bit of the first derivation!\\nWe still do not know how strong the ex-\\npected increase or decrease will be and\\nalso whether the effort will pay off: Prob-\\nably, one wrong prediction could nullify\\nthe profit of one hundred correct predic-\\ntions.\\n\\nHow can neural networks be used to pre-\\ndict share prices? Intuitively, we assume\\nthat future share prices are a function of\\nthe previous share values.\\n\\nBut this assumption is wrong: Share\\nprices are no function of their past val-\\nues, but a function of their assumed fu-\\n\\nshare price\\nfunction of\\n\\nassumed\\nfuture\\nvalue!\\n\\nture value. We do not buy shares be-\\ncause their values have been increased\\nduring the last days, but because we be-\\nlieve that they will futher increase tomor-\\nrow. If, as a consequence, many people\\nbuy a share, they will boost the price.\\nTherefore their assumption was right – a\\nself-fulfilling prophecy has been gener-\\nated, a phenomenon long known in eco-\\nnomics.\\n\\nThe same applies the other way around:\\nWe sell shares because we believe that to-\\nmorrow the prices will decrease. This will\\nbeat down the prices the next day and gen-\\nerally even more the day after the next.\\n\\nAgain and again some software appears\\nwhich uses scientific key words such as\\n”neural networks” to purport that it is ca-\\npable to predict where share prices are go-\\ning. Do not buy such software! In addi-\\ntion to the aforementioned scientific exclu-\\nsions there is one simple reason for this:\\nIf these tools work – why should the man-\\nufacturer sell them? Normally, useful eco-\\nnomic knowledge is kept secret. If we knew\\na way to definitely gain wealth by means\\nof shares, we would earn our millions by\\nusing this knowledge instead of selling it\\nfor 30 euros, wouldn’t we?\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 189\\n\\n\\n\\n\\n\\nAppendix C\\n\\nExcursus: reinforcement learning\\nWhat if there were no training samples but it would nevertheless be possible\\nto evaluate how well we have learned to solve a problem? Let us examine a\\n\\nlearning paradigm that is situated between supervised and unsupervised\\nlearning.\\n\\nI now want to introduce a more exotic ap-\\nproach of learning – just to leave the usual\\npaths. We know learning procedures in\\nwhich the network is exactly told what to\\ndo, i.e. we provide exemplary output val-\\nues. We also know learning procedures\\nlike those of the self-organizing maps, into\\nwhich only input values are entered.\\n\\nNow we want to explore something in-\\nbetween: The learning paradigm of rein-\\nforcement learning – reinforcement learn-\\ning according to Sutton and Barto\\n[SB98].\\n\\nReinforcement learning in itself is no neu-\\nral network but only one of the three learn-\\ning paradigms already mentioned in chap-\\nter 4. In some sources it is counted among\\nthe supervised learning procedures since a\\nfeedback is given. Due to its very rudimen-no\\n\\nsamples\\nbut\\n\\nfeedback\\n\\ntary feedback it is reasonable to separate\\nit from the supervised learning procedures\\n– apart from the fact that there are no\\ntraining samples at all.\\n\\nWhile it is generally known that pro-\\ncedures such as backpropagation cannot\\nwork in the human brain itself, reinforce-\\nment learning is usually considered as be-\\ning biologically more motivated.\\n\\nThe term reinforcement learning\\ncomes from cognitive science and\\npsychology and it describes the learning\\nsystem of carrot and stick, which occurs\\neverywhere in nature, i.e. learning by\\nmeans of good or bad experience, reward\\nand punishment. But there is no learning\\naid that exactly explains what we have\\nto do: We only receive a total result\\nfor a process (Did we win the game of\\nchess or not? And how sure was this\\nvictory?), but no results for the individual\\nintermediate steps.\\n\\nFor example, if we ride our bike with worn\\ntires and at a speed of exactly 21, 5kmh\\nthrough a turn over some sand with a\\ngrain size of 0.1mm, on the average, then\\nnobody could tell us exactly which han-\\n\\n191\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\ndlebar angle we have to adjust or, even\\nworse, how strong the great number of\\nmuscle parts in our arms or legs have to\\ncontract for this. Depending on whether\\nwe reach the end of the curve unharmed or\\nnot, we soon have to face the learning expe-\\nrience, a feedback or a reward, be it good\\nor bad. Thus, the reward is very simple\\n- but on the other hand it is considerably\\neasier to obtain. If we now have tested dif-\\nferent velocities and turning angles often\\nenough and received some rewards, we will\\nget a feel for what works and what does\\nnot. The aim of reinforcement learning is\\nto maintain exactly this feeling.\\n\\nAnother example for the quasi-\\nimpossibility to achieve a sort of cost or\\nutility function is a tennis player who\\ntries to maximize his athletic success\\non the long term by means of complex\\nmovements and ballistic trajectories in\\nthe three-dimensional space including the\\nwind direction, the importance of the\\ntournament, private factors and many\\nmore.\\n\\nTo get straight to the point: Since we\\nreceive only little feedback, reinforcement\\nlearning often means trial and error – and\\ntherefore it is very slow.\\n\\nC.1 System structure\\n\\nNow we want to briefly discuss different\\nsizes and components of the system. We\\nwill define them more precisely in the fol-\\nlowing sections. Broadly speaking, rein-\\nforcement learning represents the mutual\\n\\ninteraction between an agent and an envi-\\nronmental system (fig. C.2).\\n\\nThe agent shall solve some problem. He\\ncould, for instance, be an autonomous\\nrobot that shall avoid obstacles. The\\nagent performs some actions within the\\nenvironment and in return receives a feed-\\nback from the environment, which in the\\nfollowing is called reward. This cycle of ac-\\ntion and reward is characteristic for rein-\\nforcement learning. The agent influences\\nthe system, the system provides a reward\\nand then changes.\\n\\nThe reward is a real or discrete scalar\\nwhich describes, as mentioned above, how\\nwell we achieve our aim, but it does not\\ngive any guidance how we can achieve it.\\nThe aim is always to make the sum of\\nrewards as high as possible on the long\\nterm.\\n\\nC.1.1 The gridworld\\n\\nAs a learning example for reinforcement\\nlearning I would like to use the so-called\\ngridworld. We will see that its struc-\\nture is very simple and easy to figure out\\nand therefore reinforcement is actually not\\nnecessary. However, it is very suitable\\n\\nsimple\\nexamplary\\nworld\\n\\nfor representing the approach of reinforce-\\nment learning. Now let us exemplary de-\\nfine the individual components of the re-\\ninforcement system by means of the grid-\\nworld. Later, each of these components\\nwill be examined more exactly.\\n\\nEnvironment: The gridworld (fig. C.1 on\\nthe facing page) is a simple, discrete\\n\\n192 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.1 System structure\\n\\nworld in two dimensions which in the\\nfollowing we want to use as environ-\\nmental system.\\n\\nAgent: As an Agent we use a simple robot\\nbeing situated in our gridworld.\\n\\nState space: As we can see, our gridworld\\nhas 5× 7 fields with 6 fields being un-\\naccessible. Therefore, our agent can\\noccupy 29 positions in the grid world.\\nThese positions are regarded as states\\nfor the agent.\\n\\nAction space: The actions are still miss-\\ning. We simply define that the robot\\ncould move one field up or down, to\\nthe right or to the left (as long as\\nthere is no obstacle or the edge of our\\ngridworld).\\n\\nTask: Our agent’s task is to leave the grid-\\nworld. The exit is located on the right\\nof the light-colored field.\\n\\nNon-determinism: The two obstacles can\\nbe connected by a \\"door\\". When the\\ndoor is closed (lower part of the illus-\\ntration), the corresponding field is in-\\naccessible. The position of the door\\ncannot change during a cycle but only\\nbetween the cycles.\\n\\nWe now have created a small world that\\nwill accompany us through the following\\nlearning strategies and illustrate them.\\n\\nC.1.2 Agent und environment\\n\\nOur aim is that the agent learns what hap-\\npens by means of the reward. Thus, it\\n\\n×\\n\\n×\\n\\nFigure C.1: A graphical representation of our\\ngridworld. Dark-colored cells are obstacles and\\ntherefore inaccessible. The exit is located on the\\nright side of the light-colored field. The symbol\\n× marks the starting position of our agent. In\\nthe upper part of our figure the door is open, in\\nthe lower part it is closed.\\n\\nAgent\\n\\naction\\n\\n__\\nenvironment\\n\\nreward / new situation\\n\\n??\\n\\nFigure C.2: The agent performs some actions\\nwithin the environment and in return receives a\\nreward.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 193\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\nis trained over, of and by means of a dy-\\nnamic system, the environment, in order\\nto reach an aim. But what does learning\\nmean in this context?\\n\\nThe agent shall learn a mapping of sit-agent\\nacts in\\n\\nenvironment\\nuations to actions (called policy), i.e. it\\nshall learn what to do in which situation\\nto achieve a certain (given) aim. The aim\\nis simply shown to the agent by giving an\\naward for the achievement.\\n\\nSuch an award must not be mistaken for\\nthe reward – on the agent’s way to the\\nsolution it may sometimes be useful to\\nreceive a smaller award or a punishment\\nwhen in return the longterm result is max-\\nimum (similar to the situation when an\\ninvestor just sits out the downturn of the\\nshare price or to a pawn sacrifice in a chess\\ngame). So, if the agent is heading into\\nthe right direction towards the target, it\\nreceives a positive reward, and if not it re-\\nceives no reward at all or even a negative\\nreward (punishment). The award is, so to\\nspeak, the final sum of all rewards – which\\nis also called return.\\n\\nAfter having colloquially named all the ba-\\nsic components, we want to discuss more\\nprecisely which components can be used to\\nmake up our abstract reinforcement learn-\\ning system.\\n\\nIn the gridworld: In the gridworld, the\\nagent is a simple robot that should find the\\nexit of the gridworld. The environment\\nis the gridworld itself, which is a discrete\\ngridworld.\\n\\nDefinition C.1 (Agent). In reinforce-\\nment learning the agent can be formally\\n\\ndescribed as a mapping of the situation\\nspace S into the action space A(st). The\\nmeaning of situations st will be defined\\nlater and should only indicate that the ac-\\ntion space depends on the current situa-\\ntion.\\n\\nAgent: S → A(st) (C.1)\\n\\nDefinition C.2 (Environment). The en-\\nvironment represents a stochastic map-\\nping of an action A in the current situa-\\ntion st to a reward rt and a new situation\\nst+1.\\n\\nEnvironment: S ×A→ P (S × rt) (C.2)\\n\\nC.1.3 States, situations and actions\\n\\nAs already mentioned, an agent can be in\\ndifferent states: In case of the gridworld,\\nfor example, it can be in different positions\\n(here we get a two-dimensional state vec-\\ntor).\\n\\nFor an agent is ist not always possible to\\nrealize all information about its current\\nstate so that we have to introduce the term\\nsituation. A situation is a state from the\\nagent’s point of view, i.e. only a more or\\nless precise approximation of a state.\\n\\nTherefore, situations generally do not al-\\nlow to clearly \\"predict\\" successor situa-\\ntions – even with a completely determin-\\nistic system this may not be applicable.\\nIf we knew all states and the transitions\\nbetween them exactly (thus, the complete\\nsystem), it would be possible to plan op-\\ntimally and also easy to find an optimal\\n\\n194 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.1 System structure\\n\\npolicy (methods are provided, for example,\\nby dynamic programming).\\n\\nNow we know that reinforcement learning\\nis an interaction between the agent and\\nthe system including actions at and sit-\\nuations st. The agent cannot determine\\nby itself whether the current situation is\\ngood or bad: This is exactly the reason\\nwhy it receives the said reward from the\\nenvironment.\\n\\nIn the gridworld: States are positions\\nwhere the agent can be situated. Sim-\\nply said, the situations equal the states\\nin the gridworld. Possible actions would\\nbe to move towards north, south, east or\\nwest.\\n\\nSituation and action can be vectorial, the\\nreward is always a scalar (in an extreme\\ncase even only a binary value) since the\\naim of reinforcement learning is to get\\nalong with little feedback. A complex vec-\\ntorial reward would equal a real teaching\\ninput.\\n\\nBy the way, the cost function should be\\nminimized, which would not be possible,\\nhowever, with a vectorial reward since we\\ndo not have any intuitive order relations\\nin multi-dimensional space, i.e. we do not\\ndirectly know what is better or worse.\\n\\nDefinition C.3 (State). Within its en-\\nvironment the agent is in a state. States\\ncontain any information about the agent\\nwithin the environmental system. Thus,\\nit is theoretically possible to clearly pre-\\ndict a successor state to a performed ac-\\ntion within a deterministic system out of\\nthis godlike state knowledge.\\n\\nDefinition C.4 (Situation). Situations\\nst (here at time t) of a situation space JstS are the agent’s limited, approximate\\n\\nJSknowledge about its state. This approx-\\nimation (about which the agent cannot\\neven know how good it is) makes clear pre-\\ndictions impossible.\\n\\nDefinition C.5 (Action). Actions at can Jatbe performed by the agent (whereupon it\\ncould be possible that depending on the\\nsituation another action space A(S) ex-\\n\\nJA(S)ists). They cause state transitions and\\ntherefore a new situation from the agent’s\\npoint of view.\\n\\nC.1.4 Reward and return\\n\\nAs in real life it is our aim to receive\\nan award that is as high as possible, i.e.\\nto maximize the sum of the expected re-\\nwards r, called return R, on the long\\nterm. For finitely many time steps1 the\\nrewards can simply be added:\\n\\nRt = rt+1 + rt+2 + . . . (C.3)\\n\\n=\\n∞∑\\nx=1\\n\\nrt+x (C.4)\\n\\nCertainly, the return is only estimated\\nhere (if we knew all rewards and therefore\\nthe return completely, it would no longer\\nbe necessary to learn).\\n\\nDefinition C.6 (Reward). A reward rt is Jrta scalar, real or discrete (even sometimes\\nonly binary) reward or punishment which\\n\\n1 In practice, only finitely many time steps will be\\npossible, even though the formulas are stated with\\nan infinite sum in the first place\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 195\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\nthe environmental system returns to the\\nagent as reaction to an action.\\n\\nDefinition C.7 (Return). The return Rt\\nis the accumulation of all received rewards\\n\\nRtI until time t.\\n\\nC.1.4.1 Dealing with long periods of\\ntime\\n\\nHowever, not every problem has an ex-\\nplicit target and therefore a finite sum (e.g.\\nour agent can be a robot having the task\\nto drive around again and again and to\\navoid obstacles). In order not to receive a\\ndiverging sum in case of an infinite series\\nof reward estimations a weakening factor\\n0 < γ < 1 is used, which weakens the in-\\n\\nγI fluence of future rewards. This is not only\\nuseful if there exists no target but also if\\nthe target is very far away:\\n\\nRt = rt+1 + γ1rt+2 + γ2rt+3 + . . . (C.5)\\n\\n=\\n∞∑\\nx=1\\n\\nγx−1rt+x (C.6)\\n\\nThe farther the reward is away, the smaller\\nis the influence it has in the agent’s deci-\\nsions.\\n\\nAnother possibility to handle the return\\nsum would be a limited time horizon\\nτ so that only τ many following rewards\\n\\nτI\\nrt+1, . . . , rt+τ are regarded:\\n\\nRt = rt+1 + . . .+ γτ−1rt+τ (C.7)\\n\\n=\\nτ∑\\nx=1\\n\\nγx−1rt+x (C.8)\\n\\nThus, we divide the timeline into\\nepisodes. Usually, one of the two meth-\\nods is used to limit the sum, if not both\\nmethods together.\\n\\nAs in daily living we try to approximate\\nour current situation to a desired state.\\nSince it is not mandatory that only the\\nnext expected reward but the expected to-\\ntal sum decides what the agent will do, it\\nis also possible to perform actions that, on\\nshort notice, result in a negative reward\\n(e.g. the pawn sacrifice in a chess game)\\nbut will pay off later.\\n\\nC.1.5 The policy\\n\\nAfter having considered and formalized\\nsome system components of reinforcement\\nlearning the actual aim is still to be dis-\\ncussed:\\n\\nDuring reinforcement learning the agent\\nlearns a policy JΠ\\n\\nΠ : S → P (A),\\n\\nThus, it continuously adjusts a mapping\\nof the situations to the probabilities P (A),\\nwith which any action A is performed in\\nany situation S. A policy can be defined\\nas a strategy to select actions that would\\nmaximize the reward in the long term.\\n\\nIn the gridworld: In the gridworld the pol-\\nicy is the strategy according to which the\\nagent tries to exit the gridworld.\\n\\nDefinition C.8 (Policy). The policy Π\\ns a mapping of situations to probabilities\\n\\n196 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.1 System structure\\n\\nto perform every action out of the action\\nspace. So it can be formalized as\\n\\nΠ : S → P (A). (C.9)\\n\\nBasically, we distinguish between two pol-\\nicy paradigms: An open loop policy rep-\\nresents an open control chain and creates\\nout of an initial situation s0 a sequence of\\nactions a0, a1, . . . with ai 6= ai(si); i > 0.\\nThus, in the beginning the agent develops\\na plan and consecutively executes it to the\\nend without considering the intermediate\\nsituations (therefore ai 6= ai(si), actions af-\\nter a0 do not depend on the situations).\\n\\nIn the gridworld: In the gridworld, an\\nopen-loop policy would provide a precise\\ndirection towards the exit, such as the way\\nfrom the given starting position to (in ab-\\nbreviations of the directions) EEEEN.\\n\\nSo an open-loop policy is a sequence of\\nactions without interim feedback. A se-\\nquence of actions is generated out of a\\nstarting situation. If the system is known\\nwell and truly, such an open-loop policy\\ncan be used successfully and lead to use-\\nful results. But, for example, to know the\\nchess game well and truly it would be nec-\\nessary to try every possible move, which\\nwould be very time-consuming. Thus, for\\nsuch problems we have to find an alterna-\\ntive to the open-loop policy, which incorpo-\\nrates the current situations into the action\\nplan:\\n\\nA closed loop policy is a closed loop, a\\nfunction\\n\\nΠ : si → ai with ai = ai(si),\\n\\nin a manner of speaking. Here, the envi-\\nronment influences our action or the agent\\nresponds to the input of the environment,\\nrespectively, as already illustrated in fig.\\nC.2. A closed-loop policy, so to speak, is\\na reactive plan to map current situations\\nto actions to be performed.\\n\\nIn the gridworld: A closed-loop policy\\nwould be responsive to the current posi-\\ntion and choose the direction according to\\nthe action. In particular, when an obsta-\\ncle appears dynamically, such a policy is\\nthe better choice.\\n\\nWhen selecting the actions to be per-\\nformed, again two basic strategies can be\\nexamined.\\n\\nC.1.5.1 Exploitation vs. exploration\\n\\nAs in real life, during reinforcement learn-\\ning often the question arises whether the\\nexisiting knowledge is only willfully ex-\\nploited or new ways are also explored.\\nInitially, we want to discuss the two ex-\\ntremes:\\n\\nresearch\\nor safety?A greedy policy always chooses the way\\n\\nof the highest reward that can be deter-\\nmined in advance, i.e. the way of the high-\\nest known reward. This policy represents\\nthe exploitation approach and is very\\npromising when the used system is already\\nknown.\\n\\nIn contrast to the exploitation approach it\\nis the aim of the exploration approach\\nto explore a system as detailed as possible\\nso that also such paths leading to the tar-\\nget can be found which may be not very\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 197\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\npromising at first glance but are in fact\\nvery successful.\\n\\nLet us assume that we are looking for the\\nway to a restaurant, a safe policy would\\nbe to always take the way we already\\nknow, not matter how unoptimal and long\\nit may be, and not to try to explore bet-\\nter ways. Another approach would be to\\nexplore shorter ways every now and then,\\neven at the risk of taking a long time and\\nbeing unsuccessful, and therefore finally\\nhaving to take the original way and arrive\\ntoo late at the restaurant.\\n\\nIn reality, often a combination of both\\nmethods is applied: In the beginning of\\nthe learning process it is researched with\\na higher probability while at the end more\\nexisting knowledge is exploited. Here, a\\nstatic probability distribution is also pos-\\nsible and often applied.\\n\\nIn the gridworld: For finding the way in\\nthe gridworld, the restaurant example ap-\\nplies equally.\\n\\nC.2 Learning process\\n\\nLet us again take a look at daily life. Ac-\\ntions can lead us from one situation into\\ndifferent subsituations, from each subsit-\\nuation into further sub-subsituations. In\\na sense, we get a situation tree where\\nlinks between the nodes must be consid-\\nered (often there are several ways to reach\\na situation – so the tree could more accu-\\nrately be referred to as a situation graph).\\n\\nhe leaves of such a tree are the end situ-\\nations of the system. The exploration ap-\\nproach would search the tree as thoroughly\\nas possible and become acquainted with all\\nleaves. The exploitation approach would\\nunerringly go to the best known leave.\\n\\nAnalogous to the situation tree, we also\\ncan create an action tree. Here, the re-\\nwards for the actions are within the nodes.\\nNow we have to adapt from daily life how\\nwe learn exactly.\\n\\nC.2.1 Rewarding strategies\\n\\nInteresting and very important is the ques-\\ntion for what a reward and what kind of\\nreward is awarded since the design of the\\nreward significantly controls system behav-\\nior. As we have seen above, there gener-\\nally are (again as in daily life) various ac-\\ntions that can be performed in any situa-\\ntion. There are different strategies to eval-\\nuate the selected situations and to learn\\nwhich series of actions would lead to the\\ntarget. First of all, this principle should\\nbe explained in the following.\\n\\nWe now want to indicate some extreme\\ncases as design examples for the reward:\\n\\nA rewarding similar to the rewarding in a\\nchess game is referred to as pure delayed\\nreward: We only receive the reward at\\nthe end of and not during the game. This\\nmethod is always advantageous when we\\nfinally can say whether we were succesful\\nor not, but the interim steps do not allow\\n\\n198 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.2 Learning process\\n\\nan estimation of our situation. If we win,\\nthen\\n\\nrt = 0 ∀t < τ (C.10)\\n\\nas well as rτ = 1. If we lose, then rτ = −1.\\nWith this rewarding strategy a reward is\\nonly returned by the leaves of the situation\\ntree.\\n\\nPure negative reward: Here,\\n\\nrt = −1 ∀t < τ. (C.11)\\n\\nThis system finds the most rapid way to\\nreach the target because this way is auto-\\nmatically the most favorable one in respect\\nof the reward. The agent receives punish-\\nment for anything it does – even if it does\\nnothing. As a result it is the most inex-\\npensive method for the agent to reach the\\ntarget fast.\\n\\nAnother strategy is the avoidance strat-\\negy: Harmful situations are avoided.\\nHere,\\n\\nrt ∈ {0,−1}, (C.12)\\n\\nMost situations do not receive any reward,\\nonly a few of them receive a negative re-\\nward. The agent agent will avoid getting\\ntoo close to such negative situations\\n\\nWarning: Rewarding strategies can have\\nunexpected consequences. A robot that is\\ntold \\"have it your own way but if you touch\\nan obstacle you will be punished\\" will sim-\\nply stand still. If standing still is also pun-\\nished, it will drive in small circles. Recon-\\nsidering this, we will understand that this\\nbehavior optimally fulfills the return of the\\n\\nrobot but unfortunately was not intended\\nto do so.\\n\\nFurthermore, we can show that especially\\nsmall tasks can be solved better by means\\nof negative rewards while positive, more\\ndifferentiated rewards are useful for large,\\ncomplex tasks.\\n\\nFor our gridworld we want to apply the\\npure negative reward strategy: The robot\\nshall find the exit as fast as possible.\\n\\nC.2.2 The state-value function\\n\\nUnlike our agent we have a godlike view state\\nevaluationof our gridworld so that we can swiftly de-\\n\\ntermine which robot starting position can\\nprovide which optimal return.\\n\\nIn figure C.3 on the next page these opti-\\nmal returns are applied per field.\\n\\nIn the gridworld: The state-value function\\nfor our gridworld exactly represents such\\na function per situation (= position) with\\nthe difference being that here the function\\nis unknown and has to be learned.\\n\\nThus, we can see that it would be more\\npractical for the robot to be capable to\\nevaluate the current and future situations.\\nSo let us take a look at another system\\ncomponent of reinforcement learning: the\\nstate-value function V (s), which with\\nregard to a policy Π is often called VΠ(s).\\nBecause whether a situation is bad often\\ndepends on the general behavior Π of the\\nagent.\\n\\nA situation being bad under a policy that\\nis searching risks and checking out limits\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 199\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\n-6 -5 -4 -3 -2\\n-7 -1\\n-6 -5 -4 -3 -2\\n-7 -6 -5 -3\\n-8 -7 -6 -4\\n-9 -8 -7 -5\\n-10 -9 -8 -7 -6\\n\\n-6 -5 -4 -3 -2\\n-7 -1\\n-8 -9 -10 -2\\n-9 -10 -11 -3\\n-10 -11 -10 -4\\n-11 -10 -9 -5\\n-10 -9 -8 -7 -6\\n\\nFigure C.3: Representation of each optimal re-\\nturn per field in our gridworld by means of pure\\nnegative reward awarding, at the top with an\\nopen and at the bottom with a closed door.\\n\\nwould be, for instance, if an agent on a bi-\\ncycle turns a corner and the front wheel\\nbegins to slide out. And due to its dare-\\ndevil policy the agent would not brake in\\nthis situation. With a risk-aware policy\\nthe same situations would look much bet-\\nter, thus it would be evaluated higher by\\na good state-value function\\n\\nVΠ(s) simply returns the value the current\\nVΠ(s)I situation s has for the agent under policy\\n\\nΠ. Abstractly speaking, according to the\\nabove definitions, the value of the state-\\nvalue function corresponds to the return\\nRt (the expected value) of a situation st.\\n\\nEΠ denotes the set of the expected returns\\nunder Π and the current situation st.\\n\\nVΠ(s) = EΠ{Rt|s = st}\\n\\nDefinition C.9 (State-value function).\\nThe state-value function VΠ(s) has the\\ntask of determining the value of situations\\nunder a policy, i.e. to answer the agent’s\\nquestion of whether a situation s is good\\nor bad or how good or bad it is. For this\\npurpose it returns the expectation of the\\nreturn under the situation:\\n\\nVΠ(s) = EΠ{Rt|s = st} (C.13)\\n\\nThe optimal state-value function is called\\nV ∗Π(s).\\n\\nJV ∗Π(s)\\n\\nUnfortunaely, unlike us our robot does not\\nhave a godlike view of its environment. It\\ndoes not have a table with optimal returns\\nlike the one shown above to orient itself.\\nThe aim of reinforcement learning is that\\nthe robot generates its state-value func-\\ntion bit by bit on the basis of the returns of\\nmany trials and approximates the optimal\\nstate-value function V ∗ (if there is one).\\n\\nIn this context I want to introduce two\\nterms closely related to the cycle between\\nstate-value function and policy:\\n\\nC.2.2.1 Policy evaluation\\n\\nPolicy evaluation is the approach to try\\na policy a few times, to provide many re-\\nwards that way and to gradually accumu-\\nlate a state-value function by means of\\nthese rewards.\\n\\n200 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.2 Learning process\\n\\nV\\n))\\n\\n��\\n\\nΠii\\n\\n��\\nV ∗ Π∗\\n\\nFigure C.4: The cycle of reinforcement learning\\nwhich ideally leads to optimal Π∗ and V ∗.\\n\\nC.2.2.2 Policy improvement\\n\\nPolicy improvement means to improve\\na policy itself, i.e. to turn it into a new and\\nbetter one. In order to improve the policy\\nwe have to aim at the return finally having\\na larger value than before, i.e. until we\\nhave found a shorter way to the restaurant\\nand have walked it successfully\\n\\nThe principle of reinforcement learning is\\nto realize an interaction. It is tried to eval-\\nuate how good a policy is in individual\\nsituations. The changed state-value func-\\ntion provides information about the sys-\\ntem with which we again improve our pol-\\nicy. These two values lift each other, which\\ncan mathematically be proved, so that the\\nfinal result is an optimal policy Π∗ and an\\noptimal state-value function V ∗ (fig. C.4).\\nThis cycle sounds simple but is very time-\\nconsuming.\\n\\nAt first, let us regard a simple, random pol-\\nicy by which our robot could slowly fulfill\\nand improve its state-value function with-\\nout any previous knowledge.\\n\\nC.2.3 Monte Carlo method\\n\\nThe easiest approach to accumulate a\\nstate-value function is mere trial and er-\\nror. Thus, we select a randomly behaving\\npolicy which does not consider the accumu-\\nlated state-value function for its random\\ndecisions. It can be proved that at some\\npoint we will find the exit of our gridworld\\nby chance.\\n\\nInspired by random-based games of chance\\nthis approach is called Monte Carlo\\nmethod.\\n\\nIf we additionally assume a pure negative\\nreward, it is obvious that we can receive\\nan optimum value of −6 for our starting\\nfield in the state-value function. Depend-\\ning on the random way the random policy\\ntakes values other (smaller) than −6 can\\noccur for the starting field. Intuitively, we\\nwant to memorize only the better value for\\none state (i.e. one field). But here caution\\nis advised: In this way, the learning proce-\\ndure would work only with deterministic\\nsystems. Our door, which can be open or\\nclosed during a cycle, would produce oscil-\\nlations for all fields and such oscillations\\nwould influence their shortest way to the\\ntarget.\\n\\nWith the Monte Carlo method we prefer\\nto use the learning rule2\\n\\nV (st)new = V (st)alt + α(Rt − V (st)alt),\\n\\nin which the update of the state-value func-\\ntion is obviously influenced by both the\\n\\n2 The learning rule is, among others, derived by\\nmeans of the Bellman equation, but this deriva-\\ntion is not discussed in this chapter.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 201\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\nold state value and the received return (α\\nis the learning rate). Thus, the agent gets\\n\\nαI some kind of memory, new findings always\\nchange the situation value just a little bit.\\nAn exemplary learning step is shown in\\nfig. C.5.\\n\\nIn this example, the computation of the\\nstate value was applied for only one single\\nstate (our initial state). It should be ob-\\nvious that it is possible (and often done)\\nto train the values for the states visited in-\\nbetween (in case of the gridworld our ways\\nto the target) at the same time. The result\\nof such a calculation related to our exam-\\nple is illustrated in fig. C.6 on the facing\\npage.\\n\\nThe Monte Carlo method seems to be\\nsuboptimal and usually it is significantly\\nslower than the following methods of re-\\ninforcement learning. But this method is\\nthe only one for which it can be mathemat-\\nically proved that it works and therefore\\nit is very useful for theoretical considera-\\ntions.\\n\\nDefinition C.10 (Monte Carlo learning).\\nActions are randomly performed regard-\\nless of the state-value function and in the\\nlong term an expressive state-value func-\\ntion is accumulated by means of the fol-\\nlowing learning rule.\\n\\nV (st)new = V (st)alt + α(Rt − V (st)alt),\\n\\nC.2.4 Temporal difference learning\\n\\nMost of the learning is the result of ex-\\nperiences; e.g. walking or riding a bicycle\\n\\n-1\\n-6 -5 -4 -3 -2\\n\\n-1\\n-14 -13 -12 -2\\n\\n-11 -3\\n-10 -4\\n-9 -5\\n-8 -7 -6\\n\\n-10\\n\\nFigure C.5: Application of the Monte Carlo\\nlearning rule with a learning rate of α = 0.5.\\nTop: two exemplary ways the agent randomly\\nselects are applied (one with an open and one\\nwith a closed door). Bottom: The result of the\\nlearning rule for the value of the initial state con-\\nsidering both ways. Due to the fact that in the\\ncourse of time many different ways are walked\\ngiven a random policy, a very expressive state-\\nvalue function is obtained.\\n\\n202 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.2 Learning process\\n\\n-1\\n-10 -9 -8 -3 -2\\n\\n-11 -3\\n-10 -4\\n-9 -5\\n-8 -7 -6\\n\\nFigure C.6: Extension of the learning example\\nin fig. C.5 in which the returns for intermedi-\\nate states are also used to accumulate the state-\\nvalue function. Here, the low value on the door\\nfield can be seen very well: If this state is possi-\\nble, it must be very positive. If the door is closed,\\nthis state is impossible.\\n\\nΠ\\n\\nEvaluation\\n\\n!!\\nQ\\n\\npolicy improvement\\n\\naa\\n\\nFigure C.7: We try different actions within the\\nenvironment and as a result we learn and improve\\nthe policy.\\n\\nwithout getting injured (or not), even men-\\ntal skills like mathematical problem solv-\\ning benefit a lot from experience and sim-\\nple trial and error. Thus, we initialize our\\npolicy with arbitrary values – we try, learn\\nand improve the policy due to experience\\n(fig. C.7). In contrast to the Monte Carlo\\nmethod we want to do this in a more di-\\nrected manner.\\n\\nJust as we learn from experience to re-\\nact on different situations in different ways\\n\\nthe temporal difference learning (abbre-\\nviated: TD learning), does the same by\\ntraining VΠ(s) (i.e. the agent learns to esti-\\nmate which situations are worth a lot and\\nwhich are not). Again the current situa-\\ntion is identified with st, the following sit-\\nuations with st+1 and so on. Thus, the\\nlearning formula for the state-value func-\\ntion VΠ(st) is\\n\\nV (st)new =V (st)\\n+ α(rt+1 + γV (st+1)− V (st))︸ ︷︷ ︸\\n\\nchange of previous value\\n\\nWe can see that the change in value of the\\ncurrent situation st, which is proportional\\nto the learning rate α, is influenced by\\n\\n. the received reward rt+1,\\n\\n. the previous return weighted with a\\nfactor γ of the following situation\\nV (st+1),\\n\\n. the previous value of the situation\\nV (st).\\n\\nDefinition C.11 (Temporal difference\\nlearning). Unlike the Monte Carlo\\nmethod, TD learning looks ahead by re-\\ngarding the following situation st+1. Thus,\\nthe learning rule is given by\\n\\nV (st)new =V (st) (C.14)\\n\\n+ α(rt+1 + γV (st+1)− V (st))︸ ︷︷ ︸\\nchange of previous value\\n\\n.\\n\\nC.2.5 The action-value function\\n\\nAnalogous to the state-value function\\nVΠ(s), the action-value function action\\n\\nevaluation\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 203\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\n0\\n× +1\\n-1\\n\\nFigure C.8: Exemplary values of an action-\\nvalue function for the position ×. Moving right,\\none remains on the fastest way towards the tar-\\nget, moving up is still a quite fast way, moving\\ndown is not a good way at all (provided that the\\ndoor is open for all cases).\\n\\nQΠ(s, a) is another system component of\\nQΠ(s, a)I reinforcement learning, which evaluates a\\n\\ncertain action a under a certain situation\\ns and the policy Π.\\n\\nIn the gridworld: In the gridworld, the\\naction-value function tells us how good it\\nis to move from a certain field into a cer-\\ntain direction (fig. C.8).\\n\\nDefinition C.12 (Action-value function).\\nLike the state-value function, the action-\\nvalue function QΠ(st, a) evaluates certain\\nactions on the basis of certain situations\\nunder a policy. The optimal action-value\\nfunction is called Q∗Π(st, a).\\n\\nQ∗Π(s, a)I\\n\\nAs shown in fig. C.9, the actions are per-\\nformed until a target situation (here re-\\nferred to as sτ ) is achieved (if there exists a\\ntarget situation, otherwise the actions are\\nsimply performed again and again).\\n\\nC.2.6 Q learning\\n\\nThis implies QΠ(s, a) as learning fomula\\nfor the action-value function, and – analo-\\ngously to TD learning – its application is\\ncalled Q learning:\\n\\nQ(st, a)new =Q(st, a)\\n+ α(rt+1 + γmax\\n\\na\\nQ(st+1, a)︸ ︷︷ ︸\\n\\ngreedy strategy\\n\\n−Q(st, a))\\n\\n︸ ︷︷ ︸\\nchange of previous value\\n\\n.\\n\\nAgain we break down the change of the\\ncurrent action value (proportional to the\\nlearning rate α) under the current situa-\\ntion. It is influenced by\\n\\n. the received reward rt+1,\\n\\n. the maximum action over the follow-\\ning actions weighted with γ (Here, a\\ngreedy strategy is applied since it can\\nbe assumed that the best known ac-\\ntion is selected. With TD learning,\\non the other hand, we do not mind to\\nalways get into the best known next\\nsituation.),\\n\\n. the previous value of the action under\\nour situation st known as Q(st, a) (re-\\nmember that this is also weighted by\\nmeans of α).\\n\\nUsually, the action-value function learns\\nconsiderably faster than the state-value\\nfunction. But we must not disregard that\\nreinforcement learning is generally quite\\nslow: The system has to find out itself\\nwhat is good. But the advantage of Q\\n\\n204 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.3 Example applications\\n\\nGFED@ABCs0\\na0 //\\n\\ndirection of actions\\n\\n((GFED@ABCs1\\na1 //\\n\\nr1\\nkk GFED@ABC· · · aτ−2 //\\n\\nr2\\nkk ONMLHIJKsτ−1\\n\\naτ−1 //\\nrτ−1\\nkk GFED@ABCsτ\\n\\nrτ\\nll\\n\\ndirection of reward\\n\\nhh\\n\\nFigure C.9: Actions are performed until the desired target situation is achieved. Attention should\\nbe paid to numbering: Rewards are numbered beginning with 1, actions and situations beginning\\nwith 0 (This has simply been adopted as a convention).\\n\\nlearning is: Π can be initialized arbitrar-\\nily, and by means of Q learning the result\\nis always Q∗.\\n\\nDefinition C.13 (Q learning). Q learn-\\ning trains the action-value function by\\nmeans of the learning rule\\n\\nQ(st, a)new =Q(st, a) (C.15)\\n+ α(rt+1 + γmax\\n\\na\\nQ(st+1, a) −Q(st, a)).\\n\\nand thus finds Q∗ in any case.\\n\\nC.3 Example applications\\n\\nC.3.1 TD gammon\\n\\nTD gammon is a very successful\\nbackgammon game based on TD learn-\\ning invented by Gerald Tesauro. The\\nsituation here is the current configura-\\ntion of the board. Anyone who has ever\\n\\nplayed backgammon knows that the situ-\\nation space is huge (approx. 1020 situa-\\ntions). As a result, the state-value func-\\ntions cannot be computed explicitly (par-\\nticularly in the late eighties when TD gam-\\nmon was introduced). The selected re-\\nwarding strategy was the pure delayed re-\\nward, i.e. the system receives the reward\\nnot before the end of the game and at the\\nsame time the reward is the return. Then\\nthe system was allowed to practice itself\\n(initially against a backgammon program,\\nthen against an entity of itself). The result\\nwas that it achieved the highest ranking in\\na computer-backgammon league and strik-\\ningly disproved the theory that a computer\\nprogramm is not capable to master a task\\nbetter than its programmer.\\n\\nC.3.2 The car in the pit\\n\\nLet us take a look at a car parking on a\\none-dimensional road at the bottom of a\\ndeep pit without being able to get over\\nthe slope on both sides straight away by\\nmeans of its engine power in order to leave\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 205\\n\\n\\n\\nAppendix C Excursus: reinforcement learning dkriesel.com\\n\\nthe pit. Trivially, the executable actions\\nhere are the possibilities to drive forwards\\nand backwards. The intuitive solution we\\nthink of immediately is to move backwards,\\nto gain momentum at the opposite slope\\nand oscillate in this way several times to\\ndash out of the pit.\\n\\nThe actions of a reinforcement learning\\nsystem would be \\"full throttle forward\\",\\n\\"full reverse\\" and \\"doing nothing\\".\\n\\nHere, \\"everything costs\\" would be a good\\nchoice for awarding the reward so that the\\nsystem learns fast how to leave the pit and\\nrealizes that our problem cannot be solved\\nby means of mere forward directed engine\\npower. So the system will slowly build up\\nthe movement.\\n\\nThe policy can no longer be stored as a\\ntable since the state space is hard to dis-\\ncretize. As policy a function has to be\\ngenerated.\\n\\nC.3.3 The pole balancer\\n\\nThe pole balancer was developed by\\nBarto, Sutton and Anderson.\\n\\nLet be given a situation including a vehicle\\nthat is capable to move either to the right\\nat full throttle or to the left at full throt-\\ntle (bang bang control). Only these two\\nactions can be performed, standing still\\nis impossible. On the top of this car is\\nhinged an upright pole that could tip over\\nto both sides. The pole is built in such a\\nway that it always tips over to one side so\\nit never stands still (let us assume that the\\npole is rounded at the lower end).\\n\\nThe angle of the pole relative to the verti-\\ncal line is referred to as α. Furthermore,\\nthe vehicle always has a fixed position x an\\nour one-dimensional world and a velocity\\nof ẋ. Our one-dimensional world is lim-\\nited, i.e. there are maximum values and\\nminimum values x can adopt.\\n\\nThe aim of our system is to learn to steer\\nthe car in such a way that it can balance\\nthe pole, to prevent the pole from tipping\\nover. This is achieved best by an avoid-\\nance strategy: As long as the pole is bal-\\nanced the reward is 0. If the pole tips over,\\nthe reward is -1.\\n\\nInterestingly, the system is soon capable\\nto keep the pole balanced by tilting it suf-\\nficiently fast and with small movements.\\nAt this the system mostly is in the cen-\\nter of the space since this is farthest from\\nthe walls which it understands as negative\\n(if it touches the wall, the pole will tip\\nover).\\n\\nC.3.3.1 Swinging up an inverted\\npendulum\\n\\nMore difficult for the system is the fol-\\nlowing initial situation: the pole initially\\nhangs down, has to be swung up over the\\nvehicle and finally has to be stabilized. In\\nthe literature this task is called swing up\\nan inverted pendulum.\\n\\n206 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com C.4 Reinforcement learning in connection with neural networks\\n\\nC.4 Reinforcement learning in\\nconnection with neural\\nnetworks\\n\\nFinally, the reader would like to ask why a\\ntext on \\"neural networks\\" includes a chap-\\nter about reinforcement learning.\\n\\nThe answer is very simple. We have al-\\nready been introduced to supervised and\\nunsupervised learning procedures. Al-\\nthough we do not always have an om-\\nniscient teacher who makes unsupervised\\nlearning possible, this does not mean that\\nwe do not receive any feedback at all.\\nThere is often something in between, some\\nkind of criticism or school mark. Problems\\nlike this can be solved by means of rein-\\nforcement learning.\\n\\nBut not every problem is that easily solved\\nlike our gridworld: In our backgammon ex-\\nample we have approx. 1020 situations and\\nthe situation tree has a large branching fac-\\ntor, let alone other games. Here, the tables\\nused in the gridworld can no longer be re-\\nalized as state- and action-value functions.\\nThus, we have to find approximators for\\nthese functions.\\n\\nAnd which learning approximators for\\nthese reinforcement learning components\\ncome immediately into our mind? Exactly:\\nneural networks.\\n\\nExercises\\n\\nExercise 19. A robot control system\\nshall be persuaded by means of reinforce-\\n\\nment learning to find a strategy in order\\nto exit a maze as fast as possible.\\n\\n. What could an appropriate state-\\nvalue function look like?\\n\\n. How would you generate an appropri-\\nate reward?\\n\\nAssume that the robot is capable to avoid\\nobstacles and at any time knows its posi-\\ntion (x, y) and orientation φ.\\n\\nExercise 20. Describe the function of\\nthe two components ASE and ACE as\\nthey have been proposed by Barto, Sut-\\nton and Anderson to control the pole\\nbalancer.\\n\\nBibliography: [BSA83].\\n\\nExercise 21. Indicate several \\"classical\\"\\nproblems of informatics which could be\\nsolved efficiently by means of reinforce-\\nment learning. Please give reasons for\\nyour answers.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 207\\n\\n\\n\\n\\n\\nBibliography\\n\\n[And72] James A. Anderson. A simple neural network generating an interactive\\nmemory. Mathematical Biosciences, 14:197–220, 1972.\\n\\n[APZ93] D. Anguita, G. Parodi, and R. Zunino. Speed improvement of the back-\\npropagation on current-generation workstations. In WCNN’93, Portland:\\nWorld Congress on Neural Networks, July 11-15, 1993, Oregon Convention\\nCenter, Portland, Oregon, volume 1. Lawrence Erlbaum, 1993.\\n\\n[BSA83] A. Barto, R. Sutton, and C. Anderson. Neuron-like adaptive elements\\nthat can solve difficult learning control problems. IEEE Transactions on\\nSystems, Man, and Cybernetics, 13(5):834–846, September 1983.\\n\\n[CG87] G. A. Carpenter and S. Grossberg. ART2: Self-organization of stable cate-\\ngory recognition codes for analog input patterns. Applied Optics, 26:4919–\\n4930, 1987.\\n\\n[CG88] M.A. Cohen and S. Grossberg. Absolute stability of global pattern forma-\\ntion and parallel memory storage by competitive neural networks. Com-\\nputer Society Press Technology Series Neural Networks, pages 70–81, 1988.\\n\\n[CG90] G. A. Carpenter and S. Grossberg. ART 3: Hierarchical search using\\nchemical transmitters in self-organising pattern recognition architectures.\\nNeural Networks, 3(2):129–152, 1990.\\n\\n[CH67] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE\\nTransactions on Information Theory, 13(1):21–27, 1967.\\n\\n[CR00] N.A. Campbell and JB Reece. Biologie. Spektrum. Akademischer Verlag,\\n2000.\\n\\n[Cyb89] G. Cybenko. Approximation by superpositions of a sigmoidal function.\\nMathematics of Control, Signals, and Systems (MCSS), 2(4):303–314,\\n1989.\\n\\n[DHS01] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern classification. Wiley New\\nYork, 2001.\\n\\n209\\n\\n\\n\\nBibliography dkriesel.com\\n\\n[Elm90] Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–\\n211, April 1990.\\n\\n[Fah88] S. E. Fahlman. An empirical sudy of learning speed in back-propagation\\nnetworks. Technical Report CMU-CS-88-162, CMU, 1988.\\n\\n[FMI83] K. Fukushima, S. Miyake, and T. Ito. Neocognitron: A neural network\\nmodel for a mechanism of visual pattern recognition. IEEE Transactions\\non Systems, Man, and Cybernetics, 13(5):826–834, September/October\\n1983.\\n\\n[Fri94] B. Fritzke. Fast learning with incremental RBF networks. Neural Process-\\ning Letters, 1(1):2–5, 1994.\\n\\n[GKE01a] N. Goerke, F. Kintzler, and R. Eckmiller. Self organized classification of\\nchaotic domains from a nonlinearattractor. In Neural Networks, 2001. Pro-\\nceedings. IJCNN’01. International Joint Conference on, volume 3, 2001.\\n\\n[GKE01b] N. Goerke, F. Kintzler, and R. Eckmiller. Self organized partitioning of\\nchaotic attractors for control. Lecture notes in computer science, pages\\n851–856, 2001.\\n\\n[Gro76] S. Grossberg. Adaptive pattern classification and universal recoding, I:\\nParallel development and coding of neural feature detectors. Biological\\nCybernetics, 23:121–134, 1976.\\n\\n[GS06] Nils Goerke and Alexandra Scherbart. Classification using multi-soms and\\nmulti-neural gas. In IJCNN, pages 3895–3902, 2006.\\n\\n[Heb49] Donald O. Hebb. The Organization of Behavior: A Neuropsychological\\nTheory. Wiley, New York, 1949.\\n\\n[Hop82] John J. Hopfield. Neural networks and physical systems with emergent col-\\nlective computational abilities. Proc. of the National Academy of Science,\\nUSA, 79:2554–2558, 1982.\\n\\n[Hop84] JJ Hopfield. Neurons with graded response have collective computational\\nproperties like those of two-state neurons. Proceedings of the National\\nAcademy of Sciences, 81(10):3088–3092, 1984.\\n\\n[HT85] JJ Hopfield and DW Tank. Neural computation of decisions in optimiza-\\ntion problems. Biological cybernetics, 52(3):141–152, 1985.\\n\\n[Jor86] M. I. Jordan. Attractor dynamics and parallelism in a connectionist se-\\nquential machine. In Proceedings of the Eighth Conference of the Cognitive\\nScience Society, pages 531–546. Erlbaum, 1986.\\n\\n210 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Bibliography\\n\\n[Kau90] L. Kaufman. Finding groups in data: an introduction to cluster analysis.\\nIn Finding Groups in Data: An Introduction to Cluster Analysis. Wiley,\\nNew York, 1990.\\n\\n[Koh72] T. Kohonen. Correlation matrix memories. IEEEtC, C-21:353–359, 1972.\\n\\n[Koh82] Teuvo Kohonen. Self-organized formation of topologically correct feature\\nmaps. Biological Cybernetics, 43:59–69, 1982.\\n\\n[Koh89] Teuvo Kohonen. Self-Organization and Associative Memory. Springer-\\nVerlag, Berlin, third edition, 1989.\\n\\n[Koh98] T. Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1–6, 1998.\\n\\n[KSJ00] E.R. Kandel, J.H. Schwartz, and T.M. Jessell. Principles of neural science.\\nAppleton & Lange, 2000.\\n\\n[lCDS90] Y. le Cun, J. S. Denker, and S. A. Solla. Optimal brain damage. In\\nD. Touretzky, editor, Advances in Neural Information Processing Systems\\n2, pages 598–605. Morgan Kaufmann, 1990.\\n\\n[Mac67] J. MacQueen. Some methods for classification and analysis of multivariate\\nobservations. In Proceedings of the Fifth Berkeley Symposium on Mathe-\\nmatics, Statistics and Probability, Vol. 1, pages 281–296, 1967.\\n\\n[MBS93] Thomas M. Martinetz, Stanislav G. Berkovich, and Klaus J. Schulten.\\n’Neural-gas’ network for vector quantization and its application to time-\\nseries prediction. IEEE Trans. on Neural Networks, 4(4):558–569, 1993.\\n\\n[MBW+10] K.D. Micheva, B. Busse, N.C. Weiler, N. O’Rourke, and S.J. Smith. Single-\\nsynapse analysis of a diverse synapse population: proteomic imaging meth-\\nods and markers. Neuron, 68(4):639–653, 2010.\\n\\n[MP43] W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent\\nin nervous activity. Bulletin of Mathematical Biology, 5(4):115–133, 1943.\\n\\n[MP69] M. Minsky and S. Papert. Perceptrons. MIT Press, Cambridge, Mass,\\n1969.\\n\\n[MR86] J. L. McClelland and D. E. Rumelhart. Parallel Distributed Processing:\\nExplorations in the Microstructure of Cognition, volume 2. MIT Press,\\nCambridge, 1986.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 211\\n\\n\\n\\nBibliography dkriesel.com\\n\\n[Par87] David R. Parker. Optimal algorithms for adaptive networks: Second or-\\nder back propagation, second order direct propagation, and second order\\nhebbian learning. In Maureen Caudill and Charles Butler, editors, IEEE\\nFirst International Conference on Neural Networks (ICNN’87), volume II,\\npages II–593–II–600, San Diego, CA, June 1987. IEEE.\\n\\n[PG89] T. Poggio and F. Girosi. A theory of networks for approximation and\\nlearning. MIT Press, Cambridge Mass., 1989.\\n\\n[Pin87] F. J. Pineda. Generalization of back-propagation to recurrent neural net-\\nworks. Physical Review Letters, 59:2229–2232, 1987.\\n\\n[PM47] W. Pitts and W.S. McCulloch. How we know universals the perception of\\nauditory and visual forms. Bulletin of Mathematical Biology, 9(3):127–147,\\n1947.\\n\\n[Pre94] L. Prechelt. Proben1: A set of neural network benchmark problems and\\nbenchmarking rules. Technical Report, 21:94, 1994.\\n\\n[RB93] M. Riedmiller and H. Braun. A direct adaptive method for faster back-\\npropagation learning: The rprop algorithm. In Neural Networks, 1993.,\\nIEEE International Conference on, pages 586–591. IEEE, 1993.\\n\\n[RD05] G. Roth and U. Dicke. Evolution of the brain and intelligence. Trends in\\nCognitive Sciences, 9(5):250–257, 2005.\\n\\n[RHW86a] D. Rumelhart, G. Hinton, and R. Williams. Learning representations by\\nback-propagating errors. Nature, 323:533–536, October 1986.\\n\\n[RHW86b] David E. Rumelhart, Geoffrey E. Hinton, and R. J. Williams. Learning\\ninternal representations by error propagation. In D. E. Rumelhart, J. L.\\nMcClelland, and the PDP research group., editors, Parallel distributed pro-\\ncessing: Explorations in the microstructure of cognition, Volume 1: Foun-\\ndations. MIT Press, 1986.\\n\\n[Rie94] M. Riedmiller. Rprop - description and implementation details. Technical\\nreport, University of Karlsruhe, 1994.\\n\\n[Ros58] F. Rosenblatt. The perceptron: a probabilistic model for information\\nstorage and organization in the brain. Psychological Review, 65:386–408,\\n1958.\\n\\n[Ros62] F. Rosenblatt. Principles of Neurodynamics. Spartan, New York, 1962.\\n\\n[SB98] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction.\\nMIT Press, Cambridge, MA, 1998.\\n\\n212 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Bibliography\\n\\n[SG06] A. Scherbart and N. Goerke. Unsupervised system for discovering patterns\\nin time-series, 2006.\\n\\n[SGE05] Rolf Schatten, Nils Goerke, and Rolf Eckmiller. Regional and online learn-\\nable fields. In Sameer Singh, Maneesha Singh, Chidanand Apté, and Petra\\nPerner, editors, ICAPR (2), volume 3687 of Lecture Notes in Computer\\nScience, pages 74–83. Springer, 2005.\\n\\n[Ste61] K. Steinbuch. Die lernmatrix. Kybernetik (Biological Cybernetics), 1:36–45,\\n1961.\\n\\n[vdM73] C. von der Malsburg. Self-organizing of orientation sensitive cells in striate\\ncortex. Kybernetik, 14:85–100, 1973.\\n\\n[Was89] P. D. Wasserman. Neural Computing Theory and Practice. New York :\\nVan Nostrand Reinhold, 1989.\\n\\n[Wer74] P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis\\nin the Behavioral Sciences. PhD thesis, Harvard University, 1974.\\n\\n[Wer88] P. J. Werbos. Backpropagation: Past and future. In Proceedings ICNN-88,\\nSan Diego, pages 343–353, 1988.\\n\\n[WG94] A.S. Weigend and N.A. Gershenfeld. Time series prediction. Addison-\\nWesley, 1994.\\n\\n[WH60] B. Widrow and M. E. Hoff. Adaptive switching circuits. In Proceedings\\nWESCON, pages 96–104, 1960.\\n\\n[Wid89] R. Widner. Single-stage logic. AIEE Fall General Meeting, 1960. Wasser-\\nman, P. Neural Computing, Theory and Practice, Van Nostrand Reinhold,\\n1989.\\n\\n[Zel94] Andreas Zell. Simulation Neuronaler Netze. Addison-Wesley, 1994. Ger-\\nman.\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 213\\n\\n\\n\\n\\n\\nList of Figures\\n\\n1.1 Robot with 8 sensors and 2 motors . . . . . . . . . . . . . . . . . . . . . 6\\n1.3 Black box with eight inputs and two outputs . . . . . . . . . . . . . . . 7\\n1.2 Learning samples for the example robot . . . . . . . . . . . . . . . . . . 8\\n1.4 Institutions of the field of neural networks . . . . . . . . . . . . . . . . . 9\\n\\n2.1 Central nervous system . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.2 Brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.3 Biological neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.4 Action potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2.5 Compound eye . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n\\n3.1 Data processing of a neuron . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n3.2 Various popular activation functions . . . . . . . . . . . . . . . . . . . . 38\\n3.3 Feedforward network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n3.4 Feedforward network with shortcuts . . . . . . . . . . . . . . . . . . . . 41\\n3.5 Directly recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n3.6 Indirectly recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n3.7 Laterally recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.8 Completely linked network . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n3.10 Examples for different types of neurons . . . . . . . . . . . . . . . . . . 45\\n3.9 Example network with and without bias neuron . . . . . . . . . . . . . . 46\\n\\n4.1 Training samples and network capacities . . . . . . . . . . . . . . . . . . 56\\n4.2 Learning curve with different scalings . . . . . . . . . . . . . . . . . . . 60\\n4.3 Gradient descent, 2D visualization . . . . . . . . . . . . . . . . . . . . . 62\\n4.4 Possible errors during a gradient descent . . . . . . . . . . . . . . . . . . 63\\n4.5 The 2-spiral problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n4.6 Checkerboard problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n\\n5.1 The perceptron in three different views . . . . . . . . . . . . . . . . . . . 72\\n5.2 Singlelayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n5.3 Singlelayer perceptron with several output neurons . . . . . . . . . . . . 74\\n5.4 AND and OR singlelayer perceptron . . . . . . . . . . . . . . . . . . . . 75\\n\\n215\\n\\n\\n\\nList of Figures dkriesel.com\\n\\n5.5 Error surface of a network with 2 connections . . . . . . . . . . . . . . . 78\\n5.6 Sketch of a XOR-SLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\n5.7 Two-dimensional linear separation . . . . . . . . . . . . . . . . . . . . . 82\\n5.8 Three-dimensional linear separation . . . . . . . . . . . . . . . . . . . . 83\\n5.9 The XOR network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n5.10 Multilayer perceptrons and output sets . . . . . . . . . . . . . . . . . . . 85\\n5.11 Position of an inner neuron for derivation of backpropagation . . . . . . 87\\n5.12 Illustration of the backpropagation derivation . . . . . . . . . . . . . . . 89\\n5.13 Momentum term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\\n5.14 Fermi function and hyperbolic tangent . . . . . . . . . . . . . . . . . . . 102\\n5.15 Functionality of 8-2-8 encoding . . . . . . . . . . . . . . . . . . . . . . . 103\\n\\n6.1 RBF network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n6.2 Distance function in the RBF network . . . . . . . . . . . . . . . . . . . 108\\n6.3 Individual Gaussian bells in one- and two-dimensional space . . . . . . . 109\\n6.4 Accumulating Gaussian bells in one-dimensional space . . . . . . . . . . 109\\n6.5 Accumulating Gaussian bells in two-dimensional space . . . . . . . . . . 110\\n6.6 Even coverage of an input space with radial basis functions . . . . . . . 116\\n6.7 Uneven coverage of an input space with radial basis functions . . . . . . 117\\n6.8 Random, uneven coverage of an input space with radial basis functions . 117\\n\\n7.1 Roessler attractor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n7.2 Jordan network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n7.3 Elman network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\\n7.4 Unfolding in time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\n\\n8.1 Hopfield network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\\n8.2 Binary threshold function . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n8.3 Convergence of a Hopfield network . . . . . . . . . . . . . . . . . . . . . 134\\n8.4 Fermi function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\n\\n9.1 Examples for quantization . . . . . . . . . . . . . . . . . . . . . . . . . . 141\\n\\n10.1 Example topologies of a SOM . . . . . . . . . . . . . . . . . . . . . . . . 148\\n10.2 Example distances of SOM topologies . . . . . . . . . . . . . . . . . . . 151\\n10.3 SOM topology functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n10.4 First example of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n10.7 Topological defect of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n10.5 Training a SOM with one-dimensional topology . . . . . . . . . . . . . . 157\\n10.6 SOMs with one- and two-dimensional topologies and different input spaces158\\n10.8 Resolution optimization of a SOM to certain areas . . . . . . . . . . . . 160\\n\\n216 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com List of Figures\\n\\n10.9 Shape to be classified by neural gas . . . . . . . . . . . . . . . . . . . . . 162\\n\\n11.1 Structure of an ART network . . . . . . . . . . . . . . . . . . . . . . . . 166\\n11.2 Learning process of an ART network . . . . . . . . . . . . . . . . . . . . 168\\n\\nA.1 Comparing cluster analysis methods . . . . . . . . . . . . . . . . . . . . 174\\nA.2 ROLF neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\\nA.3 Clustering by means of a ROLF . . . . . . . . . . . . . . . . . . . . . . . 179\\n\\nB.1 Neural network reading time series . . . . . . . . . . . . . . . . . . . . . 182\\nB.2 One-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 184\\nB.3 Two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\nB.4 Direct two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . 186\\nB.5 Heterogeneous one-step-ahead prediction . . . . . . . . . . . . . . . . . . 188\\nB.6 Heterogeneous one-step-ahead prediction with two outputs . . . . . . . . 188\\n\\nC.1 Gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\\nC.2 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\\nC.3 Gridworld with optimal returns . . . . . . . . . . . . . . . . . . . . . . . 200\\nC.4 Reinforcement learning cycle . . . . . . . . . . . . . . . . . . . . . . . . 201\\nC.5 The Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . . 202\\nC.6 Extended Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . 203\\nC.7 Improving the policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\\nC.8 Action-value function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\\nC.9 Reinforcement learning timeline . . . . . . . . . . . . . . . . . . . . . . . 205\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 217\\n\\n\\n\\n\\n\\nIndex\\n\\n*\\n100-step rule . . . . . . . . . . . . . . . . . . . . . . . . 5\\n\\nA\\nAction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\\naction potential . . . . . . . . . . . . . . . . . . . . 21\\naction space . . . . . . . . . . . . . . . . . . . . . . . 195\\naction-value function . . . . . . . . . . . . . . 203\\nactivation . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\nactivation function . . . . . . . . . . . . . . . . . 36\\n\\nselection of . . . . . . . . . . . . . . . . . . . . . 98\\nADALINE . . see adaptive linear neuron\\nadaptive linear element . . . see adaptive\\n\\nlinear neuron\\nadaptive linear neuron . . . . . . . . . . . . . . 10\\nadaptive resonance theory . . . . . 11, 165\\nagent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .194\\nalgorithm. . . . . . . . . . . . . . . . . . . . . . . . . . .50\\namacrine cell . . . . . . . . . . . . . . . . . . . . . . . 28\\napproximation. . . . . . . . . . . . . . . . . . . . .110\\nART . . . . see adaptive resonance theory\\nART-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\nART-2A. . . . . . . . . . . . . . . . . . . . . . . . . . .168\\nART-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\nartificial intelligence . . . . . . . . . . . . . . . . 10\\nassociative data storage . . . . . . . . . . . 157\\n\\nATP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nattractor . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\nautoassociator . . . . . . . . . . . . . . . . . . . . . 131\\naxon . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18, 23\\n\\nB\\nbackpropagation . . . . . . . . . . . . . . . . . . . . 88\\n\\nsecond order . . . . . . . . . . . . . . . . . . . 95\\nbackpropagation of error. . . . . . . . . . . .84\\n\\nrecurrent . . . . . . . . . . . . . . . . . . . . . . 125\\nbar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\nbasis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\\nbias neuron. . . . . . . . . . . . . . . . . . . . . . . . .44\\nbinary threshold function . . . . . . . . . . 37\\nbipolar cell . . . . . . . . . . . . . . . . . . . . . . . . . 27\\nblack box . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\nbrain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\nbrainstem . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n\\nC\\ncapability to learn . . . . . . . . . . . . . . . . . . . 4\\ncenter\\n\\nof a ROLF neuron . . . . . . . . . . . . 176\\nof a SOM neuron. . . . . . . . . . . . . .146\\n\\n219\\n\\n\\n\\nIndex dkriesel.com\\n\\nof an RBF neuron . . . . . . . . . . . . . 104\\ndistance to the . . . . . . . . . . . . . . 107\\n\\ncentral nervous system . . . . . . . . . . . . . 14\\ncerebellum . . . . . . . . . . . . . . . . . . . . . . . . . 15\\ncerebral cortex . . . . . . . . . . . . . . . . . . . . . 14\\ncerebrum . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\nchange in weight. . . . . . . . . . . . . . . . . . . .64\\ncluster analysis . . . . . . . . . . . . . . . . . . . . 171\\nclusters . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\nCNS . . . . . . . see central nervous system\\ncodebook vector . . . . . . . . . . . . . . 138, 172\\ncomplete linkage. . . . . . . . . . . . . . . . . . . .39\\ncompound eye . . . . . . . . . . . . . . . . . . . . . . 26\\nconcentration gradient . . . . . . . . . . . . . . 19\\ncone function . . . . . . . . . . . . . . . . . . . . . .150\\nconnection. . . . . . . . . . . . . . . . . . . . . . . . . .34\\ncontext-based search . . . . . . . . . . . . . . 157\\ncontinuous . . . . . . . . . . . . . . . . . . . . . . . . 137\\ncortex . . . . . . . . . . . . . . see cerebral cortex\\n\\nvisual . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\ncortical field . . . . . . . . . . . . . . . . . . . . . . . . 14\\n\\nassociation . . . . . . . . . . . . . . . . . . . . . 15\\nprimary . . . . . . . . . . . . . . . . . . . . . . . . 15\\n\\ncylinder function . . . . . . . . . . . . . . . . . . 150\\n\\nD\\nDartmouth Summer Research Project9\\ndeep networks . . . . . . . . . . . . . . . . . . 93, 97\\nDelta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\ndelta rule . . . . . . . . . . . . . . . . . . . . . . . . . . .79\\ndendrite . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n\\ntree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\ndepolarization . . . . . . . . . . . . . . . . . . . . . . 21\\ndiencephalon . . . . . . . . . . . . see interbrain\\ndifference vector . . . . . . . see error vector\\ndigital filter . . . . . . . . . . . . . . . . . . . . . . . 183\\n\\ndigitization . . . . . . . . . . . . . . . . . . . . . . . . 138\\ndiscrete . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\ndiscretization . . . . . . . . . see quantization\\ndistance\\n\\nEuclidean . . . . . . . . . . . . . . . . . 56, 171\\nsquared. . . . . . . . . . . . . . . . . . . .76, 171\\n\\ndynamical system . . . . . . . . . . . . . . . . . 119\\n\\nE\\nearly stopping . . . . . . . . . . . . . . . . . . . . . . 59\\nelectronic brain . . . . . . . . . . . . . . . . . . . . . . 9\\nElman network . . . . . . . . . . . . . . . . . . . . 121\\nenvironment . . . . . . . . . . . . . . . . . . . . . . .193\\nepisode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\\nepoch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nepsilon-nearest neighboring . . . . . . . . 173\\nerror\\n\\nspecific . . . . . . . . . . . . . . . . . . . . . . . . . 56\\ntotal . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n\\nerror function . . . . . . . . . . . . . . . . . . . . . . 75\\nspecific . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n\\nerror vector . . . . . . . . . . . . . . . . . . . . . . . . 53\\nevolutionary algorithms . . . . . . . . . . . 125\\nexploitation approach . . . . . . . . . . . . . 197\\nexploration approach . . . . . . . . . . . . . . 197\\nexteroceptor . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\nF\\nfastprop . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nfault tolerance . . . . . . . . . . . . . . . . . . . . . . . 4\\nfeedforward. . . . . . . . . . . . . . . . . . . . . . . . .39\\nFermi function . . . . . . . . . . . . . . . . . . . . . 37\\nflat spot elimination . . . . . . . . . . . . . . . . 95\\n\\n220 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Index\\n\\nfudging . . . . . . . see flat spot elimination\\nfunction approximation . . . . . . . . . . . . . 98\\nfunction approximator\\n\\nuniversal . . . . . . . . . . . . . . . . . . . . . . . 82\\n\\nG\\nganglion cell . . . . . . . . . . . . . . . . . . . . . . . . 27\\nGauss-Markov model . . . . . . . . . . . . . . 111\\nGaussian bell . . . . . . . . . . . . . . . . . . . . . .149\\ngeneralization . . . . . . . . . . . . . . . . . . . . 4, 49\\nglial cell . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\ngradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\ngradient descent . . . . . . . . . . . . . . . . . . . . 59\\n\\nproblems . . . . . . . . . . . . . . . . . . . . . . . 60\\ngrid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\\ngridworld. . . . . . . . . . . . . . . . . . . . . . . . . .192\\n\\nH\\nHeaviside function see binary threshold\\n\\nfunction\\nHebbian rule . . . . . . . . . . . . . . . . . . . . . . . 64\\n\\ngeneralized form. . . . . . . . . . . . . . . .65\\nheteroassociator . . . . . . . . . . . . . . . . . . . 132\\nHinton diagram . . . . . . . . . . . . . . . . . . . . 34\\nhistory of development. . . . . . . . . . . . . . .8\\nHopfield networks . . . . . . . . . . . . . . . . . 127\\n\\ncontinuous . . . . . . . . . . . . . . . . . . . . 134\\nhorizontal cell . . . . . . . . . . . . . . . . . . . . . . 28\\nhyperbolic tangent . . . . . . . . . . . . . . . . . 37\\nhyperpolarization . . . . . . . . . . . . . . . . . . . 21\\nhypothalamus . . . . . . . . . . . . . . . . . . . . . . 15\\n\\nI\\nindividual eye . . . . . . . . see ommatidium\\ninput dimension . . . . . . . . . . . . . . . . . . . . 48\\ninput patterns . . . . . . . . . . . . . . . . . . . . . . 50\\ninput vector . . . . . . . . . . . . . . . . . . . . . . . . 48\\ninterbrain . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\ninternodes . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\ninteroceptor . . . . . . . . . . . . . . . . . . . . . . . . 24\\ninterpolation\\n\\nprecise . . . . . . . . . . . . . . . . . . . . . . . . 110\\nion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\niris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n\\nJ\\nJordan network. . . . . . . . . . . . . . . . . . . .120\\n\\nK\\nk-means clustering . . . . . . . . . . . . . . . . 172\\nk-nearest neighboring. . . . . . . . . . . . . .172\\n\\nL\\nlayer\\n\\nhidden . . . . . . . . . . . . . . . . . . . . . . . . . 39\\ninput . . . . . . . . . . . . . . . . . . . . . . . . . . .39\\noutput . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n\\nlearnability . . . . . . . . . . . . . . . . . . . . . . . . . 97\\nlearning\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 221\\n\\n\\n\\nIndex dkriesel.com\\n\\nbatch . . . . . . . . . . see learning, offline\\noffline . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nonline . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nreinforcement . . . . . . . . . . . . . . . . . . 51\\nsupervised. . . . . . . . . . . . . . . . . . . . . .51\\nunsupervised . . . . . . . . . . . . . . . . . . . 50\\n\\nlearning rate . . . . . . . . . . . . . . . . . . . . . . . 89\\nvariable . . . . . . . . . . . . . . . . . . . . . . . . 90\\n\\nlearning strategy . . . . . . . . . . . . . . . . . . . 39\\nlearning vector quantization . . . . . . . 137\\nlens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\nlinear separability . . . . . . . . . . . . . . . . . . 81\\nlinearer associator . . . . . . . . . . . . . . . . . . 11\\nlocked-in syndrome . . . . . . . . . . . . . . . . . 16\\nlogistic function . . . . see Fermi function\\n\\ntemperature parameter . . . . . . . . . 37\\nLVQ . . see learning vector quantization\\nLVQ1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\\nLVQ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\\nLVQ3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\\n\\nM\\nM-SOM. see self-organizing map, multi\\nMark I perceptron . . . . . . . . . . . . . . . . . . 10\\nMathematical Symbols\\n\\n(t) . . . . . . . . . . . . . . . see time concept\\nA(S) . . . . . . . . . . . . . see action space\\nEp . . . . . . . . . . . . . . . . see error vector\\nG . . . . . . . . . . . . . . . . . . . . see topology\\nN . . see self-organizing map, input\\n\\ndimension\\nP . . . . . . . . . . . . . . . . . see training set\\nQ∗Π(s, a) . see action-value function,\\n\\noptimal\\nQΠ(s, a) . see action-value function\\nRt . . . . . . . . . . . . . . . . . . . . . . see return\\n\\nS . . . . . . . . . . . . . . see situation space\\nT . . . . . . see temperature parameter\\nV ∗Π(s) . . . . . see state-value function,\\n\\noptimal\\nVΠ(s) . . . . . see state-value function\\nW . . . . . . . . . . . . . . see weight matrix\\n∆wi,j . . . . . . . . see change in weight\\nΠ . . . . . . . . . . . . . . . . . . . . . . . see policy\\nΘ . . . . . . . . . . . . . .see threshold value\\nα . . . . . . . . . . . . . . . . . . see momentum\\nβ . . . . . . . . . . . . . . . . see weight decay\\nδ . . . . . . . . . . . . . . . . . . . . . . . . see Delta\\nη . . . . . . . . . . . . . . . . .see learning rate\\nη↑ . . . . . . . . . . . . . . . . . . . . . . see Rprop\\nη↓ . . . . . . . . . . . . . . . . . . . . . . see Rprop\\nηmax . . . . . . . . . . . . . . . . . . . . see Rprop\\nηmin . . . . . . . . . . . . . . . . . . . . see Rprop\\nηi,j . . . . . . . . . . . . . . . . . . . . . see Rprop\\n∇ . . . . . . . . . . . . . . see nabla operator\\nρ . . . . . . . . . . . . . see radius multiplier\\nErr . . . . . . . . . . . . . . . . see error, total\\nErr(W ) . . . . . . . . . see error function\\nErrp . . . . . . . . . . . . . see error, specific\\nErrp(W )see error function, specific\\nErrWD . . . . . . . . . . . see weight decay\\nat . . . . . . . . . . . . . . . . . . . . . . see action\\nc . . . . . . . . . . . . . . . . . . . . . . . .see center\\n\\nof an RBF neuron, see neuron,\\nself-organizing map, center\\n\\nm . . . . . . . . . . . see output dimension\\nn . . . . . . . . . . . . . see input dimension\\np . . . . . . . . . . . . . see training pattern\\nrh . . . see center of an RBF neuron,\\n\\ndistance to the\\nrt . . . . . . . . . . . . . . . . . . . . . . see reward\\nst . . . . . . . . . . . . . . . . . . . . see situation\\nt . . . . . . . . . . . . . . . see teaching input\\nwi,j . . . . . . . . . . . . . . . . . . . . see weight\\nx . . . . . . . . . . . . . . . . . see input vector\\ny . . . . . . . . . . . . . . . . see output vector\\n\\n222 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Index\\n\\nfact . . . . . . . . see activation function\\nfout . . . . . . . . . . . see output function\\n\\nmembrane . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n-potential . . . . . . . . . . . . . . . . . . . . . . 19\\n\\nmemorized . . . . . . . . . . . . . . . . . . . . . . . . . 54\\nmetric . . . . . . . . . . . . . . . . . . . . . . . . . . . . .171\\nMexican hat function . . . . . . . . . . . . . . 150\\nMLP. . . . . . . .see perceptron, multilayer\\nmomentum . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nmomentum term. . . . . . . . . . . . . . . . . . . .94\\nMonte Carlo method . . . . . . . . . . . . . . 201\\nMoore-Penrose pseudo inverse . . . . . 110\\nmoving average procedure . . . . . . . . . 184\\nmyelin sheath . . . . . . . . . . . . . . . . . . . . . . 23\\n\\nN\\nnabla operator. . . . . . . . . . . . . . . . . . . . . .59\\nNeocognitron . . . . . . . . . . . . . . . . . . . . . . . 12\\nnervous system . . . . . . . . . . . . . . . . . . . . . 13\\nnetwork input . . . . . . . . . . . . . . . . . . . . . . 35\\nneural gas . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n\\ngrowing . . . . . . . . . . . . . . . . . . . . . . . 162\\nmulti- . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n\\nneural network . . . . . . . . . . . . . . . . . . . . . 34\\nrecurrent . . . . . . . . . . . . . . . . . . . . . . 119\\n\\nneuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\naccepting . . . . . . . . . . . . . . . . . . . . . 177\\nbinary. . . . . . . . . . . . . . . . . . . . . . . . . .71\\ncontext. . . . . . . . . . . . . . . . . . . . . . . .120\\nFermi . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\nidentity . . . . . . . . . . . . . . . . . . . . . . . . 71\\ninformation processing . . . . . . . . . 71\\ninput . . . . . . . . . . . . . . . . . . . . . . . . . . .71\\nRBF . . . . . . . . . . . . . . . . . . . . . . . . . . 104\\noutput . . . . . . . . . . . . . . . . . . . . . . 104\\n\\nROLF. . . . . . . . . . . . . . . . . . . . . . . . .176\\n\\nself-organizing map. . . . . . . . . . . .146\\ntanh . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\nwinner . . . . . . . . . . . . . . . . . . . . . . . . 148\\n\\nneuron layers . . . . . . . . . . . . . . . . . see layer\\nneurotransmitters . . . . . . . . . . . . . . . . . . 17\\nnodes of Ranvier . . . . . . . . . . . . . . . . . . . 23\\n\\nO\\noligodendrocytes . . . . . . . . . . . . . . . . . . . 23\\nOLVQ. . . . . . . . . . . . . . . . . . . . . . . . . . . . .141\\non-neuron . . . . . . . . . . . . . see bias neuron\\none-step-ahead prediction . . . . . . . . . 183\\n\\nheterogeneous . . . . . . . . . . . . . . . . . 187\\nopen loop learning. . . . . . . . . . . . . . . . .125\\noptimal brain damage . . . . . . . . . . . . . . 96\\norder of activation . . . . . . . . . . . . . . . . . . 45\\n\\nasynchronous\\nfixed order . . . . . . . . . . . . . . . . . . . 47\\nrandom order . . . . . . . . . . . . . . . . 46\\nrandomly permuted order . . . . 46\\ntopological order . . . . . . . . . . . . . 47\\n\\nsynchronous . . . . . . . . . . . . . . . . . . . . 46\\noutput dimension . . . . . . . . . . . . . . . . . . . 48\\noutput function. . . . . . . . . . . . . . . . . . . . .38\\noutput vector. . . . . . . . . . . . . . . . . . . . . . .48\\n\\nP\\nparallelism . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\npattern . . . . . . . . . . . see training pattern\\npattern recognition . . . . . . . . . . . . 98, 131\\nperceptron . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n\\nmultilayer . . . . . . . . . . . . . . . . . . . . . . 82\\nrecurrent . . . . . . . . . . . . . . . . . . . . 119\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 223\\n\\n\\n\\nIndex dkriesel.com\\n\\nsinglelayer . . . . . . . . . . . . . . . . . . . . . .72\\nperceptron convergence theorem . . . . 73\\nperceptron learning algorithm . . . . . . 73\\nperiod . . . . . . . . . . . . . . . . . . . . . . . . . . . . .119\\nperipheral nervous system . . . . . . . . . . 13\\nPersons\\n\\nAnderson . . . . . . . . . . . . . . . . . . . . 206 f.\\nAnderson, James A. . . . . . . . . . . . . 11\\nAnguita . . . . . . . . . . . . . . . . . . . . . . . . 37\\nBarto . . . . . . . . . . . . . . . . . . . 191, 206 f.\\nCarpenter, Gail . . . . . . . . . . . .11, 165\\nElman . . . . . . . . . . . . . . . . . . . . . . . . 120\\nFukushima . . . . . . . . . . . . . . . . . . . . . 12\\nGirosi . . . . . . . . . . . . . . . . . . . . . . . . . 103\\nGrossberg, Stephen . . . . . . . . 11, 165\\nHebb, Donald O. . . . . . . . . . . . . 9, 64\\nHinton . . . . . . . . . . . . . . . . . . . . . . . . . 12\\nHoff, Marcian E. . . . . . . . . . . . . . . . 10\\nHopfield, John . . . . . . . . . . . 11 f., 127\\nIto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\nJordan . . . . . . . . . . . . . . . . . . . . . . . . 120\\nKohonen, Teuvo . 11, 137, 145, 157\\nLashley, Karl . . . . . . . . . . . . . . . . . . . . 9\\nMacQueen, J. . . . . . . . . . . . . . . . . . 172\\nMartinetz, Thomas . . . . . . . . . . . . 159\\nMcCulloch, Warren . . . . . . . . . . . . 8 f.\\nMinsky, Marvin . . . . . . . . . . . . . . . . 9 f.\\nMiyake . . . . . . . . . . . . . . . . . . . . . . . . . 12\\nNilsson, Nils. . . . . . . . . . . . . . . . . . . .10\\nPapert, Seymour . . . . . . . . . . . . . . . 10\\nParker, David . . . . . . . . . . . . . . . . . . 95\\nPitts, Walter . . . . . . . . . . . . . . . . . . . 8 f.\\nPoggio . . . . . . . . . . . . . . . . . . . . . . . . 103\\nPythagoras . . . . . . . . . . . . . . . . . . . . . 56\\nRiedmiller, Martin . . . . . . . . . . . . . 90\\nRosenblatt, Frank . . . . . . . . . . 10, 69\\nRumelhart . . . . . . . . . . . . . . . . . . . . . 12\\nSteinbuch, Karl . . . . . . . . . . . . . . . . 10\\nSutton . . . . . . . . . . . . . . . . . . 191, 206 f.\\nTesauro, Gerald . . . . . . . . . . . . . . . 205\\n\\nvon der Malsburg, Christoph . . . 11\\nWerbos, Paul . . . . . . . . . . . 11, 84, 96\\nWidrow, Bernard . . . . . . . . . . . . . . . 10\\nWightman, Charles . . . . . . . . . . . . .10\\nWilliams . . . . . . . . . . . . . . . . . . . . . . . 12\\nZuse, Konrad . . . . . . . . . . . . . . . . . . . . 9\\n\\npinhole eye . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nPNS . . . . see peripheral nervous system\\npole balancer . . . . . . . . . . . . . . . . . . . . . . 206\\npolicy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\\n\\nclosed loop . . . . . . . . . . . . . . . . . . . . 197\\nevaluation . . . . . . . . . . . . . . . . . . . . . 200\\ngreedy . . . . . . . . . . . . . . . . . . . . . . . . 197\\nimprovement . . . . . . . . . . . . . . . . . . 200\\nopen loop . . . . . . . . . . . . . . . . . . . . . 197\\n\\npons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\npropagation function . . . . . . . . . . . . . . . 35\\npruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\\npupil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n\\nQ\\nQ learning . . . . . . . . . . . . . . . . . . . . . . . . 204\\nquantization. . . . . . . . . . . . . . . . . . . . . . .137\\nquickpropagation . . . . . . . . . . . . . . . . . . . 95\\n\\nR\\nRBF network. . . . . . . . . . . . . . . . . . . . . .104\\n\\ngrowing . . . . . . . . . . . . . . . . . . . . . . . 115\\nreceptive field . . . . . . . . . . . . . . . . . . . . . . 27\\nreceptor cell . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\nphoto-. . . . . . . . . . . . . . . . . . . . . . . . . .27\\nprimary . . . . . . . . . . . . . . . . . . . . . . . . 24\\nsecondary . . . . . . . . . . . . . . . . . . . . . . 24\\n\\n224 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\ndkriesel.com Index\\n\\nrecurrence . . . . . . . . . . . . . . . . . . . . . 40, 119\\ndirect . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\nindirect . . . . . . . . . . . . . . . . . . . . . . . . 41\\nlateral . . . . . . . . . . . . . . . . . . . . . . . . . .42\\n\\nrefractory period . . . . . . . . . . . . . . . . . . . 23\\nregional and online learnable fields 175\\nreinforcement learning . . . . . . . . . . . . . 191\\nrepolarization . . . . . . . . . . . . . . . . . . . . . . 21\\nrepresentability . . . . . . . . . . . . . . . . . . . . . 97\\nresilient backpropagation . . . . . . . . . . . 90\\nresonance . . . . . . . . . . . . . . . . . . . . . . . . . 166\\nretina. . . . . . . . . . . . . . . . . . . . . . . . . . .27, 71\\nreturn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\\nreward . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\\n\\navoidance strategy . . . . . . . . . . . . 199\\npure delayed . . . . . . . . . . . . . . . . . . 198\\npure negative . . . . . . . . . . . . . . . . . 198\\n\\nRMS . . . . . . . . . . . . see root mean square\\nROLFs . . . . . . . . . see regional and online\\n\\nlearnable fields\\nroot mean square . . . . . . . . . . . . . . . . . . . 56\\nRprop . . . see resilient backpropagation\\n\\nS\\nsaltatory conductor . . . . . . . . . . . . . . . . . 23\\nSchwann cell . . . . . . . . . . . . . . . . . . . . . . . 23\\nself-fulfilling prophecy . . . . . . . . . . . . . 189\\nself-organizing feature maps . . . . . . . . 11\\nself-organizing map . . . . . . . . . . . . . . . . 145\\n\\nmulti- . . . . . . . . . . . . . . . . . . . . . . . . . 161\\nsensory adaptation . . . . . . . . . . . . . . . . . 25\\nsensory transduction. . . . . . . . . . . . . . . .24\\nshortcut connections . . . . . . . . . . . . . . . .39\\nsilhouette coefficient . . . . . . . . . . . . . . . 175\\nsingle lense eye . . . . . . . . . . . . . . . . . . . . . 27\\nSingle Shot Learning . . . . . . . . . . . . . . 130\\n\\nsituation . . . . . . . . . . . . . . . . . . . . . . . . . . 194\\nsituation space . . . . . . . . . . . . . . . . . . . . 195\\nsituation tree . . . . . . . . . . . . . . . . . . . . . . 198\\nSLP . . . . . . . . see perceptron, singlelayer\\nSnark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\nSNIPE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vi\\nsodium-potassium pump. . . . . . . . . . . . 20\\nSOM . . . . . . . . . . see self-organizing map\\nsoma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nspin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\\nspinal cord . . . . . . . . . . . . . . . . . . . . . . . . . 14\\nstability / plasticity dilemma . . . . . . 165\\nstate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\\nstate space forecasting . . . . . . . . . . . . .183\\nstate-value function . . . . . . . . . . . . . . . 200\\nstimulus . . . . . . . . . . . . . . . . . . . . . . . 21, 147\\nstimulus-conducting apparatus. . . . . .24\\nsurface, perceptive. . . . . . . . . . . . . . . . .176\\nswing up an inverted pendulum. . . .206\\nsymmetry breaking . . . . . . . . . . . . . . . . . 98\\nsynapse\\n\\nchemical . . . . . . . . . . . . . . . . . . . . . . . 17\\nelectrical . . . . . . . . . . . . . . . . . . . . . . . 17\\n\\nsynapses. . . . . . . . . . . . . . . . . . . . . . . . . . . .17\\nsynaptic cleft . . . . . . . . . . . . . . . . . . . . . . . 17\\n\\nT\\ntarget . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\nTD gammon . . . . . . . . . . . . . . . . . . . . . . 205\\nTD learning. . . .see temporal difference\\n\\nlearning\\nteacher forcing . . . . . . . . . . . . . . . . . . . . 125\\nteaching input . . . . . . . . . . . . . . . . . . . . . . 53\\ntelencephalon . . . . . . . . . . . . see cerebrum\\ntemporal difference learning . . . . . . . 202\\nthalamus . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n\\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 225\\n\\n\\n\\nIndex dkriesel.com\\n\\nthreshold potential . . . . . . . . . . . . . . . . . 21\\nthreshold value . . . . . . . . . . . . . . . . . . . . . 36\\ntime concept . . . . . . . . . . . . . . . . . . . . . . . 33\\ntime horizon . . . . . . . . . . . . . . . . . . . . . . 196\\ntime series . . . . . . . . . . . . . . . . . . . . . . . . 181\\ntime series prediction . . . . . . . . . . . . . . 181\\ntopological defect. . . . . . . . . . . . . . . . . .154\\ntopology . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\ntopology function . . . . . . . . . . . . . . . . . 148\\ntraining pattern . . . . . . . . . . . . . . . . . . . . 53\\n\\nset of . . . . . . . . . . . . . . . . . . . . . . . . . . .53\\ntraining set . . . . . . . . . . . . . . . . . . . . . . . . . 50\\ntransfer functionsee activation function\\ntruncus cerebri . . . . . . . . . . see brainstem\\ntwo-step-ahead prediction . . . . . . . . . 185\\n\\ndirect . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n\\nU\\nunfolding in time . . . . . . . . . . . . . . . . . . 123\\n\\nV\\nvoronoi diagram . . . . . . . . . . . . . . . . . . . 138\\n\\nW\\nweight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\nweight matrix . . . . . . . . . . . . . . . . . . . . . . 34\\n\\nbottom-up . . . . . . . . . . . . . . . . . . . . 166\\ntop-down. . . . . . . . . . . . . . . . . . . . . .165\\n\\nweight vector . . . . . . . . . . . . . . . . . . . . . . . 34\\n\\nweighted sum. . . . . . . . . . . . . . . . . . . . . . . 35\\nWidrow-Hoff rule . . . . . . . . see delta rule\\nwinner-takes-all scheme . . . . . . . . . . . . . 42\\n\\n226 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\\n\\n\\n\\tA small preface\\n\\tI From biology to formalization – motivation, philosophy, history and realization of neural models\\n\\t1 Introduction, motivation and history\\n\\t1.1 Why neural networks?\\n\\t1.1.1 The 100-step rule\\n\\t1.1.2 Simple application examples\\n\\n\\t1.2 History of neural networks\\n\\t1.2.1 The beginning\\n\\t1.2.2 Golden age\\n\\t1.2.3 Long silence and slow reconstruction\\n\\t1.2.4 Renaissance\\n\\n\\tExercises\\n\\n\\t2 Biological neural networks\\n\\t2.1 The vertebrate nervous system\\n\\t2.1.1 Peripheral and central nervous system\\n\\t2.1.2 Cerebrum\\n\\t2.1.3 Cerebellum\\n\\t2.1.4 Diencephalon\\n\\t2.1.5 Brainstem\\n\\n\\t2.2 The neuron\\n\\t2.2.1 Components\\n\\t2.2.2 Electrochemical processes in the neuron\\n\\n\\t2.3 Receptor cells\\n\\t2.3.1 Various types\\n\\t2.3.2 Information processing within the nervous system\\n\\t2.3.3 Light sensing organs\\n\\n\\t2.4 The amount of neurons in living organisms\\n\\t2.5 Technical neurons as caricature of biology\\n\\tExercises\\n\\n\\t3 Components of artificial neural networks (fundamental)\\n\\t3.1 The concept of time in neural networks\\n\\t3.2 Components of neural networks\\n\\t3.2.1 Connections\\n\\t3.2.2 Propagation function and network input\\n\\t3.2.3 Activation\\n\\t3.2.4 Threshold value\\n\\t3.2.5 Activation function\\n\\t3.2.6 Common activation functions\\n\\t3.2.7 Output function\\n\\t3.2.8 Learning strategy\\n\\n\\t3.3 Network topologies\\n\\t3.3.1 Feedforward\\n\\t3.3.2 Recurrent networks\\n\\t3.3.3 Completely linked networks\\n\\n\\t3.4 The bias neuron\\n\\t3.5 Representing neurons\\n\\t3.6 Orders of activation\\n\\t3.6.1 Synchronous activation\\n\\t3.6.2 Asynchronous activation\\n\\n\\t3.7 Input and output of data\\n\\tExercises\\n\\n\\t4 Fundamentals on learning and training samples (fundamental)\\n\\t4.1 Paradigms of learning\\n\\t4.1.1 Unsupervised learning\\n\\t4.1.2 Reinforcement learning\\n\\t4.1.3 Supervised learning\\n\\t4.1.4 Offline or online learning?\\n\\t4.1.5 Questions in advance\\n\\n\\t4.2 Training patterns and teaching input\\n\\t4.3 Using training samples\\n\\t4.3.1 Division of the training set\\n\\t4.3.2 Order of pattern representation\\n\\n\\t4.4 Learning curve and error measurement\\n\\t4.4.1 When do we stop learning?\\n\\n\\t4.5 Gradient optimization procedures\\n\\t4.5.1 Problems of gradient procedures\\n\\n\\t4.6 Exemplary problems\\n\\t4.6.1 Boolean functions\\n\\t4.6.2 The parity function\\n\\t4.6.3 The 2-spiral problem\\n\\t4.6.4 The checkerboard problem\\n\\t4.6.5 The identity function\\n\\t4.6.6 Other exemplary problems\\n\\n\\t4.7 Hebbian rule\\n\\t4.7.1 Original rule\\n\\t4.7.2 Generalized form\\n\\n\\tExercises\\n\\n\\n\\tII Supervised learning network paradigms\\n\\t5 The perceptron, backpropagation and its variants\\n\\t5.1 The singlelayer perceptron\\n\\t5.1.1 Perceptron learning algorithm and convergence theorem\\n\\t5.1.2 Delta rule\\n\\n\\t5.2 Linear separability\\n\\t5.3 The multilayer perceptron\\n\\t5.4 Backpropagation of error\\n\\t5.4.1 Derivation\\n\\t5.4.2 Boiling backpropagation down to the delta rule\\n\\t5.4.3 Selecting a learning rate\\n\\n\\t5.5 Resilient backpropagation\\n\\t5.5.1 Adaption of weights\\n\\t5.5.2 Dynamic learning rate adjustment\\n\\t5.5.3 Rprop in practice\\n\\n\\t5.6 Further variations and extensions to backpropagation\\n\\t5.6.1 Momentum term\\n\\t5.6.2 Flat spot elimination\\n\\t5.6.3 Second order backpropagation\\n\\t5.6.4 Weight decay\\n\\t5.6.5 Pruning and Optimal Brain Damage\\n\\n\\t5.7 Initial configuration of a multilayer perceptron\\n\\t5.7.1 Number of layers\\n\\t5.7.2 The number of neurons\\n\\t5.7.3 Selecting an activation function\\n\\t5.7.4 Initializing weights\\n\\n\\t5.8 The 8-3-8 encoding problem and related problems\\n\\tExercises\\n\\n\\t6 Radial basis functions\\n\\t6.1 Components and structure\\n\\t6.2 Information processing of an RBF network\\n\\t6.2.1 Information processing in RBF neurons\\n\\t6.2.2 Analytical thoughts prior to the training\\n\\n\\t6.3 Training of RBF networks\\n\\t6.3.1 Centers and widths of RBF neurons\\n\\n\\t6.4 Growing RBF networks\\n\\t6.4.1 Adding neurons\\n\\t6.4.2 Limiting the number of neurons\\n\\t6.4.3 Deleting neurons\\n\\n\\t6.5 Comparing RBF networks and multilayer perceptrons\\n\\tExercises\\n\\n\\t7 Recurrent perceptron-like networks (depends on chapter 5)\\n\\t7.1 Jordan networks\\n\\t7.2 Elman networks\\n\\t7.3 Training recurrent networks\\n\\t7.3.1 Unfolding in time\\n\\t7.3.2 Teacher forcing\\n\\t7.3.3 Recurrent backpropagation\\n\\t7.3.4 Training with evolution\\n\\n\\n\\t8 Hopfield networks\\n\\t8.1 Inspired by magnetism\\n\\t8.2 Structure and functionality\\n\\t8.2.1 Input and output of a Hopfield network\\n\\t8.2.2 Significance of weights\\n\\t8.2.3 Change in the state of neurons\\n\\n\\t8.3 Generating the weight matrix\\n\\t8.4 Autoassociation and traditional application\\n\\t8.5 Heteroassociation and analogies to neural data storage\\n\\t8.5.1 Generating the heteroassociative matrix\\n\\t8.5.2 Stabilizing the heteroassociations\\n\\t8.5.3 Biological motivation of heterassociation\\n\\n\\t8.6 Continuous Hopfield networks\\n\\tExercises\\n\\n\\t9 Learning vector quantization\\n\\t9.1 About quantization\\n\\t9.2 Purpose of LVQ\\n\\t9.3 Using codebook vectors\\n\\t9.4 Adjusting codebook vectors\\n\\t9.4.1 The procedure of learning\\n\\n\\t9.5 Connection to neural networks\\n\\tExercises\\n\\n\\n\\tIII Unsupervised learning network paradigms\\n\\t10 Self-organizing feature maps\\n\\t10.1 Structure\\n\\t10.2 Functionality and output interpretation\\n\\t10.3 Training\\n\\t10.3.1 The topology function\\n\\t10.3.2 Monotonically decreasing learning rate and neighborhood\\n\\n\\t10.4 Examples\\n\\t10.4.1 Topological defects\\n\\n\\t10.5 Adjustment of resolution and position-dependent learning rate\\n\\t10.6 Application\\n\\t10.6.1 Interaction with RBF networks\\n\\n\\t10.7 Variations\\n\\t10.7.1 Neural gas\\n\\t10.7.2 Multi-SOMs\\n\\t10.7.3 Multi-neural gas\\n\\t10.7.4 Growing neural gas\\n\\n\\tExercises\\n\\n\\t11 Adaptive resonance theory\\n\\t11.1 Task and structure of an ART network\\n\\t11.1.1 Resonance\\n\\n\\t11.2 Learning process\\n\\t11.2.1 Pattern input and top-down learning\\n\\t11.2.2 Resonance and bottom-up learning\\n\\t11.2.3 Adding an output neuron\\n\\n\\t11.3 Extensions\\n\\n\\n\\tIV Excursi, appendices and registers\\n\\tA Excursus: Cluster analysis and regional and online learnable fields\\n\\tA.1 k-means clustering\\n\\tA.2 k-nearest neighboring\\n\\tA.3 -nearest neighboring\\n\\tA.4 The silhouette coefficient\\n\\tA.5 Regional and online learnable fields\\n\\tA.5.1 Structure of a ROLF\\n\\tA.5.2 Training a ROLF\\n\\tA.5.3 Evaluating a ROLF\\n\\tA.5.4 Comparison with popular clustering methods\\n\\tA.5.5 Initializing radii, learning rates and multiplier\\n\\tA.5.6 Application examples\\n\\n\\tExercises\\n\\n\\tB Excursus: neural networks used for prediction\\n\\tB.1 About time series\\n\\tB.2 One-step-ahead prediction\\n\\tB.3 Two-step-ahead prediction\\n\\tB.3.1 Recursive two-step-ahead prediction\\n\\tB.3.2 Direct two-step-ahead prediction\\n\\n\\tB.4 Additional optimization approaches for prediction\\n\\tB.4.1 Changing temporal parameters\\n\\tB.4.2 Heterogeneous prediction\\n\\n\\tB.5 Remarks on the prediction of share prices\\n\\n\\tC Excursus: reinforcement learning\\n\\tC.1 System structure\\n\\tC.1.1 The gridworld\\n\\tC.1.2 Agent und environment\\n\\tC.1.3 States, situations and actions\\n\\tC.1.4 Reward and return\\n\\tC.1.5 The policy\\n\\n\\tC.2 Learning process\\n\\tC.2.1 Rewarding strategies\\n\\tC.2.2 The state-value function\\n\\tC.2.3 Monte Carlo method\\n\\tC.2.4 Temporal difference learning\\n\\tC.2.5 The action-value function\\n\\tC.2.6 Q learning\\n\\n\\tC.3 Example applications\\n\\tC.3.1 TD gammon\\n\\tC.3.2 The car in the pit\\n\\tC.3.3 The pole balancer\\n\\n\\tC.4 Reinforcement learning in connection with neural networks\\n\\tExercises\\n\\n\\tBibliography\\n\\tList of Figures\\n\\tIndex\\n\\n\\n", "metadata"=>{"pdf:docinfo:modified"=>"2012-05-16T07:01:55Z", "pdf:docinfo:creator"=>"DavidKriesel", "resourceName"=>"b\'A Brief Introduction to Neural Networks (neuronalenetze-en-zeta2-2col-dkrieselcom).pdf\'", "pdf:docinfo:created"=>"2012-05-16T07:01:55Z"}, "filename"=>"A Brief Introduction to Neural Networks (neuronalenetze-en-zeta2-2col-dkrieselcom).pdf"}', 'filename': 'A Brief Introduction to Neural Networks (neuronalenetze-en-zeta2-2col-dkrieselcom).pdf', 'metadata_pdf:docinfo:created': '2012-05-16T07:01:55Z', 'mongo_id': '63cffa2778b994746c729ce6', '@version': '1', 'host': 'bdvm', 'logdate': '2023-01-24T15:32:55+00:00', '@timestamp': '2023-01-24T15:32:58.417165108Z', 'content': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n \n \n \n \n \n \n \n \n \n \n \n \n\n\xa0A\xa0Brief\xa0Introduction\xa0to\xa0\n\nNeural\xa0Networks\xa0\n\xa0 \xa0\n\n\xa0David\xa0Kriesel\xa0\n\xa0dkriesel.com\xa0\n\nDownload\xa0location:\nhttp://www.dkriesel.com/en/science/neural_networks\n\nNEW\xa0–\xa0for\xa0the\xa0programmers:\xa0\nScalable\xa0and\xa0efficient\xa0NN\xa0framework,\xa0written\xa0in\xa0JAVA\xa0\n\nhttp://www.dkriesel.com/en/tech/snipe\n\n\n\n\n\ndkriesel.com\n\nIn remembrance of\nDr. Peter Kemp, Notary (ret.), Bonn, Germany.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) iii\n\n\n\n\n\nA small preface\n"Originally, this work has been prepared in the framework of a seminar of the\nUniversity of Bonn in Germany, but it has been and will be extended (after\n\nbeing presented and published online under www.dkriesel.com on\n5/27/2005). First and foremost, to provide a comprehensive overview of the\n\nsubject of neural networks and, second, just to acquire more and more\nknowledge about LATEX . And who knows – maybe one day this summary will\n\nbecome a real preface!"\n\nAbstract of this work, end of 2005\n\nThe above abstract has not yet become a\npreface but at least a little preface, ever\nsince the extended text (then 40 pages\nlong) has turned out to be a download\nhit.\n\nAmbition and intention of this\nmanuscript\n\nThe entire text is written and laid out\nmore effectively and with more illustra-\ntions than before. I did all the illustra-\ntions myself, most of them directly in\nLATEX by using XYpic. They reflect what\nI would have liked to see when becoming\nacquainted with the subject: Text and il-\nlustrations should be memorable and easy\nto understand to offer as many people as\npossible access to the field of neural net-\nworks.\n\nNevertheless, the mathematically and for-\nmally skilled readers will be able to under-\n\nstand the definitions without reading the\nrunning text, while the opposite holds for\nreaders only interested in the subject mat-\nter; everything is explained in both collo-\nquial and formal language. Please let me\nknow if you find out that I have violated\nthis principle.\n\nThe sections of this text are mostly\nindependent from each other\n\nThe document itself is divided into differ-\nent parts, which are again divided into\nchapters. Although the chapters contain\ncross-references, they are also individually\naccessible to readers with little previous\nknowledge. There are larger and smaller\nchapters: While the larger chapters should\nprovide profound insight into a paradigm\nof neural networks (e.g. the classic neural\nnetwork structure: the perceptron and its\nlearning procedures), the smaller chapters\ngive a short overview – but this is also ex-\n\nv\n\n\n\ndkriesel.com\n\nplained in the introduction of each chapter.\nIn addition to all the definitions and expla-\nnations I have included some excursuses\nto provide interesting information not di-\nrectly related to the subject.\n\nUnfortunately, I was not able to find free\nGerman sources that are multi-faceted\nin respect of content (concerning the\nparadigms of neural networks) and, nev-\nertheless, written in coherent style. The\naim of this work is (even if it could not\nbe fulfilled at first go) to close this gap bit\nby bit and to provide easy access to the\nsubject.\n\nWant to learn not only by\nreading, but also by coding?\nUse SNIPE!\n\nSNIPE1 is a well-documented JAVA li-\nbrary that implements a framework for\nneural networks in a speedy, feature-rich\nand usable way. It is available at no\ncost for non-commercial purposes. It was\noriginally designed for high performance\nsimulations with lots and lots of neural\nnetworks (even large ones) being trained\nsimultaneously. Recently, I decided to\ngive it away as a professional reference im-\nplementation that covers network aspects\nhandled within this work, while at the\nsame time being faster and more efficient\nthan lots of other implementations due to\n\n1 Scalable and Generalized Neural Information Pro-\ncessing Engine, downloadable at http://www.\ndkriesel.com/tech/snipe, online JavaDoc at\nhttp://snipe.dkriesel.com\n\nthe original high-performance simulation\ndesign goal. Those of you who are up for\nlearning by doing and/or have to use a\nfast and stable neural networks implemen-\ntation for some reasons, should definetely\nhave a look at Snipe.\n\nHowever, the aspects covered by Snipe are\nnot entirely congruent with those covered\nby this manuscript. Some of the kinds\nof neural networks are not supported by\nSnipe, while when it comes to other kinds\nof neural networks, Snipe may have lots\nand lots more capabilities than may ever\nbe covered in the manuscript in the form\nof practical hints. Anyway, in my experi-\nence almost all of the implementation re-\nquirements of my readers are covered well.\nOn the Snipe download page, look for the\nsection "Getting started with Snipe" – you\nwill find an easy step-by-step guide con-\ncerning Snipe and its documentation, as\nwell as some examples.\n\nSNIPE: This manuscript frequently incor-\nporates Snipe. Shaded Snipe-paragraphs\nlike this one are scattered among large\nparts of the manuscript, providing infor-\nmation on how to implement their con-\ntext in Snipe. This also implies that\nthose who do not want to use Snipe,\njust have to skip the shaded Snipe-\nparagraphs! The Snipe-paragraphs as-\nsume the reader has had a close look at\nthe "Getting started with Snipe" section.\nOften, class names are used. As Snipe con-\nsists of only a few different packages, I omit-\nted the package names within the qualified\nclass names for the sake of readability.\n\nvi D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\nhttp://www.dkriesel.com/tech/snipe\nhttp://www.dkriesel.com/tech/snipe\nhttp://snipe.dkriesel.com\n\n\ndkriesel.com\n\nIt’s easy to print this\nmanuscript\n\nThis text is completely illustrated in\ncolor, but it can also be printed as is in\nmonochrome: The colors of figures, tables\nand text are well-chosen so that in addi-\ntion to an appealing design the colors are\nstill easy to distinguish when printed in\nmonochrome.\n\nThere are many tools directly\nintegrated into the text\n\nDifferent aids are directly integrated in the\ndocument to make reading more flexible:\nHowever, anyone (like me) who prefers\nreading words on paper rather than on\nscreen can also enjoy some features.\n\nIn the table of contents, different\ntypes of chapters are marked\n\nDifferent types of chapters are directly\nmarked within the table of contents. Chap-\nters, that are marked as "fundamental"\nare definitely ones to read because almost\nall subsequent chapters heavily depend on\nthem. Other chapters additionally depend\non information given in other (preceding)\nchapters, which then is marked in the ta-\nble of contents, too.\n\nSpeaking headlines throughout the\ntext, short ones in the table of\ncontents\n\nThe whole manuscript is now pervaded by\nsuch headlines. Speaking headlines are\nnot just title-like ("Reinforcement Learn-\ning"), but centralize the information given\nin the associated section to a single sen-\ntence. In the named instance, an appro-\npriate headline would be "Reinforcement\nlearning methods provide feedback to the\nnetwork, whether it behaves good or bad".\nHowever, such long headlines would bloat\nthe table of contents in an unacceptable\nway. So I used short titles like the first one\nin the table of contents, and speaking ones,\nlike the latter, throughout the text.\n\nMarginal notes are a navigational\naid\n\nThe entire document contains marginal\nnotes in colloquial language (see the exam-\n\nHypertext\non paper\n:-)\n\nple in the margin), allowing you to "scan"\nthe document quickly to find a certain pas-\nsage in the text (including the titles).\n\nNew mathematical symbols are marked by\nspecific marginal notes for easy finding Jx(see the example for x in the margin).\n\nThere are several kinds of indexing\n\nThis document contains different types of\nindexing: If you have found a word in\nthe index and opened the corresponding\npage, you can easily find it by searching\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) vii\n\n\n\ndkriesel.com\n\nfor highlighted text – all indexed words\nare highlighted like this.\n\nMathematical symbols appearing in sev-\neral chapters of this document (e.g. Ω for\nan output neuron; I tried to maintain a\nconsistent nomenclature for regularly re-\ncurring elements) are separately indexed\nunder "Mathematical Symbols", so they\ncan easily be assigned to the correspond-\ning term.\n\nNames of persons written in small caps\nare indexed in the category "Persons" and\nordered by the last names.\n\nTerms of use and license\n\nBeginning with the epsilon edition, the\ntext is licensed under the Creative Com-\nmons Attribution-No Derivative Works\n3.0 Unported License2, except for some\nlittle portions of the work licensed under\nmore liberal licenses as mentioned (mainly\nsome figures from Wikimedia Commons).\nA quick license summary:\n\n1. You are free to redistribute this docu-\nment (even though it is a much better\nidea to just distribute the URL of my\nhomepage, for it always contains the\nmost recent version of the text).\n\n2. You may not modify, transform, or\nbuild upon the document except for\npersonal use.\n\n2 http://creativecommons.org/licenses/\nby-nd/3.0/\n\n3. You must maintain the author’s attri-\nbution of the document at all times.\n\n4. You may not use the attribution to\nimply that the author endorses you\nor your document use.\n\nFor I’m no lawyer, the above bullet-point\nsummary is just informational: if there is\nany conflict in interpretation between the\nsummary and the actual license, the actual\nlicense always takes precedence. Note that\nthis license does not extend to the source\nfiles used to produce the document. Those\nare still mine.\n\nHow to cite this manuscript\n\nThere’s no official publisher, so you need\nto be careful with your citation. Please\nfind more information in English and\nGerman language on my homepage, re-\nspectively the subpage concerning the\nmanuscript3.\n\nAcknowledgement\n\nNow I would like to express my grati-\ntude to all the people who contributed, in\nwhatever manner, to the success of this\nwork, since a work like this needs many\nhelpers. First of all, I want to thank\nthe proofreaders of this text, who helped\nme and my readers very much. In al-\nphabetical order: Wolfgang Apolinarski,\nKathrin Gräve, Paul Imhoff, Thomas\n\n3 http://www.dkriesel.com/en/science/\nneural_networks\n\nviii D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\nhttp://creativecommons.org/licenses/by-nd/3.0/\nhttp://creativecommons.org/licenses/by-nd/3.0/\nhttp://www.dkriesel.com/en/science/neural_networks\nhttp://www.dkriesel.com/en/science/neural_networks\n\n\ndkriesel.com\n\nKühn, Christoph Kunze, Malte Lohmeyer,\nJoachim Nock, Daniel Plohmann, Daniel\nRosenthal, Christian Schulz and Tobias\nWilken.\n\nAdditionally, I want to thank the readers\nDietmar Berger, Igor Buchmüller, Marie\nChrist, Julia Damaschek, Jochen Döll,\nMaximilian Ernestus, Hardy Falk, Anne\nFeldmeier, Sascha Fink, Andreas Fried-\nmann, Jan Gassen, Markus Gerhards, Se-\nbastian Hirsch, Andreas Hochrath, Nico\nHöft, Thomas Ihme, Boris Jentsch, Tim\nHussein, Thilo Keller, Mario Krenn, Mirko\nKunze, Maikel Linke, Adam Maciak,\nBenjamin Meier, David Möller, Andreas\nMüller, Rainer Penninger, Lena Reichel,\nAlexander Schier, Matthias Siegmund,\nMathias Tirtasana, Oliver Tischler, Max-\nimilian Voit, Igor Wall, Achim Weber,\nFrank Weinreis, Gideon Maillette de Buij\nWenniger, Philipp Woock and many oth-\ners for their feedback, suggestions and re-\nmarks.\n\nAdditionally, I’d like to thank Sebastian\nMerzbach, who examined this work in a\nvery conscientious way finding inconsisten-\ncies and errors. In particular, he cleared\nlots and lots of language clumsiness from\nthe English version.\n\nEspecially, I would like to thank Beate\nKuhl for translating the entire text from\nGerman to English, and for her questions\nwhich made me think of changing the\nphrasing of some paragraphs.\n\nI would particularly like to thank Prof.\nRolf Eckmiller and Dr. Nils Goerke as\nwell as the entire Division of Neuroinfor-\nmatics, Department of Computer Science\n\nof the University of Bonn – they all made\nsure that I always learned (and also had\nto learn) something new about neural net-\nworks and related subjects. Especially Dr.\nGoerke has always been willing to respond\nto any questions I was not able to answer\nmyself during the writing process. Conver-\nsations with Prof. Eckmiller made me step\nback from the whiteboard to get a better\noverall view on what I was doing and what\nI should do next.\n\nGlobally, and not only in the context of\nthis work, I want to thank my parents who\nnever get tired to buy me specialized and\ntherefore expensive books and who have\nalways supported me in my studies.\n\nFor many "remarks" and the very special\nand cordial atmosphere ;-) I want to thank\nAndreas Huber and Tobias Treutler. Since\nour first semester it has rarely been boring\nwith you!\n\nNow I would like to think back to my\nschool days and cordially thank some\nteachers who (in my opinion) had im-\nparted some scientific knowledge to me –\nalthough my class participation had not\nalways been wholehearted: Mr. Wilfried\nHartmann, Mr. Hubert Peters and Mr.\nFrank Nökel.\n\nFurthermore I would like to thank the\nwhole team at the notary’s office of Dr.\nKemp and Dr. Kolb in Bonn, where I have\nalways felt to be in good hands and who\nhave helped me to keep my printing costs\nlow - in particular Christiane Flamme and\nDr. Kemp!\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) ix\n\n\n\ndkriesel.com\n\nThanks go also to the Wikimedia Com-\nmons, where I took some (few) images and\naltered them to suit this text.\n\nLast but not least I want to thank two\npeople who made outstanding contribu-\ntions to this work who occupy, so to speak,\na place of honor: My girlfriend Verena\nThomas, who found many mathematical\nand logical errors in my text and dis-\ncussed them with me, although she has\nlots of other things to do, and Chris-\ntiane Schultze, who carefully reviewed the\ntext for spelling mistakes and inconsisten-\ncies.\n\nDavid Kriesel\n\nx D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\nContents\n\nA small preface v\n\nI From biology to formalization – motivation, philosophy, history and\nrealization of neural models 1\n\n1 Introduction, motivation and history 3\n1.1 Why neural networks? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.1.1 The 100-step rule . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.1.2 Simple application examples . . . . . . . . . . . . . . . . . . . . . 6\n\n1.2 History of neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.1 The beginning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.2 Golden age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.2.3 Long silence and slow reconstruction . . . . . . . . . . . . . . . . 11\n1.2.4 Renaissance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\n2 Biological neural networks 13\n2.1 The vertebrate nervous system . . . . . . . . . . . . . . . . . . . . . . . 13\n\n2.1.1 Peripheral and central nervous system . . . . . . . . . . . . . . . 13\n2.1.2 Cerebrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.3 Cerebellum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.1.4 Diencephalon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.1.5 Brainstem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n2.2 The neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.2.1 Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.2.2 Electrochemical processes in the neuron . . . . . . . . . . . . . . 19\n\n2.3 Receptor cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.3.1 Various types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.3.2 Information processing within the nervous system . . . . . . . . 25\n2.3.3 Light sensing organs . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n2.4 The amount of neurons in living organisms . . . . . . . . . . . . . . . . 28\n\nxi\n\n\n\nContents dkriesel.com\n\n2.5 Technical neurons as caricature of biology . . . . . . . . . . . . . . . . . 30\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\n3 Components of artificial neural networks (fundamental) 33\n3.1 The concept of time in neural networks . . . . . . . . . . . . . . . . . . 33\n3.2 Components of neural networks . . . . . . . . . . . . . . . . . . . . . . . 33\n\n3.2.1 Connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.2.2 Propagation function and network input . . . . . . . . . . . . . . 34\n3.2.3 Activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.2.4 Threshold value . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.2.5 Activation function . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.2.6 Common activation functions . . . . . . . . . . . . . . . . . . . . 37\n3.2.7 Output function . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.2.8 Learning strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n\n3.3 Network topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.3.1 Feedforward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.3.2 Recurrent networks . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.3.3 Completely linked networks . . . . . . . . . . . . . . . . . . . . . 42\n\n3.4 The bias neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Representing neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3.6 Orders of activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n\n3.6.1 Synchronous activation . . . . . . . . . . . . . . . . . . . . . . . 45\n3.6.2 Asynchronous activation . . . . . . . . . . . . . . . . . . . . . . . 46\n\n3.7 Input and output of data . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n\n4 Fundamentals on learning and training samples (fundamental) 51\n4.1 Paradigms of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n\n4.1.1 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . 52\n4.1.2 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . 53\n4.1.3 Supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.1.4 Offline or online learning? . . . . . . . . . . . . . . . . . . . . . . 54\n4.1.5 Questions in advance . . . . . . . . . . . . . . . . . . . . . . . . . 54\n\n4.2 Training patterns and teaching input . . . . . . . . . . . . . . . . . . . . 54\n4.3 Using training samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\n4.3.1 Division of the training set . . . . . . . . . . . . . . . . . . . . . 57\n4.3.2 Order of pattern representation . . . . . . . . . . . . . . . . . . . 57\n\n4.4 Learning curve and error measurement . . . . . . . . . . . . . . . . . . . 58\n4.4.1 When do we stop learning? . . . . . . . . . . . . . . . . . . . . . 59\n\nxii D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Contents\n\n4.5 Gradient optimization procedures . . . . . . . . . . . . . . . . . . . . . . 61\n4.5.1 Problems of gradient procedures . . . . . . . . . . . . . . . . . . 62\n\n4.6 Exemplary problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.6.1 Boolean functions . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.6.2 The parity function . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.6.3 The 2-spiral problem . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.6.4 The checkerboard problem . . . . . . . . . . . . . . . . . . . . . . 65\n4.6.5 The identity function . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.6.6 Other exemplary problems . . . . . . . . . . . . . . . . . . . . . 66\n\n4.7 Hebbian rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.7.1 Original rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.7.2 Generalized form . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\nII Supervised learning network paradigms 69\n\n5 The perceptron, backpropagation and its variants 71\n5.1 The singlelayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n\n5.1.1 Perceptron learning algorithm and convergence theorem . . . . . 75\n5.1.2 Delta rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n\n5.2 Linear separability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n5.3 The multilayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.4 Backpropagation of error . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n\n5.4.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n5.4.2 Boiling backpropagation down to the delta rule . . . . . . . . . . 91\n5.4.3 Selecting a learning rate . . . . . . . . . . . . . . . . . . . . . . . 92\n\n5.5 Resilient backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n5.5.1 Adaption of weights . . . . . . . . . . . . . . . . . . . . . . . . . 94\n5.5.2 Dynamic learning rate adjustment . . . . . . . . . . . . . . . . . 94\n5.5.3 Rprop in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n\n5.6 Further variations and extensions to backpropagation . . . . . . . . . . 96\n5.6.1 Momentum term . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n5.6.2 Flat spot elimination . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.6.3 Second order backpropagation . . . . . . . . . . . . . . . . . . . 98\n5.6.4 Weight decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n5.6.5 Pruning and Optimal Brain Damage . . . . . . . . . . . . . . . . 98\n\n5.7 Initial configuration of a multilayer perceptron . . . . . . . . . . . . . . 99\n5.7.1 Number of layers . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.7.2 The number of neurons . . . . . . . . . . . . . . . . . . . . . . . 100\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) xiii\n\n\n\nContents dkriesel.com\n\n5.7.3 Selecting an activation function . . . . . . . . . . . . . . . . . . . 100\n5.7.4 Initializing weights . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n\n5.8 The 8-3-8 encoding problem and related problems . . . . . . . . . . . . 101\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n\n6 Radial basis functions 105\n6.1 Components and structure . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.2 Information processing of an RBF network . . . . . . . . . . . . . . . . 106\n\n6.2.1 Information processing in RBF neurons . . . . . . . . . . . . . . 108\n6.2.2 Analytical thoughts prior to the training . . . . . . . . . . . . . . 111\n\n6.3 Training of RBF networks . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n6.3.1 Centers and widths of RBF neurons . . . . . . . . . . . . . . . . 115\n\n6.4 Growing RBF networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n6.4.1 Adding neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n6.4.2 Limiting the number of neurons . . . . . . . . . . . . . . . . . . . 119\n6.4.3 Deleting neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n\n6.5 Comparing RBF networks and multilayer perceptrons . . . . . . . . . . 119\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n\n7 Recurrent perceptron-like networks (depends on chapter 5) 121\n7.1 Jordan networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n7.2 Elman networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n7.3 Training recurrent networks . . . . . . . . . . . . . . . . . . . . . . . . . 124\n\n7.3.1 Unfolding in time . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n7.3.2 Teacher forcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n7.3.3 Recurrent backpropagation . . . . . . . . . . . . . . . . . . . . . 127\n7.3.4 Training with evolution . . . . . . . . . . . . . . . . . . . . . . . 127\n\n8 Hopfield networks 129\n8.1 Inspired by magnetism . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n8.2 Structure and functionality . . . . . . . . . . . . . . . . . . . . . . . . . 129\n\n8.2.1 Input and output of a Hopfield network . . . . . . . . . . . . . . 130\n8.2.2 Significance of weights . . . . . . . . . . . . . . . . . . . . . . . . 131\n8.2.3 Change in the state of neurons . . . . . . . . . . . . . . . . . . . 131\n\n8.3 Generating the weight matrix . . . . . . . . . . . . . . . . . . . . . . . . 132\n8.4 Autoassociation and traditional application . . . . . . . . . . . . . . . . 133\n8.5 Heteroassociation and analogies to neural data storage . . . . . . . . . . 134\n\n8.5.1 Generating the heteroassociative matrix . . . . . . . . . . . . . . 135\n8.5.2 Stabilizing the heteroassociations . . . . . . . . . . . . . . . . . . 135\n8.5.3 Biological motivation of heterassociation . . . . . . . . . . . . . . 136\n\nxiv D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Contents\n\n8.6 Continuous Hopfield networks . . . . . . . . . . . . . . . . . . . . . . . . 136\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n\n9 Learning vector quantization 139\n9.1 About quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n9.2 Purpose of LVQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n9.3 Using codebook vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n9.4 Adjusting codebook vectors . . . . . . . . . . . . . . . . . . . . . . . . . 141\n\n9.4.1 The procedure of learning . . . . . . . . . . . . . . . . . . . . . . 141\n9.5 Connection to neural networks . . . . . . . . . . . . . . . . . . . . . . . 143\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n\nIII Unsupervised learning network paradigms 145\n\n10 Self-organizing feature maps 147\n10.1 Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n10.2 Functionality and output interpretation . . . . . . . . . . . . . . . . . . 149\n10.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n\n10.3.1 The topology function . . . . . . . . . . . . . . . . . . . . . . . . 150\n10.3.2 Monotonically decreasing learning rate and neighborhood . . . . 152\n\n10.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n10.4.1 Topological defects . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\n10.5 Adjustment of resolution and position-dependent learning rate . . . . . 156\n10.6 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n\n10.6.1 Interaction with RBF networks . . . . . . . . . . . . . . . . . . . 161\n10.7 Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n\n10.7.1 Neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n10.7.2 Multi-SOMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n10.7.3 Multi-neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n10.7.4 Growing neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n\n11 Adaptive resonance theory 165\n11.1 Task and structure of an ART network . . . . . . . . . . . . . . . . . . . 165\n\n11.1.1 Resonance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n11.2 Learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n\n11.2.1 Pattern input and top-down learning . . . . . . . . . . . . . . . . 167\n11.2.2 Resonance and bottom-up learning . . . . . . . . . . . . . . . . . 167\n11.2.3 Adding an output neuron . . . . . . . . . . . . . . . . . . . . . . 167\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) xv\n\n\n\nContents dkriesel.com\n\n11.3 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n\nIV Excursi, appendices and registers 169\n\nA Excursus: Cluster analysis and regional and online learnable fields 171\nA.1 k-means clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\nA.2 k-nearest neighboring . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\nA.3 ε-nearest neighboring . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\nA.4 The silhouette coefficient . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\nA.5 Regional and online learnable fields . . . . . . . . . . . . . . . . . . . . . 175\n\nA.5.1 Structure of a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . 176\nA.5.2 Training a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\nA.5.3 Evaluating a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . 178\nA.5.4 Comparison with popular clustering methods . . . . . . . . . . . 179\nA.5.5 Initializing radii, learning rates and multiplier . . . . . . . . . . . 180\nA.5.6 Application examples . . . . . . . . . . . . . . . . . . . . . . . . 180\n\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n\nB Excursus: neural networks used for prediction 181\nB.1 About time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\nB.2 One-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 183\nB.3 Two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n\nB.3.1 Recursive two-step-ahead prediction . . . . . . . . . . . . . . . . 185\nB.3.2 Direct two-step-ahead prediction . . . . . . . . . . . . . . . . . . 185\n\nB.4 Additional optimization approaches for prediction . . . . . . . . . . . . . 185\nB.4.1 Changing temporal parameters . . . . . . . . . . . . . . . . . . . 185\nB.4.2 Heterogeneous prediction . . . . . . . . . . . . . . . . . . . . . . 187\n\nB.5 Remarks on the prediction of share prices . . . . . . . . . . . . . . . . . 187\n\nC Excursus: reinforcement learning 191\nC.1 System structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n\nC.1.1 The gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\nC.1.2 Agent und environment . . . . . . . . . . . . . . . . . . . . . . . 193\nC.1.3 States, situations and actions . . . . . . . . . . . . . . . . . . . . 194\nC.1.4 Reward and return . . . . . . . . . . . . . . . . . . . . . . . . . . 195\nC.1.5 The policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n\nC.2 Learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\nC.2.1 Rewarding strategies . . . . . . . . . . . . . . . . . . . . . . . . . 198\nC.2.2 The state-value function . . . . . . . . . . . . . . . . . . . . . . . 199\n\nxvi D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Contents\n\nC.2.3 Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . 201\nC.2.4 Temporal difference learning . . . . . . . . . . . . . . . . . . . . 202\nC.2.5 The action-value function . . . . . . . . . . . . . . . . . . . . . . 203\nC.2.6 Q learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n\nC.3 Example applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\nC.3.1 TD gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\nC.3.2 The car in the pit . . . . . . . . . . . . . . . . . . . . . . . . . . 205\nC.3.3 The pole balancer . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n\nC.4 Reinforcement learning in connection with neural networks . . . . . . . 207\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n\nBibliography 209\n\nList of Figures 215\n\nIndex 219\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) xvii\n\n\n\n\n\nPart I\n\nFrom biology to formalization –\nmotivation, philosophy, history and\n\nrealization of neural models\n\n1\n\n\n\n\n\nChapter 1\n\nIntroduction, motivation and history\nHow to teach a computer? You can either write a fixed program – or you can\n\nenable the computer to learn on its own. Living beings do not have any\nprogrammer writing a program for developing their skills, which then only has\nto be executed. They learn by themselves – without the previous knowledge\n\nfrom external impressions – and thus can solve problems better than any\ncomputer today. What qualities are needed to achieve such a behavior for\n\ndevices like computers? Can such cognition be adapted from biology? History,\ndevelopment, decline and resurgence of a wide approach to solve problems.\n\n1.1 Why neural networks?\n\nThere are problem categories that cannot\nbe formulated as an algorithm. Problems\nthat depend on many subtle factors, for ex-\nample the purchase price of a real estate\nwhich our brain can (approximately) cal-\nculate. Without an algorithm a computer\ncannot do the same. Therefore the ques-\ntion to be asked is: How do we learn to\nexplore such problems?\n\nExactly – we learn; a capability comput-\ners obviously do not have. Humans have\n\nComputers\ncannot\nlearn\n\na brain that can learn. Computers have\nsome processing units and memory. They\nallow the computer to perform the most\ncomplex numerical calculations in a very\nshort time, but they are not adaptive.\n\nIf we compare computer and brain1, we\nwill note that, theoretically, the computer\nshould be more powerful than our brain:\nIt comprises 109 transistors with a switch-\ning time of 10−9 seconds. The brain con-\ntains 1011 neurons, but these only have a\nswitching time of about 10−3 seconds.\n\nThe largest part of the brain is work-\ning continuously, while the largest part of\nthe computer is only passive data storage.\nThus, the brain is parallel and therefore\n\nparallelism\nperforming close to its theoretical maxi-\n\n1 Of course, this comparison is - for obvious rea-\nsons - controversially discussed by biologists and\ncomputer scientists, since response time and quan-\ntity do not tell anything about quality and perfor-\nmance of the processing units as well as neurons\nand transistors cannot be compared directly. Nev-\nertheless, the comparison serves its purpose and\nindicates the advantage of parallelism by means\nof processing time.\n\n3\n\n\n\nChapter 1 Introduction, motivation and history dkriesel.com\n\nBrain Computer\nNo. of processing units ≈ 1011 ≈ 109\n\nType of processing units Neurons Transistors\nType of calculation massively parallel usually serial\nData storage associative address-based\nSwitching time ≈ 10−3s ≈ 10−9s\nPossible switching operations ≈ 1013 1\n\ns ≈ 1018 1\ns\n\nActual switching operations ≈ 1012 1\ns ≈ 1010 1\n\ns\n\nTable 1.1: The (flawed) comparison between brain and computer at a glance. Inspired by: [Zel94]\n\nmum, from which the computer is orders\nof magnitude away (Table 1.1). Addition-\nally, a computer is static - the brain as\na biological neural network can reorganize\nitself during its "lifespan" and therefore is\nable to learn, to compensate errors and so\nforth.\n\nWithin this text I want to outline how\nwe can use the said characteristics of our\nbrain for a computer system.\n\nSo the study of artificial neural networks\nis motivated by their similarity to success-\nfully working biological systems, which - in\ncomparison to the overall system - consist\nof very simple but numerous nerve cells\n\nsimple\nbut many\nprocessing\n\nunits\n\nthat work massively in parallel and (which\nis probably one of the most significant\naspects) have the capability to learn.\nThere is no need to explicitly program a\nneural network. For instance, it can learn\nfrom training samples or by means of en-\n\nn. network\ncapable\nto learn\n\ncouragement - with a carrot and a stick,\nso to speak (reinforcement learning).\n\nOne result from this learning procedure is\nthe capability of neural networks to gen-\n\neralize and associate data: After suc-\ncessful training a neural network can find\nreasonable solutions for similar problems\nof the same class that were not explicitly\ntrained. This in turn results in a high de-\ngree of fault tolerance against noisy in-\nput data.\n\nFault tolerance is closely related to biolog-\nical neural networks, in which this charac-\nteristic is very distinct: As previously men-\ntioned, a human has about 1011 neurons\nthat continuously reorganize themselves\nor are reorganized by external influences\n(about 105 neurons can be destroyed while\nin a drunken stupor, some types of food\nor environmental influences can also de-\nstroy brain cells). Nevertheless, our cogni-\ntive abilities are not significantly affected.\n\nn. network\nfault\ntolerant\n\nThus, the brain is tolerant against internal\nerrors – and also against external errors,\nfor we can often read a really "dreadful\nscrawl" although the individual letters are\nnearly impossible to read.\n\nOur modern technology, however, is not\nautomatically fault-tolerant. I have never\nheard that someone forgot to install the\n\n4 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 1.1 Why neural networks?\n\nhard disk controller into a computer and\ntherefore the graphics card automatically\ntook over its tasks, i.e. removed con-\nductors and developed communication, so\nthat the system as a whole was affected\nby the missing component, but not com-\npletely destroyed.\n\nA disadvantage of this distributed fault-\ntolerant storage is certainly the fact that\nwe cannot realize at first sight what a neu-\nral neutwork knows and performs or where\nits faults lie. Usually, it is easier to per-\nform such analyses for conventional algo-\nrithms. Most often we can only trans-\nfer knowledge into our neural network by\nmeans of a learning procedure, which can\ncause several errors and is not always easy\nto manage.\n\nFault tolerance of data, on the other hand,\nis already more sophisticated in state-of-\nthe-art technology: Let us compare a\nrecord and a CD. If there is a scratch on a\nrecord, the audio information on this spot\nwill be completely lost (you will hear a\npop) and then the music goes on. On a CD\nthe audio data are distributedly stored: A\nscratch causes a blurry sound in its vicin-\nity, but the data stream remains largely\nunaffected. The listener won’t notice any-\nthing.\n\nSo let us summarize the main characteris-\ntics we try to adapt from biology:\n\n. Self-organization and learning capa-\nbility,\n\n. Generalization capability and\n\n. Fault tolerance.\n\nWhat types of neural networks particu-\nlarly develop what kinds of abilities and\ncan be used for what problem classes will\nbe discussed in the course of this work.\n\nIn the introductory chapter I want to\nclarify the following: "The neural net-\nwork" does not exist. There are differ- Important!\nent paradigms for neural networks, how\nthey are trained and where they are used.\nMy goal is to introduce some of these\nparadigms and supplement some remarks\nfor practical application.\n\nWe have already mentioned that our brain\nworks massively in parallel, in contrast to\nthe functioning of a computer, i.e. every\ncomponent is active at any time. If we\nwant to state an argument for massive par-\nallel processing, then the 100-step rule\ncan be cited.\n\n1.1.1 The 100-step rule\n\nExperiments showed that a human can\nrecognize the picture of a familiar object\nor person in ≈ 0.1 seconds, which cor-\nresponds to a neuron switching time of\n≈ 10−3 seconds in ≈ 100 discrete time\nsteps of parallel processing.\n\nparallel\nprocessing\n\nA computer following the von Neumann\narchitecture, however, can do practically\nnothing in 100 time steps of sequential pro-\ncessing, which are 100 assembler steps or\ncycle steps.\n\nNow we want to look at a simple applica-\ntion example for a neural network.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 5\n\n\n\nChapter 1 Introduction, motivation and history dkriesel.com\n\nFigure 1.1: A small robot with eight sensors\nand two motors. The arrow indicates the driv-\ning direction.\n\n1.1.2 Simple application examples\n\nLet us assume that we have a small robot\nas shown in fig. 1.1. This robot has eight\ndistance sensors from which it extracts in-\nput data: Three sensors are placed on the\nfront right, three on the front left, and two\non the back. Each sensor provides a real\nnumeric value at any time, that means we\nare always receiving an input I ∈ R8.\n\nDespite its two motors (which will be\nneeded later) the robot in our simple ex-\nample is not capable to do much: It shall\nonly drive on but stop when it might col-\nlide with an obstacle. Thus, our output\nis binary: H = 0 for "Everything is okay,\ndrive on" and H = 1 for "Stop" (The out-\n\nput is called H for "halt signal"). There-\nfore we need a mapping\n\nf : R8 → B1,\n\nthat applies the input signals to a robot\nactivity.\n\n1.1.2.1 The classical way\n\nThere are two ways of realizing this map-\nping. On the one hand, there is the clas-\nsical way: We sit down and think for a\nwhile, and finally the result is a circuit or\na small computer program which realizes\nthe mapping (this is easily possible, since\nthe example is very simple). After that\nwe refer to the technical reference of the\nsensors, study their characteristic curve in\norder to learn the values for the different\nobstacle distances, and embed these values\ninto the aforementioned set of rules. Such\nprocedures are applied in the classic artifi-\ncial intelligence, and if you know the exact\nrules of a mapping algorithm, you are al-\nways well advised to follow this scheme.\n\n1.1.2.2 The way of learning\n\nOn the other hand, more interesting and\nmore successful for many mappings and\nproblems that are hard to comprehend\nstraightaway is the way of learning: We\nshow different possible situations to the\nrobot (fig. 1.2 on page 8), – and the robot\nshall learn on its own what to do in the\ncourse of its robot life.\n\nIn this example the robot shall simply\nlearn when to stop. We first treat the\n\n6 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 1.1 Why neural networks?\n\nFigure 1.3: Initially, we regard the robot control\nas a black box whose inner life is unknown. The\nblack box receives eight real sensor values and\nmaps these values to a binary output value.\n\nneural network as a kind of black box\n(fig. 1.3). This means we do not know its\nstructure but just regard its behavior in\npractice.\n\nThe situations in form of simply mea-\nsured sensor values (e.g. placing the robot\nin front of an obstacle, see illustration),\nwhich we show to the robot and for which\nwe specify whether to drive on or to stop,\nare called training samples. Thus, a train-\ning sample consists of an exemplary input\nand a corresponding desired output. Now\nthe question is how to transfer this knowl-\nedge, the information, into the neural net-\nwork.\n\nThe samples can be taught to a neural\nnetwork by using a simple learning pro-\ncedure (a learning procedure is a simple\nalgorithm or a mathematical formula. If\nwe have done everything right and chosen\ngood samples, the neural network will gen-\neralize from these samples and find a uni-\nversal rule when it has to stop.\n\nOur example can be optionally expanded.\nFor the purpose of direction control it\nwould be possible to control the motors\nof our robot separately2, with the sensor\nlayout being the same. In this case we are\nlooking for a mapping\n\nf : R8 → R2,\n\nwhich gradually controls the two motors\nby means of the sensor inputs and thus\ncannot only, for example, stop the robot\nbut also lets it avoid obstacles. Here it\nis more difficult to analytically derive the\nrules, and de facto a neural network would\nbe more appropriate.\n\nOur goal is not to learn the samples by\nheart, but to realize the principle behind\nthem: Ideally, the robot should apply the\nneural network in any situation and be\nable to avoid obstacles. In particular, the\nrobot should query the network continu-\nously and repeatedly while driving in order\nto continously avoid obstacles. The result\nis a constant cycle: The robot queries the\nnetwork. As a consequence, it will drive\nin one direction, which changes the sen-\nsors values. Again the robot queries the\nnetwork and changes its position, the sen-\nsor values are changed once again, and so\non. It is obvious that this system can also\nbe adapted to dynamic, i.e changing, en-\nvironments (e.g. the moving obstacles in\nour example).\n\n2 There is a robot called Khepera with more or less\nsimilar characteristics. It is round-shaped, approx.\n7 cm in diameter, has two motors with wheels\nand various sensors. For more information I rec-\nommend to refer to the internet.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 7\n\n\n\nChapter 1 Introduction, motivation and history dkriesel.com\n\nFigure 1.2: The robot is positioned in a landscape that provides sensor values for different situa-\ntions. We add the desired output values H and so receive our learning samples. The directions in\nwhich the sensors are oriented are exemplarily applied to two robots.\n\n1.2 A brief history of neural\nnetworks\n\nThe field of neural networks has, like any\nother field of science, a long history of\ndevelopment with many ups and downs,\nas we will see soon. To continue the style\nof my work I will not represent this history\nin text form but more compact in form of a\ntimeline. Citations and bibliographical ref-\nerences are added mainly for those topics\nthat will not be further discussed in this\ntext. Citations for keywords that will be\nexplained later are mentioned in the corre-\nsponding chapters.\n\nThe history of neural networks begins in\nthe early 1940’s and thus nearly simulta-\n\nneously with the history of programmable\nelectronic computers. The youth of this\nfield of research, as with the field of com-\nputer science itself, can be easily recog-\nnized due to the fact that many of the\ncited persons are still with us.\n\n1.2.1 The beginning\n\nAs soon as 1943 Warren McCulloch\nand Walter Pitts introduced mod-\nels of neurological networks, recre-\nated threshold switches based on neu-\nrons and showed that even simple\nnetworks of this kind are able to\ncalculate nearly any logic or arith-\nmetic function [MP43]. Further-\n\n8 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 1.2 History of neural networks\n\nFigure 1.4: Some institutions of the field of neural networks. From left to right: John von Neu-\nmann, Donald O. Hebb, Marvin Minsky, Bernard Widrow, Seymour Papert, Teuvo Kohonen, John\nHopfield, "in the order of appearance" as far as possible.\n\nmore, the first computer precur-\nsors ("electronic brains")were de-\nveloped, among others supported by\nKonrad Zuse, who was tired of cal-\nculating ballistic trajectories by hand.\n\n1947: Walter Pitts and Warren Mc-\nCulloch indicated a practical field\nof application (which was not men-\ntioned in their work from 1943),\nnamely the recognition of spacial pat-\nterns by neural networks [PM47].\n\n1949: Donald O. Hebb formulated the\nclassical Hebbian rule [Heb49] which\nrepresents in its more generalized\nform the basis of nearly all neural\nlearning procedures. The rule im-\nplies that the connection between two\nneurons is strengthened when both\nneurons are active at the same time.\nThis change in strength is propor-\ntional to the product of the two activ-\nities. Hebb could postulate this rule,\nbut due to the absence of neurological\nresearch he was not able to verify it.\n\n1950: The neuropsychologist Karl\nLashley defended the thesis that\n\nbrain information storage is realized\nas a distributed system. His thesis\nwas based on experiments on rats,\nwhere only the extent but not the\nlocation of the destroyed nerve tissue\ninfluences the rats’ performance to\nfind their way out of a labyrinth.\n\n1.2.2 Golden age\n\n1951: For his dissertation Marvin Min-\nsky developed the neurocomputer\nSnark, which has already been capa-\nble to adjust its weights3 automati-\ncally. But it has never been practi-\ncally implemented, since it is capable\nto busily calculate, but nobody really\nknows what it calculates.\n\n1956: Well-known scientists and ambi-\ntious students met at the Dart-\nmouth Summer Research Project\nand discussed, to put it crudely, how\nto simulate a brain. Differences be-\ntween top-down and bottom-up re-\nsearch developed. While the early\n\n3 We will learn soon what weights are.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 9\n\n\n\nChapter 1 Introduction, motivation and history dkriesel.com\n\nsupporters of artificial intelligence\nwanted to simulate capabilities by\nmeans of software, supporters of neu-\nral networks wanted to achieve sys-\ntem behavior by imitating the small-\nest parts of the system – the neurons.\n\n1957-1958: At the MIT, Frank Rosen-\nblatt, Charles Wightman and\ntheir coworkers developed the first\nsuccessful neurocomputer, the Mark\nI perceptron, which was capable to\n\ndevelopment\naccelerates recognize simple numerics by means\n\nof a 20 × 20 pixel image sensor and\nelectromechanically worked with 512\nmotor driven potentiometers - each\npotentiometer representing one vari-\nable weight.\n\n1959: Frank Rosenblatt described dif-\nferent versions of the perceptron, for-\nmulated and verified his perceptron\nconvergence theorem. He described\nneuron layers mimicking the retina,\nthreshold switches, and a learning\nrule adjusting the connecting weights.\n\n1960: Bernard Widrow and Mar-\ncian E. Hoff introduced the ADA-\nLINE (ADAptive LInear NEu-\nron) [WH60], a fast and precise\nadaptive learning system being the\nfirst widely commercially used neu-\nral network: It could be found in\nnearly every analog telephone for real-\ntime adaptive echo filtering and was\ntrained by menas of the Widrow-Hoff\n\nfirst\nspread\n\nuse\nrule or delta rule. At that time Hoff,\nlater co-founder of Intel Corporation,\nwas a PhD student of Widrow, who\nhimself is known as the inventor of\n\nmodern microprocessors. One advan-\ntage the delta rule had over the origi-\nnal perceptron learning algorithm was\nits adaptivity: If the difference be-\ntween the actual output and the cor-\nrect solution was large, the connect-\ning weights also changed in larger\nsteps – the smaller the steps, the\ncloser the target was. Disadvantage:\nmissapplication led to infinitesimal\nsmall steps close to the target. In the\nfollowing stagnation and out of fear\nof scientific unpopularity of the neu-\nral networks ADALINE was renamed\nin adaptive linear element – which\nwas undone again later on.\n\n1961: Karl Steinbuch introduced tech-\nnical realizations of associative mem-\nory, which can be seen as predecessors\nof today’s neural associative mem-\nories [Ste61]. Additionally, he de-\nscribed concepts for neural techniques\nand analyzed their possibilities and\nlimits.\n\n1965: In his book Learning Machines,\nNils Nilsson gave an overview of\nthe progress and works of this period\nof neural network research. It was\nassumed that the basic principles of\nself-learning and therefore, generally\nspeaking, "intelligent" systems had al-\nready been discovered. Today this as-\nsumption seems to be an exorbitant\noverestimation, but at that time it\nprovided for high popularity and suf-\nficient research funds.\n\n1969: Marvin Minsky and Seymour\nPapert published a precise mathe-\n\n10 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 1.2 History of neural networks\n\nmatical analysis of the perceptron\n[MP69] to show that the perceptron\nmodel was not capable of representing\nmany important problems (keywords:\nXOR problem and linear separability),\nand so put an end to overestimation,\npopularity and research funds. The\n\nresearch\nfunds were\n\nstopped\nimplication that more powerful mod-\nels would show exactly the same prob-\nlems and the forecast that the entire\nfield would be a research dead end re-\nsulted in a nearly complete decline in\nresearch funds for the next 15 years\n– no matter how incorrect these fore-\ncasts were from today’s point of view.\n\n1.2.3 Long silence and slow\nreconstruction\n\nThe research funds were, as previously-\nmentioned, extremely short. Everywhere\nresearch went on, but there were neither\nconferences nor other events and therefore\nonly few publications. This isolation of\nindividual researchers provided for many\nindependently developed neural network\nparadigms: They researched, but there\nwas no discourse among them.\n\nIn spite of the poor appreciation the field\nreceived, the basic theories for the still\ncontinuing renaissance were laid at that\ntime:\n\n1972: Teuvo Kohonen introduced a\nmodel of the linear associator,\na model of an associative memory\n[Koh72]. In the same year, such a\nmodel was presented independently\nand from a neurophysiologist’s point\n\nof view by James A. Anderson\n[And72].\n\n1973: Christoph von der Malsburg\nused a neuron model that was non-\nlinear and biologically more moti-\nvated [vdM73].\n\n1974: For his dissertation in Harvard\nPaul Werbos developed a learning\nprocedure called backpropagation of\nerror [Wer74], but it was not until\none decade later that this procedure\nreached today’s importance.\n\nbackprop\ndeveloped\n\n1976-1980 and thereafter: Stephen\nGrossberg presented many papers\n(for instance [Gro76]) in which\nnumerous neural models are analyzed\nmathematically. Furthermore, he\ndedicated himself to the problem of\nkeeping a neural network capable\nof learning without destroying\nalready learned associations. Under\ncooperation of Gail Carpenter\nthis led to models of adaptive\nresonance theory (ART).\n\n1982: Teuvo Kohonen described the\nself-organizing feature maps\n(SOM) [Koh82, Koh98] – also\nknown as Kohonen maps. He was\nlooking for the mechanisms involving\nself-organization in the brain (He\nknew that the information about the\ncreation of a being is stored in the\ngenome, which has, however, not\nenough memory for a structure like\nthe brain. As a consequence, the\nbrain has to organize and create\nitself for the most part).\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 11\n\n\n\nChapter 1 Introduction, motivation and history dkriesel.com\n\nJohn Hopfield also invented the\nso-called Hopfield networks [Hop82]\nwhich are inspired by the laws of mag-\nnetism in physics. They were not\nwidely used in technical applications,\nbut the field of neural networks slowly\nregained importance.\n\n1983: Fukushima, Miyake and Ito in-\ntroduced the neural model of the\nNeocognitron which could recognize\nhandwritten characters [FMI83] and\nwas an extension of the Cognitron net-\nwork already developed in 1975.\n\n1.2.4 Renaissance\n\nThrough the influence of John Hopfield,\nwho had personally convinced many re-\nsearchers of the importance of the field,\nand the wide publication of backpro-\npagation by Rumelhart, Hinton and\nWilliams, the field of neural networks\nslowly showed signs of upswing.\n\n1985: John Hopfield published an arti-\ncle describing a way of finding accept-\nable solutions for the Travelling Sales-\nman problem by using Hopfield nets.\n\nRenaissance\n\n1986: The backpropagation of error learn-\ning procedure as a generalization of\nthe delta rule was separately devel-\noped and widely published by the Par-\nallel Distributed Processing Group\n[RHW86a]: Non-linearly-separable\nproblems could be solved by multi-\nlayer perceptrons, and Marvin Min-\nsky’s negative evaluations were dis-\nproven at a single blow. At the same\n\ntime a certain kind of fatigue spread\nin the field of artificial intelligence,\ncaused by a series of failures and un-\nfulfilled hopes.\n\nFrom this time on, the development of\nthe field of research has almost been\nexplosive. It can no longer be item-\nized, but some of its results will be\nseen in the following.\n\nExercises\n\nExercise 1. Give one example for each\nof the following topics:\n\n. A book on neural networks or neuroin-\nformatics,\n\n. A collaborative group of a university\nworking with neural networks,\n\n. A software tool realizing neural net-\nworks ("simulator"),\n\n. A company using neural networks,\nand\n\n. A product or service being realized by\nmeans of neural networks.\n\nExercise 2. Show at least four applica-\ntions of technical neural networks: two\nfrom the field of pattern recognition and\ntwo from the field of function approxima-\ntion.\n\nExercise 3. Briefly characterize the four\ndevelopment phases of neural networks\nand give expressive examples for each\nphase.\n\n12 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\nChapter 2\n\nBiological neural networks\nHow do biological systems solve problems? How does a system of neurons\n\nwork? How can we understand its functionality? What are different quantities\nof neurons able to do? Where in the nervous system does information\n\nprocessing occur? A short biological overview of the complexity of simple\nelements of neural information processing followed by some thoughts about\n\ntheir simplification in order to technically adapt them.\n\nBefore we begin to describe the technical\nside of neural networks, it would be use-\nful to briefly discuss the biology of neu-\nral networks and the cognition of living\norganisms – the reader may skip the fol-\nlowing chapter without missing any tech-\nnical information. On the other hand I\nrecommend to read the said excursus if\nyou want to learn something about the\nunderlying neurophysiology and see that\nour small approaches, the technical neural\nnetworks, are only caricatures of nature\n– and how powerful their natural counter-\nparts must be when our small approaches\nare already that effective. Now we want\nto take a brief look at the nervous system\nof vertebrates: We will start with a very\nrough granularity and then proceed with\nthe brain and up to the neural level. For\nfurther reading I want to recommend the\nbooks [CR00,KSJ00], which helped me a\nlot during this chapter.\n\n2.1 The vertebrate nervous\nsystem\n\nThe entire information processing system,\ni.e. the vertebrate nervous system, con-\nsists of the central nervous system and the\nperipheral nervous system, which is only\na first and simple subdivision. In real-\nity, such a rigid subdivision does not make\nsense, but here it is helpful to outline the\ninformation processing in a body.\n\n2.1.1 Peripheral and central\nnervous system\n\nThe peripheral nervous system (PNS)\ncomprises the nerves that are situated out-\nside of the brain or the spinal cord. These\nnerves form a branched and very dense net-\nwork throughout the whole body. The pe-\n\n13\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nripheral nervous system includes, for ex-\nample, the spinal nerves which pass out\nof the spinal cord (two within the level of\neach vertebra of the spine) and supply ex-\ntremities, neck and trunk, but also the cra-\nnial nerves directly leading to the brain.\n\nThe central nervous system (CNS),\nhowever, is the "main-frame" within the\nvertebrate. It is the place where infor-\nmation received by the sense organs are\nstored and managed. Furthermore, it con-\ntrols the inner processes in the body and,\nlast but not least, coordinates the mo-\ntor functions of the organism. The ver-\ntebrate central nervous system consists of\nthe brain and the spinal cord (Fig. 2.1).\nHowever, we want to focus on the brain,\nwhich can - for the purpose of simplifica-\ntion - be divided into four areas (Fig. 2.2\non the next page) to be discussed here.\n\n2.1.2 The cerebrum is responsible\nfor abstract thinking\nprocesses.\n\nThe cerebrum (telencephalon) is one of\nthe areas of the brain that changed most\nduring evolution. Along an axis, running\nfrom the lateral face to the back of the\nhead, this area is divided into two hemi-\nspheres, which are organized in a folded\nstructure. These cerebral hemispheres\nare connected by one strong nerve cord\n("bar") and several small ones. A large\nnumber of neurons are located in the cere-\nbral cortex (cortex) which is approx. 2-\n4 cm thick and divided into different cor-\ntical fields, each having a specific task to Figure 2.1: Illustration of the central nervous\n\nsystem with spinal cord and brain.\n\n14 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.1 The vertebrate nervous system\n\nFigure 2.2: Illustration of the brain. The col-\nored areas of the brain are discussed in the text.\nThe more we turn from abstract information pro-\ncessing to direct reflexive processing, the darker\nthe areas of the brain are colored.\n\nfulfill. Primary cortical fields are re-\nsponsible for processing qualitative infor-\nmation, such as the management of differ-\nent perceptions (e.g. the visual cortex\nis responsible for the management of vi-\nsion). Association cortical fields, how-\never, perform more abstract association\nand thinking processes; they also contain\nour memory.\n\n2.1.3 The cerebellum controls and\ncoordinates motor functions\n\nThe cerebellum is located below the cere-\nbrum, therefore it is closer to the spinal\ncord. Accordingly, it serves less abstract\nfunctions with higher priority: Here, large\nparts of motor coordination are performed,\ni.e., balance and movements are controlled\n\nand errors are continually corrected. For\nthis purpose, the cerebellum has direct\nsensory information about muscle lengths\nas well as acoustic and visual informa-\ntion. Furthermore, it also receives mes-\nsages about more abstract motor signals\ncoming from the cerebrum.\n\nIn the human brain the cerebellum is con-\nsiderably smaller than the cerebrum, but\nthis is rather an exception. In many ver-\ntebrates this ratio is less pronounced. If\nwe take a look at vertebrate evolution, we\nwill notice that the cerebellum is not "too\nsmall" but the cerebum is "too large" (at\nleast, it is the most highly developed struc-\nture in the vertebrate brain). The two re-\nmaining brain areas should also be briefly\ndiscussed: the diencephalon and the brain-\nstem.\n\n2.1.4 The diencephalon controls\nfundamental physiological\nprocesses\n\nThe interbrain (diencephalon) includes\nparts of which only the thalamus will\n\nthalamus\nfilters\nincoming\ndata\n\nbe briefly discussed: This part of the di-\nencephalon mediates between sensory and\nmotor signals and the cerebrum. Particu-\nlarly, the thalamus decides which part of\nthe information is transferred to the cere-\nbrum, so that especially less important\nsensory perceptions can be suppressed at\nshort notice to avoid overloads. Another\npart of the diencephalon is the hypotha-\nlamus, which controls a number of pro-\ncesses within the body. The diencephalon\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 15\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nis also heavily involved in the human cir-\ncadian rhythm ("internal clock") and the\nsensation of pain.\n\n2.1.5 The brainstem connects the\nbrain with the spinal cord and\ncontrols reflexes.\n\nIn comparison with the diencephalon the\nbrainstem or the (truncus cerebri) re-\nspectively is phylogenetically much older.\nRoughly speaking, it is the "extended\nspinal cord" and thus the connection be-\ntween brain and spinal cord. The brain-\nstem can also be divided into different ar-\neas, some of which will be exemplarily in-\ntroduced in this chapter. The functions\nwill be discussed from abstract functions\ntowards more fundamental ones. One im-\nportant component is the pons (=bridge),\na kind of transit station for many nerve sig-\nnals from brain to body and vice versa.\n\nIf the pons is damaged (e.g. by a cere-\nbral infarct), then the result could be the\nlocked-in syndrome – a condition in\nwhich a patient is "walled-in" within his\nown body. He is conscious and aware\nwith no loss of cognitive function, but can-\nnot move or communicate by any means.\nOnly his senses of sight, hearing, smell and\ntaste are generally working perfectly nor-\nmal. Locked-in patients may often be able\nto communicate with others by blinking or\nmoving their eyes.\n\nFurthermore, the brainstem is responsible\nfor many fundamental reflexes, such as the\nblinking reflex or coughing.\n\nAll parts of the nervous system have one\nthing in common: information processing.\nThis is accomplished by huge accumula-\ntions of billions of very similar cells, whose\nstructure is very simple but which com-\nmunicate continuously. Large groups of\nthese cells send coordinated signals and\nthus reach the enormous information pro-\ncessing capacity we are familiar with from\nour brain. We will now leave the level of\nbrain areas and continue with the cellular\nlevel of the body - the level of neurons.\n\n2.2 Neurons are information\nprocessing cells\n\nBefore specifying the functions and pro-\ncesses within a neuron, we will give a\nrough description of neuron functions: A\nneuron is nothing more than a switch with\ninformation input and output. The switch\nwill be activated if there are enough stim-\nuli of other neurons hitting the informa-\ntion input. Then, at the information out-\nput, a pulse is sent to, for example, other\nneurons.\n\n2.2.1 Components of a neuron\n\nNow we want to take a look at the com-\nponents of a neuron (Fig. 2.3 on the fac-\ning page). In doing so, we will follow the\nway the electrical information takes within\nthe neuron. The dendrites of a neuron\nreceive the information by special connec-\ntions, the synapses.\n\n16 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.2 The neuron\n\nFigure 2.3: Illustration of a biological neuron with the components discussed in this text.\n\n2.2.1.1 Synapses weight the individual\nparts of information\n\nIncoming signals from other neurons or\ncells are transferred to a neuron by special\nconnections, the synapses. Such connec-\ntions can usually be found at the dendrites\nof a neuron, sometimes also directly at the\nsoma. We distinguish between electrical\nand chemical synapses.\n\nThe electrical synapse is the simpler\nelectrical\nsynapse:\nsimple\n\nvariant. An electrical signal received by\nthe synapse, i.e. coming from the presy-\nnaptic side, is directly transferred to the\npostsynaptic nucleus of the cell. Thus,\nthere is a direct, strong, unadjustable\nconnection between the signal transmitter\nand the signal receiver, which is, for exam-\nple, relevant to shortening reactions that\nmust be "hard coded" within a living or-\nganism.\n\nThe chemical synapse is the more dis-\ntinctive variant. Here, the electrical cou-\npling of source and target does not take\nplace, the coupling is interrupted by the\nsynaptic cleft. This cleft electrically sep-\narates the presynaptic side from the post-\nsynaptic one. You might think that, never-\ntheless, the information has to flow, so we\nwill discuss how this happens: It is not an\nelectrical, but a chemical process. On the\npresynaptic side of the synaptic cleft the\nelectrical signal is converted into a chemi-\ncal signal, a process induced by chemical\ncues released there (the so-called neuro-\ntransmitters). These neurotransmitters\ncross the synaptic cleft and transfer the\ninformation into the nucleus of the cell\n(this is a very simple explanation, but later\non we will see how this exactly works),\nwhere it is reconverted into electrical in-\nformation. The neurotransmitters are de-\ngraded very fast, so that it is possible to re-\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 17\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nlease very precise information pulses here,\ntoo.\n\nIn spite of the more complex function-\ncemical\nsynapse\nis more\ncomplex\nbut also\n\nmore\npowerful\n\ning, the chemical synapse has - compared\nwith the electrical synapse - utmost advan-\ntages:\n\nOne-way connection: A chemical\nsynapse is a one-way connection.\nDue to the fact that there is no direct\nelectrical connection between the\npre- and postsynaptic area, electrical\npulses in the postsynaptic area\ncannot flash over to the presynaptic\narea.\n\nAdjustability: There is a large number of\ndifferent neurotransmitters that can\nalso be released in various quantities\nin a synaptic cleft. There are neuro-\ntransmitters that stimulate the post-\nsynaptic cell nucleus, and others that\nslow down such stimulation. Some\nsynapses transfer a strongly stimulat-\ning signal, some only weakly stimu-\nlating ones. The adjustability varies\na lot, and one of the central points\nin the examination of the learning\nability of the brain is, that here the\nsynapses are variable, too. That is,\nover time they can form a stronger or\nweaker connection.\n\n2.2.1.2 Dendrites collect all parts of\ninformation\n\nDendrites branch like trees from the cell\nnucleus of the neuron (which is called\nsoma) and receive electrical signals from\n\nmany different sources, which are then\ntransferred into the nucleus of the cell.\nThe amount of branching dendrites is also\ncalled dendrite tree.\n\n2.2.1.3 In the soma the weighted\ninformation is accumulated\n\nAfter the cell nucleus (soma) has re-\nceived a plenty of activating (=stimulat-\ning) and inhibiting (=diminishing) signals\nby synapses or dendrites, the soma accu-\nmulates these signals. As soon as the ac-\ncumulated signal exceeds a certain value\n(called threshold value), the cell nucleus\nof the neuron activates an electrical pulse\nwhich then is transmitted to the neurons\nconnected to the current one.\n\n2.2.1.4 The axon transfers outgoing\npulses\n\nThe pulse is transferred to other neurons\nby means of the axon. The axon is a\nlong, slender extension of the soma. In\nan extreme case, an axon can stretch up\nto one meter (e.g. within the spinal cord).\nThe axon is electrically isolated in order\nto achieve a better conduction of the elec-\ntrical signal (we will return to this point\nlater on) and it leads to dendrites, which\ntransfer the information to, for example,\nother neurons. So now we are back at the\nbeginning of our description of the neuron\nelements. An axon can, however, transfer\ninformation to other kinds of cells in order\nto control them.\n\n18 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.2 The neuron\n\n2.2.2 Electrochemical processes in\nthe neuron and its\ncomponents\n\nAfter having pursued the path of an elec-\ntrical signal from the dendrites via the\nsynapses to the nucleus of the cell and\nfrom there via the axon into other den-\ndrites, we now want to take a small step\nfrom biology towards technology. In doing\nso, a simplified introduction of the electro-\nchemical information processing should be\nprovided.\n\n2.2.2.1 Neurons maintain electrical\nmembrane potential\n\nOne fundamental aspect is the fact that\ncompared to their environment the neu-\nrons show a difference in electrical charge,\na potential. In the membrane (=enve-\nlope) of the neuron the charge is different\nfrom the charge on the outside. This dif-\nference in charge is a central concept that\nis important to understand the processes\nwithin the neuron. The difference is called\nmembrane potential. The membrane\npotential, i.e., the difference in charge, is\ncreated by several kinds of charged atoms\n(ions), whose concentration varies within\nand outside of the neuron. If we penetrate\nthe membrane from the inside outwards,\nwe will find certain kinds of ions more of-\nten or less often than on the inside. This\ndescent or ascent of concentration is called\na concentration gradient.\n\nLet us first take a look at the membrane\npotential in the resting state of the neu-\n\nron, i.e., we assume that no electrical sig-\nnals are received from the outside. In this\ncase, the membrane potential is −70 mV.\nSince we have learned that this potential\ndepends on the concentration gradients of\nvarious ions, there is of course the central\nquestion of how to maintain these concen-\ntration gradients: Normally, diffusion pre-\ndominates and therefore each ion is eager\nto decrease concentration gradients and\nto spread out evenly. If this happens,\nthe membrane potential will move towards\n0 mV, so finally there would be no mem-\nbrane potential anymore. Thus, the neu-\nron actively maintains its membrane po-\ntential to be able to process information.\nHow does this work?\n\nThe secret is the membrane itself, which is\npermeable to some ions, but not for others.\nTo maintain the potential, various mecha-\nnisms are in progress at the same time:\n\nConcentration gradient: As described\nabove the ions try to be as uniformly\ndistributed as possible. If the\nconcentration of an ion is higher on\nthe inside of the neuron than on\nthe outside, it will try to diffuse\nto the outside and vice versa.\nThe positively charged ion K+\n\n(potassium) occurs very frequently\nwithin the neuron but less frequently\noutside of the neuron, and therefore\nit slowly diffuses out through the\nneuron’s membrane. But another\ngroup of negative ions, collectively\ncalled A−, remains within the neuron\nsince the membrane is not permeable\nto them. Thus, the inside of the\nneuron becomes negatively charged.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 19\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nNegative A ions remain, positive K\nions disappear, and so the inside of\nthe cell becomes more negative. The\nresult is another gradient.\n\nElectrical Gradient: The electrical gradi-\nent acts contrary to the concentration\ngradient. The intracellular charge is\nnow very strong, therefore it attracts\npositive ions: K+ wants to get back\ninto the cell.\n\nIf these two gradients were now left alone,\nthey would eventually balance out, reach\na steady state, and a membrane poten-\ntial of −85 mV would develop. But we\nwant to achieve a resting membrane po-\ntential of −70 mV, thus there seem to ex-\nist some disturbances which prevent this.\nFurthermore, there is another important\nion, Na+ (sodium), for which the mem-\nbrane is not very permeable but which,\nhowever, slowly pours through the mem-\nbrane into the cell. As a result, the sodium\nis driven into the cell all the more: On the\none hand, there is less sodium within the\nneuron than outside the neuron. On the\nother hand, sodium is positively charged\nbut the interior of the cell has negative\ncharge, which is a second reason for the\nsodium wanting to get into the cell.\n\nDue to the low diffusion of sodium into the\ncell the intracellular sodium concentration\nincreases. But at the same time the inside\nof the cell becomes less negative, so that\nK+ pours in more slowly (we can see that\nthis is a complex mechanism where every-\nthing is influenced by everything). The\nsodium shifts the intracellular equilibrium\nfrom negative to less negative, compared\n\nwith its environment. But even with these\ntwo ions a standstill with all gradients be-\ning balanced out could still be achieved.\nNow the last piece of the puzzle gets into\nthe game: a "pump" (or rather, the protein\nATP) actively transports ions against the\ndirection they actually want to take!\n\nSodium is actively pumped out of the cell,\nalthough it tries to get into the cell\nalong the concentration gradient and\nthe electrical gradient.\n\nPotassium, however, diffuses strongly out\nof the cell, but is actively pumped\nback into it.\n\nFor this reason the pump is also called\nsodium-potassium pump. The pump\nmaintains the concentration gradient for\nthe sodium as well as for the potassium,\nso that some sort of steady state equilib-\nrium is created and finally the resting po-\ntential is −70 mV as observed. All in all\nthe membrane potential is maintained by\nthe fact that the membrane is imperme-\nable to some ions and other ions are ac-\ntively pumped against the concentration\nand electrical gradients. Now that we\nknow that each neuron has a membrane\npotential we want to observe how a neu-\nron receives and transmits signals.\n\n2.2.2.2 The neuron is activated by\nchanges in the membrane\npotential\n\nAbove we have learned that sodium and\npotassium can diffuse through the mem-\nbrane - sodium slowly, potassium faster.\n\n20 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.2 The neuron\n\nThey move through channels within the\nmembrane, the sodium and potassium\nchannels. In addition to these per-\nmanently open channels responsible for\ndiffusion and balanced by the sodium-\npotassium pump, there also exist channels\nthat are not always open but which only\nresponse "if required". Since the opening\nof these channels changes the concentra-\ntion of ions within and outside of the mem-\nbrane, it also changes the membrane po-\ntential.\n\nThese controllable channels are opened as\nsoon as the accumulated received stimulus\nexceeds a certain threshold. For example,\nstimuli can be received from other neurons\nor have other causes. There exist, for ex-\nample, specialized forms of neurons, the\nsensory cells, for which a light incidence\ncould be such a stimulus. If the incom-\ning amount of light exceeds the threshold,\ncontrollable channels are opened.\n\nThe said threshold (the threshold poten-\ntial) lies at about −55 mV. As soon as the\nreceived stimuli reach this value, the neu-\nron is activated and an electrical signal,\nan action potential, is initiated. Then\nthis signal is transmitted to the cells con-\nnected to the observed neuron, i.e. the\ncells "listen" to the neuron. Now we want\nto take a closer look at the different stages\nof the action potential (Fig. 2.4 on the next\npage):\n\nResting state: Only the permanently\nopen sodium and potassium channels\nare permeable. The membrane\npotential is at −70 mV and actively\nkept there by the neuron.\n\nStimulus up to the threshold: A stimu-\nlus opens channels so that sodium\ncan pour in. The intracellular charge\nbecomes more positive. As soon as\nthe membrane potential exceeds the\nthreshold of −55 mV, the action po-\ntential is initiated by the opening of\nmany sodium channels.\n\nDepolarization: Sodium is pouring in. Re-\nmember: Sodium wants to pour into\nthe cell because there is a lower in-\ntracellular than extracellular concen-\ntration of sodium. Additionally, the\ncell is dominated by a negative en-\nvironment which attracts the posi-\ntive sodium ions. This massive in-\nflux of sodium drastically increases\nthe membrane potential - up to ap-\nprox. +30 mV - which is the electrical\npulse, i.e., the action potential.\n\nRepolarization: Now the sodium channels\nare closed and the potassium channels\nare opened. The positively charged\nions want to leave the positive inte-\nrior of the cell. Additionally, the intra-\ncellular concentration is much higher\nthan the extracellular one, which in-\ncreases the efflux of ions even more.\nThe interior of the cell is once again\nmore negatively charged than the ex-\nterior.\n\nHyperpolarization: Sodium as well as\npotassium channels are closed again.\nAt first the membrane potential is\nslightly more negative than the rest-\ning potential. This is due to the\nfact that the potassium channels close\nmore slowly. As a result, (positively\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 21\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nFigure 2.4: Initiation of action potential over time.\n\n22 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.2 The neuron\n\ncharged) potassium effuses because of\nits lower extracellular concentration.\nAfter a refractory period of 1 − 2\nms the resting state is re-established\nso that the neuron can react to newly\napplied stimuli with an action poten-\ntial. In simple terms, the refractory\nperiod is a mandatory break a neu-\nron has to take in order to regenerate.\nThe shorter this break is, the more\noften a neuron can fire per time.\n\nThen the resulting pulse is transmitted by\nthe axon.\n\n2.2.2.3 In the axon a pulse is\nconducted in a saltatory way\n\nWe have already learned that the axon\nis used to transmit the action potential\nacross long distances (remember: You will\nfind an illustration of a neuron including\nan axon in Fig. 2.3 on page 17). The axon\nis a long, slender extension of the soma.\nIn vertebrates it is normally coated by a\nmyelin sheath that consists of Schwann\ncells (in the PNS) or oligodendrocytes\n(in the CNS) 1, which insulate the axon\nvery well from electrical activity. At a dis-\ntance of 0.1−2mm there are gaps between\nthese cells, the so-called nodes of Ran-\nvier. The said gaps appear where one in-\nsulate cell ends and the next one begins.\nIt is obvious that at such a node the axon\nis less insulated.\n1 Schwann cells as well as oligodendrocytes are vari-\neties of the glial cells. There are about 50 times\nmore glial cells than neurons: They surround the\nneurons (glia = glue), insulate them from each\nother, provide energy, etc.\n\nNow you may assume that these less in-\nsulated nodes are a disadvantage of the\naxon - however, they are not. At the\nnodes, mass can be transferred between\nthe intracellular and extracellular area, a\ntransfer that is impossible at those parts\nof the axon which are situated between\ntwo nodes (internodes) and therefore in-\nsulated by the myelin sheath. This mass\ntransfer permits the generation of signals\nsimilar to the generation of the action po-\ntential within the soma. The action po-\ntential is transferred as follows: It does\nnot continuously travel along the axon but\njumps from node to node. Thus, a series\nof depolarization travels along the nodes of\nRanvier. One action potential initiates the\nnext one, and mostly even several nodes\nare active at the same time here. The\npulse "jumping" from node to node is re-\nsponsible for the name of this pulse con-\nductor: saltatory conductor.\n\nObviously, the pulse will move faster if its\njumps are larger. Axons with large in-\nternodes (2 mm) achieve a signal disper-\nsion of approx. 180 meters per second.\nHowever, the internodes cannot grow in-\ndefinitely, since the action potential to be\ntransferred would fade too much until it\nreaches the next node. So the nodes have\na task, too: to constantly amplify the sig-\nnal. The cells receiving the action poten-\ntial are attached to the end of the axon –\noften connected by dendrites and synapses.\nAs already indicated above, the action po-\ntentials are not only generated by informa-\ntion received by the dendrites from other\nneurons.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 23\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\n2.3 Receptor cells are\nmodified neurons\n\nAction potentials can also be generated by\nsensory information an organism receives\nfrom its environment through its sensory\ncells. Specialized receptor cells are able\nto perceive specific stimulus energies such\nas light, temperature and sound or the ex-\nistence of certain molecules (like, for exam-\nple, the sense of smell). This is working\nbecause of the fact that these sensory cells\nare actually modified neurons. They do\nnot receive electrical signals via dendrites\nbut the existence of the stimulus being\nspecific for the receptor cell ensures that\nthe ion channels open and an action po-\ntential is developed. This process of trans-\nforming stimulus energy into changes in\nthe membrane potential is called sensory\ntransduction. Usually, the stimulus en-\nergy itself is too weak to directly cause\nnerve signals. Therefore, the signals are\namplified either during transduction or by\nmeans of the stimulus-conducting ap-\nparatus. The resulting action potential\ncan be processed by other neurons and is\nthen transmitted into the thalamus, which\nis, as we have already learned, a gateway\nto the cerebral cortex and therefore can re-\nject sensory impressions according to cur-\nrent relevance and thus prevent an abun-\ndance of information to be managed.\n\n2.3.1 There are different receptor\ncells for various types of\nperceptions\n\nPrimary receptors transmit their pulses\ndirectly to the nervous system. A good\nexample for this is the sense of pain.\nHere, the stimulus intensity is propor-\ntional to the amplitude of the action po-\ntential. Technically, this is an amplitude\nmodulation.\n\nSecondary receptors, however, continu-\nously transmit pulses. These pulses con-\ntrol the amount of the related neurotrans-\nmitter, which is responsible for transfer-\nring the stimulus. The stimulus in turn\ncontrols the frequency of the action poten-\ntial of the receiving neuron. This process\nis a frequency modulation, an encoding of\nthe stimulus, which allows to better per-\nceive the increase and decrease of a stimu-\nlus.\n\nThere can be individual receptor cells or\ncells forming complex sensory organs (e.g.\neyes or ears). They can receive stimuli\nwithin the body (by means of the intero-\nceptors) as well as stimuli outside of the\nbody (by means of the exteroceptors).\n\nAfter having outlined how information is\nreceived from the environment, it will be\ninteresting to look at how the information\nis processed.\n\n24 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.3 Receptor cells\n\n2.3.2 Information is processed on\nevery level of the nervous\nsystem\n\nThere is no reason to believe that all re-\nceived information is transmitted to the\nbrain and processed there, and that the\nbrain ensures that it is "output" in the\nform of motor pulses (the only thing an\norganism can actually do within its envi-\nronment is to move). The information pro-\ncessing is entirely decentralized. In order\nto illustrate this principle, we want to take\na look at some examples, which leads us\nagain from the abstract to the fundamen-\ntal in our hierarchy of information process-\ning.\n\n. It is certain that information is pro-\ncessed in the cerebrum, which is the\nmost developed natural information\nprocessing structure.\n\n. The midbrain and the thalamus,\nwhich serves – as we have already\nlearned – as a gateway to the cere-\nbral cortex, are situated much lower\nin the hierarchy. The filtering of in-\nformation with respect to the current\nrelevance executed by the midbrain\nis a very important method of infor-\nmation processing, too. But even the\nthalamus does not receive any prepro-\ncessed stimuli from the outside. Now\nlet us continue with the lowest level,\nthe sensory cells.\n\n. On the lowest level, i.e. at the recep-\ntor cells, the information is not only\nreceived and transferred but directly\nprocessed. One of the main aspects of\n\nthis subject is to prevent the transmis-\nsion of "continuous stimuli" to the cen-\ntral nervous system because of sen-\nsory adaptation: Due to continu-\nous stimulation many receptor cells\nautomatically become insensitive to\nstimuli. Thus, receptor cells are not\na direct mapping of specific stimu-\nlus energy onto action potentials but\ndepend on the past. Other sensors\nchange their sensitivity according to\nthe situation: There are taste recep-\ntors which respond more or less to the\nsame stimulus according to the nutri-\ntional condition of the organism.\n\n. Even before a stimulus reaches the\nreceptor cells, information processing\ncan already be executed by a preced-\ning signal carrying apparatus, for ex-\nample in the form of amplification:\nThe external and the internal ear\nhave a specific shape to amplify the\nsound, which also allows – in asso-\nciation with the sensory cells of the\nsense of hearing – the sensory stim-\nulus only to increase logarithmically\nwith the intensity of the heard sig-\nnal. On closer examination, this is\nnecessary, since the sound pressure of\nthe signals for which the ear is con-\nstructed can vary over a wide expo-\nnential range. Here, a logarithmic\nmeasurement is an advantage. Firstly,\nan overload is prevented and secondly,\nthe fact that the intensity measure-\nment of intensive signals will be less\nprecise, doesn’t matter as well. If a jet\nfighter is starting next to you, small\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 25\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nchanges in the noise level can be ig-\nnored.\n\nJust to get a feeling for sensory organs\nand information processing in the organ-\nism, we will briefly describe "usual" light\nsensing organs, i.e. organs often found in\nnature. For the third light sensing organ\ndescribed below, the single lens eye, we\nwill discuss the information processing in\nthe eye.\n\n2.3.3 An outline of common light\nsensing organs\n\nFor many organisms it turned out to be ex-\ntremely useful to be able to perceive elec-\ntromagnetic radiation in certain regions of\nthe spectrum. Consequently, sensory or-\ngans have been developed which can de-\ntect such electromagnetic radiation and\nthe wavelength range of the radiation per-\nceivable by the human eye is called visible\nrange or simply light. The different wave-\nlengths of this electromagnetic radiation\nare perceived by the human eye as differ-\nent colors. The visible range of the elec-\ntromagnetic radiation is different for each\norganism. Some organisms cannot see the\ncolors (=wavelength ranges) we can see,\nothers can even perceive additional wave-\nlength ranges (e.g. in the UV range). Be-\nfore we begin with the human being – in\norder to get a broader knowledge of the\nsense of sight– we briefly want to look at\ntwo organs of sight which, from an evolu-\ntionary point of view, exist much longer\nthan the human.\n\n2.3.3.1 Compound eyes and pinhole\neyes only provide high temporal\nor spatial resolution\n\nLet us first take a look at the so-called\ncompound eye (Fig. 2.5 on the next\npage), which is, for example, common in\ninsects and crustaceans. The compound\n\nCompound eye:\nhigh temp.,\nlow\nspatial\nresolution\n\neye consists of a great number of small,\nindividual eyes. If we look at the com-\npound eye from the outside, the individ-\nual eyes are clearly visible and arranged\nin a hexagonal pattern. Each individual\neye has its own nerve fiber which is con-\nnected to the insect brain. Since the indi-\nvidual eyes can be distinguished, it is ob-\nvious that the number of pixels, i.e. the\nspatial resolution, of compound eyes must\nbe very low and the image is blurred. But\ncompound eyes have advantages, too, espe-\ncially for fast-flying insects. Certain com-\npound eyes process more than 300 images\nper second (to the human eye, however,\nmovies with 25 images per second appear\nas a fluent motion).\n\nPinhole eyes are, for example, found in\noctopus species and work – as you can\nguess – similar to a pinhole camera. A\n\npinhole\ncamera:\nhigh spat.,\nlow\ntemporal\nresolution\n\npinhole eye has a very small opening for\nlight entry, which projects a sharp image\nonto the sensory cells behind. Thus, the\nspatial resolution is much higher than in\nthe compound eye. But due to the very\nsmall opening for light entry the resulting\nimage is less bright.\n\n26 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.3 Receptor cells\n\nFigure 2.5: Compound eye of a robber fly\n\n2.3.3.2 Single lens eyes combine the\nadvantages of the other two\neye types, but they are more\ncomplex\n\nThe light sensing organ common in verte-\nbrates is the single lense eye. The result-\ning image is a sharp, high-resolution image\nof the environment at high or variable light\nintensity. On the other hand it is more\ncomplex. Similar to the pinhole eye the\nlight enters through an opening (pupil)\nand is projected onto a layer of sensory\ncells in the eye. (retina). But in contrast\n\nSingle\nlense eye:\n\nhigh temp.\nand spat.\nresolution\n\nto the pinhole eye, the size of the pupil can\nbe adapted to the lighting conditions (by\nmeans of the iris muscle, which expands\nor contracts the pupil). These differences\nin pupil dilation require to actively focus\nthe image. Therefore, the single lens eye\ncontains an additional adjustable lens.\n\n2.3.3.3 The retina does not only\nreceive information but is also\nresponsible for information\nprocessing\n\nThe light signals falling on the eye are\nreceived by the retina and directly pre-\nprocessed by several layers of information-\nprocessing cells. We want to briefly dis-\ncuss the different steps of this informa-\ntion processing and in doing so, we follow\nthe way of the information carried by the\nlight:\n\nPhotoreceptors receive the light signal\nund cause action potentials (there\nare different receptors for different\ncolor components and light intensi-\nties). These receptors are the real\nlight-receiving part of the retina and\nthey are sensitive to such an extent\nthat only one single photon falling\non the retina can cause an action po-\ntential. Then several photoreceptors\ntransmit their signals to one single\n\nbipolar cell. This means that here the in-\nformation has already been summa-\nrized. Finally, the now transformed\nlight signal travels from several bipo-\nlar cells 2 into\n\nganglion cells. Various bipolar cells can\ntransmit their information to one gan-\nglion cell. The higher the number\nof photoreceptors that affect the gan-\nglion cell, the larger the field of per-\nception, the receptive field, which\ncovers the ganglions – and the less\n\n2 There are different kinds of bipolar cells, as well,\nbut to discuss all of them would go too far.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 27\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nsharp is the image in the area of this\nganglion cell. So the information is\nalready reduced directly in the retina\nand the overall image is, for exam-\nple, blurred in the peripheral field\nof vision. So far, we have learned\nabout the information processing in\nthe retina only as a top-down struc-\nture. Now we want to take a look at\nthe\n\nhorizontal and amacrine cells. These\ncells are not connected from the\nfront backwards but laterally. They\nallow the light signals to influence\nthemselves laterally directly during\nthe information processing in the\nretina – a much more powerful\nmethod of information processing\nthan compressing and blurring.\nWhen the horizontal cells are excited\nby a photoreceptor, they are able to\nexcite other nearby photoreceptors\nand at the same time inhibit more\ndistant bipolar cells and receptors.\nThis ensures the clear perception of\noutlines and bright points. Amacrine\ncells can further intensify certain\nstimuli by distributing information\nfrom bipolar cells to several ganglion\ncells or by inhibiting ganglions.\n\nThese first steps of transmitting visual in-\nformation to the brain show that informa-\ntion is processed from the first moment the\ninformation is received and, on the other\nhand, is processed in parallel within mil-\nlions of information-processing cells. The\nsystem’s power and resistance to errors\nis based upon this massive division of\nwork.\n\n2.4 The amount of neurons in\nliving organisms at\ndifferent stages of\ndevelopment\n\nAn overview of different organisms and\ntheir neural capacity (in large part from\n[RD05]):\n\n302 neurons are required by the nervous\nsystem of a nematode worm, which\nserves as a popular model organism\nin biology. Nematodes live in the soil\nand feed on bacteria.\n\n104 neurons make an ant (To simplify\nmatters we neglect the fact that some\nant species also can have more or less\nefficient nervous systems). Due to the\nuse of different attractants and odors,\nants are able to engage in complex\nsocial behavior and form huge states\nwith millions of individuals. If you re-\ngard such an ant state as an individ-\nual, it has a cognitive capacity similar\nto a chimpanzee or even a human.\n\nWith 105 neurons the nervous system of\na fly can be constructed. A fly can\nevade an object in real-time in three-\ndimensional space, it can land upon\nthe ceiling upside down, has a consid-\nerable sensory system because of com-\npound eyes, vibrissae, nerves at the\nend of its legs and much more. Thus,\na fly has considerable differential and\nintegral calculus in high dimensions\nimplemented "in hardware". We all\nknow that a fly is not easy to catch.\nOf course, the bodily functions are\n\n28 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.4 The amount of neurons in living organisms\n\nalso controlled by neurons, but these\nshould be ignored here.\n\nWith 0.8 · 106 neurons we have enough\ncerebral matter to create a honeybee.\nHoneybees build colonies and have\namazing capabilities in the field of\naerial reconnaissance and navigation.\n\n4 · 106 neurons result in a mouse, and\nhere the world of vertebrates already\nbegins.\n\n1.5 · 107 neurons are sufficient for a rat,\nan animal which is denounced as be-\ning extremely intelligent and are of-\nten used to participate in a variety\nof intelligence tests representative for\nthe animal world. Rats have an ex-\ntraordinary sense of smell and orien-\ntation, and they also show social be-\nhavior. The brain of a frog can be\npositioned within the same dimension.\nThe frog has a complex build with\nmany functions, it can swim and has\nevolved complex behavior. A frog\ncan continuously target the said fly\nby means of his eyes while jumping\nin three-dimensional space and and\ncatch it with its tongue with consid-\nerable probability.\n\n5 · 107 neurons make a bat. The bat can\nnavigate in total darkness through a\nroom, exact up to several centime-\nters, by only using their sense of hear-\ning. It uses acoustic signals to localize\nself-camouflaging insects (e.g. some\nmoths have a certain wing structure\nthat reflects less sound waves and the\necho will be small) and also eats its\nprey while flying.\n\n1.6 · 108 neurons are required by the\nbrain of a dog, companion of man for\nages. Now take a look at another pop-\nular companion of man:\n\n3 · 108 neurons can be found in a cat,\nwhich is about twice as much as in\na dog. We know that cats are very\nelegant, patient carnivores that can\nshow a variety of behaviors. By the\nway, an octopus can be positioned\nwithin the same magnitude. Only\nvery few people know that, for exam-\nple, in labyrinth orientation the octo-\npus is vastly superior to the rat.\n\nFor 6 · 109 neurons you already get a\nchimpanzee, one of the animals being\nvery similar to the human.\n\n1011 neurons make a human. Usually,\nthe human has considerable cognitive\ncapabilities, is able to speak, to ab-\nstract, to remember and to use tools\nas well as the knowledge of other hu-\nmans to develop advanced technolo-\ngies and manifold social structures.\n\nWith 2 · 1011 neurons there are nervous\nsystems having more neurons than\nthe human nervous system. Here we\nshould mention elephants and certain\nwhale species.\n\nOur state-of-the-art computers are not\nable to keep up with the aforementioned\nprocessing power of a fly. Recent research\nresults suggest that the processes in ner-\nvous systems might be vastly more pow-\nerful than people thought until not long\nago: Michaeva et al. describe a separate,\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 29\n\n\n\nChapter 2 Biological neural networks dkriesel.com\n\nsynapse-integrated information way of in-\nformation processing [MBW+10]. Poster-\nity will show if they are right.\n\n2.5 Transition to technical\nneurons: neural networks\nare a caricature of biology\n\nHow do we change from biological neural\nnetworks to the technical ones? Through\nradical simplification. I want to briefly\nsummarize the conclusions relevant for the\ntechnical part:\n\nWe have learned that the biological neu-\nrons are linked to each other in a weighted\nway and when stimulated they electrically\ntransmit their signal via the axon. From\nthe axon they are not directly transferred\nto the succeeding neurons, but they first\nhave to cross the synaptic cleft where the\nsignal is changed again by variable chem-\nical processes. In the receiving neuron\nthe various inputs that have been post-\nprocessed in the synaptic cleft are summa-\nrized or accumulated to one single pulse.\nDepending on how the neuron is stimu-\nlated by the cumulated input, the neuron\nitself emits a pulse or not – thus, the out-\nput is non-linear and not proportional to\nthe cumulated input. Our brief summary\ncorresponds exactly with the few elements\nof biological neural networks we want to\ntake over into the technical approxima-\ntion:\n\nVectorial input: The input of technical\nneurons consists of many components,\n\ntherefore it is a vector. In nature a\nneuron receives pulses of 103 to 104\n\nother neurons on average.\n\nScalar output: The output of a neuron is\na scalar, which means that the neu-\nron only consists of one component.\nSeveral scalar outputs in turn form\nthe vectorial input of another neuron.\nThis particularly means that some-\nwhere in the neuron the various input\ncomponents have to be summarized in\nsuch a way that only one component\nremains.\n\nSynapses change input: In technical neu-\nral networks the inputs are prepro-\ncessed, too. They are multiplied by\na number (the weight) – they are\nweighted. The set of such weights rep-\nresents the information storage of a\nneural network – in both biological\noriginal and technical adaptation.\n\nAccumulating the inputs: In biology, the\ninputs are summarized to a pulse ac-\ncording to the chemical change, i.e.,\nthey are accumulated – on the techni-\ncal side this is often realized by the\nweighted sum, which we will get to\nknow later on. This means that after\naccumulation we continue with only\none value, a scalar, instead of a vec-\ntor.\n\nNon-linear characteristic: The input of\nour technical neurons is also not pro-\nportional to the output.\n\nAdjustable weights: The weights weight-\ning the inputs are variable, similar to\n\n30 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 2.5 Technical neurons as caricature of biology\n\nthe chemical processes at the synap-\ntic cleft. This adds a great dynamic\nto the network because a large part of\nthe "knowledge" of a neural network is\nsaved in the weights and in the form\nand power of the chemical processes\nin a synaptic cleft.\n\nSo our current, only casually formulated\nand very simple neuron model receives a\nvectorial input\n\n~x,\n\nwith components xi. These are multiplied\nby the appropriate weights wi and accumu-\nlated: ∑\n\ni\n\nwixi.\n\nThe aforementioned term is called\nweighted sum. Then the nonlinear\nmapping f defines the scalar output y:\n\ny = f\n\n(∑\ni\n\nwixi\n\n)\n.\n\nAfter this transition we now want to spec-\nify more precisely our neuron model and\nadd some odds and ends. Afterwards we\nwill take a look at how the weights can be\nadjusted.\n\nExercises\n\nExercise 4. It is estimated that a hu-\nman brain consists of approx. 1011 nerve\ncells, each of which has about 103 to 104\n\nsynapses. For this exercise we assume 103\n\nsynapses per neuron. Let us further as-\nsume that a single synapse could save 4\n\nbits of information. Naïvely calculated:\nHow much storage capacity does the brain\nhave? Note: The information which neu-\nron is connected to which other neuron is\nalso important.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 31\n\n\n\n\n\nChapter 3\n\nComponents of artificial neural networks\nFormal definitions and colloquial explanations of the components that realize\nthe technical adaptations of biological neural networks. Initial descriptions of\n\nhow to combine these components into a neural network.\n\nThis chapter contains the formal defini-\ntions for most of the neural network com-\nponents used later in the text. After this\nchapter you will be able to read the indi-\nvidual chapters of this work without hav-\ning to know the preceding ones (although\nthis would be useful).\n\n3.1 The concept of time in\nneural networks\n\nIn some definitions of this text we use the\nterm time or the number of cycles of the\nneural network, respectively. Time is di-\nvided into discrete time steps:\n\ndiscrete\ntime steps\n\nDefinition 3.1 (The concept of time).\nThe current time (present time) is referred\nto as (t), the next time step as (t + 1),\n\n(t)I the preceding one as (t − 1). All other\ntime steps are referred to analogously. If in\nthe following chapters several mathemati-\ncal variables (e.g. netj or oi) refer to a\n\ncertain point in time, the notation will be,\nfor example, netj(t− 1) or oi(t).\n\nFrom a biological point of view this is, of\ncourse, not very plausible (in the human\nbrain a neuron does not wait for another\none), but it significantly simplifies the im-\nplementation.\n\n3.2 Components of neural\nnetworks\n\nA technical neural network consists of sim-\nple processing units, the neurons, and\ndirected, weighted connections between\nthose neurons. Here, the strength of a\nconnection (or the connecting weight) be-\n\n33\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\ntween two neurons i and j is referred to as\nwi,j\n\n1.\n\nDefinition 3.2 (Neural network). A\nneural network is a sorted triple\n(N,V,w) with two sets N , V and a func-\ntion w, where N is the set of neurons and\nV a set {(i, j)|i, j ∈ N} whose elements are\ncalled connections between neuron i and\nneuron j. The function w : V → R defines\n\nn. network\n= neurons\n+ weighted\nconnection\n\nthe weights, where w((i, j)), the weight of\nthe connection between neuron i and neu-\nron j, is shortened to wi,j . Depending on\n\nwi,jI the point of view it is either undefined or\n0 for connections that do not exist in the\nnetwork.\n\nSNIPE: In Snipe, an instance of the class\nNeuralNetworkDescriptor is created in\nthe first place. The descriptor object\nroughly outlines a class of neural networks,\ne.g. it defines the number of neuron lay-\ners in a neural network. In a second step,\nthe descriptor object is used to instantiate\nan arbitrary number of NeuralNetwork ob-\njects. To get started with Snipe program-\nming, the documentations of exactly these\ntwo classes are – in that order – the right\nthing to read. The presented layout involv-\ning descriptor and dependent neural net-\nworks is very reasonable from the imple-\nmentation point of view, because it is en-\nables to create and maintain general param-\neters of even very large sets of similar (but\nnot neccessarily equal) networks.\n\nSo the weights can be implemented in a\nsquare weight matrix W or, optionally,\nin a weight vector W with the row num-\n\nWI\n1 Note: In some of the cited literature i and j could\nbe interchanged in wi,j . Here, a consistent stan-\ndard does not exist. But in this text I try to use\nthe notation I found more frequently and in the\nmore significant citations.\n\nber of the matrix indicating where the con-\nnection begins, and the column number of\nthe matrix indicating, which neuron is the\ntarget. Indeed, in this case the numeric\n0 marks a non-existing connection. This\nmatrix representation is also called Hin-\nton diagram2.\n\nThe neurons and connections comprise the\nfollowing components and variables (I’m\nfollowing the path of the data within a\nneuron, which is according to fig. 3.1 on\nthe facing page in top-down direction):\n\n3.2.1 Connections carry information\nthat is processed by neurons\n\nData are transferred between neurons via\nconnections with the connecting weight be-\ning either excitatory or inhibitory. The\ndefinition of connections has already been\nincluded in the definition of the neural net-\nwork.\n\nSNIPE: Connection weights\ncan be set using the method\nNeuralNetwork.setSynapse.\n\n3.2.2 The propagation function\nconverts vector inputs to\nscalar network inputs\n\nLooking at a neuron j, we will usually find\na lot of neurons with a connection to j, i.e.\nwhich transfer their output to j.\n\n2 Note that, here again, in some of the cited liter-\nature axes and rows could be interchanged. The\npublished literature is not consistent here, as well.\n\n34 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.2 Components of neural networks\n\n \n\nPropagierungsfunktion \n(oft gewichtete Summe, verarbeitet \n\nEingaben zur Netzeingabe) \n\nAusgabefunktion \n(Erzeugt aus Aktivierung die Ausgabe, \n\nist oft Identität) \n\nAktivierungsfunktion \n(Erzeugt aus Netzeingabe und alter \n\nAktivierung die neue Aktivierung)\n\nEingaben anderer \nNeuronen Netzeingabe\n\nAktivierung Ausgabe zu \nanderen Neuronen \n\nPropagation function \n(often weighted sum, transforms \n\noutputs of other neurons to net input) \n\nOutput function \n(often identity function, transforms \n\nactivation to output for other neurons) \n\nActivation function \n(Transforms net input and sometimes \n\nold activation to new activation)\n\nData Input of \nother Neurons \n\nNetwork Input\n\nActivation\n\nData Output to \nother Neurons \n\nFigure 3.1: Data processing of a neuron. The\nactivation function of a neuron implies the\nthreshold value.\n\nFor a neuron j the propagation func-\ntion receives the outputs oi1 , . . . , oin of\nother neurons i1, i2, . . . , in (which are con-\nnected to j), and transforms them in con- manages\n\ninputssideration of the connecting weights wi,j\ninto the network input netj that can be fur-\nther processed by the activation function.\nThus, the network input is the result of\nthe propagation function.\n\nDefinition 3.3 (Propagation func-\ntion and network input). Let\nI = {i1, i2, . . . , in} be the set of neurons,\nsuch that ∀z ∈ {1, . . . , n} : ∃wiz ,j . Then\nthe network input of j, called netj , is\ncalculated by the propagation function\nfprop as follows:\n\nnetj = fprop(oi1 , . . . , oin , wi1,j , . . . , win,j)\n(3.1)\n\nHere the weighted sum is very popular:\nThe multiplication of the output of each\nneuron i by wi,j , and the summation of\nthe results:\n\nnetj =\n∑\ni∈I\n\n(oi · wi,j) (3.2)\n\nSNIPE: The propagation function in\nSnipe was implemented using the weighted\nsum.\n\n3.2.3 The activation is the\n"switching status" of a\nneuron\n\nBased on the model of nature every neuron\nis, to a certain extent, at all times active,\nexcited or whatever you will call it. The\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 35\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\nreactions of the neurons to the input val-\nues depend on this activation state. The\n\nHow active\nis a\n\nneuron?\nactivation state indicates the extent of a\nneuron’s activation and is often shortly re-\nferred to as activation. Its formal defini-\ntion is included in the following definition\nof the activation function. But generally,\nit can be defined as follows:\n\nDefinition 3.4 (Activation state / activa-\ntion in general). Let j be a neuron. The\nactivation state aj , in short activation, is\nexplicitly assigned to j, indicates the ex-\ntent of the neuron’s activity and results\nfrom the activation function.\n\nSNIPE: It is possible to get and set activa-\ntion states of neurons by using the meth-\nods getActivation or setActivation in\nthe class NeuralNetwork.\n\n3.2.4 Neurons get activated if the\nnetwork input exceeds their\ntreshold value\n\nNear the threshold value, the activation\nfunction of a neuron reacts particularly\nsensitive. From the biological point of\nview the threshold value represents the\nthreshold at which a neuron starts fir-\ning. The threshold value is also mostly\n\nhighest\npoint of\n\nsensation\nincluded in the definition of the activation\nfunction, but generally the definition is the\nfollowing:\n\nDefinition 3.5 (Threshold value in gen-\neral). Let j be a neuron. The threshold\nvalue Θj is uniquely assigned to j and\n\nΘI marks the position of the maximum gradi-\nent value of the activation function.\n\n3.2.5 The activation function\ndetermines the activation of a\nneuron dependent on network\ninput and treshold value\n\nAt a certain time – as we have already\nlearned – the activation aj of a neuron j\ndepends on the previous3 activation state\nof the neuron and the external input.\n\nDefinition 3.6 (Activation function and\nActivation). Let j be a neuron. The ac-\n\ncalculates\nactivationtivation function is defined as\n\naj(t) = fact(netj(t), aj(t− 1),Θj). (3.3)\n\nIt transforms the network input netj , Jfactas well as the previous activation state\naj(t− 1) into a new activation state aj(t),\nwith the threshold value Θ playing an im-\nportant role, as already mentioned.\n\nUnlike the other variables within the neu-\nral network (particularly unlike the ones\ndefined so far) the activation function is\noften defined globally for all neurons or\nat least for a set of neurons and only the\nthreshold values are different for each neu-\nron. We should also keep in mind that\nthe threshold values can be changed, for\nexample by a learning procedure. So it\ncan in particular become necessary to re-\nlate the threshold value to the time and to\nwrite, for instance Θj as Θj(t) (but for rea-\nsons of clarity, I omitted this here). The\nactivation function is also called transfer\nfunction.\n\n3 The previous activation is not always relevant for\nthe current – we will see examples for both vari-\nants.\n\n36 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.2 Components of neural networks\n\nSNIPE: In Snipe, activation functions are\ngeneralized to neuron behaviors. Such\nbehaviors can represent just normal acti-\nvation functions, or even incorporate in-\nternal states and dynamics. Correspond-\ning parts of Snipe can be found in the\npackage neuronbehavior, which also con-\ntains some of the activation functions in-\ntroduced in the next section. The inter-\nface NeuronBehavior allows for implemen-\ntation of custom behaviors. Objects that\ninherit from this interface can be passed to\na NeuralNetworkDescriptor instance. It\nis possible to define individual behaviors\nper neuron layer.\n\n3.2.6 Common activation functions\n\nThe simplest activation function is the bi-\nnary threshold function (fig. 3.2 on the\nnext page), which can only take on two val-\nues (also referred to as Heaviside func-\ntion). If the input is above a certain\nthreshold, the function changes from one\nvalue to another, but otherwise remains\nconstant. This implies that the function\nis not differentiable at the threshold and\nfor the rest the derivative is 0. Due to\nthis fact, backpropagation learning, for ex-\nample, is impossible (as we will see later).\nAlso very popular is the Fermi function\nor logistic function (fig. 3.2)\n\n1\n1 + e−x , (3.4)\n\nwhich maps to the range of values of (0, 1)\nand the hyperbolic tangent (fig. 3.2)\nwhich maps to (−1, 1). Both functions are\ndifferentiable. The Fermi function can be\nexpanded by a temperature parameter\nT into the form\n\nTI\n\n1\n1 + e−xT\n\n. (3.5)\n\nThe smaller this parameter, the more does\nit compress the function on the x axis.\nThus, one can arbitrarily approximate the\nHeaviside function. Incidentally, there ex-\nist activation functions which are not ex-\nplicitly defined but depend on the input ac-\ncording to a random distribution (stochas-\ntic activation function).\n\nA alternative to the hypberbolic tangent\nthat is really worth mentioning was sug-\ngested by Anguita et al. [APZ93], who\nhave been tired of the slowness of the work-\nstations back in 1993. Thinking about\nhow to make neural network propagations\nfaster, they quickly identified the approx-\nimation of the e-function used in the hy-\nperbolic tangent as one of the causes of\nslowness. Consequently, they "engineered"\nan approximation to the hyperbolic tan-\ngent, just using two parabola pieces and\ntwo half-lines. At the price of delivering\na slightly smaller range of values than the\nhyperbolic tangent ([−0.96016; 0.96016] in-\nstead of [−1; 1]), dependent on what CPU\none uses, it can be calculated 200 times\nfaster because it just needs two multipli-\ncations and one addition. What’s more,\nit has some other advantages that will be\nmentioned later.\n\nSNIPE: The activation functions intro-\nduced here are implemented within the\nclasses Fermi and TangensHyperbolicus,\nboth of which are located in the package\nneuronbehavior. The fast hyperbolic tan-\ngent approximation is located within the\nclass TangensHyperbolicusAnguita.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 37\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\n−1\n\n−0.5\n\n 0\n\n 0.5\n\n 1\n\n−4 −2  0  2  4\n\nf(\nx)\n\nx\n\nHeaviside Function\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−4 −2  0  2  4\n\nf(\nx)\n\nx\n\nFermi Function with Temperature Parameter\n\n−1\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−4 −2  0  2  4\n\nta\nnh\n\n(x\n)\n\nx\n\nHyperbolic Tangent\n\nFigure 3.2: Various popular activation func-\ntions, from top to bottom: Heaviside or binary\nthreshold function, Fermi function, hyperbolic\ntangent. The Fermi function was expanded by\na temperature parameter. The original Fermi\nfunction is represented by dark colors, the tem-\nperature parameters of the modified Fermi func-\ntions are, ordered ascending by steepness, 1\n\n2 ,\n1\n5 ,1\n\n10 und 1\n25 .\n\n3.2.7 An output function may be\nused to process the activation\nonce again\n\nThe output function of a neuron j cal-\nculates the values which are transferred to\nthe other neurons connected to j. More\nformally:\n\nDefinition 3.7 (Output function). Let j\ninforms\nother\nneurons\n\nbe a neuron. The output function\n\nfout(aj) = oj (3.6)\n\ncalculates the output value oj of the neu- Jfoutron j from its activation state aj .\n\nGenerally, the output function is defined\nglobally, too. Often this function is the\nidentity, i.e. the activation aj is directly\noutput4:\n\nfout(aj) = aj , so oj = aj (3.7)\n\nUnless explicitly specified differently, we\nwill use the identity as output function\nwithin this text.\n\n3.2.8 Learning strategies adjust a\nnetwork to fit our needs\n\nSince we will address this subject later in\ndetail and at first want to get to know the\nprinciples of neural network structures, I\nwill only provide a brief and general defi-\nnition here:\n4 Other definitions of output functions may be use-\nful if the range of values of the activation function\nis not sufficient.\n\n38 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.3 Network topologies\n\nDefinition 3.8 (General learning rule).\nThe learning strategy is an algorithm\nthat can be used to change and thereby\ntrain the neural network, so that the net-\nwork produces a desired output for a given\ninput.\n\n3.3 Network topologies\n\nAfter we have become acquainted with the\ncomposition of the elements of a neural\nnetwork, I want to give an overview of\nthe usual topologies (= designs) of neural\nnetworks, i.e. to construct networks con-\nsisting of these elements. Every topology\ndescribed in this text is illustrated by a\nmap and its Hinton diagram so that the\nreader can immediately see the character-\nistics and apply them to other networks.\n\nIn the Hinton diagram the dotted weights\nare represented by light grey fields, the\nsolid ones by dark grey fields. The input\nand output arrows, which were added for\nreasons of clarity, cannot be found in the\nHinton diagram. In order to clarify that\nthe connections are between the line neu-\nrons and the column neurons, I have in-\nserted the small arrow � in the upper-left\ncell.\n\nSNIPE: Snipe is designed for realization\nof arbitrary network topologies. In this\nrespect, Snipe defines different kinds of\nsynapses depending on their source and\ntheir target. Any kind of synapse can sep-\narately be allowed or forbidden for a set of\nnetworks using the setAllowed methods in\na NeuralNetworkDescriptor instance.\n\n3.3.1 Feedforward networks consist\nof layers and connections\ntowards each following layer\n\nFeedforward In this text feedforward net-\nworks (fig. 3.3 on the following page) are\nthe networks we will first explore (even if\nwe will use different topologies later). The\nneurons are grouped in the following lay-\ners: One input layer, n hidden pro-\n\nnetwork of\nlayerscessing layers (invisible from the out-\n\nside, that’s why the neurons are also re-\nferred to as hidden neurons) and one out-\nput layer. In a feedforward network each\nneuron in one layer has only directed con-\nnections to the neurons of the next layer\n(towards the output layer). In fig. 3.3 on\nthe next page the connections permitted\nfor a feedforward network are represented\nby solid lines. We will often be confronted\nwith feedforward networks in which every\nneuron i is connected to all neurons of the\nnext layer (these layers are called com-\npletely linked). To prevent naming con-\nflicts the output neurons are often referred\nto as Ω.\n\nDefinition 3.9 (Feedforward network).\nThe neuron layers of a feedforward net-\nwork (fig. 3.3 on the following page) are\nclearly separated: One input layer, one\noutput layer and one or more processing\nlayers which are invisible from the outside\n(also called hidden layers). Connections\nare only permitted to neurons of the fol-\nlowing layer.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 39\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\n�� ��GFED@ABCi1\n\n~~}}}}}}}}}\n\n  AAAAAAAAA\n\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\n~~}}}}}}}}}\n\n  AAAAAAAAA\n\nGFED@ABCh1\n\n  AAAAAAAAA\n\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCh2\n\n~~}}}}}}}}}\n\n  AAAAAAAAA\nGFED@ABCh3\n\n~~}}}}}}}}}\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\nGFED@ABCΩ1\n\n��\n\nGFED@ABCΩ2\n\n��\n\n� i1 i2 h1 h2 h3 Ω1 Ω2\ni1\ni2\nh1\nh2\nh3\nΩ1\nΩ2\n\nFigure 3.3: A feedforward network with three\nlayers: two input neurons, three hidden neurons\nand two output neurons. Characteristic for the\nHinton diagram of completely linked feedforward\nnetworks is the formation of blocks above the\ndiagonal.\n\n3.3.1.1 Shortcut connections skip layers\nShortcuts\nskip\nlayersSome feedforward networks permit the so-\n\ncalled shortcut connections (fig. 3.4 on\nthe next page): connections that skip one\nor more levels. These connections may\nonly be directed towards the output layer,\ntoo.\n\nDefinition 3.10 (Feedforward network\nwith shortcut connections). Similar to the\nfeedforward network, but the connections\nmay not only be directed towards the next\nlayer but also towards any other subse-\nquent layer.\n\n3.3.2 Recurrent networks have\ninfluence on themselves\n\nRecurrence is defined as the process of a\nneuron influencing itself by any means or\nby any connection. Recurrent networks do\nnot always have explicitly defined input or\noutput neurons. Therefore in the figures\nI omitted all markings that concern this\nmatter and only numbered the neurons.\n\n3.3.2.1 Direct recurrences start and\nend at the same neuron\n\nSome networks allow for neurons to be\nconnected to themselves, which is called\ndirect recurrence (or sometimes self-\nrecurrence (fig. 3.5 on the facing page).\nAs a result, neurons inhibit and therefore\nstrengthen themselves in order to reach\ntheir activation limits.\n\n40 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.3 Network topologies\n\n�� ��GFED@ABCi1\n\n��\n++\n\n~~   **\n\nGFED@ABCi2\n\nss\n��\n\ntt ~~   GFED@ABCh1\n\n  **\n\nGFED@ABCh2\n\n~~   \n\nGFED@ABCh3\n\n~~ttGFED@ABCΩ1\n\n��\n\nGFED@ABCΩ2\n\n��\n\n� i1 i2 h1 h2 h3 Ω1 Ω2\ni1\ni2\nh1\nh2\nh3\nΩ1\nΩ2\n\nFigure 3.4: A feedforward network with short-\ncut connections, which are represented by solid\nlines. On the right side of the feedforward blocks\nnew connections have been added to the Hinton\ndiagram.\n\n?>=<89:;1\nvv\n\n�� �� ))\n\n?>=<89:;2\nvv\n\nuu �� ��?>=<89:;3\nvv\n\n�� ))\n\n?>=<89:;4\nvv\n\n�� ��\n\n?>=<89:;5\nvv\n\n��uu?>=<89:;6\nvv ?>=<89:;7\n\nvv\n\n� 1 2 3 4 5 6 7\n1\n2\n3\n4\n5\n6\n7\n\nFigure 3.5: A network similar to a feedforward\nnetwork with directly recurrent neurons. The di-\nrect recurrences are represented by solid lines and\nexactly correspond to the diagonal in the Hinton\ndiagram matrix.\n\nDefinition 3.11 (Direct recurrence).\nNow we expand the feedforward network neurons\n\ninfluence\nthemselves\n\nby connecting a neuron j to itself, with the\nweights of these connections being referred\nto as wj,j . In other words: the diagonal\nof the weight matrix W may be different\nfrom 0.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 41\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\n3.3.2.2 Indirect recurrences can\ninfluence their starting neuron\nonly by making detours\n\nIf connections are allowed towards the in-\nput layer, they will be called indirect re-\ncurrences. Then a neuron j can use in-\ndirect forwards connections to influence it-\nself, for example, by influencing the neu-\nrons of the next layer and the neurons of\nthis next layer influencing j (fig. 3.6).\n\nDefinition 3.12 (Indirect recurrence).\nAgain our network is based on a feedfor-\nward network, now with additional connec-\ntions between neurons and their preceding\nlayer being allowed. Therefore, below the\ndiagonal of W is different from 0.\n\n3.3.2.3 Lateral recurrences connect\nneurons within one layer\n\nConnections between neurons within one\nlayer are called lateral recurrences\n(fig. 3.7 on the facing page). Here, each\nneuron often inhibits the other neurons of\nthe layer and strengthens itself. As a re-\nsult only the strongest neuron becomes ac-\ntive (winner-takes-all scheme).\nDefinition 3.13 (Lateral recurrence). A\nlaterally recurrent network permits con-\nnections within one layer.\n\n3.3.3 Completely linked networks\nallow any possible connection\n\nCompletely linked networks permit connec-\ntions between all neurons, except for direct\n\n?>=<89:;1\n\n�� �� ))\n\n?>=<89:;2\n\nuu �� ��?>=<89:;3\n\n88 22\n\n�� ))\n\n?>=<89:;4\n\nXX 88\n\n�� ��\n\n?>=<89:;5\n\nXXgg\n\n��uu?>=<89:;6\n\nXX 88 22\n\n?>=<89:;7\n\ngg XX 88\n\n� 1 2 3 4 5 6 7\n1\n2\n3\n4\n5\n6\n7\n\nFigure 3.6: A network similar to a feedforward\nnetwork with indirectly recurrent neurons. The\nindirect recurrences are represented by solid lines.\nAs we can see, connections to the preceding lay-\ners can exist here, too. The fields that are sym-\nmetric to the feedforward blocks in the Hinton\ndiagram are now occupied.\n\n42 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.4 The bias neuron\n\n?>=<89:;1 ++kk\n\n�� �� ))\n\n?>=<89:;2\n\nuu �� ��?>=<89:;3 ++kk **\njj\n\n�� ))\n\n?>=<89:;4 ++kk\n\n�� ��\n\n?>=<89:;5\n\n��uu?>=<89:;6 ++kk ?>=<89:;7\n\n� 1 2 3 4 5 6 7\n1\n2\n3\n4\n5\n6\n7\n\nFigure 3.7: A network similar to a feedforward\nnetwork with laterally recurrent neurons. The\ndirect recurrences are represented by solid lines.\nHere, recurrences only exist within the layer.\nIn the Hinton diagram, filled squares are con-\ncentrated around the diagonal in the height of\nthe feedforward blocks, but the diagonal is left\nuncovered.\n\nrecurrences. Furthermore, the connections\nmust be symmetric (fig. 3.8 on the next\npage). A popular example are the self-\norganizing maps, which will be introduced\nin chapter 10.\n\nDefinition 3.14 (Complete interconnec-\ntion). In this case, every neuron is always\nallowed to be connected to every other neu-\nron – but as a result every neuron can\nbecome an input neuron. Therefore, di-\nrect recurrences normally cannot be ap-\nplied here and clearly defined layers do not\nlonger exist. Thus, the matrix W may be\nunequal to 0 everywhere, except along its\ndiagonal.\n\n3.4 The bias neuron is a\ntechnical trick to consider\nthreshold values as\nconnection weights\n\nBy now we know that in many network\nparadigms neurons have a threshold value\nthat indicates when a neuron becomes ac-\ntive. Thus, the threshold value is an\nactivation function parameter of a neu-\nron. From the biological point of view\nthis sounds most plausible, but it is com-\nplicated to access the activation function\nat runtime in order to train the threshold\nvalue.\n\nBut threshold values Θj1 , . . . ,Θjn for neu-\nrons j1, j2, . . . , jn can also be realized as\nconnecting weight of a continuously fir-\ning neuron: For this purpose an addi-\ntional bias neuron whose output value\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 43\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\n?>=<89:;1 ii\n\n��\n\nii\n\n))TTTTTTTTTTTTTTTTTTTTTTT\nOO\n\n��\n\noo //\n^^\n\n��>>>>>>>>>\n?>=<89:;255\n\nuujjjjjjjjjjjjjjjjjjjjjjj OO\n\n��\n\n@@\n\n����������� ^^\n\n��>>>>>>>>>\n\n?>=<89:;3 ii\n\n))TTTTTTTTTTTTTTTTTTTTTTToo //\n��\n\n@@��������� ?>=<89:;4 ?>=<89:;544jj 55\n\nuujjjjjjjjjjjjjjjjjjjjjjj//oo\n@@\n\n�����������\n\n?>=<89:;6\n\n\n\n\n55\n\n��\n\n@@�����������\n\n^>̂>>>>>>>> ?>=<89:;7//oo\n��\n\n^>̂>>>>>>>>\n\n� 1 2 3 4 5 6 7\n1\n2\n3\n4\n5\n6\n7\n\nFigure 3.8: A completely linked network with\nsymmetric connections and without direct recur-\nrences. In the Hinton diagram only the diagonal\nis left blank.\n\nis always 1 is integrated in the network\nand connected to the neurons j1, j2, . . . , jn.\nThese new connections get the weights\n−Θj1 , . . . ,−Θjn , i.e. they get the negative\nthreshold values.\n\nDefinition 3.15. A bias neuron is a\nneuron whose output value is always 1 and\nwhich is represented by\n\nGFED@ABCBIAS .\n\nIt is used to represent neuron biases as con-\nnection weights, which enables any weight-\ntraining algorithm to train the biases at\nthe same time.\n\nThen the threshold value of the neurons\nj1, j2, . . . , jn is set to 0. Now the thresh-\nold values are implemented as connection\nweights (fig. 3.9 on page 46) and can di-\nrectly be trained together with the con-\nnection weights, which considerably facil-\nitates the learning process.\n\nIn other words: Instead of including the\nthreshold value in the activation function,\nit is now included in the propagation func-\ntion. Or even shorter: The threshold value\nis subtracted from the network input, i.e.\nit is part of the network input. More for-\nmally:\n\nbias neuron\nreplaces\nthresh. value\nwith weights\n\nLet j1, j2, . . . , jn be neurons with thresh-\nold values Θj1 , . . . ,Θjn . By inserting a\nbias neuron whose output value is always\n1, generating connections between the said\nbias neuron and the neurons j1, j2, . . . , jn\nand weighting these connections\nwBIAS,j1 , . . . , wBIAS,jnwith −Θj1 , . . . ,−Θjn ,\nwe can set Θj1 = . . . = Θjn = 0 and\n\n44 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.6 Orders of activation\n\nreceive an equivalent neural network\nwhose threshold values are realized by\nconnection weights.\n\nUndoubtedly, the advantage of the bias\nneuron is the fact that it is much easier\nto implement it in the network. One dis-\nadvantage is that the representation of the\nnetwork already becomes quite ugly with\nonly a few neurons, let alone with a great\nnumber of them. By the way, a bias neu-\nron is often referred to as on neuron.\n\nFrom now on, the bias neuron is omit-\nted for clarity in the following illustrations,\nbut we know that it exists and that the\nthreshold values can simply be treated as\nweights because of it.\n\nSNIPE: In Snipe, a bias neuron was imple-\nmented instead of neuron-individual biases.\nThe neuron index of the bias neuron is 0.\n\n3.5 Representing neurons\n\nWe have already seen that we can either\nwrite its name or its threshold value into\na neuron. Another useful representation,\nwhich we will use several times in the\nfollowing, is to illustrate neurons accord-\ning to their type of data processing. See\nfig. 3.10 for some examples without fur-\nther explanation – the different types of\nneurons are explained as soon as we need\nthem.\n\nWVUTPQRS||c,x||\nGauß\n\nGFED@ABC� ONMLHIJKΣ\n�\n\nWVUTPQRSΣ\nL|H\n\nWVUTPQRSΣ\nTanh\n\nWVUTPQRSΣ\nFermi\n\nONMLHIJKΣ\nfact\n\nGFED@ABCBIAS\n\nFigure 3.10: Different types of neurons that will\nappear in the following text.\n\n3.6 Take care of the order in\nwhich neuron activations\nare calculated\n\nFor a neural network it is very important\nin which order the individual neurons re-\nceive and process the input and output the\nresults. Here, we distinguish two model\nclasses:\n\n3.6.1 Synchronous activation\n\nAll neurons change their values syn-\nchronously, i.e. they simultaneously cal-\nculate network inputs, activation and out-\nput, and pass them on. Synchronous ac-\ntivation corresponds closest to its biolog-\nical counterpart, but it is – if to be im-\nplemented in hardware – only useful on\ncertain parallel computers and especially\nnot for feedforward networks. This order\nof activation is the most generic and can\nbe used with networks of arbitrary topol-\nogy.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 45\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\n��GFED@ABCΘ1\n\n  BBBBBBBBB\n\n~~|||||||||\n\nGFED@ABCΘ2\n\n��\n\nGFED@ABCΘ3\n\n��\n\n��GFED@ABCBIAS −Θ1 //\n\n−Θ2\nAAAA\n\n  AAAA −Θ3\nTTTTTTTTTT\n\n**TTTTTTTTTT\n\n?>=<89:;0\n\n����?>=<89:;0\n\n��\n\n?>=<89:;0\n\n��\n\nFigure 3.9: Two equivalent neural networks, one without bias neuron on the left, one with bias\nneuron on the right. The neuron threshold values can be found in the neurons, the connecting\nweights at the connections. Furthermore, I omitted the weights of the already existing connections\n(represented by dotted lines on the right side).\n\nDefinition 3.16 (Synchronous activa-\ntion). All neurons of a network calculate\n\nbiologically\nplausible network inputs at the same time by means\n\nof the propagation function, activation by\nmeans of the activation function and out-\nput by means of the output function. Af-\nter that the activation cycle is complete.\n\nSNIPE: When implementing in software,\none could model this very general activa-\ntion order by every time step calculating\nand caching every single network input,\nand after that calculating all activations.\nThis is exactly how it is done in Snipe, be-\ncause Snipe has to be able to realize arbi-\ntrary network topologies.\n\n3.6.2 Asynchronous activation\n\nHere, the neurons do not change their val-\nues simultaneously but at different points\n\nof time. For this, there exist different or-\nders, some of which I want to introduce in\nthe following: easier to\n\nimplement\n\n3.6.2.1 Random order\n\nDefinition 3.17 (Random order of acti-\nvation). With random order of acti-\nvation a neuron i is randomly chosen and\nits neti, ai and oi are updated. For n neu-\nrons a cycle is the n-fold execution of this\nstep. Obviously, some neurons are repeat-\nedly updated during one cycle, and others,\nhowever, not at all.\n\nApparently, this order of activation is not\nalways useful.\n\n46 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.6 Orders of activation\n\n3.6.2.2 Random permutation\n\nWith random permutation each neuron\nis chosen exactly once, but in random or-\nder, during one cycle.\n\nDefinition 3.18 (Random permutation).\nInitially, a permutation of the neurons is\ncalculated randomly and therefore defines\nthe order of activation. Then the neurons\nare successively processed in this order.\n\nThis order of activation is as well used\nrarely because firstly, the order is gener-\nally useless and, secondly, it is very time-\nconsuming to compute a new permutation\nfor every cycle. A Hopfield network (chap-\nter 8) is a topology nominally having a\nrandom or a randomly permuted order of\nactivation. But note that in practice, for\nthe previously mentioned reasons, a fixed\norder of activation is preferred.\n\nFor all orders either the previous neuron\nactivations at time t or, if already existing,\nthe neuron activations at time t + 1, for\nwhich we are calculating the activations,\ncan be taken as a starting point.\n\n3.6.2.3 Topological order\n\nDefinition 3.19 (Topological activation).\nWith topological order of activation\n\noften very\nuseful the neurons are updated during one cycle\n\nand according to a fixed order. The order\nis defined by the network topology.\n\nThis procedure can only be considered for\nnon-cyclic, i.e. non-recurrent, networks,\n\nsince otherwise there is no order of activa-\ntion. Thus, in feedforward networks (for\nwhich the procedure is very reasonable)\nthe input neurons would be updated first,\nthen the inner neurons and finally the out-\nput neurons. This may save us a lot of\ntime: Given a synchronous activation or-\nder, a feedforward network with n layers\nof neurons would need n full propagation\ncycles in order to enable input data to\nhave influence on the output of the net-\nwork. Given the topological activation or-\nder, we just need one single propagation.\nHowever, not every network topology al-\nlows for finding a special activation order\nthat enables saving time.\n\nSNIPE: Those who want to use Snipe\nfor implementing feedforward networks\nmay save some calculation time by us-\ning the feature fastprop (mentioned\nwithin the documentation of the class\nNeuralNetworkDescriptor. Once fastprop\nis enabled, it will cause the data propaga-\ntion to be carried out in a slightly different\nway. In the standard mode, all net inputs\nare calculated first, followed by all activa-\ntions. In the fastprop mode, for every neu-\nron, the activation is calculated right after\nthe net input. The neuron values are calcu-\nlated in ascending neuron index order. The\nneuron numbers are ascending from input\nto output layer, which provides us with the\nperfect topological activation order for feed-\nforward networks.\n\n3.6.2.4 Fixed orders of activation\nduring implementation\n\nObviously, fixed orders of activation\ncan be defined as well. Therefore, when\nimplementing, for instance, feedforward\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 47\n\n\n\nChapter 3 Components of artificial neural networks (fundamental) dkriesel.com\n\nnetworks it is very popular to determine\nthe order of activation once according to\nthe topology and to use this order without\nfurther verification at runtime. But this is\nnot necessarily useful for networks that are\ncapable to change their topology.\n\n3.7 Communication with the\noutside world: input and\noutput of data in and\nfrom neural networks\n\nFinally, let us take a look at the fact that,\nof course, many types of neural networks\npermit the input of data. Then these data\nare processed and can produce output.\nLet us, for example, regard the feedfor-\nward network shown in fig. 3.3 on page 40:\nIt has two input neurons and two output\nneurons, which means that it also has two\nnumerical inputs x1, x2 and outputs y1, y2.\nAs a simplification we summarize the in-\nput and output components for n input\nor output neurons within the vectors x =\n(x1, x2, . . . , xn) and y = (y1, y2, . . . , yn).\n\nDefinition 3.20 (Input vector). A net-\nxI work with n input neurons needs n inputs\n\nx1, x2, . . . , xn. They are considered as in-\nput vector x = (x1, x2, . . . , xn). As a\nconsequence, the input dimension is re-\nferred to as n. Data is put into a neural\n\nnI network by using the components of the in-\nput vector as network inputs of the input\nneurons.\n\nDefinition 3.21 (Output vector). A net-\nyI work with m output neurons provides m\n\noutputs y1, y2, . . . , ym. They are regarded\nas output vector y = (y1, y2, . . . , ym).\nThus, the output dimension is referred\nto as m. Data is output by a neural net- Jmwork by the output neurons adopting the\ncomponents of the output vector in their\noutput values.\n\nSNIPE: In order to propagate data through\na NeuralNetwork-instance, the propagate\nmethod is used. It receives the input vector\nas array of doubles, and returns the output\nvector in the same way.\n\nNow we have defined and closely examined\nthe basic components of neural networks –\nwithout having seen a network in action.\nBut first we will continue with theoretical\nexplanations and generally describe how a\nneural network could learn.\n\nExercises\n\nExercise 5. Would it be useful (from\nyour point of view) to insert one bias neu-\nron in each layer of a layer-based network,\nsuch as a feedforward network? Discuss\nthis in relation to the representation and\nimplementation of the network. Will the\nresult of the network change?\n\nExercise 6. Show for the Fermi function\nf(x) as well as for the hyperbolic tangent\ntanh(x), that their derivatives can be ex-\npressed by the respective functions them-\nselves so that the two statements\n\n1. f ′(x) = f(x) · (1− f(x)) and\n\n2. tanh′(x) = 1− tanh2(x)\n\n48 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 3.7 Input and output of data\n\nare true.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 49\n\n\n\n\n\nChapter 4\n\nFundamentals on learning and training\nsamples\n\nApproaches and thoughts of how to teach machines. Should neural networks\nbe corrected? Should they only be encouraged? Or should they even learn\n\nwithout any help? Thoughts about what we want to change during the\nlearning procedure and how we will change it, about the measurement of\n\nerrors and when we have learned enough.\n\nAs written above, the most interesting\ncharacteristic of neural networks is their\ncapability to familiarize with problems\nby means of training and, after sufficient\ntraining, to be able to solve unknown prob-\nlems of the same class. This approach is re-\nferred to as generalization. Before intro-\nducing specific learning procedures, I want\nto propose some basic principles about the\nlearning procedure in this chapter.\n\n4.1 There are different\nparadigms of learning\n\nLearning is a comprehensive term. A\nlearning system changes itself in order to\nadapt to e.g. environmental changes. A\nneural network could learn from many\nthings but, of course, there will always be\n\nFrom what\ndo we learn?\n\nthe question of how to implement it. In\nprinciple, a neural network changes when\nits components are changing, as we have\nlearned above. Theoretically, a neural net-\nwork could learn by\n\n1. developing new connections,\n\n2. deleting existing connections,\n\n3. changing connecting weights,\n\n4. changing the threshold values of neu-\nrons,\n\n5. varying one or more of the three neu-\nron functions (remember: activation\nfunction, propagation function and\noutput function),\n\n6. developing new neurons, or\n\n7. deleting existing neurons (and so, of\ncourse, existing connections).\n\n51\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\nAs mentioned above, we assume the\nchange in weight to be the most common\nprocedure. Furthermore, deletion of con-\nnections can be realized by additionally\ntaking care that a connection is no longer\ntrained when it is set to 0. Moreover, we\ncan develop further connections by setting\na non-existing connection (with the value\n0 in the connection matrix) to a value dif-\nferent from 0. As for the modification of\nthreshold values I refer to the possibility\nof implementing them as weights (section\n3.4). Thus, we perform any of the first four\nof the learning paradigms by just training\nsynaptic weights.\n\nThe change of neuron functions is difficult\nto implement, not very intuitive and not\nexactly biologically motivated. Therefore\nit is not very popular and I will omit this\ntopic here. The possibilities to develop or\ndelete neurons do not only provide well\nadjusted weights during the training of a\nneural network, but also optimize the net-\nwork topology. Thus, they attract a grow-\ning interest and are often realized by using\nevolutionary procedures. But, since we ac-\ncept that a large part of learning possibil-\nities can already be covered by changes in\nweight, they are also not the subject mat-\nter of this text (however, it is planned to\nextend the text towards those aspects of\ntraining).\n\nSNIPE: Methods of the class\nNeuralNetwork allow for changes in\nconnection weights, and addition and\nremoval of both connections and neurons.\nMethods in NeuralNetworkDescriptor\nenable the change of neuron behaviors,\nrespectively activation functions per\nlayer.\n\nThus, we let our neural network learn by\nmodifying the connecting weights accord-\ning to rules that can be formulated as al-\n\nLearning\nby changes\nin weight\n\ngorithms. Therefore a learning procedure\nis always an algorithm that can easily be\nimplemented by means of a programming\nlanguage. Later in the text I will assume\nthat the definition of the term desired out-\nput which is worth learning is known (and\nI will define formally what a training pat-\ntern is) and that we have a training set\nof learning samples. Let a training set be\ndefined as follows:\n\nDefinition 4.1 (Training set). A train- JPing set (named P ) is a set of training\npatterns, which we use to train our neu-\nral net.\n\nI will now introduce the three essential\nparadigms of learning by presenting the\ndifferences between their regarding train-\ning sets.\n\n4.1.1 Unsupervised learning\nprovides input patterns to the\nnetwork, but no learning aides\n\nUnsupervised learning is the biologi-\ncally most plausible method, but is not\nsuitable for all problems. Only the in-\nput patterns are given; the network tries\nto identify similar patterns and to classify\nthem into similar categories.\n\nDefinition 4.2 (Unsupervised learning).\nThe training set only consists of input\npatterns, the network tries by itself to de-\ntect similarities and to generate pattern\nclasses.\n\n52 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.1 Paradigms of learning\n\nHere I want to refer again to the popu-\nlar example of Kohonen’s self-organising\nmaps (chapter 10).\n\n4.1.2 Reinforcement learning\nmethods provide feedback to\nthe network, whether it\nbehaves well or bad\n\nIn reinforcement learning the network\nreceives a logical or a real value after\n\nnetwork\nreceives\n\nreward or\npunishment\n\ncompletion of a sequence, which defines\nwhether the result is right or wrong. Intu-\nitively it is clear that this procedure should\nbe more effective than unsupervised learn-\ning since the network receives specific crit-\nera for problem-solving.\n\nDefinition 4.3 (Reinforcement learning).\nThe training set consists of input patterns,\nafter completion of a sequence a value is re-\nturned to the network indicating whether\nthe result was right or wrong and, possibly,\nhow right or wrong it was.\n\n4.1.3 Supervised learning methods\nprovide training patterns\ntogether with appropriate\ndesired outputs\n\nIn supervised learning the training set\nconsists of input patterns as well as their\ncorrect results in the form of the precise ac-\ntivation of all output neurons. Thus, for\neach training set that is fed into the net-\nwork the output, for instance, can directly\n\nnetwork\nreceives\ncorrect\n\nresults for\nsamples\n\nbe compared with the correct solution and\nand the network weights can be changed\n\naccording to their difference. The objec-\ntive is to change the weights to the effect\nthat the network cannot only associate in-\nput and output patterns independently af-\nter the training, but can provide plausible\nresults to unknown, similar input patterns,\ni.e. it generalises.\n\nDefinition 4.4 (Supervised learning).\nThe training set consists of input patterns\nwith correct results so that the network can\nreceive a precise error vector1 can be re-\nturned.\n\nThis learning procedure is not always bio-\nlogically plausible, but it is extremely ef-\nfective and therefore very practicable.\n\nAt first we want to look at the the su-\npervised learning procedures in general,\nwhich - in this text - are corresponding\nto the following steps:\n\nEntering the input pattern (activation of\ninput neurons),\n\nForward propagation of the input by the\nnetwork, generation of the output,\n\nlearning\nscheme\n\nComparing the output with the desired\noutput (teaching input), provides er-\nror vector (difference vector),\n\nCorrections of the network are\ncalculated based on the error vector,\n\nCorrections are applied.\n\n1 The term error vector will be defined in section\n4.2, where mathematical formalisation of learning\nis discussed.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 53\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\n4.1.4 Offline or online learning?\n\nIt must be noted that learning can be\noffline (a set of training samples is pre-\nsented, then the weights are changed, the\ntotal error is calculated by means of a error\nfunction operation or simply accumulated -\nsee also section 4.4) or online (after every\nsample presented the weights are changed).\nBoth procedures have advantages and dis-\nadvantages, which will be discussed in the\nlearning procedures section if necessary.\nOffline training procedures are also called\nbatch training procedures since a batch\nof results is corrected all at once. Such a\ntraining section of a whole batch of train-\ning samples including the related change\nin weight values is called epoch.\n\nDefinition 4.5 (Offline learning). Sev-\neral training patterns are entered into the\nnetwork at once, the errors are accumu-\nlated and it learns for all patterns at the\nsame time.\n\nDefinition 4.6 (Online learning). The\nnetwork learns directly from the errors of\neach training sample.\n\n4.1.5 Questions you should answer\nbefore learning\n\nThe application of such schemes certainly\nrequires preliminary thoughts about some\nquestions, which I want to introduce now\nas a check list and, if possible, answer\nthem in the course of this text:\n\n. Where does the learning input come\nfrom and in what form?\n\n. How must the weights be modified to\nallow fast and reliable learning?\n\n. How can the success of a learning pro-\ncess be measured in an objective way?\n\n. Is it possible to determine the "best"\nlearning procedure?\n\n. Is it possible to predict if a learning\nprocedure terminates, i.e. whether it\nwill reach an optimal state after a fi-\nnite time or if it, for example, will os-\ncillate between different states?\n\n. How can the learned patterns be\nstored in the network?\n\n. Is it possible to avoid that newly\nlearned patterns destroy previously\nlearned associations (the so-called sta-\nbility/plasticity dilemma)?\n\nWe will see that all these questions cannot\nbe generally answered but that they have JJJ\n\nno easy\nanswers!\n\nto be discussed for each learning procedure\nand each network topology individually.\n\n4.2 Training patterns and\nteaching input\n\nBefore we get to know our first learning\nrule, we need to introduce the teaching\ninput. In (this) case of supervised learn-\ning we assume a training set consisting\nof training patterns and the correspond-\ning correct output values we want to see\n\ndesired\noutputat the output neurons after the training.\n\nWhile the network has not finished train-\ning, i.e. as long as it is generating wrong\noutputs, these output values are referred\n\n54 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.2 Training patterns and teaching input\n\nto as teaching input, and that for each neu-\nron individually. Thus, for a neuron j with\nthe incorrect output oj , tj is the teaching\ninput, which means it is the correct or de-\nsired output for a training pattern p.\n\nDefinition 4.7 (Training patterns). A\npI training pattern is an input vector p\n\nwith the components p1, p2, . . . , pn whose\ndesired output is known. By entering the\ntraining pattern into the network we re-\nceive an output that can be compared with\nthe teaching input, which is the desired\noutput. The set of training patterns is\ncalled P . It contains a finite number of or-\ndered pairs(p, t) of training patterns with\ncorresponding desired output.\n\nTraining patterns are often simply called\npatterns, that is why they are referred\nto as p. In the literature as well as in\nthis text they are called synonymously pat-\nterns, training samples etc.\n\nDefinition 4.8 (Teaching input). Let j\ntI be an output neuron. The teaching in-\n\nput tj is the desired and correct value j\ndesired\noutput should output after the input of a certain\n\ntraining pattern. Analogously to the vec-\ntor p the teaching inputs t1, t2, . . . , tn of\nthe neurons can also be combined into a\nvector t. t always refers to a specific train-\ning pattern p and is, as already mentioned,\ncontained in the set P of the training pat-\nterns.\n\nSNIPE: Classes that are relevant\nfor training data are located in\nthe package training. The class\nTrainingSampleLesson allows for storage\nof training patterns and teaching inputs,\n\nas well as simple preprocessing of the\ntraining data.\n\nDefinition 4.9 (Error vector). For sev- JEperal output neurons Ω1,Ω2, . . . ,Ωn the dif-\nference between output vector and teach-\ning input under a training input p\n\nEp =\n\n\uf8eb\uf8ec\uf8ed t1 − y1\n...\n\ntn − yn\n\n\uf8f6\uf8f7\uf8f8\nis referred to as error vector, sometimes\nit is also called difference vector. De-\npending on whether you are learning of-\nfline or online, the difference vector refers\nto a specific training pattern, or to the er-\nror of a set of training patterns which is\nnormalized in a certain way.\n\nNow I want to briefly summarize the vec-\ntors we have yet defined. There is the\n\ninput vector x, which can be entered into\nthe neural network. Depending on\nthe type of network being used the\nneural network will output an\n\noutput vector y. Basically, the\n\ntraining sample p is nothing more than\nan input vector. We only use it for\ntraining purposes because we know\nthe corresponding\n\nteaching input t which is nothing more\nthan the desired output vector to the\ntraining sample. The\n\nerror vector Ep is the difference between\nthe teaching input t and the actural\noutput y.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 55\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\nSo, what x and y are for the general net-\nwork operation are p and t for the networkImportant!\ntraining - and during training we try to\nbring y as close to t as possible. One ad-\nvice concerning notation: We referred to\nthe output values of a neuron i as oi. Thus,\nthe output of an output neuron Ω is called\noΩ. But the output values of a network are\nreferred to as yΩ. Certainly, these network\noutputs are only neuron outputs, too, but\nthey are outputs of output neurons. In\nthis respect\n\nyΩ = oΩ\n\nis true.\n\n4.3 Using training samples\n\nWe have seen how we can learn in prin-\nciple and which steps are required to do\nso. Now we should take a look at the se-\nlection of training data and the learning\ncurve. After successful learning it is par-\nticularly interesting whether the network\nhas onlymemorized – i.e. whether it can\nuse our training samples to quite exactly\nproduce the right output but to provide\nwrong answers for all other problems of\nthe same class.\n\nSuppose that we want the network to train\na mapping R2 → B1 and therefor use the\ntraining samples from fig. 4.1: Then there\ncould be a chance that, finally, the net-\nwork will exactly mark the colored areas\naround the training samples with the out-\nput 1 (fig. 4.1, top), and otherwise will\noutput 0 . Thus, it has sufficient storage\ncapacity to concentrate on the six training\n\nFigure 4.1: Visualization of training results of\nthe same training set on networks with a capacity\nbeing too high (top), correct (middle) or too low\n(bottom).\n\n56 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.3 Using training samples\n\nsamples with the output 1. This implies\nan oversized network with too much free\nstorage capacity.\n\nOn the other hand a network could have\ninsufficient capacity (fig. 4.1, bottom) –\nthis rough presentation of input data does\nnot correspond to the good generalization\nperformance we desire. Thus, we have to\nfind the balance (fig. 4.1, middle).\n\n4.3.1 It is useful to divide the set of\ntraining samples\n\nAn often proposed solution for these prob-\nlems is to divide, the training set into\n\n. one training set really used to train ,\n\n. and one verification set to test our\nprogress\n\n– provided that there are enough train-\ning samples. The usual division relations\nare, for instance, 70% for training data\nand 30% for verification data (randomly\nchosen). We can finish the training when\nthe network provides good results on the\ntraining data as well as on the verification\ndata.\n\nSNIPE: The method splitLesson within\nthe class TrainingSampleLesson allows for\nsplitting a TrainingSampleLesson with re-\nspect to a given ratio.\n\nBut note: If the verification data provide\npoor results, do not modify the network\nstructure until these data provide good re-\nsults – otherwise you run the risk of tai-\nloring the network to the verification data.\n\nThis means, that these data are included\nin the training, even if they are not used\nexplicitly for the training. The solution\nis a third set of validation data used only\nfor validation after a supposably success-\nful training.\n\nBy training less patterns, we obviously\nwithhold information from the network\nand risk to worsen the learning perfor-\nmance. But this text is not about 100%\nexact reproduction of given samples but\nabout successful generalization and ap-\nproximation of a whole function – for\nwhich it can definitely be useful to train\nless information into the network.\n\n4.3.2 Order of pattern\nrepresentation\n\nYou can find different strategies to choose\nthe order of pattern presentation: If pat-\nterns are presented in random sequence,\nthere is no guarantee that the patterns\nare learned equally well (however, this is\nthe standard method). Always the same\nsequence of patterns, on the other hand,\nprovokes that the patterns will be memo-\nrized when using recurrent networks (later,\nwe will learn more about this type of net-\nworks). A random permutation would\nsolve both problems, but it is – as already\nmentioned – very time-consuming to cal-\nculate such a permutation.\n\nSNIPE: The method shuffleSamples lo-\ncated in the class TrainingSampleLesson\npermutes a lesson.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 57\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\n4.4 Learning curve and error\nmeasurement\n\nThe learning curve indicates the progress\nof the error, which can be determined innorm\n\nto\ncompare\n\nvarious ways. The motivation to create a\nlearning curve is that such a curve can in-\ndicate whether the network is progressing\nor not. For this, the error should be nor-\nmalized, i.e. represent a distance measure\nbetween the correct and the current out-\nput of the network. For example, we can\ntake the same pattern-specific, squared er-\nror with a prefactor, which we are also go-\ning to use to derive the backpropagation\nof error (let Ω be output neurons and O\nthe set of output neurons):\n\nErrp = 1\n2\n∑\nΩ∈O\n\n(tΩ − yΩ)2 (4.1)\n\nDefinition 4.10 (Specific error). The\nspecific error Errp is based on a single\n\nErrpI training sample, which means it is gener-\nated online.\n\nAdditionally, the root mean square (ab-\nbreviated: RMS) and the Euclidean\ndistance are often used.\n\nThe Euclidean distance (generalization of\nthe theorem of Pythagoras) is useful for\nlower dimensions where we can still visual-\nize its usefulness.\n\nDefinition 4.11 (Euclidean distance).\nThe Euclidean distance between two vec-\ntors t and y is defined as\n\nErrp =\n√∑\n\nΩ∈O\n(tΩ − yΩ)2. (4.2)\n\nGenerally, the root mean square is com-\nmonly used since it considers extreme out-\nliers to a greater extent.\n\nDefinition 4.12 (Root mean square).\nThe root mean square of two vectors t and\ny is defined as\n\nErrp =\n√∑\n\nΩ∈O(tΩ − yΩ)2\n\n|O|\n. (4.3)\n\nAs for offline learning, the total error in\nthe course of one training epoch is inter-\nesting and useful, too:\n\nErr =\n∑\np∈P\n\nErrp (4.4)\n\nDefinition 4.13 (Total error). The total\nerror Err is based on all training samples, JErrthat means it is generated offline.\n\nAnalogously we can generate a total RMS\nand a total Euclidean distance in the\ncourse of a whole epoch. Of course, it is\npossible to use other types of error mea-\nsurement. To get used to further error\nmeasurement methods, I suggest to have a\nlook into the technical report of Prechelt\n[Pre94]. In this report, both error mea-\nsurement methods and sample problems\nare discussed (this is why there will be a\nsimmilar suggestion during the discussion\nof exemplary problems).\n\nSNIPE: There are several static meth-\nods representing different methods of er-\nror measurement implemented in the class\nErrorMeasurement.\n\nDepending on our method of error mea-\nsurement our learning curve certainly\n\n58 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.4 Learning curve and error measurement\n\nchanges, too. A perfect learning curve\nlooks like a negative exponential func-\ntion, that means it is proportional to e−t\n(fig. 4.2 on the following page). Thus, the\nrepresentation of the learning curve can be\nillustrated by means of a logarithmic scale\n(fig. 4.2, second diagram from the bot-\ntom) – with the said scaling combination\na descending line implies an exponential\ndescent of the error.\n\nWith the network doing a good job, the\nproblems being not too difficult and the\nlogarithmic representation of Err you can\nsee - metaphorically speaking - a descend-\ning line that often forms "spikes" at the\nbottom – here, we reach the limit of the\n64-bit resolution of our computer and our\nnetwork has actually learned the optimum\nof what it is capable of learning.\n\nTypical learning curves can show a few flat\nareas as well, i.e. they can show some\nsteps, which is no sign of a malfunctioning\nlearning process. As we can also see in fig.\n4.2, a well-suited representation can make\nany slightly decreasing learning curve look\ngood – so just be cautious when reading\nthe literature.\n\n4.4.1 When do we stop learning?\n\nNow, the big question is: When do we\nstop learning? Generally, the training is\nstopped when the user in front of the learn-\ning computer "thinks" the error was small\nenough. Indeed, there is no easy answer\nand thus I can once again only give you\nsomething to think about, which, however,\n\ndepends on a more objective view on the\ncomparison of several learning curves.\n\nConfidence in the results, for example, is\nboosted, when the network always reaches\n\nobjectivity\nnearly the same final error-rate for differ-\nent random initializations – so repeated\ninitialization and training will provide a\nmore objective result.\n\nOn the other hand, it can be possible that\na curve descending fast in the beginning\ncan, after a longer time of learning, be\novertaken by another curve: This can indi-\ncate that either the learning rate of the\nworse curve was too high or the worse\ncurve itself simply got stuck in a local min-\nimum, but was the first to find it.\n\nRemember: Larger error values are worse\nthan the small ones.\n\nBut, in any case, note: Many people only\ngenerate a learning curve in respect of the\ntraining data (and then they are surprised\nthat only a few things will work) – but for\nreasons of objectivity and clarity it should\nnot be forgotten to plot the verification\ndata on a second learning curve, which\ngenerally provides values that are slightly\nworse and with stronger oscillation. But\nwith good generalization the curve can de-\ncrease, too.\n\nWhen the network eventually begins to\nmemorize the samples, the shape of the\nlearning curve can provide an indication:\nIf the learning curve of the verification\nsamples is suddenly and rapidly rising\nwhile the learning curve of the verification\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 59\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\n 0\n\n 5e−005\n\n 0.0001\n\n 0.00015\n\n 0.0002\n\n 0.00025\n\n 0  100  200  300  400  500  600  700  800  900 1000\n\nF\neh\n\nle\nr\n\nEpoche\n\n 0\n\n 2e−005\n\n 4e−005\n\n 6e−005\n\n 8e−005\n\n 0.0001\n\n 0.00012\n\n 0.00014\n\n 0.00016\n\n 0.00018\n\n 0.0002\n\n 1  10  100  1000\n\nF\neh\n\nle\nr\n\nEpoche\n\n 1e−035\n\n 1e−030\n\n 1e−025\n\n 1e−020\n\n 1e−015\n\n 1e−010\n\n 1e−005\n\n 1\n\n 0  100  200  300  400  500  600  700  800  900 1000\n\nF\neh\n\nle\nr\n\nEpoche\n\n 1e−035\n\n 1e−030\n\n 1e−025\n\n 1e−020\n\n 1e−015\n\n 1e−010\n\n 1e−005\n\n 1\n\n 1  10  100  1000\n\nF\neh\n\nle\nr\n\nEpoche\n\nFigure 4.2: All four illustrations show the same (idealized, because very smooth) learning curve.\nNote the alternating logarithmic and linear scalings! Also note the small "inaccurate spikes" visible\nin the sharp bend of the curve in the first and second diagram from bottom.\n\n60 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.5 Gradient optimization procedures\n\ndata is continuously falling, this could indi-\ncate memorizing and a generalization get-\nting poorer and poorer. At this point it\ncould be decided whether the network has\nalready learned well enough at the next\npoint of the two curves, and maybe the\nfinal point of learning is to be applied\nhere (this procedure is called early stop-\nping).\n\nOnce again I want to remind you that they\nare all acting as indicators and not to draw\nIf-Then conclusions.\n\n4.5 Gradient optimization\nprocedures\n\nIn order to establish the mathematical ba-\nsis for some of the following learning pro-\ncedures I want to explain briefly what is\nmeant by gradient descent: the backpro-\npagation of error learning procedure, for\nexample, involves this mathematical basis\nand thus inherits the advantages and dis-\nadvantages of the gradient descent.\n\nGradient descent procedures are generally\nused where we want to maximize or mini-\nmize n-dimensional functions. Due to clar-\nity the illustration (fig. 4.3 on the next\npage) shows only two dimensions, but prin-\ncipally there is no limit to the number of\ndimensions.\n\nThe gradient is a vector g that is de-\nfined for any differentiable point of a func-\ntion, that points from this point exactly\ntowards the steepest ascent and indicates\nthe gradient in this direction by means\n\nof its norm |g|. Thus, the gradient is a\ngeneralization of the derivative for multi-\ndimensional functions. Accordingly, the\nnegative gradient −g exactly points to-\nwards the steepest descent. The gradient\noperator ∇ is referred to as nabla op- J∇\n\ngradient is\nmulti-dim.\nderivative\n\nerator, the overall notation of the the\ngradient g of the point (x, y) of a two-\ndimensional function f being g(x, y) =\n∇f(x, y).\nDefinition 4.14 (Gradient). Let g be\na gradient. Then g is a vector with n\ncomponents that is defined for any point\nof a (differential) n-dimensional function\nf(x1, x2, . . . , xn). The gradient operator\nnotation is defined as\ng(x1, x2, . . . , xn) = ∇f(x1, x2, . . . , xn).\n\ng directs from any point of f towards\nthe steepest ascent from this point, with\n|g| corresponding to the degree of this as-\ncent.\n\nGradient descent means to going downhill\nin small steps from any starting point of\nour function towards the gradient g (which\nmeans, vividly speaking, the direction to\nwhich a ball would roll from the starting\npoint), with the size of the steps being pro-\nportional to |g| (the steeper the descent,\nthe longer the steps). Therefore, we move\nslowly on a flat plateau, and on a steep as-\ncent we run downhill rapidly. If we came\ninto a valley, we would - depending on the\nsize of our steps - jump over it or we would\nreturn into the valley across the opposite\nhillside in order to come closer and closer\nto the deepest point of the valley by walk-\ning back and forth, similar to our ball mov-\ning within a round bowl.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 61\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\nFigure 4.3: Visualization of the gradient descent on a two-dimensional error function. We\nmove forward in the opposite direction of g, i.e. with the steepest descent towards the lowest\npoint, with the step width being proportional to |g| (the steeper the descent, the faster the\nsteps). On the left the area is shown in 3D, on the right the steps over the contour lines are\nshown in 2D. Here it is obvious how a movement is made in the opposite direction of g towards\nthe minimum of the function and continuously slows down proportionally to |g|. Source:\nhttp://webster.fhs-hagenberg.ac.at/staff/sdreisei/Teaching/WS2001-2002/\nPatternClassification/graddescent.pdf\n\nDefinition 4.15 (Gradient descent).\nLet f be an n-dimensional function and\n\nWe go\ntowards the\n\ngradient\ns = (s1, s2, . . . , sn) the given starting\npoint. Gradient descent means going\nfrom f(s) against the direction of g, i.e.\ntowards −g with steps of the size of |g|\ntowards smaller and smaller values of f .\n\nGradient descent procedures are not an er-\nrorless optimization procedure at all (as\nwe will see in the following sections) – how-\never, they work still well on many prob-\nlems, which makes them an optimization\nparadigm that is frequently used. Anyway,\nlet us have a look on their potential disad-\nvantages so we can keep them in mind a\nbit.\n\n4.5.1 Gradient procedures\nincorporate several problems\n\nAs already implied in section 4.5, the gra-\ndient descent (and therefore the backpro-\npagation) is promising but not foolproof.\nOne problem, is that the result does not\nalways reveal if an error has occurred.\n\ngradient\ndescent\nwith errors\n\n4.5.1.1 Often, gradient descents\nconverge against suboptimal\nminima\n\nEvery gradient descent procedure can, for\nexample, get stuck within a local mini-\nmum (part a of fig. 4.4 on the facing page).\n\n62 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.5 Gradient optimization procedures\n\nFigure 4.4: Possible errors during a gradient descent: a) Detecting bad minima, b) Quasi-standstill\nwith small gradient, c) Oscillation in canyons, d) Leaving good minima.\n\nThis problem is increasing proportionally\nto the size of the error surface, and there\nis no universal solution. In reality, one\ncannot know if the optimal minimum is\nreached and considers a training success-\nful, if an acceptable minimum is found.\n\n4.5.1.2 Flat plataeus on the error\nsurface may cause training\nslowness\n\nWhen passing a flat plateau, for instance,\nthe gradient also becomes negligibly small\nbecause there is hardly a descent (part b\nof fig. 4.4), which requires many further\nsteps. A hypothetically possible gradient\nof 0 would completely stop the descent.\n\n4.5.1.3 Even if good minima are\nreached, they may be left\nafterwards\n\nOn the other hand the gradient is very\nlarge at a steep slope so that large steps\ncan be made and a good minimum can pos-\nsibly be missed (part d of fig. 4.4).\n\n4.5.1.4 Steep canyons in the error\nsurface may cause oscillations\n\nA sudden alternation from one very strong\nnegative gradient to a very strong positive\none can even result in oscillation (part c\nof fig. 4.4). In nature, such an error does\nnot occur very often so that we can think\nabout the possibilities b and d.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 63\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\n4.6 Exemplary problems allow\nfor testing self-coded\nlearning strategies\n\nWe looked at learning from the formal\npoint of view – not much yet but a little.\nNow it is time to look at a few exemplary\nproblem you can later use to test imple-\nmented networks and learning rules.\n\n4.6.1 Boolean functions\n\nA popular example is the one that did\nnot work in the nineteen-sixties: the XOR\nfunction (B2 → B1). We need a hidden\nneuron layer, which we have discussed in\ndetail. Thus, we need at least two neu-\nrons in the inner layer. Let the activation\nfunction in all layers (except in the input\nlayer, of course) be the hyperbolic tangent.\nTrivially, we now expect the outputs 1.0\nor −1.0, depending on whether the func-\ntion XOR outputs 1 or 0 - and exactly\nhere is where the first beginner’s mistake\noccurs.\n\nFor outputs close to 1 or -1, i.e. close to\nthe limits of the hyperbolic tangent (or\nin case of the Fermi function 0 or 1), we\nneed very large network inputs. The only\nchance to reach these network inputs are\nlarge weights, which have to be learned:\nThe learning process is largely extended.\nTherefore it is wiser to enter the teaching\ninputs 0.9 or −0.9 as desired outputs or\nto be satisfied when the network outputs\nthose values instead of 1 and −1.\n\ni1 i2 i3 Ω\n0 0 0 1\n0 0 1 0\n0 1 0 0\n0 1 1 1\n1 0 0 0\n1 0 1 1\n1 1 0 1\n1 1 1 0\n\nTable 4.1: Illustration of the parity function\nwith three inputs.\n\nAnother favourite example for singlelayer\nperceptrons are the boolean functions\nAND and OR.\n\n4.6.2 The parity function\n\nThe parity function maps a set of bits to 1\nor 0, depending on whether an even num-\nber of input bits is set to 1 or not. Ba-\nsically, this is the function Bn → B1. It\nis characterized by easy learnability up to\napprox. n = 3 (shown in table 4.1), but\nthe learning effort rapidly increases from\nn = 4. The reader may create a score ta-\nble for the 2-bit parity function. What is\nconspicuous?\n\n4.6.3 The 2-spiral problem\n\nAs a training sample for a function let\nus take two spirals coiled into each other\n(fig. 4.5 on the facing page) with the\nfunction certainly representing a mapping\n\n64 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.6 Exemplary problems\n\nFigure 4.5: Illustration of the training samples\nof the 2-spiral problem\n\nR2 → B1. One of the spirals is assigned\nto the output value 1, the other spiral to\n0. Here, memorizing does not help. The\nnetwork has to understand the mapping it-\nself. This example can be solved by means\nof an MLP, too.\n\n4.6.4 The checkerboard problem\n\nWe again create a two-dimensional func-\ntion of the form R2 → B1 and specify\ncheckered training samples (fig. 4.6) with\none colored field representing 1 and all the\nrest of them representing 0. The difficulty\nincreases proportionally to the size of the\nfunction: While a 3×3 field is easy to learn,\nthe larger fields are more difficult (here\nwe eventually use methods that are more\n\nFigure 4.6: Illustration of training samples for\nthe checkerboard problem\n\nsuitable for this kind of problems than the\nMLP).\n\nThe 2-spiral problem is very similar to the\ncheckerboard problem, only that, mathe-\nmatically speaking, the first problem is us-\ning polar coordinates instead of Cartesian\ncoordinates. I just want to introduce as\nan example one last trivial case: the iden-\ntity.\n\n4.6.5 The identity function\n\nBy using linear activation functions the\nidentity mapping from R1 to R1 (of course\nonly within the parameters of the used ac-\ntivation function) is no problem for the\nnetwork, but we put some obstacles in its\nway by using our sigmoid functions so that\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 65\n\n\n\nChapter 4 Fundamentals on learning and training samples (fundamental) dkriesel.com\n\nit would be difficult for the network to\nlearn the identity. Just try it for the fun\nof it.\n\nNow, it is time to hava a look at our first\nmathematical learning rule.\n\n4.6.6 There are lots of other\nexemplary problems\n\nFor lots and lots of further exemplary prob-\nlems, I want to recommend the technical\nreport written by prechelt [Pre94] which\nalso has been named in the sections about\nerror measurement procedures..\n\n4.7 The Hebbian learning rule\nis the basis for most\nother learning rules\n\nIn 1949, Donald O. Hebb formulated\ntheHebbian rule [Heb49] which is the ba-\nsis for most of the more complicated learn-\ning rules we will discuss in this text. We\ndistinguish between the original form and\nthe more general form, which is a kind of\nprinciple for other learning rules.\n\n4.7.1 Original rule\n\nDefinition 4.16 (Hebbian rule). "If neu-\nron j receives an input from neuron i and\nif both neurons are strongly active at the\nsame time, then increase the weight wi,j\n(i.e. the strength of the connection be-\ntween i and j)." Mathematically speaking,\nthe rule is:\n\nearly\nform of\nthe rule\n\n∆wi,j ∼ ηoiaj (4.5)\n\nwith ∆wi,j being the change in weight\nfrom i to j , which is proportional to the J∆wi,jfollowing factors:\n\n. the output oi of the predecessor neu-\nron i, as well as,\n\n. the activation aj of the successor neu-\nron j,\n\n. a constant η, i.e. the learning rate,\nwhich will be discussed in section\n5.4.3.\n\nThe changes in weight ∆wi,j are simply\nadded to the weight wi,j .\n\nWhy am I speaking twice about activation,\nbut in the formula I am using oi and aj , i.e.\nthe output of neuron of neuron i and the ac-\ntivation of neuron j? Remember that the\nidentity is often used as output function\nand therefore ai and oi of a neuron are of-\nten the same. Besides, Hebb postulated\nhis rule long before the specification of\ntechnical neurons. Considering that this\nlearning rule was preferred in binary acti-\nvations, it is clear that with the possible\nactivations (1, 0) the weights will either in-\ncrease or remain constant. Sooner or later\n\nweights\ngo ad\ninfinitum\n\nthey would go ad infinitum, since they can\nonly be corrected "upwards" when an error\noccurs. This can be compensated by using\nthe activations (-1,1)2. Thus, the weights\nare decreased when the activation of the\npredecessor neuron dissents from the one\nof the successor neuron, otherwise they are\nincreased.\n2 But that is no longer the "original version" of the\nHebbian rule.\n\n66 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 4.7 Hebbian rule\n\n4.7.2 Generalized form\n\nMost of the learning rules discussed before\nare a specialization of the mathematically\nmore general form [MR86] of the Hebbian\nrule.\n\nDefinition 4.17 (Hebbian rule, more gen-\neral). The generalized form of the\nHebbian Rule only specifies the propor-\ntionality of the change in weight to the\nproduct of two undefined functions, but\nwith defined input values.\n\n∆wi,j = η · h(oi, wi,j) · g(aj , tj) (4.6)\n\nThus, the product of the functions\n\n. g(aj , tj) and\n\n. h(oi, wi,j)\n\n. as well as the constant learning rate\nη\n\nresults in the change in weight. As you\ncan see, h receives the output of the pre-\ndecessor cell oi as well as the weight from\npredecessor to successor wi,j while g ex-\npects the actual and desired activation of\nthe successor aj and tj (here t stands for\nthe aforementioned teaching input). As al-\nready mentioned g and h are not specified\nin this general definition. Therefore, we\nwill now return to the path of specializa-\ntion we discussed before equation 4.6. Af-\nter we have had a short picture of what\na learning rule could look like and of our\nthoughts about learning itself, we will be\nintroduced to our first network paradigm\nincluding the learning procedure.\n\nExercises\n\nExercise 7. Calculate the average value\nµ and the standard deviation σ for the fol-\nlowing data points.\n\np1 = (2, 2, 2)\np2 = (3, 3, 3)\np3 = (4, 4, 4)\np4 = (6, 0, 0)\np5 = (0, 6, 0)\np6 = (0, 0, 6)\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 67\n\n\n\n\n\nPart II\n\nSupervised learning network\nparadigms\n\n69\n\n\n\n\n\nChapter 5\n\nThe perceptron, backpropagation and its\nvariants\n\nA classic among the neural networks. If we talk about a neural network, then\nin the majority of cases we speak about a percepton or a variation of it.\n\nPerceptrons are multilayer networks without recurrence and with fixed input\nand output layers. Description of a perceptron, its limits and extensions that\nshould avoid the limitations. Derivation of learning procedures and discussion\n\nof their problems.\n\nAs already mentioned in the history of neu-\nral networks, the perceptron was described\nby Frank Rosenblatt in 1958 [Ros58].\nInitially, Rosenblatt defined the already\ndiscussed weighted sum and a non-linear\nactivation function as components of the\nperceptron.\n\nThere is no established definition for a per-\nceptron, but most of the time the term\nis used to describe a feedforward network\nwith shortcut connections. This network\nhas a layer of scanner neurons (retina)\nwith statically weighted connections to\nthe following layer and is called input\nlayer (fig. 5.1 on the next page); but the\nweights of all other layers are allowed to be\nchanged. All neurons subordinate to the\nretina are pattern detectors. Here we ini-\ntially use a binary perceptron with every\noutput neuron having exactly two possi-\n\nble output values (e.g. {0, 1} or {−1, 1}).\nThus, a binary threshold function is used\nas activation function, depending on the\nthreshold value Θ of the output neuron.\n\nIn a way, the binary activation function\nrepresents an IF query which can also\nbe negated by means of negative weights.\nThe perceptron can thus be used to ac-\ncomplish true logical information process-\ning.\n\nWhether this method is reasonable is an-\nother matter – of course, this is not the\neasiest way to achieve Boolean logic. I just\nwant to illustrate that perceptrons can\nbe used as simple logical components and\nthat, theoretically speaking, any Boolean\nfunction can be realized by means of per-\nceptrons being connected in series or in-\nterconnected in a sophisticated way. But\n\n71\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nKapitel 5 Das Perceptron dkriesel.com\n\n�� "" )) ++ ,,\n�� ## )) ++|| �� ## )){{uu\n\n�� ""{{uuss\n��||uussrrGFED@ABC�\n\n\'\'OOOOOOOOOOOOOOOOO GFED@ABC�\n\n��@@@@@@@@@\nGFED@ABC�\n\n��\n\nGFED@ABC�\n\n��~~~~~~~~~\nGFED@ABC�\n\nwwooooooooooooooooo\n\nWVUTPQRSΣ\nL|H\n\n��\n\nGFED@ABCi1\n\n((PPPPPPPPPPPPPPPPPP GFED@ABCi2\n\n!!CCCCCCCCCC\nGFED@ABCi3\n\n��\n\nGFED@ABCi4\n\n}}{{{{{{{{{{\nGFED@ABCi5\n\nvvnnnnnnnnnnnnnnnnnn\n\n?>=<89:;Ω\n\n��\n\nAbbildung 5.1: Aufbau eines Perceptrons mit einer Schicht variabler Verbindungen in verschiede-\nnen Ansichten. Die durchgezogene Gewichtsschicht in den unteren beiden Abbildungen ist trainier-\nbar.\nOben: Am Beispiel der Informationsabtastung im Auge.\nMitte: Skizze desselben mit eingezeichneter fester Gewichtsschicht unter Verwendung der definier-\nten funktionsbeschreibenden Designs für Neurone.\nUnten: Ohne eingezeichnete feste Gewichtsschicht, mit Benennung der einzelnen Neuronen nach\nunserer Konvention. Wir werden die feste Gewichtschicht im weiteren Verlauf der Arbeit nicht mehr\nbetrachten.\n\n70 D. Kriesel – Ein kleiner Überblick über Neuronale Netze (EPSILON-DE)\n\nFigure 5.1: Architecture of a perceptron with one layer of variable connections in different views.\nThe solid-drawn weight layer in the two illustrations on the bottom can be trained.\nLeft side: Example of scanning information in the eye.\nRight side, upper part: Drawing of the same example with indicated fixed-weight layer using the\ndefined designs of the functional descriptions for neurons.\nRight side, lower part: Without indicated fixed-weight layer, with the name of each neuron\ncorresponding to our convention. The fixed-weight layer will no longer be taken into account in the\ncourse of this work.\n\n72 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com\n\nwe will see that this is not possible without\nconnecting them serially. Before providing\nthe definition of the perceptron, I want to\ndefine some types of neurons used in this\nchapter.\n\nDefinition 5.1 (Input neuron). An in-\nput neuron is an identity neuron. It\nexactly forwards the information received.\nThus, it represents the identity function,input neuron\n\nonly forwards\ndata\n\nwhich should be indicated by the symbol\n�. Therefore the input neuron is repre-\nsented by the symbol GFED@ABC� .\n\nDefinition 5.2 (Information process-\ning neuron). Information processing\nneurons somehow process the input infor-\nmation, i.e. do not represent the identity\nfunction. A binary neuron sums up all\ninputs by using the weighted sum as prop-\nagation function, which we want to illus-\ntrate by the sign Σ. Then the activation\nfunction of the neuron is the binary thresh-\nold function, which can be illustrated by\nL|H. This leads us to the complete de-\npiction of information processing neurons,\n\nnamely WVUTPQRSΣ\nL|H\n\n. Other neurons that use\n\nthe weighted sum as propagation function\nbut the activation functions hyperbolic tan-\ngent or Fermi function, or with a sepa-\nrately defined activation function fact, are\nsimilarly represented by\n\nWVUTPQRSΣ\nTanh\n\nWVUTPQRSΣ\nFermi\n\nONMLHIJKΣ\nfact\n\n.\n\nThese neurons are also referred to as\nFermi neurons or Tanh neuron.\n\nNow that we know the components of a\nperceptron we should be able to define\nit.\n\nDefinition 5.3 (Perceptron). The per-\nceptron (fig. 5.1 on the facing page) is1 a\nfeedforward network containing a retina\nthat is used only for data acquisition and\nwhich has fixed-weighted connections with\nthe first neuron layer (input layer). The\nfixed-weight layer is followed by at least\none trainable weight layer. One neuron\nlayer is completely linked with the follow-\ning layer. The first layer of the percep-\ntron consists of the input neurons defined\nabove.\n\nA feedforward network often contains\nshortcuts which does not exactly corre-\nspond to the original description and there-\nfore is not included in the definition. We\ncan see that the retina is not included in\nthe lower part of fig. 5.1. As a matter\nof fact the first neuron layer is often un-\nderstood (simplified and sufficient for this\nmethod) as input layer, because this layer retina is\n\nunconsideredonly forwards the input values. The retina\nitself and the static weights behind it are\nno longer mentioned or displayed, since\nthey do not process information in any\ncase. So, the depiction of a perceptron\nstarts with the input neurons.\n\n1 It may confuse some readers that I claim that there\nis no definition of a perceptron but then define the\nperceptron in the following section. I therefore\nsuggest keeping my definition in the back of your\nmind and just take it for granted in the course of\nthis work.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 73\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nSNIPE: The methods\nsetSettingsTopologyFeedForward\nand the variation -WithShortcuts in\na NeuralNetworkDescriptor-Instance\napply settings to a descriptor, which\nare appropriate for feedforward networks\nor feedforward networks with shortcuts.\nThe respective kinds of connections are\nallowed, all others are not, and fastprop is\nactivated.\n\n5.1 The singlelayer\nperceptron provides only\none trainable weight layer\n\nHere, connections with trainable weights\ngo from the input layer to an output\nneuron Ω, which returns the information\n\n1 trainable\nlayer whether the pattern entered at the input\n\nneurons was recognized or not. Thus, a\nsinglelayer perception (abbreviated SLP)\nhas only one level of trainable weights\n(fig. 5.1 on page 72).\n\nDefinition 5.4 (Singlelayer perceptron).\nA singlelayer perceptron (SLP) is a\nperceptron having only one layer of vari-\nable weights and one layer of output neu-\nrons Ω. The technical view of an SLP is\nshown in fig. 5.2.\n\nCertainly, the existence of several output\nneurons Ω1,Ω2, . . . ,Ωn does not consider-\nably change the concept of the perceptronImportant!\n(fig. 5.3): A perceptron with several out-\nput neurons can also be regarded as sev-\neral different perceptrons with the same\ninput.\n\nGFED@ABCBIAS\n\nwBIAS,Ω\n\n  \n\nGFED@ABCi1\n\nwi1,Ω\n\n��\n\nGFED@ABCi2\n\nwi2,Ω\n����\n\n������\n\n?>=<89:;Ω\n\n��\n\nFigure 5.2: A singlelayer perceptron with two in-\nput neurons and one output neuron. The net-\nwork returns the output by means of the ar-\nrow leaving the network. The trainable layer of\nweights is situated in the center (labeled). As a\nreminder, the bias neuron is again included here.\nAlthough the weight wBIAS,Ω is a normal weight\nand also treated like this, I have represented it\nby a dotted line – which significantly increases\nthe clarity of larger networks. In future, the bias\nneuron will no longer be included.\n\nGFED@ABCi1\n\n  @@@@@@@@@\n\n**UUUUUUUUUUUUUUUUUUUUUUUUUU\n\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCi2\n\n��\n((PPPPPPPPPPPPPPPPPP\n\n  AAAAAAAAA\nGFED@ABCi3\n\n~~}}}}}}}}}\n\n  AAAAAAAAA\n\n��\n\nGFED@ABCi4\n\nvvnnnnnnnnnnnnnnnnnn\n\n��~~}}}}}}}}}\nGFED@ABCi5\n\n~~~~~~~~~~~\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\nwwnnnnnnnnnnnnnnnnn\n\nGFED@ABCΩ1\n\n��\n\nGFED@ABCΩ2\n\n��\n\nGFED@ABCΩ3\n\n��\n\nFigure 5.3: Singlelayer perceptron with several\noutput neurons\n\n74 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.1 The singlelayer perceptron\n\nGFED@ABC�\n\n1\nAAAA\n\n  AAAA\n\nGFED@ABC�\n\n1}}}}\n\n~~}}}}\n\nGFED@ABC1.5\n\n��\n\nGFED@ABC�\n\n1\nAAAA\n\n  AAAA\n\nGFED@ABC�\n\n1}}}}\n\n~~}}}}\n\nGFED@ABC0.5\n\n��\n\nFigure 5.4: Two singlelayer perceptrons for\nBoolean functions. The upper singlelayer per-\nceptron realizes an AND, the lower one realizes\nan OR. The activation function of the informa-\ntion processing neuron is the binary threshold\nfunction. Where available, the threshold values\nare written into the neurons.\n\nThe Boolean functions AND and OR shown\nin fig. 5.4 are trivial examples that can eas-\nily be composed.\n\nNow we want to know how to train a single-\nlayer perceptron. We will therefore at first\ntake a look at the perceptron learning al-\ngorithm and then we will look at the delta\nrule.\n\n5.1.1 Perceptron learning algorithm\nand convergence theorem\n\nThe original perceptron learning algo-\nrithm with binary neuron activation func-\ntion is described in alg. 1. It has been\nproven that the algorithm converges in\nfinite time – so in finite time the per-\nceptron can learn anything it can repre-\nsent (perceptron convergence theorem,\n[Ros62]). But please do not get your hopes\nup too soon! What the perceptron is capa-\nble to represent will be explored later.\n\nDuring the exploration of linear separabil-\nity of problems we will cover the fact that\nat least the singlelayer perceptron unfor-\ntunately cannot represent a lot of prob-\nlems.\n\n5.1.2 The delta rule as a gradient\nbased learning strategy for\nSLPs\n\nIn the following we deviate from our bi-\nnary threshold value as activation function\nbecause at least for backpropagation of er-\nror we need, as you will see, a differen-\n\nfact now differ-\nentiabletiable or even a semi-linear activation func-\n\ntion. For the now following delta rule (like\nbackpropagation derived in [MR86]) it is\nnot always necessary but useful. This fact,\nhowever, will also be pointed out in the\nappropriate part of this work. Compared\nwith the aforementioned perceptron learn-\ning algorithm, the delta rule has the ad-\nvantage to be suitable for non-binary acti-\nvation functions and, being far away from\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 75\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\n1: while ∃p ∈ P and error too large do\n2: Input p into the network, calculate output y {P set of training patterns}\n3: for all output neurons Ω do\n4: if yΩ = tΩ then\n5: Output is okay, no correction of weights\n6: else\n7: if yΩ = 0 then\n8: for all input neurons i do\n9: wi,Ω := wi,Ω + oi {...increase weight towards Ω by oi}\n\n10: end for\n11: end if\n12: if yΩ = 1 then\n13: for all input neurons i do\n14: wi,Ω := wi,Ω − oi {...decrease weight towards Ω by oi}\n15: end for\n16: end if\n17: end if\n18: end for\n19: end while\nAlgorithm 1: Perceptron learning algorithm. The perceptron learning algorithm\nreduces the weights to output neurons that return 1 instead of 0, and in the inverse\ncase increases weights.\n\n76 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.1 The singlelayer perceptron\n\nthe learning target, to automatically learn\nfaster.\n\nSuppose that we have a singlelayer percep-\ntron with randomly set weights which we\nwant to teach a function by means of train-\ning samples. The set of these training sam-\nples is called P . It contains, as already de-\nfined, the pairs (p, t) of the training sam-\nples p and the associated teaching input t.\nI also want to remind you that\n\n. x is the input vector and\n\n. y is the output vector of a neural net-\nwork,\n\n. output neurons are referred to as\nΩ1,Ω2, . . . ,Ω|O|,\n\n. i is the input and\n\n. o is the output of a neuron.\n\nAdditionally, we defined that\n\n. the error vector Ep represents the dif-\nference (t−y) under a certain training\nsample p.\n\n. Furthermore, let O be the set of out-\nput neurons and\n\n. I be the set of input neurons.\n\nAnother naming convention shall be that,\nfor example, for an output o and a teach-\ning input t an additional index p may be\nset in order to indicate that these values\nare pattern-specific. Sometimes this will\nconsiderably enhance clarity.\n\nNow our learning target will certainly be,\nthat for all training samples the output y\n\nof the network is approximately the de-\nsired output t, i.e. formally it is true\nthat\n\n∀p : y ≈ t or ∀p : Ep ≈ 0.\n\nThis means we first have to understand the\ntotal error Err as a function of the weights:\nThe total error increases or decreases de-\npending on how we change the weights.\n\nDefinition 5.5 (Error function). The er-\nror function\n\nJErr(W )\nErr : W → R\n\nregards the set2 of weights W as a vector\nand maps the values onto the normalized error as\n\nfunctionoutput error (normalized because other-\nwise not all errors can be mapped onto\none single e ∈ R to perform a gradient de-\nscent). It is obvious that a specific error\nfunction can analogously be generated\n\nJErrp(W )for a single pattern p.\n\nAs already shown in section 4.5, gradient\ndescent procedures calculate the gradient\nof an arbitrary but finite-dimensional func-\ntion (here: of the error function Err(W ))\nand move down against the direction of\nthe gradient until a minimum is reached.\nErr(W ) is defined on the set of all weights\nwhich we here regard as the vector W .\nSo we try to decrease or to minimize the\nerror by simply tweaking the weights –\nthus one receives information about how\nto change the weights (the change in all\n\n2 Following the tradition of the literature, I previ-\nously defined W as a weight matrix. I am aware\nof this conflict but it should not bother us here.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 77\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\n−2\n−1\n\n 0\n 1\n\n 2\nw1\n\n−2\n−1\n\n 0\n 1\n\n 2\n\nw2\n\n 0\n\n 1\n\n 2\n\n 3\n\n 4\n\n 5\n\nFigure 5.5: Exemplary error surface of a neural\nnetwork with two trainable connections w1 und\nw2. Generally, neural networks have more than\ntwo connections, but this would have made the\nillustration too complex. And most of the time\nthe error surface is too craggy, which complicates\nthe search for the minimum.\n\nweights is referred to as ∆W ) by calcu-\nlating the gradient ∇Err(W ) of the error\nfunction Err(W ):\n\n∆W ∼ −∇Err(W ). (5.1)\n\nDue to this relation there is a proportional-\nity constant η for which equality holds (η\nwill soon get another meaning and a real\npractical use beyond the mere meaning of\na proportionality constant. I just ask the\nreader to be patient for a while.):\n\n∆W = −η∇Err(W ). (5.2)\n\nTo simplify further analysis, we now\nrewrite the gradient of the error-function\naccording to all weights as an usual par-\ntial derivative according to a single weight\nwi,Ω (the only variable weights exists be-\ntween the hidden and the output layer Ω).\n\nThus, we tweak every single weight and ob-\nserve how the error function changes, i.e.\nwe derive the error function according to\na weight wi,Ω and obtain the value ∆wi,Ω\nof how to change this weight.\n\n∆wi,Ω = −η∂Err(W )\n∂wi,Ω\n\n. (5.3)\n\nNow the following question arises: How\nis our error function defined exactly? It\nis not good if many results are far away\nfrom the desired ones; the error function\nshould then provide large values – on the\nother hand, it is similarly bad if many\nresults are close to the desired ones but\nthere exists an extremely far outlying re-\nsult. The squared distance between the\noutput vector y and the teaching input t\nappears adequate to our needs. It provides\nthe error Errp that is specific for a train-\ning sample p over the output of all output\nneurons Ω:\n\nErrp(W ) = 1\n2\n∑\nΩ∈O\n\n(tp,Ω − yp,Ω)2. (5.4)\n\nThus, we calculate the squared difference\nof the components of the vectors t and\ny, given the pattern p, and sum up these\nsquares. The summation of the specific er-\nrors Errp(W ) of all patterns p then yields\nthe definition of the error Err and there-\n\n78 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.1 The singlelayer perceptron\n\nfore the definition of the error function\nErr(W ):\n\nErr(W ) =\n∑\np∈P\n\nErrp(W ) (5.5)\n\n= 1\n2\n\nsum over all p︷ ︸︸ ︷∑\np∈P\n\n\uf8eb\uf8ed∑\nΩ∈O\n\n(tp,Ω − yp,Ω)2\n\n\uf8f6\uf8f8\n︸ ︷︷ ︸\n\nsum over all Ω\n\n.\n\n(5.6)\n\nThe observant reader will certainly wonder\nwhere the factor 1\n\n2 in equation 5.4 on the\npreceding page suddenly came from and\nwhy there is no root in the equation, as\nthis formula looks very similar to the Eu-\nclidean distance. Both facts result from\nsimple pragmatics: Our intention is to\nminimize the error. Because the root func-\ntion decreases with its argument, we can\nsimply omit it for reasons of calculation\nand implementation efforts, since we do\nnot need it for minimization. Similarly, it\ndoes not matter if the term to be mini-\nmized is divided by 2: Therefore I am al-\nlowed to multiply by 1\n\n2 . This is just done\nso that it cancels with a 2 in the course of\nour calculation.\n\nNow we want to continue deriving the\ndelta rule for linear activation functions.\nWe have already discussed that we tweak\nthe individual weights wi,Ω a bit and see\nhow the error Err(W ) is changing – which\ncorresponds to the derivative of the er-\nror function Err(W ) according to the very\nsame weight wi,Ω. This derivative cor-\nresponds to the sum of the derivatives\nof all specific errors Errp according to\nthis weight (since the total error Err(W )\n\nresults from the sum of the specific er-\nrors):\n\n∆wi,Ω = −η∂Err(W )\n∂wi,Ω\n\n(5.7)\n\n=\n∑\np∈P\n−η∂Errp(W )\n\n∂wi,Ω\n. (5.8)\n\nOnce again I want to think about the ques-\ntion of how a neural network processes\ndata. Basically, the data is only trans-\nferred through a function, the result of the\nfunction is sent through another one, and\nso on. If we ignore the output function,\nthe path of the neuron outputs oi1 and oi2 ,\nwhich the neurons i1 and i2 entered into a\nneuron Ω, initially is the propagation func-\ntion (here weighted sum), from which the\nnetwork input is going to be received. This\nis then sent through the activation func-\ntion of the neuron Ω so that we receive\nthe output of this neuron which is at the\nsame time a component of the output vec-\ntor y:\n\nnetΩ → fact\n\n= fact(netΩ)\n= oΩ\n\n= yΩ.\n\nAs we can see, this output results from\nmany nested functions:\n\noΩ = fact(netΩ) (5.9)\n\n= fact(oi1 · wi1,Ω + oi2 · wi2,Ω). (5.10)\n\nIt is clear that we could break down the\noutput into the single input neurons (this\nis unnecessary here, since they do not\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 79\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nprocess information in an SLP). Thus,\nwe want to calculate the derivatives of\nequation 5.8 on the preceding page and\ndue to the nested functions we can apply\nthe chain rule to factorize the derivative\n∂Errp(W )\n∂wi,Ω\n\nin equation 5.8 on the previous\npage.\n\n∂Errp(W )\n∂wi,Ω\n\n= ∂Errp(W )\n∂op,Ω\n\n· ∂op,Ω\n∂wi,Ω\n\n. (5.11)\n\nLet us take a look at the first multiplica-\ntive factor of the above equation 5.11\nwhich represents the derivative of the spe-\ncific error Errp(W ) according to the out-\nput, i.e. the change of the error Errp\nwith an output op,Ω: The examination\nof Errp (equation 5.4 on page 78) clearly\nshows that this change is exactly the dif-\nference between teaching input and out-\nput (tp,Ω− op,Ω) (remember: Since Ω is an\noutput neuron, op,Ω = yp,Ω). The closer\nthe output is to the teaching input, the\nsmaller is the specific error. Thus we can\nreplace one by the other. This difference\nis also called δp,Ω (which is the reason for\nthe name delta rule):\n\n∂Errp(W )\n∂wi,Ω\n\n= −(tp,Ω − op,Ω) · ∂op,Ω\n∂wi,Ω\n\n(5.12)\n\n= −δp,Ω ·\n∂op,Ω\n∂wi,Ω\n\n(5.13)\n\nThe second multiplicative factor of equa-\ntion 5.11 and of the following one is the\nderivative of the output specific to the pat-\ntern p of the neuron Ω according to the\nweight wi,Ω. So how does op,Ω change\nwhen the weight from i to Ω is changed?\n\nDue to the requirement at the beginning of\nthe derivation, we only have a linear acti-\nvation function fact, therefore we can just\nas well look at the change of the network\ninput when wi,Ω is changing:\n\n∂Errp(W )\n∂wi,Ω\n\n= −δp,Ω ·\n∂\n∑\ni∈I(op,iwi,Ω)\n∂wi,Ω\n\n.\n\n(5.14)\n\nThe resulting derivative ∂\n∑\n\ni∈I(op,iwi,Ω)\n∂wi,Ω\n\ncan now be simplified: The function∑\ni∈I(op,iwi,Ω) to be derived consists of\n\nmany summands, and only the sum-\nmand op,iwi,Ω contains the variable wi,Ω,\naccording to which we derive. Thus,\n∂\n∑\n\ni∈I(op,iwi,Ω)\n∂wi,Ω\n\n= op,i and therefore:\n\n∂Errp(W )\n∂wi,Ω\n\n= −δp,Ω · op,i (5.15)\n\n= −op,i · δp,Ω. (5.16)\n\nWe insert this in equation 5.8 on the previ-\nous page, which results in our modification\nrule for a weight wi,Ω:\n\n∆wi,Ω = η ·\n∑\np∈P\n\nop,i · δp,Ω. (5.17)\n\nHowever: From the very beginning the\nderivation has been intended as an offline\nrule by means of the question of how to\nadd the errors of all patterns and how to\nlearn them after all patterns have been\nrepresented. Although this approach is\nmathematically correct, the implementa-\ntion is far more time-consuming and, as\nwe will see later in this chapter, partially\n\n80 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.2 Linear separability\n\nneeds a lot of compuational effort during\ntraining.\n\nThe "online-learning version" of the delta\nrule simply omits the summation and\nlearning is realized immediately after the\npresentation of each pattern, this also sim-\nplifies the notation (which is no longer nec-\nessarily related to a pattern p):\n\n∆wi,Ω = η · oi · δΩ. (5.18)\n\nThis version of the delta rule shall be used\nfor the following definition:\n\nDefinition 5.6 (Delta rule). If we deter-\nmine, analogously to the aforementioned\nderivation, that the function h of the Heb-\nbian theory (equation 4.6 on page 67) only\nprovides the output oi of the predecessor\nneuron i and if the function g is the differ-\nence between the desired activation tΩ and\nthe actual activation aΩ, we will receive\nthe delta rule, also known as Widrow-\nHoff rule:\n\n∆wi,Ω = η · oi · (tΩ − aΩ) = ηoiδΩ (5.19)\n\nIf we use the desired output (instead of the\nactivation) as teaching input, and there-\nfore the output function of the output neu-\nrons does not represent an identity, we ob-\ntain\n\n∆wi,Ω = η · oi · (tΩ − oΩ) = ηoiδΩ (5.20)\n\nand δΩ then corresponds to the difference\nbetween tΩ and oΩ.\n\nIn the case of the delta rule, the change\nof all weights to an output neuron Ω is\nproportional\n\nIn. 1 In. 2 Output\n0 0 0\n0 1 1\n1 0 1\n1 1 0\n\nTable 5.1: Definition of the logical XOR. The\ninput values are shown of the left, the output\nvalues on the right.\n\n. to the difference between the current\nactivation or output aΩ or oΩ and the\ncorresponding teaching input tΩ. We\nwant to refer to this factor as δΩ , Jδwhich is also referred to as "Delta".\n\nApparently the delta rule only applies for\nSLPs, since the formula is always related\nto the teaching input, and there is no\n\ndelta rule\nonly for SLPteaching input for the inner processing lay-\n\ners of neurons.\n\n5.2 A SLP is only capable of\nrepresenting linearly\nseparable data\n\nLet f be the XOR function which expects\ntwo binary inputs and generates a binary\noutput (for the precise definition see ta-\nble 5.1).\n\nLet us try to represent the XOR func-\ntion by means of an SLP with two input\nneurons i1, i2 and one output neuron Ω\n(fig. 5.6 on the following page).\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 81\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\n�� ��GFED@ABCi1\n\nwi1,Ω\nBBBB\n\n  BBBB\n\nGFED@ABCi2\n\nwi2,Ω\n||||\n\n~~||||\n\n?>=<89:;Ω\n\n��\nXOR?\n\nFigure 5.6: Sketch of a singlelayer perceptron\nthat shall represent the XOR function - which is\nimpossible.\n\nHere we use the weighted sum as propaga-\ntion function, a binary activation function\nwith the threshold value Θ and the iden-\ntity as output function. Depending on i1\nand i2, Ω has to output the value 1 if the\nfollowing holds:\n\nnetΩ = oi1wi1,Ω + oi2wi2,Ω ≥ ΘΩ (5.21)\n\nWe assume a positive weight wi2,Ω, the in-\nequality 5.21 is then equivalent to\n\noi1 ≥\n1\n\nwi1,Ω\n(ΘΩ − oi2wi2,Ω) (5.22)\n\nWith a constant threshold value ΘΩ, the\nright part of inequation 5.22 is a straight\nline through a coordinate system defined\nby the possible outputs oi1 und oi2 of the\ninput neurons i1 and i2 (fig. 5.7).\n\nFor a (as required for inequation 5.22) pos-\nitive wi2,Ω the output neuron Ω fires for\n\nFigure 5.7: Linear separation of n = 2 inputs of\nthe input neurons i1 and i2 by a 1-dimensional\nstraight line. A and B show the corners belong-\ning to the sets of the XOR function that are to\nbe separated.\n\n82 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.2 Linear separability\n\nn number of\nbinary\nfunctions\n\nlin.\nseparable\nones\n\nshare\n\n1 4 4 100%\n2 16 14 87.5%\n3 256 104 40.6%\n4 65, 536 1, 772 2.7%\n5 4.3 · 109 94, 572 0.002%\n6 1.8 · 1019 5, 028, 134 ≈ 0%\n\nTable 5.2: Number of functions concerning n bi-\nnary inputs, and number and proportion of the\nfunctions thereof which can be linearly separated.\nIn accordance with [Zel94,Wid89,Was89].\n\ninput combinations lying above the gener-\nated straight line. For a negative wi2,Ω it\nwould fire for all input combinations lying\nbelow the straight line. Note that only the\nfour corners of the unit square are possi-\nble inputs because the XOR function only\nknows binary inputs.\n\nIn order to solve the XOR problem, we\nhave to turn and move the straight line so\nthat input set A = {(0, 0), (1, 1)} is sepa-\nrated from input set B = {(0, 1), (1, 0)} –\nthis is, obviously, impossible.\n\nGenerally, the input parameters of n many\ninput neurons can be represented in an n-\ndimensional cube which is separated by an\n\nSLP cannot\ndo everything SLP through an (n−1)-dimensional hyper-\n\nplane (fig. 5.8). Only sets that can be sep-\narated by such a hyperplane, i.e. which\nare linearly separable, can be classified\nby an SLP.\n\nFigure 5.8: Linear separation of n = 3 inputs\nfrom input neurons i1, i2 and i3 by 2-dimensional\nplane.\n\nUnfortunately, it seems that the percent-\nage of the linearly separable problems\nrapidly decreases with increasing n (see\ntable 5.2), which limits the functionality\n\nfew tasks\nare linearly\nseparable\n\nof the SLP. Additionally, tests for linear\nseparability are difficult. Thus, for more\ndifficult tasks with more inputs we need\nsomething more powerful than SLP. The\nXOR problem itself is one of these tasks,\nsince a perceptron that is supposed to rep-\nresent the XOR function already needs a\nhidden layer (fig. 5.9 on the next page).\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 83\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nGFED@ABC�\n\n1\nAAAA\n\n  AAAA\n\n1\n11111111\n\n��1\n1111111\n\nGFED@ABC�\n\n1}}}}\n\n~~}}}}\n\n1\r\r\r\r\r\r\r\r\n\n��\r\r\r\r\r\r\r\r\nGFED@ABC1.5\n\n−2\n��GFED@ABC0.5\n\n��\nXOR\n\nFigure 5.9: Neural network realizing the XOR\nfunction. Threshold values (as far as they are\nexisting) are located within the neurons.\n\n5.3 A multilayer perceptron\ncontains more trainable\nweight layers\n\nA perceptron with two or more trainable\nweight layers (called multilayer perceptron\nor MLP) is more powerful than an SLP. As\nwe know, a singlelayer perceptron can di-\nvide the input space by means of a hyper-\nplane (in a two-dimensional input space\nby means of a straight line). A two-\nstage perceptron (two trainable weight lay-\n\nmore planes\ners, three neuron layers) can classify con-\nvex polygons by further processing these\nstraight lines, e.g. in the form "recognize\npatterns lying above straight line 1, be-\nlow straight line 2 and below straight line\n3". Thus, we – metaphorically speaking\n- took an SLP with several output neu-\nrons and "attached" another SLP (upper\n\npart of fig. 5.10 on the facing page). A\nmultilayer perceptron represents an uni-\nversal function approximator, which\nis proven by the Theorem of Cybenko\n[Cyb89].\n\nAnother trainable weight layer proceeds\nanalogously, now with the convex poly-\ngons. Those can be added, subtracted or\nsomehow processed with other operations\n(lower part of fig. 5.10 on the next page).\n\nGenerally, it can be mathematically\nproven that even a multilayer perceptron\nwith one layer of hidden neurons can ar-\nbitrarily precisely approximate functions\nwith only finitely many discontinuities as\nwell as their first derivatives. Unfortu-\nnately, this proof is not constructive and\ntherefore it is left to us to find the correct\nnumber of neurons and weights.\n\nIn the following we want to use a\nwidespread abbreviated form for different\nmultilayer perceptrons: We denote a two-\nstage perceptron with 5 neurons in the in-\nput layer, 3 neurons in the hidden layer\nand 4 neurons in the output layer as a 5-\n3-4-MLP.\n\nDefinition 5.7 (Multilayer perceptron).\nPerceptrons with more than one layer of\nvariably weighted connections are referred\nto as multilayer perceptrons (MLP).\nAn n-layer or n-stage perceptron has\nthereby exactly n variable weight layers\nand n + 1 neuron layers (the retina is dis-\nregarded here) with neuron layer 1 being\nthe input layer.\n\nSince three-stage perceptrons can classify\nsets of any form by combining and sepa- 3-stage\n\nMLP is\nsufficient\n\n84 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.3 The multilayer perceptron\n\nGFED@ABCi1\n\n�����������\n\n��@@@@@@@@@\n\n**UUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\n\n�����������\n\n��@@@@@@@@@\n\nttjjjjjjjjjjjjjjjjjjjjjjjjj\n\nGFED@ABCh1\n\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCh2\n\n��\n\nGFED@ABCh3\n\nwwooooooooooooooooo\n\n?>=<89:;Ω\n\n��\n\nGFED@ABCi1\n\n~~~~~~~~~~~\n\n��   @@@@@@@@@\n\n\'\' )) **\n\nGFED@ABCi2\n\ntt uu ww ~~~~~~~~~~~\n\n��   @@@@@@@@@\n\nGFED@ABCh1\n\n\'\'PPPPPPPPPPPPPPPPP\n\n--\n\nGFED@ABCh2\n\n  @@@@@@@@@\n\n,,\n\nGFED@ABCh3\n\n��\n**\n\nGFED@ABCh4\n\n��\ntt\n\nGFED@ABCh5\n\n~~~~~~~~~~~\n\nrr\n\nGFED@ABCh6\n\nwwnnnnnnnnnnnnnnnnn\n\nqqGFED@ABCh7\n\n��@@@@@@@@@\nGFED@ABCh8\n\n��~~~~~~~~~\n\n?>=<89:;Ω\n\n��\n\nFigure 5.10: We know that an SLP represents a straight line. With 2 trainable weight layers,\nseveral straight lines can be combined to form convex polygons (above). By using 3 trainable\nweight layers several polygons can be formed into arbitrary sets (below).\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 85\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nn classifiable sets\n1 hyperplane\n2 convex polygon\n3 any set\n4 any set as well, i.e. no\n\nadvantage\n\nTable 5.3: Representation of which perceptron\ncan classify which types of sets with n being the\nnumber of trainable weight layers.\n\nrating arbitrarily many convex polygons,\nanother step will not be advantageous\nwith respect to function representations.\nBe cautious when reading the literature:\nThere are many different definitions of\nwhat is counted as a layer. Some sources\ncount the neuron layers, some count the\nweight layers. Some sources include the\nretina, some the trainable weight layers.\nSome exclude (for some reason) the out-\nput neuron layer. In this work, I chose\nthe definition that provides, in my opinion,\nthe most information about the learning\ncapabilities – and I will use it cosistently.\nRemember: An n-stage perceptron has ex-\nactly n trainable weight layers. You can\nfind a summary of which perceptrons can\nclassify which types of sets in table 5.3.\nWe now want to face the challenge of train-\ning perceptrons with more than one weight\nlayer.\n\n5.4 Backpropagation of error\ngeneralizes the delta rule\nto allow for MLP training\n\nNext, I want to derive and explain the\nbackpropagation of error learning rule\n(abbreviated: backpropagation, backprop\nor BP), which can be used to train multi-\nstage perceptrons with semi-linear3 activa-\ntion functions. Binary threshold functions\nand other non-differentiable functions are\nno longer supported, but that doesn’t mat-\nter: We have seen that the Fermi func-\ntion or the hyperbolic tangent can arbi-\ntrarily approximate the binary threshold\nfunction by means of a temperature pa-\nrameter T . To a large extent I will fol-\nlow the derivation according to [Zel94] and\n[MR86]. Once again I want to point out\nthat this procedure had previously been\npublished by Paul Werbos in [Wer74]\nbut had consideraby less readers than in\n[MR86].\n\nBackpropagation is a gradient descent pro-\ncedure (including all strengths and weak-\nnesses of the gradient descent) with the\nerror function Err(W ) receiving all n\nweights as arguments (fig. 5.5 on page 78)\nand assigning them to the output error, i.e.\nbeing n-dimensional. On Err(W ) a point\nof small error or even a point of the small-\nest error is sought by means of the gradi-\nent descent. Thus, in analogy to the delta\nrule, backpropagation trains the weights\nof the neural network. And it is exactly\n\n3 Semilinear functions are monotonous and differen-\ntiable – but generally they are not linear.\n\n86 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.4 Backpropagation of error\n\nthe delta rule or its variable δi for a neu-\nron i which is expanded from one trainable\nweight layer to several ones by backpropa-\ngation.\n\n5.4.1 The derivation is similar to\nthe one of the delta rule, but\nwith a generalized delta\n\nLet us define in advance that the network\ninput of the individual neurons i results\nfrom the weighted sum. Furthermore, as\nwith the derivation of the delta rule, let\nop,i, netp,i etc. be defined as the already\nfamiliar oi, neti, etc. under the input pat-\ntern p we used for the training. Let the\noutput function be the identity again, thus\noi = fact(netp,i) holds for any neuron i.\nSince this is a generalization of the delta\nrule, we use the same formula framework\nas with the delta rule (equation 5.20 on\n\ngeneral-\nization\n\nof δ\npage 81). As already indicated, we have\nto generalize the variable δ for every neu-\nron.\n\nFirst of all: Where is the neuron for which\nwe want to calculate δ? It is obvious to\nselect an arbitrary inner neuron h having\na set K of predecessor neurons k as well\nas a set of L successor neurons l, which\nare also inner neurons (see fig. 5.11). It\nis therefore irrelevant whether the prede-\ncessor neurons are already the input neu-\nrons.\n\nNow we perform the same derivation as\nfor the delta rule and split functions by\nmeans the chain rule. I will not discuss\nthis derivation in great detail, but the prin-\ncipal is similar to that of the delta rule (the\n\n/.-,()*+\n\n&&LLLLLLLLLLLLLLL /.-,()*+\n\n��========== /.-,()*+\n\n��\n\n. . . ?>=<89:;k\nwk,h\n\npppppppp\n\nwwppppppp\n\nK\n\nONMLHIJKΣ\nfact\n\nxxrrrrrrrrrrrrrrr\n\n������������\n\n��\n\nwh,l\nNNNNNNN\n\n\'\'NNNNNNNN\n\nh H\n\n/.-,()*+ /.-,()*+ /.-,()*+ . . . ?>=<89:;l L\n\nFigure 5.11: Illustration of the position of our\nneuron h within the neural network. It is lying in\nlayerH, the preceding layer isK, the subsequent\nlayer is L.\n\ndifferences are, as already mentioned, in\nthe generalized δ). We initially derive the\nerror function Err according to a weight\nwk,h.\n\n∂Err(wk,h)\n∂wk,h\n\n= ∂Err\n∂neth︸ ︷︷ ︸\n=−δh\n\n·∂neth\n∂wk,h\n\n(5.23)\n\nThe first factor of equation 5.23 is −δh,\nwhich we will deal with later in this text.\nThe numerator of the second factor of the\nequation includes the network input, i.e.\nthe weighted sum is included in the numer-\nator so that we can immediately derive it.\nAgain, all summands of the sum drop out\napart from the summand containing wk,h.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 87\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nThis summand is referred to as wk,h ·ok. If\nwe calculate the derivative, the output of\nneuron k becomes:\n\n∂neth\n∂wk,h\n\n= ∂\n∑\nk∈K wk,hok\n∂wk,h\n\n(5.24)\n\n= ok (5.25)\n\nAs promised, we will now discuss the −δh\nof equation 5.23 on the previous page,\nwhich is split up again according of the\nchain rule:\n\nδh = − ∂Err\n∂neth\n\n(5.26)\n\n= −∂Err\n∂oh\n\n· ∂oh\n∂neth\n\n(5.27)\n\nThe derivation of the output according to\nthe network input (the second factor in\nequation 5.27) clearly equals the deriva-\ntion of the activation function according\nto the network input:\n\n∂oh\n∂neth\n\n= ∂fact(neth)\n∂neth\n\n(5.28)\n\n= fact\n′(neth) (5.29)\n\nConsider this an important passage! We\nnow analogously derive the first factor in\nequation 5.27. Therefore, we have to point\nout that the derivation of the error func-\ntion according to the output of an inner\nneuron layer depends on the vector of all\nnetwork inputs of the next following layer.\nThis is reflected in equation 5.30:\n\n−∂Err\n∂oh\n\n= −\n∂Err(netl1 , . . . ,netl|L|)\n\n∂oh\n(5.30)\n\nAccording to the definition of the multi-\ndimensional chain rule, we immediately ob-\ntain equation 5.31:\n\n−∂Err\n∂oh\n\n=\n∑\nl∈L\n\n(\n− ∂Err\n∂netl\n\n· ∂netl\n∂oh\n\n)\n(5.31)\n\nThe sum in equation 5.31 contains two fac-\ntors. Now we want to discuss these factors\nbeing added over the subsequent layer L.\nWe simply calculate the second factor in\nthe following equation 5.33:\n\n∂netl\n∂oh\n\n= ∂\n∑\nh∈H wh,l · oh\n∂oh\n\n(5.32)\n\n= wh,l (5.33)\n\nThe same applies for the first factor accord-\ning to the definition of our δ:\n\n− ∂Err\n∂netl\n\n= δl (5.34)\n\nNow we insert:\n\n⇒ −∂Err\n∂oh\n\n=\n∑\nl∈L\n\nδlwh,l (5.35)\n\nYou can find a graphic version of the δ\ngeneralization including all splittings in\nfig. 5.12 on the facing page.\n\nThe reader might already have noticed\nthat some intermediate results were shown\nin frames. Exactly those intermediate re-\nsults were highlighted in that way, which\nare a factor in the change in weight of\nwk,h. If the aforementioned equations are\n\n88 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.4 Backpropagation of error\n\nδh\n\n− ∂Err\n∂neth\n\n�� ��\n∂oh\n∂neth −∂Err\n\n∂oh\n\n\r\r ��\nf ′act(neth) − ∂Err\n\n∂netl\n∑\nl∈L\n\n∂netl\n∂oh\n\nδl\n∂\n∑\n\nh∈H wh,l·oh\n∂oh\n\nwh,l\n\nFigure 5.12: Graphical representation of the equations (by equal signs) and chain rule splittings\n(by arrows) in the framework of the backpropagation derivation. The leaves of the tree reflect the\nfinal results from the generalization of δ, which are framed in the derivation.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 89\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\ncombined with the highlighted intermedi-\nate results, the outcome of this will be the\nwanted change in weight ∆wk,h to\n\n∆wk,h = ηokδh with (5.36)\n\nδh = f ′act(neth) ·\n∑\nl∈L\n\n(δlwh,l)\n\n– of course only in case of h being an inner\nneuron (otherweise there would not be a\nsubsequent layer L).\n\nThe case of h being an output neuron has\nalready been discussed during the deriva-\ntion of the delta rule. All in all, the re-\nsult is the generalization of the delta rule,\ncalled backpropagation of error :\n\n∆wk,h = ηokδh with\n\nδh =\n{\nf ′act(neth) · (th − yh) (h outside)\nf ′act(neth) ·∑l∈L(δlwh,l) (h inside)\n\n(5.37)\n\nIn contrast to the delta rule, δ is treated\ndifferently depending on whether h is an\noutput or an inner (i.e. hidden) neuron:\n\n1. If h is an output neuron, then\n\nδp,h = f ′act(netp,h) · (tp,h − yp,h)\n(5.38)\n\nThus, under our training pattern p\nthe weight wk,h from k to h is changed\nproportionally according to\n\n. the learning rate η,\n\n. the output op,k of the predeces-\nsor neuron k,\n\n. the gradient of the activation\nfunction at the position of the\nnetwork input of the successor\nneuron f ′act(netp,h) and\n\n. the difference between teaching\ninput tp,h and output yp,h of the\nsuccessor neuron h.\n\nTeach. Input\nchanged for\nthe outer\nweight layer\n\nIn this case, backpropagation is work-\ning on two neuron layers, the output\nlayer with the successor neuron h and\nthe preceding layer with the predeces-\nsor neuron k.\n\n2. If h is an inner, hidden neuron, then\n\nδp,h = f ′act(netp,h) ·\n∑\nl∈L\n\n(δp,l · wh,l)\n\n(5.39)\n\nholds. I want to explicitly mention\nback-\npropagation\nfor inner\nlayers\n\nthat backpropagation is now working\non three layers. Here, neuron k is\nthe predecessor of the connection to\nbe changed with the weight wk,h, the\nneuron h is the successor of the con-\nnection to be changed and the neu-\nrons l are lying in the layer follow-\ning the successor neuron. Thus, ac-\ncording to our training pattern p, the\nweight wk,h from k to h is proportion-\nally changed according to\n\n. the learning rate η,\n\n. the output of the predecessor\nneuron op,k,\n\n. the gradient of the activation\nfunction at the position of the\nnetwork input of the successor\nneuron f ′act(netp,h),\n\n. as well as, and this is the\ndifference, according to the\nweighted sum of the changes in\nweight to all neurons following h,∑\nl∈L(δp,l · wh,l).\n\n90 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.4 Backpropagation of error\n\nDefinition 5.8 (Backpropagation). If we\nsummarize formulas 5.38 on the preceding\npage and 5.39 on the facing page, we re-\nceive the following final formula for back-\npropagation (the identifiers p are om-\nmited for reasons of clarity):\n\n∆wk,h = ηokδh with\n\nδh =\n{\nf ′act(neth) · (th − yh) (h outside)\nf ′act(neth) ·∑l∈L(δlwh,l) (h inside)\n\n(5.40)\n\nSNIPE: An online variant of backpro-\npagation is implemented in the method\ntrainBackpropagationOfError within the\nclass NeuralNetwork.\n\nIt is obvious that backpropagation ini-\ntially processes the last weight layer di-\nrectly by means of the teaching input and\nthen works backwards from layer to layer\nwhile considering each preceding change in\nweights. Thus, the teaching input leaves\ntraces in all weight layers. Here I describe\nthe first (delta rule) and the second part\nof backpropagation (generalized delta rule\non more layers) in one go, which may meet\nthe requirements of the matter but not\nof the research. The first part is obvious,\nwhich you will soon see in the framework\nof a mathematical gimmick. Decades of\ndevelopment time and work lie between the\nfirst and the second, recursive part. Like\nmany groundbreaking inventions, it was\nnot until its development that it was recog-\nnized how plausible this invention was.\n\n5.4.2 Heading back: Boiling\nbackpropagation down to\ndelta rule\n\nAs explained above, the delta rule is a\nspecial case of backpropagation for one-\nstage perceptrons and linear activation\nfunctions – I want to briefly explain this\n\nbackprop\nexpands\ndelta rule\n\ncircumstance and develop the delta rule\nout of backpropagation in order to aug-\nment the understanding of both rules. We\nhave seen that backpropagation is defined\nby\n\n∆wk,h = ηokδh with\n\nδh =\n{\nf ′act(neth) · (th − yh) (h outside)\nf ′act(neth) ·∑l∈L(δlwh,l) (h inside)\n\n(5.41)\n\nSince we only use it for one-stage percep-\ntrons, the second part of backpropagation\n(light-colored) is omitted without substitu-\ntion. The result is:\n\n∆wk,h = ηokδh with\nδh = f ′act(neth) · (th − oh) (5.42)\n\nFurthermore, we only want to use linear\nactivation functions so that f ′act (light-\ncolored) is constant. As is generally\nknown, constants can be combined, and\ntherefore we directly merge the constant\nderivative f ′act and (being constant for at\nleast one lerning cycle) the learning rate η\n(also light-colored) in η. Thus, the result\nis:\n\n∆wk,h = ηokδh = ηok · (th − oh) (5.43)\n\nThis exactly corresponds to the delta rule\ndefinition.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 91\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\n5.4.3 The selection of the learning\nrate has heavy influence on\nthe learning process\n\nIn the meantime we have often seen that\nthe change in weight is, in any case, pro-\nportional to the learning rate η. Thus, the\nselection of η is crucial for the behaviour\nof backpropagation and for learning proce-\ndures in general.\n\nhow fast\nwill be\n\nlearned? Definition 5.9 (Learning rate). Speed\nand accuracy of a learning procedure can\nalways be controlled by and are always pro-\nportional to a learning rate which is writ-\nten as η.\n\nηI\n\nIf the value of the chosen η is too large,\nthe jumps on the error surface are also\ntoo large and, for example, narrow valleys\ncould simply be jumped over. Addition-\nally, the movements across the error sur-\nface would be very uncontrolled. Thus, a\nsmall η is the desired input, which, how-\never, can cost a huge, often unacceptable\namount of time. Experience shows that\ngood learning rate values are in the range\nof\n\n0.01 ≤ η ≤ 0.9.\n\nThe selection of η significantly depends on\nthe problem, the network and the training\ndata, so that it is barely possible to give\npractical advise. But for instance it is pop-\nular to start with a relatively large η, e.g.\n0.9, and to slowly decrease it down to 0.1.\nFor simpler problems η can often be kept\nconstant.\n\n5.4.3.1 Variation of the learning rate\nover time\n\nDuring training, another stylistic device\ncan be a variable learning rate: In the\nbeginning, a large learning rate leads to\ngood results, but later it results in inac-\ncurate learning. A smaller learning rate\nis more time-consuming, but the result is\nmore precise. Thus, during the learning\nprocess the learning rate needs to be de-\ncreased by one order of magnitude once or\nrepeatedly.\n\nA common error (which also seems to be a\nvery neat solution at first glance) is to con-\ntinually decrease the learning rate. Here\nit quickly happens that the descent of the\nlearning rate is larger than the ascent of\na hill of the error function we are climb-\ning. The result is that we simply get stuck\nat this ascent. Solution: Rather reduce\nthe learning rate gradually as mentioned\nabove.\n\n5.4.3.2 Different layers – Different\nlearning rates\n\nThe farer we move away from the out-\nput layer during the learning process, the\nslower backpropagation is learning. Thus,\nit is a good idea to select a larger learning\nrate for the weight layers close to the in-\nput layer than for the weight layers close\nto the output layer.\n\n92 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.5 Resilient backpropagation\n\n5.5 Resilient backpropagation\nis an extension to\nbackpropagation of error\n\nWe have just raised two backpropagation-\nspecific properties that can occasionally be\na problem (in addition to those which are\nalready caused by gradient descent itself):\nOn the one hand, users of backpropaga-\ntion can choose a bad learning rate. On\nthe other hand, the further the weights are\nfrom the output layer, the slower backpro-\npagation learns. For this reason, Mar-\ntin Riedmiller et al. enhanced back-\npropagation and called their version re-\nsilient backpropagation (short Rprop)\n[RB93, Rie94]. I want to compare back-\npropagation and Rprop, without explic-\nitly declaring one version superior to the\nother. Before actually dealing with formu-\nlas, let us informally compare the two pri-\nmary ideas behind Rprop (and their con-\nsequences) to the already familiar backpro-\npagation.\n\nLearning rates: Backpropagation uses by\ndefault a learning rate η, which is se-\nlected by the user, and applies to the\nentire network. It remains static un-\ntil it is manually changed. We have\nalready explored the disadvantages of\nthis approach. Here, Rprop pursues a\ncompletely different approach: there\nis no global learning rate. First, each\nweight wi,j has its own learning rate\n\nOne learning-\nrate per\nweight\n\nηi,jI\n\nηi,j , and second, these learning rates\nare not chosen by the user, but are au-\ntomatically set by Rprop itself. Third,\n\nautomatic\nlearning rate\nadjustment\n\nthe weight changes are not static but\n\nare adapted for each time step of\nRprop. To account for the temporal\nchange, we have to correctly call it\nηi,j(t). This not only enables more\nfocused learning, also the problem of\nan increasingly slowed down learning\nthroughout the layers is solved in an\nelegant way.\n\nWeight change: When using backpropa-\ngation, weights are changed propor-\ntionally to the gradient of the error\nfunction. At first glance, this is really\nintuitive. However, we incorporate ev-\nery jagged feature of the error surface\ninto the weight changes. It is at least\nquestionable, whether this is always\nuseful. Here, Rprop takes other ways\nas well: the amount of weight change\n∆wi,j simply directly corresponds to\nthe automatically adjusted learning\nrate ηi,j . Thus the change in weight is\nnot proportional to the gradient, it is\nonly influenced by the sign of the gra-\ndient. Until now we still do not know\nhow exactly the ηi,j are adapted at\nrun time, but let me anticipate that\nthe resulting process looks consider-\n\nMuch\nsmoother learningably less rugged than an error func-\n\ntion.\n\nIn contrast to backprop the weight update\nstep is replaced and an additional step\nfor the adjustment of the learning rate is\nadded. Now how exactly are these ideas\nbeing implemented?\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 93\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\n5.5.1 Weight changes are not\nproportional to the gradient\n\nLet us first consider the change in weight.\nWe have already noticed that the weight-\nspecific learning rates directly serve as ab-\nsolute values for the changes of the re-\nspective weights. There remains the ques-\ntion of where the sign comes from – this\nis a point at which the gradient comes\ninto play. As with the derivation of back-\npropagation, we derive the error function\nErr(W ) by the individual weights wi,j and\nobtain gradients ∂Err(W )\n\n∂wi,j\n. Now, the big\n\ndifference: rather than multiplicatively\nincorporating the absolute value of the\ngradient into the weight change, we con-\nsider only the sign of the gradient. The\ngradient hence no longer determines the\n\ngradient\ndetermines only\ndirection of the\n\nupdates\n\nstrength, but only the direction of the\nweight change.\n\nIf the sign of the gradient ∂Err(W )\n∂wi,j\n\nis pos-\nitive, we must decrease the weight wi,j .\nSo the weight is reduced by ηi,j . If the\nsign of the gradient is negative, the weight\nneeds to be increased. So ηi,j is added to\nit. If the gradient is exactly 0, nothing\nhappens at all. Let us now create a for-\nmula from this colloquial description. The\ncorresponding terms are affixed with a (t)\nto show that everything happens at the\nsame time step. This might decrease clar-\nity at first glance, but is nevertheless im-\nportant because we will soon look at an-\nother formula that operates on different\ntime steps. Instead, we shorten the gra-\ndient to: g = ∂Err(W )\n\n∂wi,j\n.\n\nDefinition 5.10 (Weight change in\nRprop).\n\n∆wi,j(t) =\n\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n−ηi,j(t), if g(t) > 0\n+ηi,j(t), if g(t) < 0\n0 otherwise.\n\n(5.44)\n\nWe now know how the weights are changed\n– now remains the question how the learn-\ning rates are adjusted. Finally, once we\nhave understood the overall system, we\nwill deal with the remaining details like ini-\ntialization and some specific constants.\n\n5.5.2 Many dynamically adjusted\nlearning rates instead of one\nstatic\n\nTo adjust the learning rate ηi,j , we again\nhave to consider the associated gradients\ng of two time steps: the gradient that has\njust passed (t − 1) and the current one\n(t). Again, only the sign of the gradient\nmatters, and we now must ask ourselves:\nWhat can happen to the sign over two time\nsteps? It can stay the same, and it can\nflip.\n\nIf the sign changes from g(t − 1) to g(t),\nwe have skipped a local minimum in the\ngradient. Hence, the last update was too\nlarge and ηi,j(t) has to be reduced as com-\npared to the previous ηi,j(t− 1). One can\nsay, that the search needs to be more accu-\nrate. In mathematical terms, we obtain a\nnew ηi,j(t) by multiplying the old ηi,j(t−1)\nwith a constant η↓, which is between 1 and\n\nJη↓0. In this case we know that in the last\ntime step (t − 1) something went wrong –\n\n94 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.5 Resilient backpropagation\n\nhence we additionally reset the weight up-\ndate for the weight wi,j at time step (t) to\n0, so that it not applied at all (not shown\nin the following formula).\n\nHowever, if the sign remains the same, one\ncan perform a (careful!) increase of ηi,j to\nget past shallow areas of the error function.\nHere we obtain our new ηi,j(t) by multiply-\ning the old ηi,j(t − 1) with a constant η↑\n\nη↑I which is greater than 1.\n\nDefinition 5.11 (Adaptation of learning\nrates in Rprop).\n\nηi,j(t) =\n\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nη↑ηi,j(t− 1), g(t− 1)g(t) > 0\nη↓ηi,j(t− 1), g(t− 1)g(t) < 0\nηi,j(t− 1) otherwise.\n\n(5.45)\n\nCaution: This also implies that Rprop is\nRprop only\n\nlearns\noffline\n\nexclusively designed for offline. If the gra-\ndients do not have a certain continuity, the\nlearning process slows down to the lowest\nrates (and remains there). When learning\nonline, one changes – loosely speaking –\nthe error function with each new epoch,\nsince it is based on only one training pat-\ntern. This may be often well applicable\nin backpropagation and it is very often\neven faster than the offline version, which\nis why it is used there frequently. It lacks,\nhowever, a clear mathematical motivation,\nand that is exactly what we need here.\n\n5.5.3 We are still missing a few\ndetails to use Rprop in\npractice\n\nA few minor issues remain unanswered,\nnamely\n\n1. How large are η↑ and η↓ (i.e. how\nmuch are learning rates reinforced or\nweakened)?\n\n2. How to choose ηi,j(0) (i.e. how are\nthe weight-specific learning rates ini-\ntialized)?4\n\n3. What are the upper and lower bounds\nηmin and ηmax for ηi,j set? Jηmin\n\nJηmaxWe now answer these questions with a\nquick motivation. The initial value for the\nlearning rates should be somewhere in the\norder of the initialization of the weights.\nηi,j(0) = 0.1 has proven to be a good\nchoice. The authors of the Rprop paper\nexplain in an obvious way that this value\n– as long as it is positive and without an ex-\norbitantly high absolute value – does not\nneed to be dealt with very critically, as\nit will be quickly overridden by the auto-\nmatic adaptation anyway.\n\nEqually uncritical is ηmax, for which they\nrecommend, without further mathemati-\ncal justification, a value of 50 which is used\nthroughout most of the literature. One\ncan set this parameter to lower values in\norder to allow only very cautious updates.\nSmall update steps should be allowed in\nany case, so we set ηmin = 10−6.\n\n4 Protipp: since the ηi,j can be changed only by\nmultiplication, 0 would be a rather suboptimal ini-\ntialization :-)\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 95\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nNow we have left only the parameters η↑\nand η↓. Let us start with η↓: If this value\nis used, we have skipped a minimum, from\nwhich we do not know where exactly it lies\non the skipped track. Analogous to the\nprocedure of binary search, where the tar-\nget object is often skipped as well, we as-\nsume it was in the middle of the skipped\ntrack. So we need to halve the learning\nrate, which is why the canonical choice\nη↓ = 0.5 is being selected. If the value\nof η↑ is used, learning rates shall be in-\ncreased with caution. Here we cannot gen-\neralize the principle of binary search and\nsimply use the value 2.0, otherwise the\nlearning rate update will end up consist-\ning almost exclusively of changes in direc-\ntion. Independent of the particular prob-\nlems, a value of η↑ = 1.2 has proven to\nbe promising. Slight changes of this value\nhave not significantly affected the rate of\nconvergence. This fact allowed for setting\nthis value as a constant as well.\n\nWith advancing computational capabili-\nties of computers one can observe a more\nand more widespread distribution of net-\nworks that consist of a big number of lay-\ners, i.e. deep networks. For such net-\n\nRprop is very\ngood for\n\ndeep networks\nworks it is crucial to prefer Rprop over the\noriginal backpropagation, because back-\nprop, as already indicated, learns very\nslowly at weights wich are far from the\noutput layer. For problems with a smaller\nnumber of layers, I would recommend test-\ning the more widespread backpropagation\n(with both offline and online learning) and\nthe less common Rprop equivalently.\n\nSNIPE: In Snipe resilient backpropa-\ngation is supported via the method\ntrainResilientBackpropagation of the\nclass NeuralNetwork. Furthermore, you\ncan also use an additional improvement\nto resilient propagation, which is, however,\nnot dealt with in this work. There are get-\nters and setters for the different parameters\nof Rprop.\n\n5.6 Backpropagation has\noften been extended and\naltered besides Rprop\n\nBackpropagation has often been extended.\nMany of these extensions can simply be im-\nplemented as optional features of backpro-\npagation in order to have a larger scope for\ntesting. In the following I want to briefly\ndescribe some of them.\n\n5.6.1 Adding momentum to\nlearning\n\nLet us assume to descent a steep slope\non skis - what prevents us from immedi-\nately stopping at the edge of the slope\nto the plateau? Exactly - our momen-\ntum. With backpropagation the momen-\ntum term [RHW86b] is responsible for the\nfact that a kind of moment of inertia\n(momentum) is added to every step size\n(fig. 5.13 on the next page), by always\nadding a fraction of the previous change\nto every new change in weight:\n\n(∆pwi,j)now = ηop,iδp,j+α·(∆pwi,j)previous.\n\n96 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.6 Further variations and extensions to backpropagation\n\nOf course, this notation is only used for\na better understanding. Generally, as al-\nready defined by the concept of time, when\nreferring to the current cycle as (t), then\nthe previous cycle is identified by (t − 1),\nwhich is continued successively. And now\nwe come to the formal definition of the mo-\nmentum term:\n\nDefinition 5.12 (Momentum term). The\nmoment of\n\ninertia variation of backpropagation by means of\nthe momentum term is defined as fol-\nlows:\n\n∆wi,j(t) = ηoiδj + α ·∆wi,j(t− 1) (5.46)\n\nWe accelerate on plateaus (avoiding quasi-\nstandstill on plateaus) and slow down on\ncraggy surfaces (preventing oscillations).\nMoreover, the effect of inertia can be var-\nied via the prefactor α, common val-\n\nαI ues are between 0.6 und 0.9. Addition-\nally, the momentum enables the positive\neffect that our skier swings back and\nforth several times in a minimum, and fi-\nnally lands in the minimum. Despite its\nnice one-dimensional appearance, the oth-\nerwise very rare error of leaving good min-\nima unfortunately occurs more frequently\nbecause of the momentum term – which\nmeans that this is again no optimal solu-\ntion (but we are by now accustomed to\nthis condition).\n\n5.6.2 Flat spot elimination prevents\nneurons from getting stuck\n\nIt must be pointed out that with the hy-\nperbolic tangent as well as with the Fermi\n\nFigure 5.13: We want to execute the gradient\ndescent like a skier crossing a slope, who would\nhardly stop immediately at the edge to the\nplateau.\n\nfunction the derivative outside of the close\nproximity of Θ is nearly 0. This results\nin the fact that it becomes very difficult\nto move neurons away from the limits of\nthe activation (flat spots), which could ex- neurons\n\nget stucktremely extend the learning time. This\nproblem can be dealt with by modifying\nthe derivative, for example by adding a\nconstant (e.g. 0.1), which is called flat\nspot elimination or – more colloquial –\nfudging.\n\nIt is an interesting observation, that suc-\ncess has also been achieved by using deriva-\ntives defined as constants [Fah88]. A nice\nexample making use of this effect is the\nfast hyperbolic tangent approximation by\nAnguita et al. introduced in section 3.2.6\non page 37. In the outer regions of it’s (as\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 97\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nwell approximated and accelerated) deriva-\ntive, it makes use of a small constant.\n\n5.6.3 The second derivative can be\nused, too\n\nAccording to David Parker [Par87],\nSecond order backpropagation also us-\nese the second gradient, i.e. the second\nmulti-dimensional derivative of the error\nfunction, to obtain more precise estimates\nof the correct ∆wi,j . Even higher deriva-\ntives only rarely improve the estimations.\nThus, less training cycles are needed but\nthose require much more computational ef-\nfort.\n\nIn general, we use further derivatives (i.e.\nHessian matrices, since the functions are\nmultidimensional) for higher order meth-\nods. As expected, the procedures reduce\nthe number of learning epochs, but signifi-\ncantly increase the computational effort of\nthe individual epochs. So in the end these\nprocedures often need more learning time\nthan backpropagation.\n\nThe quickpropagation learning proce-\ndure [Fah88] uses the second derivative of\nthe error propagation and locally under-\nstands the error function to be a parabola.\nWe analytically determine the vertex (i.e.\nthe lowest point) of the said parabola and\ndirectly jump to this point. Thus, this\nlearning procedure is a second-order proce-\ndure. Of course, this does not work with\nerror surfaces that cannot locally be ap-\nproximated by a parabola (certainly it is\nnot always possible to directly say whether\nthis is the case).\n\n5.6.4 Weight decay: Punishment of\nlarge weights\n\nThe weight decay according to Paul\nWerbos [Wer88] is a modification that ex-\ntends the error by a term punishing large\nweights. So the error under weight de-\ncay\n\nErrWD\n\ndoes not only increase proportionally to JErrWDthe actual error but also proportionally to\nthe square of the weights. As a result the\nnetwork is keeping the weights small dur-\ning learning.\n\nErrWD = Err + β · 1\n2\n∑\nw∈W\n\n(w)2\n\n︸ ︷︷ ︸\npunishment\n\n(5.47)\n\nThis approach is inspired by nature where\nsynaptic weights cannot become infinitely\nstrong as well. Additionally, due to these\n\nkeep weights\nsmallsmall weights, the error function often\n\nshows weaker fluctuations, allowing easier\nand more controlled learning.\n\nThe prefactor 1\n2 again resulted from sim-\n\nple pragmatics. The factor β controls the Jβstrength of punishment: Values from 0.001\nto 0.02 are often used here.\n\n5.6.5 Cutting networks down:\nPruning and Optimal Brain\nDamage\n\nIf we have executed the weight decay long\nenough and notice that for a neuron in\nthe input layer all successor weights are\n\nprune the\nnetwork0 or close to 0, we can remove the neuron,\n\n98 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.7 Initial configuration of a multilayer perceptron\n\nhence losing this neuron and some weights\nand thereby reduce the possibility that the\nnetwork will memorize. This procedure is\ncalled pruning.\n\nSuch a method to detect and delete un-\nnecessary weights and neurons is referred\nto as optimal brain damage [lCDS90].\nI only want to describe it briefly: The\nmean error per output neuron is composed\nof two competing terms. While one term,\nas usual, considers the difference between\noutput and teaching input, the other one\ntries to "press" a weight towards 0. If a\nweight is strongly needed to minimize the\nerror, the first term will win. If this is not\nthe case, the second term will win. Neu-\nrons which only have zero weights can be\npruned again in the end.\n\nThere are many other variations of back-\nprop and whole books only about this\nsubject, but since my aim is to offer an\noverview of neural networks, I just want\nto mention the variations above as a moti-\nvation to read on.\n\nFor some of these extensions it is obvi-\nous that they cannot only be applied to\nfeedforward networks with backpropaga-\ntion learning procedures.\n\nWe have gotten to know backpropagation\nand feedforward topology – now we have\nto learn how to build a neural network. It\nis of course impossible to fully communi-\ncate this experience in the framework of\nthis work. To obtain at least some of\nthis knowledge, I now advise you to deal\nwith some of the exemplary problems from\n4.6.\n\n5.7 Getting started – Initial\nconfiguration of a\nmultilayer perceptron\n\nAfter having discussed the backpropaga-\ntion of error learning procedure and know-\ning how to train an existing network, it\nwould be useful to consider how to imple-\nment such a network.\n\n5.7.1 Number of layers: Two or\nthree may often do the job,\nbut more are also used\n\nLet us begin with the trivial circumstance\nthat a network should have one layer of in-\nput neurons and one layer of output neu-\nrons, which results in at least two layers.\n\nAdditionally, we need – as we have already\nlearned during the examination of linear\nseparability – at least one hidden layer of\nneurons, if our problem is not linearly sep-\narable (which is, as we have seen, very\nlikely).\n\nIt is possible, as already mentioned, to\nmathematically prove that this MLP with\none hidden neuron layer is already capable\nof approximating arbitrary functions with\nany accuracy 5 – but it is necessary not\nonly to discuss the representability of a\nproblem by means of a perceptron but also\nthe learnability. Representability means\nthat a perceptron can, in principle, realize\n\n5 Note: We have not indicated the number of neu-\nrons in the hidden layer, we only mentioned the\nhypothetical possibility.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 99\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\na mapping - but learnability means that\nwe are also able to teach it.\n\nIn this respect, experience shows that two\nhidden neuron layers (or three trainable\nweight layers) can be very useful to solve\na problem, since many problems can be\nrepresented by a hidden layer but are very\ndifficult to learn.\n\nOne should keep in mind that any ad-\nditional layer generates additional sub-\nminima of the error function in which we\ncan get stuck. All these things consid-\nered, a promising way is to try it with\none hidden layer at first and if that fails,\nretry with two layers. Only if that fails,\none should consider more layers. However,\ngiven the increasing calculation power of\ncurrent computers, deep networks with\na lot of layers are also used with success.\n\n5.7.2 The number of neurons has\nto be tested\n\nThe number of neurons (apart from input\nand output layer, where the number of in-\nput and output neurons is already defined\nby the problem statement) principally cor-\nresponds to the number of free parameters\nof the problem to be represented.\n\nSince we have already discussed the net-\nwork capacity with respect to memorizing\nor a too imprecise problem representation,\nit is clear that our goal is to have as few\nfree parameters as possible but as many as\nnecessary.\n\nBut we also know that there is no stan-\ndard solution for the question of how many\n\nneurons should be used. Thus, the most\nuseful approach is to initially train with\nonly a few neurons and to repeatedly train\nnew networks with more neurons until the\nresult significantly improves and, particu-\nlarly, the generalization performance is not\naffected (bottom-up approach).\n\n5.7.3 Selecting an activation\nfunction\n\nAnother very important parameter for the\nway of information processing of a neural\nnetwork is the selection of an activa-\ntion function. The activation function\nfor input neurons is fixed to the identity\nfunction, since they do not process infor-\nmation.\n\nThe first question to be asked is whether\nwe actually want to use the same acti-\nvation function in the hidden layer and\nin the ouput layer – no one prevents us\nfrom choosing different functions. Gener-\nally, the activation function is the same for\nall hidden neurons as well as for the output\nneurons respectively.\n\nFor tasks of function approximation it\nhas been found reasonable to use the hy-\nperbolic tangent (left part of fig. 5.14 on\npage 102) as activation function of the hid-\nden neurons, while a linear activation func-\ntion is used in the output. The latter is\nabsolutely necessary so that we do not gen-\nerate a limited output intervall. Contrary\nto the input layer which uses linear acti-\nvation functions as well, the output layer\nstill processes information, because it has\n\n100 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.8 The 8-3-8 encoding problem and related problems\n\nthreshold values. However, linear activa-\ntion functions in the output can also cause\nhuge learning steps and jumping over good\nminima in the error surface. This can be\navoided by setting the learning rate to very\nsmall values in the output layer.\n\nAn unlimited output interval is not essen-\ntial for pattern recognition tasks6. If\nthe hyperbolic tangent is used in any case,\nthe output interval will be a bit larger. Un-\nlike with the hyperbolic tangent, with the\nFermi function (right part of fig. 5.14 on\nthe following page) it is difficult to learn\nsomething far from the threshold value\n(where its result is close to 0). However,\nhere a lot of freedom is given for selecting\nan activation function. But generally, the\ndisadvantage of sigmoid functions is the\nfact that they hardly learn something for\nvalues far from thei threshold value, unless\nthe network is modified.\n\n5.7.4 Weights should be initialized\nwith small, randomly chosen\nvalues\n\nThe initialization of weights is not as triv-\nial as one might think. If they are simply\ninitialized with 0, there will be no change\nin weights at all. If they are all initialized\nby the same value, they will all change\nequally during training. The simple so-\nlution of this problem is called symme-\ntry breaking, which is the initialization\nof weights with small random values. The\n\nrandom\ninitial\n\nweights 6 Generally, pattern recognition is understood as a\nspecial case of function approximation with a few\ndiscrete output possibilities.\n\nrange of random values could be the in-\nterval [−0.5; 0.5] not including 0 or values\nvery close to 0. This random initialization\nhas a nice side effect: Chances are that\nthe average of network inputs is close to 0,\na value that hits (in most activation func-\ntions) the region of the greatest derivative,\nallowing for strong learning impulses right\nfrom the start of learning.\n\nSNIPE: In Snipe, weights are initial-\nized randomly (if a synapse initial-\nization is wanted). The maximum\nabsolute weight value of a synapse\ninitialized at random can be set in\na NeuralNetworkDescriptor using the\nmethod setSynapseInitialRange.\n\n5.8 The 8-3-8 encoding\nproblem and related\nproblems\n\nThe 8-3-8 encoding problem is a clas-\nsic among the multilayer perceptron test\ntraining problems. In our MLP we\nhave an input layer with eight neurons\ni1, i2, . . . , i8, an output layer with eight\nneurons Ω1,Ω2, . . . ,Ω8 and one hidden\nlayer with three neurons. Thus, this net-\nwork represents a function B8 → B8. Now\nthe training task is that an input of a value\n1 into the neuron ij should lead to an out-\nput of a value 1 from the neuron Ωj (only\none neuron should be activated, which re-\nsults in 8 training samples.\n\nDuring the analysis of the trained network\nwe will see that the network with the 3\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 101\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\n−1\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−4 −2  0  2  4\n\nta\nnh\n\n(x\n)\n\nx\n\nHyperbolic Tangent\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−4 −2  0  2  4\n\nf(\nx)\n\nx\n\nFermi Function with Temperature Parameter\n\nFigure 5.14: As a reminder the illustration of the hyperbolic tangent (left) and the Fermi function\n(right). The Fermi function was expanded by a temperature parameter. The original Fermi function\nis thereby represented by dark colors, the temperature parameter of the modified Fermi functions\nare, ordered ascending by steepness, 1\n\n2 ,\n1\n5 ,\n\n1\n10 and 1\n\n25 .\n\nhidden neurons represents some kind of bi-\nnary encoding and that the above map-\nping is possible (assumed training time:\n≈ 104 epochs). Thus, our network is a ma-\nchine in which the input is first encoded\nand afterwards decoded again.\n\nAnalogously, we can train a 1024-10-1024\nencoding problem. But is it possible to\nimprove the efficiency of this procedure?\nCould there be, for example, a 1024-9-\n1024- or an 8-2-8-encoding network?\n\nYes, even that is possible, since the net-\nwork does not depend on binary encodings:\nThus, an 8-2-8 network is sufficient for our\nproblem. But the encoding of the network\nis far more difficult to understand (fig. 5.15\non the next page) and the training of the\nnetworks requires a lot more time.\n\nSNIPE: The static method\ngetEncoderSampleLesson in the class\nTrainingSampleLesson allows for creating\nsimple training sample lessons of arbitrary\n\ndimensionality for encoder problems like\nthe above.\n\nAn 8-1-8 network, however, does not work,\nsince the possibility that the output of one\nneuron is compensated by another one is\nessential, and if there is only one hidden\nneuron, there is certainly no compensatory\nneuron.\n\nExercises\n\nExercise 8. Fig. 5.4 on page 75 shows\na small network for the boolean functions\nAND and OR. Write tables with all computa-\ntional parameters of neural networks (e.g.\nnetwork input, activation etc.). Perform\nthe calculations for the four possible in-\nputs of the networks and write down the\nvalues of these variables for each input. Do\nthe same for the XOR network (fig. 5.9 on\npage 84).\n\n102 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 5.8 The 8-3-8 encoding problem and related problems\n\nFigure 5.15: Illustration of the functionality of\n8-2-8 network encoding. The marked points rep-\nresent the vectors of the inner neuron activation\nassociated to the samples. As you can see, it\nis possible to find inner activation formations so\nthat each point can be separated from the rest\nof the points by a straight line. The illustration\nshows an exemplary separation of one point.\n\nExercise 9.\n\n1. List all boolean functions B3 → B1,\nthat are linearly separable and char-\nacterize them exactly.\n\n2. List those that are not linearly sepa-\nrable and characterize them exactly,\ntoo.\n\nExercise 10. A simple 2-1 network shall\nbe trained with one single pattern by\nmeans of backpropagation of error and\nη = 0.1. Verify if the error\n\nErr = Errp = 1\n2(t− y)2\n\nconverges and if so, at what value. How\ndoes the error curve look like? Let the\npattern (p, t) be defined by p = (p1, p2) =\n(0.3, 0.7) and tΩ = 0.4. Randomly initalize\nthe weights in the interval [1;−1].\n\nExercise 11. A one-stage perceptron\nwith two input neurons, bias neuron\nand binary threshold function as activa-\ntion function divides the two-dimensional\nspace into two regions by means of a\nstraight line g. Analytically calculate a\nset of weight values for such a perceptron\nso that the following set P of the 6 pat-\nterns of the form (p1, p2, tΩ) with ε� 1 is\ncorrectly classified.\n\nP ={(0, 0,−1);\n(2,−1, 1);\n(7 + ε, 3− ε, 1);\n(7− ε, 3 + ε,−1);\n(0,−2− ε, 1);\n(0− ε,−2,−1)}\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 103\n\n\n\nChapter 5 The perceptron, backpropagation and its variants dkriesel.com\n\nExercise 12. Calculate in a comprehen-\nsible way one vector ∆W of all changes in\nweight by means of the backpropagation of\nerror procedure with η = 1. Let a 2-2-1\nMLP with bias neuron be given and let the\npattern be defined by\n\np = (p1, p2, tΩ) = (2, 0, 0.1).\n\nFor all weights with the target Ω the ini-\ntial value of the weights should be 1. For\nall other weights the initial value should\nbe 0.5. What is conspicuous about the\nchanges?\n\n104 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\nChapter 6\n\nRadial basis functions\nRBF networks approximate functions by stretching and compressing Gaussian\nbells and then summing them spatially shifted. Description of their functions\n\nand their learning process. Comparison with multilayer perceptrons.\n\nAccording to Poggio and Girosi [PG89]\nradial basis function networks (RBF net-\nworks) are a paradigm of neural networks,\nwhich was developed considerably later\nthan that of perceptrons. Like percep-\ntrons, the RBF networks are built in layers.\nBut in this case, they have exactly three\nlayers, i.e. only one single layer of hidden\nneurons.\n\nLike perceptrons, the networks have a\nfeedforward structure and their layers are\ncompletely linked. Here, the input layer\nagain does not participate in information\nprocessing. The RBF networks are -\nlike MLPs - universal function approxima-\ntors.\n\nDespite all things in common: What is the\ndifference between RBF networks and per-\nceptrons? The difference lies in the infor-\nmation processing itself and in the compu-\ntational rules within the neurons outside\nof the input layer. So, in a moment we\nwill define a so far unknown type of neu-\nrons.\n\n6.1 Components and\nstructure of an RBF\nnetwork\n\nInitially, we want to discuss colloquially\nand then define some concepts concerning\nRBF networks.\n\nOutput neurons: In an RBF network the\noutput neurons only contain the iden-\ntity as activation function and one\nweighted sum as propagation func-\ntion. Thus, they do little more than\nadding all input values and returning\nthe sum.\n\nHidden neurons are also called RBF neu-\nrons (as well as the layer in which\nthey are located is referred to as RBF\nlayer). As propagation function, each\nhidden neuron calculates a norm that\nrepresents the distance between the\ninput to the network and the so-called\nposition of the neuron (center). This\nis inserted into a radial activation\n\n105\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\nfunction which calculates and outputs\nthe activation of the neuron.\n\nDefinition 6.1 (RBF input neuron). Def-\ninition and representation is identical toinput\n\nis linear\nagain\n\nthe definition 5.1 on page 73 of the input\nneuron.\n\nDefinition 6.2 (Center of an RBF neu-\nron). The center ch of an RBF neuron\n\ncI\nh is the point in the input space where\nthe RBF neuron is located . In general,\n\nPosition\nin the input\n\nspace\nthe closer the input vector is to the center\nvector of an RBF neuron, the higher is its\nactivation.\n\nDefinition 6.3 (RBF neuron). The so-\ncalled RBF neurons h have a propaga-\ntion function fprop that determines the dis-\ntance between the center ch of a neuronImportant!\nand the input vector y. This distance rep-\nresents the network input. Then the net-\nwork input is sent through a radial basis\nfunction fact which returns the activation\nor the output of the neuron. RBF neurons\n\nare represented by the symbol WVUTPQRS||c,x||\nGauß\n\n.\n\nDefinition 6.4 (RBF output neuron).\nRBF output neurons Ω use the\n\nweighted sum as propagation function\nfprop, and the identity as activation func-\n\nonly sums\nup tion fact. They are represented by the sym-\n\nbol ONMLHIJKΣ\n� .\n\nDefinition 6.5 (RBF network). An\nRBF network has exactly three layers in\nthe following order: The input layer con-\nsisting of input neurons, the hidden layer\n(also called RBF layer) consisting of RBF\nneurons and the output layer consisting of\n\nRBF output neurons. Each layer is com-\n3 layers,\nfeedforwardpletely linked with the following one, short-\n\ncuts do not exist (fig. 6.1 on the next page)\n– it is a feedforward topology. The connec-\ntions between input layer and RBF layer\nare unweighted, i.e. they only transmit\nthe input. The connections between RBF\nlayer and output layer are weighted. The\noriginal definition of an RBF network only\nreferred to an output neuron, but – in anal-\nogy to the perceptrons – it is apparent that\nsuch a definition can be generalized. A\nbias neuron is not used in RBF networks.\nThe set of input neurons shall be repre-\nsented by I, the set of hidden neurons by JI,H,O\nH and the set of output neurons by O.\n\nTherefore, the inner neurons are called ra-\ndial basis neurons because from their def-\ninition follows directly that all input vec-\ntors with the same distance from the cen-\nter of a neuron also produce the same out-\nput value (fig. 6.2 on page 108).\n\n6.2 Information processing of\nan RBF network\n\nNow the question is, what can be realized\nby such a network and what is its purpose.\nLet us go over the RBF network from top\nto bottom: An RBF network receives the\ninput by means of the unweighted con-\nnections. Then the input vector is sent\nthrough a norm so that the result is a\nscalar. This scalar (which, by the way, can\nonly be positive due to the norm) is pro-\ncessed by a radial basis function, for exam-\n\n106 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 6.2 Information processing of an RBF network\n\n�� ��GFED@ABC�\n\n||yyyyyyyyyy\n\n�� ""EEEEEEEEEE\n\n((RRRRRRRRRRRRRRRRRRRR\n\n++VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV GFED@ABC�\n\n""EEEEEEEEEE\n\n��||yyyyyyyyyy\n\nvvllllllllllllllllllll\n\nsshhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh i1, i2, . . . , i|I|\n\nWVUTPQRS||c,x||\nGauß\n\n!!CCCCCCCCCC\n\n((QQQQQQQQQQQQQQQQQQQQ\n\n**VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV WVUTPQRS||c,x||\nGauß\n\n�� !!CCCCCCCCCC\n\n((QQQQQQQQQQQQQQQQQQQQ\nWVUTPQRS||c,x||\n\nGauß\n\n}}{{{{{{{{{{\n\n�� !!CCCCCCCCCC\nWVUTPQRS||c,x||\n\nGauß\n\nvvmmmmmmmmmmmmmmmmmmmm\n\n}}{{{{{{{{{{\n\n��\n\nWVUTPQRS||c,x||\nGauß\n\ntthhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n\nvvmmmmmmmmmmmmmmmmmmmm\n\n}}{{{{{{{{{{\nh1, h2, . . . , h|H|\n\nONMLHIJKΣ\n�\n\n��\n\nONMLHIJKΣ\n�\n\n��\n\nONMLHIJKΣ\n�\n\n��\n\nΩ1,Ω2, . . . ,Ω|O|\n\nFigure 6.1: An exemplary RBF network with two input neurons, five hidden neurons and three\noutput neurons. The connections to the hidden neurons are not weighted, they only transmit the\ninput. Right of the illustration you can find the names of the neurons, which coincide with the\nnames of the MLP neurons: Input neurons are called i, hidden neurons are called h and output\nneurons are called Ω. The associated sets are referred to as I, H and O.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 107\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\nFigure 6.2: Let ch be the center of an RBF neu-\nron h. Then the activation function facth is ra-\ndially symmetric around ch.\n\nple by a Gaussian bell (fig. 6.3 on the next\npage) .input\n\n→ distance\n→ Gaussian bell\n\n→ sum\n→ output\n\nThe output values of the different neurons\nof the RBF layer or of the different Gaus-\nsian bells are added within the third layer:\nbasically, in relation to the whole input\nspace, Gaussian bells are added here.\n\nSuppose that we have a second, a third\nand a fourth RBF neuron and therefore\nfour differently located centers. Each of\nthese neurons now measures another dis-\ntance from the input to its own center\nand de facto provides different values, even\nif the Gaussian bell is the same. Since\nthese values are finally simply accumu-\nlated in the output layer, one can easily\nsee that any surface can be shaped by drag-\n\nging, compressing and removing Gaussian\nbells and subsequently accumulating them.\nHere, the parameters for the superposition\nof the Gaussian bells are in the weights\nof the connections between the RBF layer\nand the output layer.\n\nFurthermore, the network architecture of-\nfers the possibility to freely define or train\nheight and width of the Gaussian bells –\ndue to which the network paradigm be-\ncomes even more versatile. We will get\nto know methods and approches for this\nlater.\n\n6.2.1 Information processing in\nRBF neurons\n\nRBF neurons process information by using\nnorms and radial basis functions\n\nAt first, let us take as an example a sim-\nple 1-4-1 RBF network. It is apparent\nthat we will receive a one-dimensional out-\nput which can be represented as a func-\ntion (fig. 6.4 on the facing page). Ad-\nditionally, the network includes the cen-\nters c1, c2, . . . , c4 of the four inner neurons\nh1, h2, . . . , h4, and therefore it has Gaus-\nsian bells which are finally added within\nthe output neuron Ω. The network also\npossesses four values σ1, σ2, . . . , σ4 which\ninfluence the width of the Gaussian bells.\nOn the contrary, the height of the Gaus-\nsian bell is influenced by the subsequent\nweights, since the individual output val-\nues of the bells are multiplied by those\nweights.\n\n108 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 6.2 Information processing of an RBF network\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−2 −1.5 −1 −0.5  0  0.5  1  1.5  2\n\nh(\nr)\n\nr\n\nGaussian in 1D Gaussian in 2D\n\n−2\n−1\n\n 0\n 1\n\nx\n\n−2\n−1\n\n 0\n 1\n\n 2\n\ny\n\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n\n 1\n\nh(r)\n\nFigure 6.3: Two individual one- or two-dimensional Gaussian bells. In both cases σ = 0.4 holds\nand the centers of the Gaussian bells lie in the coordinate origin. The distance r to the center (0, 0)\nis simply calculated according to the Pythagorean theorem: r =\n\n√\nx2 + y2.\n\n−0.6\n\n−0.4\n\n−0.2\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n 1.2\n\n 1.4\n\n−2  0  2  4  6  8\n\ny\n\nx\n\nFigure 6.4: Four different Gaussian bells in one-dimensional space generated by means of RBF\nneurons are added by an output neuron of the RBF network. The Gaussian bells have different\nheights, widths and positions. Their centers c1, c2, . . . , c4 are located at 0, 1, 3, 4, the widths\nσ1, σ2, . . . , σ4 at 0.4, 1, 0.2, 0.8. You can see a two-dimensional example in fig. 6.5 on the following\npage.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 109\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\nGaussian 1\n\n−2\n−1\n\n 0\n 1x\n\n−2\n−1\n\n 0\n 1\n\n 2\n\ny\n\n−1\n−0.5\n\n 0\n 0.5\n\n 1\n 1.5\n\n 2\n\nh(r)\nGaussian 2\n\n−2\n−1\n\n 0\n 1x\n\n−2\n−1\n\n 0\n 1\n\n 2\n\ny\n\n−1\n−0.5\n\n 0\n 0.5\n\n 1\n 1.5\n\n 2\n\nh(r)\n\nGaussian 3\n\n−2\n−1\n\n 0\n 1x\n\n−2\n−1\n\n 0\n 1\n\n 2\n\ny\n\n−1\n−0.5\n\n 0\n 0.5\n\n 1\n 1.5\n\n 2\n\nh(r)\nGaussian 4\n\n−2\n−1\n\n 0\n 1x\n\n−2\n−1\n\n 0\n 1\n\n 2\n\ny\n\n−1\n−0.5\n\n 0\n 0.5\n\n 1\n 1.5\n\n 2\n\nh(r)\n\nWVUTPQRS||c,x||\nGauß\n\n((QQQQQQQQQQQQQQQQQQQQ\nWVUTPQRS||c,x||\n\nGauß\n\n  AAAAAAAAAA\nWVUTPQRS||c,x||\n\nGauß\n\n~~}}}}}}}}}}\n\nWVUTPQRS||c,x||\nGauß\n\nvvmmmmmmmmmmmmmmmmmmm\n\nONMLHIJKΣ\n�\n\n��\n\nSum of the 4 Gaussians\n\n−2\n−1.5\n\n−1\n−0.5\n\n 0\n 0.5\n\n 1\n 1.5\n\n 2\n\nx\n\n−2\n−1.5\n\n−1\n−0.5\n\n 0\n 0.5\n\n 1\n 1.5\n\n 2\n\ny\n\n−1\n−0.75\n\n−0.5\n−0.25\n\n 0\n 0.25\n\n 0.5\n 0.75\n\n 1\n 1.25\n\n 1.5\n 1.75\n\n 2\n\nFigure 6.5: Four different Gaussian bells in two-dimensional space generated by means of RBF\nneurons are added by an output neuron of the RBF network. Once again r =\n\n√\nx2 + y2 applies for\n\nthe distance. The heights w, widths σ and centers c = (x, y) are: w1 = 1, σ1 = 0.4, c1 = (0.5, 0.5),\nw2 = −1, σ2 = 0.6, c2 = (1.15,−1.15), w3 = 1.5, σ3 = 0.2, c3 = (−0.5,−1), w4 = 0.8, σ4 =\n1.4, c4 = (−2, 0).\n\n110 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 6.2 Information processing of an RBF network\n\nSince we use a norm to calculate the dis-\ntance between the input vector and the\ncenter of a neuron h, we have different\nchoices: Often the Euclidian norm is cho-\nsen to calculate the distance:\n\nrh = ||x− ch|| (6.1)\n\n=\n√∑\ni∈I\n\n(xi − ch,i)2 (6.2)\n\nRemember: The input vector was referred\nto as x. Here, the index i runs through\nthe input neurons and thereby through the\ninput vector components and the neuron\ncenter components. As we can see, the\nEuclidean distance generates the squared\ndifferences of all vector components, adds\nthem and extracts the root of the sum.\nIn two-dimensional space this corresponds\nto the Pythagorean theorem. From the\ndefinition of a norm directly follows that\nthe distance can only be positive. Strictly\nspeaking, we hence only use the positive\npart of the activation function. By the\nway, activation functions other than the\nGaussian bell are possible. Normally, func-\ntions that are monotonically decreasing\nover the interval [0;∞] are chosen.\n\nNow that we know the distance rh be-\nrhI tween the input vector x and the center\n\nch of the RBF neuron h, this distance has\nto be passed through the activation func-\ntion. Here we use, as already mentioned,\na Gaussian bell:\n\nfact(rh) = e\n\n(\n−r2\nh\n\n2σ2\nh\n\n)\n(6.3)\n\nIt is obvious that both the center ch and\nthe width σh can be seen as part of the\n\nactivation function fact, and hence the ac-\ntivation functions should not be referred\nto as fact simultaneously. One solution\nwould be to number the activation func-\ntions like fact1, fact2, . . . , fact|H| withH be-\ning the set of hidden neurons. But as a\nresult the explanation would be very con-\nfusing. So I simply use the name fact for\nall activation functions and regard σ and\nc as variables that are defined for individ-\nual neurons but no directly included in the\nactivation function.\n\nThe reader will certainly notice that in the\nliterature the Gaussian bell is often nor-\nmalized by a multiplicative factor. We\ncan, however, avoid this factor because\nwe are multiplying anyway with the subse-\nquent weights and consecutive multiplica-\ntions, first by a normalization factor and\nthen by the connections’ weights, would\nonly yield different factors there. We do\nnot need this factor (especially because for\nour purpose the integral of the Gaussian\nbell must not always be 1) and therefore\nsimply leave it out.\n\n6.2.2 Some analytical thoughts\nprior to the training\n\nThe output yΩ of an RBF output neuron\nΩ results from combining the functions of\nan RBF neuron to\n\nyΩ =\n∑\nh∈H\n\nwh,Ω · fact (||x− ch||) . (6.4)\n\nSuppose that similar to the multilayer per-\nceptron we have a set P , that contains |P |\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 111\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\ntraining samples (p, t). Then we obtain\n|P | functions of the form\n\nyΩ =\n∑\nh∈H\n\nwh,Ω · fact (||p− ch||) , (6.5)\n\ni.e. one function for each training sam-\nple.\n\nOf course, with this effort we are aiming\nat letting the output y for all training\npatterns p converge to the corresponding\nteaching input t.\n\n6.2.2.1 Weights can simply be\ncomputed as solution of a\nsystem of equations\n\nThus, we have |P | equations. Now let us\nassume that the widths σ1, σ2, . . . , σk, the\ncenters c1, c2, . . . , ck and the training sam-\nples p including the teaching input t are\ngiven. We are looking for the weights wh,Ω\nwith |H| weights for one output neuron\nΩ. Thus, our problem can be seen as a\nsystem of equations since the only thing\nwe want to change at the moment are the\nweights.\n\nThis demands a distinction of cases con-\ncerning the number of training samples |P |\nand the number of RBF neurons |H|:\n\n|P | = |H|: If the number of RBF neurons\nequals the number of patterns, i.e.\n|P | = |H|, the equation can be re-\nduced to a matrix multiplication\n\nsimply\ncalculate\nweights\n\nT = M ·G (6.6)\n\n⇔ M−1 · T = M−1 ·M ·G (6.7)\n\n⇔ M−1 · T = E ·G (6.8)\n\n⇔ M−1 · T = G, (6.9)\n\nwhere\n\n. T is the vector of the teaching JTinputs for all training samples,\n\n. M is the |P | × |H| matrix of JMthe outputs of all |H| RBF neu-\nrons to |P | samples (remember:\n|P | = |H|, the matrix is squared\nand we can therefore attempt to\ninvert it),\n\n. G is the vector of the desired JGweights and\n\n. E is a unit matrix with the same JEsize as G.\n\nMathematically speaking, we can sim-\nply calculate the weights: In the case\nof |P | = |H| there is exactly one RBF\nneuron available per training sample.\nThis means, that the network exactly\nmeets the |P | existing nodes after hav-\ning calculated the weights, i.e. it per-\nforms a precise interpolation. To\ncalculate such an equation we cer-\ntainly do not need an RBF network,\nand therefore we can proceed to the\nnext case.\n\nExact interpolation must not be mis-\ntaken for the memorizing ability men-\ntioned with the MLPs: First, we are\nnot talking about the training of RBF\n\n112 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 6.2 Information processing of an RBF network\n\nnetworks at the moment. Second,\nit could be advantageous for us and\nmight in fact be intended if the net-\nwork exactly interpolates between the\nnodes.\n\n|P | < |H|: The system of equations is\nunder-determined, there are more\nRBF neurons than training samples,\ni.e. |P | < |H|. Certainly, this case\nnormally does not occur very often.\nIn this case, there is a huge variety\nof solutions which we do not need in\nsuch detail. We can select one set of\nweights out of many obviously possi-\nble ones.\n\n|P | > |H|: But most interesting for fur-\nther discussion is the case if there\nare significantly more training sam-\nples than RBF neurons, that means\n|P | > |H|. Thus, we again want\nto use the generalization capability of\nthe neural network.\n\nIf we have more training samples than\nRBF neurons, we cannot assume that\nevery training sample is exactly hit.\nSo, if we cannot exactly hit the points\nand therefore cannot just interpolate\nas in the aforementioned ideal case\nwith |P | = |H|, we must try to find\na function that approximates our\ntraining set P as closely as possible:\nAs with the MLP we try to reduce\nthe sum of the squared error to a min-\nimum.\n\nHow do we continue the calculation\nin the case of |P | > |H|? As above,\nto solve the system of equations, we\n\nhave to find the solution M of a ma-\ntrix multiplication\n\nT = M ·G. (6.10)\n\nThe problem is that this time we can-\nnot invert the |P | × |H| matrix M be-\ncause it is not a square matrix (here,\n|P | 6= |H| is true). Here, we have\nto use the Moore-Penrose pseudo\ninverse M+ which is defined by\n\nJM+\n\nM+ = (MT ·M)−1 ·MT (6.11)\n\nAlthough the Moore-Penrose pseudo\ninverse is not the inverse of a matrix,\nit can be used similarly in this case1.\nWe get equations that are very similar\nto those in the case of |P | = |H|:\n\nT = M ·G (6.12)\n\n⇔ M+ · T = M+ ·M ·G (6.13)\n\n⇔ M+ · T = E ·G (6.14)\n\n⇔ M+ · T = G (6.15)\n\nAnother reason for the use of the\nMoore-Penrose pseudo inverse is the\nfact that it minimizes the squared\nerror (which is our goal): The esti-\nmate of the vector G in equation 6.15\ncorresponds to the Gauss-Markov\nmodel known from statistics, which\nis used to minimize the squared error.\nIn the aforementioned equations 6.11\nand the following ones please do not\nmistake the T in MT (of the trans-\npose of the matrix M) for the T of\nthe vector of all teaching inputs.\n\n1 Particularly, M+ = M−1 is true if M is invertible.\nI do not want to go into detail of the reasons for\nthese circumstances and applications ofM+ - they\ncan easily be found in literature for linear algebra.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 113\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\n6.2.2.2 The generalization on several\noutputs is trivial and not quite\ncomputationally expensive\n\nWe have found a mathematically exact\nway to directly calculate the weights.\nWhat will happen if there are several out-\nput neurons, i.e. |O| > 1, with O being, as\nusual, the set of the output neurons Ω? In\nthis case, as we have already indicated, it\ndoes not change much: The additional out-\nput neurons have their own set of weights\nwhile we do not change the σ and c of the\nRBF layer. Thus, in an RBF network it is\neasy for given σ and c to realize a lot of\noutput neurons since we only have to cal-\nculate the individual vector of weights\n\nGΩ = M+ · TΩ (6.16)\n\nfor every new output neuron Ω, whereas\nthe matrix M+, which generally requires\na lot of computational effort, always stays\nthe same: So it is quite inexpensive – atinexpensive\n\noutput\ndimension\n\nleast concerning the computational com-\nplexity – to add more output neurons.\n\n6.2.2.3 Computational effort and\naccuracy\n\nFor realistic problems it normally applies\nthat there are considerably more training\nsamples than RBF neurons, i.e. |P | �\n|H|: You can, without any difficulty, use\n106 training samples, if you like. Theoreti-\ncally, we could find the terms for the math-\nematically correct solution on the black-\nboard (after a very long time), but such\ncalculations often seem to be imprecise\n\nand very time-consuming (matrix inver-\nsions require a lot of computational ef-\nfort).\n\nFurthermore, our Moore-Penrose pseudo-\ninverse is, in spite of numeric stabil-\nity, no guarantee that the output vector\n\nM+ complex\nand imprecisecorresponds to the teaching vector, be-\n\ncause such extensive computations can be\nprone to many inaccuracies, even though\nthe calculation is mathematically correct:\nOur computers can only provide us with\n(nonetheless very good) approximations of\nthe pseudo-inverse matrices. This means\nthat we also get only approximations of\nthe correct weights (maybe with a lot of\naccumulated numerical errors) and there-\nfore only an approximation (maybe very\nrough or even unrecognizable) of the de-\nsired output.\n\nIf we have enough computing power to an-\nalytically determine a weight vector, we\nshould use it nevertheless only as an initial\nvalue for our learning process, which leads\nus to the real training methods – but oth-\nerwise it would be boring, wouldn’t it?\n\n6.3 Combinations of equation\nsystem and gradient\nstrategies are useful for\ntraining\n\nAnalogous to the MLP we perform a gra-\ndient descent to find the suitable weights\nby means of the already well known delta retraining\n\ndelta rulerule. Here, backpropagation is unneces-\nsary since we only have to train one single\n\n114 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 6.3 Training of RBF networks\n\nweight layer – which requires less comput-\ning time.\n\nWe know that the delta rule is\n\n∆wh,Ω = η · δΩ · oh, (6.17)\n\nin which we now insert as follows:\n\n∆wh,Ω = η · (tΩ − yΩ) · fact(||p− ch||)\n(6.18)\n\nHere again I explicitly want to mention\nthat it is very popular to divide the train-\ning into two phases by analytically com-\nputing a set of weights and then refining\nit by training with the delta rule.\n\nThere is still the question whether to learn\noffline or online. Here, the answer is sim-\nilar to the answer for the multilayer per-\nceptron: Initially, one often trains onlinetraining\n\nin phases (faster movement across the error surface).\nThen, after having approximated the so-\nlution, the errors are once again accumu-\nlated and, for a more precise approxima-\ntion, one trains offline in a third learn-\ning phase. However, similar to the MLPs,\nyou can be successful by using many meth-\nods.\n\nAs already indicated, in an RBF network\nnot only the weights between the hidden\nand the output layer can be optimized. So\nlet us now take a look at the possibility to\nvary σ and c.\n\n6.3.1 It is not always trivial to\ndetermine centers and widths\nof RBF neurons\n\nIt is obvious that the approximation accu-\nracy of RBF networks can be increased by\nadapting the widths and positions of the\nGaussian bells in the input space to the\nproblem that needs to be approximated.\nThere are several methods to deal with the\ncenters c and the widths σ of the Gaussian vary\n\nσ and cbells:\n\nFixed selection: The centers and widths\ncan be selected in a fixed manner and\nregardless of the training samples –\nthis is what we have assumed until\nnow.\n\nConditional, fixed selection: Again cen-\nters and widths are selected fixedly,\nbut we have previous knowledge\nabout the functions to be approxi-\nmated and comply with it.\n\nAdaptive to the learning process: This\nis definitely the most elegant variant,\nbut certainly the most challenging\none, too. A realization of this\napproach will not be discussed in\nthis chapter but it can be found in\nconnection with another network\ntopology (section 10.6.1).\n\n6.3.1.1 Fixed selection\n\nIn any case, the goal is to cover the in-\nput space as evenly as possible. Here,\nwidths of 2\n\n3 of the distance between the\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 115\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\nFigure 6.6: Example for an even coverage of a\ntwo-dimensional input space by applying radial\nbasis functions.\n\ncenters can be selected so that the Gaus-\nsian bells overlap by approx. "one third"2\n(fig. 6.6). The closer the bells are set the\nmore precise but the more time-consuming\nthe whole thing becomes.\n\nThis may seem to be very inelegant, but\nin the field of function approximation we\ncannot avoid even coverage. Here it is\nuseless if the function to be approximated\nis precisely represented at some positions\nbut at other positions the return value is\nonly 0. However, the high input dimen-\nsion requires a great many RBF neurons,\nwhich increases the computational effortinput\n\ndimension\nvery expensive\n\nexponentially with the dimension – and is\n\n2 It is apparent that a Gaussian bell is mathemati-\ncally infinitely wide, therefore I ask the reader to\napologize this sloppy formulation.\n\nresponsible for the fact that six- to ten-\ndimensional problems in RBF networks\nare already called "high-dimensional" (an\nMLP, for example, does not cause any\nproblems here).\n\n6.3.1.2 Conditional, fixed selection\n\nSuppose that our training samples are not\nevenly distributed across the input space.\nIt then seems obvious to arrange the cen-\nters and sigmas of the RBF neurons by\nmeans of the pattern distribution. So the\ntraining patterns can be analyzed by statis-\ntical techniques such as a cluster analysis,\nand so it can be determined whether there\nare statistical factors according to which\nwe should distribute the centers and sig-\nmas (fig. 6.7 on the facing page).\n\nA more trivial alternative would be to\nset |H| centers on positions randomly se-\nlected from the set of patterns. So this\nmethod would allow for every training pat-\ntern p to be directly in the center of a neu-\nron (fig. 6.8 on the next page). This is\nnot yet very elegant but a good solution\nwhen time is an issue. Generally, for this\nmethod the widths are fixedly selected.\n\nIf we have reason to believe that the set\nof training samples is clustered, we can\nuse clustering methods to determine them.\nThere are different methods to determine\nclusters in an arbitrarily dimensional set\nof points. We will be introduced to some\nof them in excursus A. One neural cluster-\ning method are the so-called ROLFs (sec-\ntion A.5), and self-organizing maps are\n\n116 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 6.3 Training of RBF networks\n\nFigure 6.7: Example of an uneven coverage of\na two-dimensional input space, of which we\nhave previous knowledge, by applying radial ba-\nsis functions.\n\nalso useful in connection with determin-\ning the position of RBF neurons (section\n10.6.1). Using ROLFs, one can also receive\nindicators for useful radii of the RBF neu-\nrons. Learning vector quantisation (chap-\nter 9) has also provided good results. All\nthese methods have nothing to do with\nthe RBF networks themselves but are only\nused to generate some previous knowledge.\nTherefore we will not discuss them in this\nchapter but independently in the indicated\nchapters.\n\nAnother approach is to use the approved\nmethods: We could slightly move the po-\nsitions of the centers and observe how our\nerror function Err is changing – a gradient\ndescent, as already known from the MLPs.\n\nFigure 6.8: Example of an uneven coverage of\na two-dimensional input space by applying radial\nbasis functions. The widths were fixedly selected,\nthe centers of the neurons were randomly dis-\ntributed throughout the training patterns. This\ndistribution can certainly lead to slightly unrepre-\nsentative results, which can be seen at the single\ndata point down to the left.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 117\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\nIn a similar manner we could look how the\nerror depends on the values σ. Analogous\nto the derivation of backpropagation we\nderive\n\n∂Err(σhch)\n∂σh\n\nand ∂Err(σhch)\n∂ch\n\n.\n\nSince the derivation of these terms corre-\nsponds to the derivation of backpropaga-\ntion we do not want to discuss it here.\n\nBut experience shows that no convincing\nresults are obtained by regarding how the\nerror behaves depending on the centers\nand sigmas. Even if mathematics claim\nthat such methods are promising, the gra-\ndient descent, as we already know, leads\nto problems with very craggy error sur-\nfaces.\n\nAnd that is the crucial point: Naturally,\nRBF networks generate very craggy er-\nror surfaces because, if we considerably\nchange a c or a σ, we will significantly\nchange the appearance of the error func-\ntion.\n\n6.4 Growing RBF networks\nautomatically adjust the\nneuron density\n\nIn growing RBF networks, the number\n|H| of RBF neurons is not constant. A\ncertain number |H| of neurons as well as\ntheir centers ch and widths σh are previ-\nously selected (e.g. by means of a cluster-\ning method) and then extended or reduced.\n\nIn the following text, only simple mecha-\nnisms are sketched. For more information,\nI refer to [Fri94].\n\n6.4.1 Neurons are added to places\nwith large error values\n\nAfter generating this initial configuration\nthe vector of the weights G is analytically\ncalculated. Then all specific errors Errp\nconcerning the set P of the training sam-\nples are calculated and the maximum spe-\ncific error\n\nmax\nP\n\n(Errp)\n\nis sought.\n\nThe extension of the network is simple:\nWe replace this maximum error with a new\n\nreplace\nerror with\nneuron\n\nRBF neuron. Of course, we have to exer-\ncise care in doing this: IF the σ are small,\nthe neurons will only influence each other\nif the distance between them is short. But\nif the σ are large, the already exisiting\nneurons are considerably influenced by the\nnew neuron because of the overlapping of\nthe Gaussian bells.\n\nSo it is obvious that we will adjust the al-\nready existing RBF neurons when adding\nthe new neuron.\n\nTo put it simply, this adjustment is made\nby moving the centers c of the other neu-\nrons away from the new neuron and re-\nducing their width σ a bit. Then the\ncurrent output vector y of the network is\ncompared to the teaching input t and the\nweight vector G is improved by means of\ntraining. Subsequently, a new neuron can\nbe inserted if necessary. This method is\n\n118 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 6.5 Comparing RBF networks and multilayer perceptrons\n\nparticularly suited for function approxima-\ntions.\n\n6.4.2 Limiting the number of\nneurons\n\nHere it is mandatory to see that the net-\nwork will not grow ad infinitum, which can\nhappen very fast. Thus, it is very useful\nto previously define a maximum number\nfor neurons |H|max.\n\n6.4.3 Less important neurons are\ndeleted\n\nWhich leads to the question whether it\nis possible to continue learning when this\nlimit |H|max is reached. The answer is:\nthis would not stop learning. We only have\nto look for the "most unimportant" neuron\nand delete it. A neuron is, for example,\nunimportant for the network if there is an-\nother neuron that has a similar function:\nIt often occurs that two Gaussian bells ex-\nactly overlap and at such a position, for\n\ndelete\nunimportant\n\nneurons\ninstance, one single neuron with a higher\nGaussian bell would be appropriate.\n\nBut to develop automated procedures in\norder to find less relevant neurons is highly\nproblem dependent and we want to leave\nthis to the programmer.\n\nWith RBF networks and multilayer per-\nceptrons we have already become ac-\nquainted with and extensivley discussed\ntwo network paradigms for similar prob-\nlems. Therefore we want to compare these\n\ntwo paradigms and look at their advan-\ntages and disadvantages.\n\n6.5 Comparing RBF networks\nand multilayer\nperceptrons\n\nWe will compare multilayer perceptrons\nand RBF networks with respect to differ-\nent aspects.\n\nInput dimension: We must be careful\nwith RBF networks in high-\ndimensional functional spaces since\nthe network could very quickly\nrequire huge memory storage and\ncomputational effort. Here, a\nmultilayer perceptron would cause\nless problems because its number of\nneuons does not grow exponentially\nwith the input dimension.\n\nCenter selection: However, selecting the\ncenters c for RBF networks is (despite\nthe introduced approaches) still a ma-\njor problem. Please use any previous\nknowledge you have when applying\nthem. Such problems do not occur\nwith the MLP.\n\nOutput dimension: The advantage of\nRBF networks is that the training is\nnot much influenced when the output\ndimension of the network is high.\nFor an MLP, a learning procedure\nsuch as backpropagation thereby will\nbe very time-consuming.\n\nExtrapolation: Advantage as well as dis-\nadvantage of RBF networks is the lack\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 119\n\n\n\nChapter 6 Radial basis functions dkriesel.com\n\nof extrapolation capability: An RBF\nnetwork returns the result 0 far away\nfrom the centers of the RBF layer. On\nthe one hand it does not extrapolate,\nunlike the MLP it cannot be used\nfor extrapolation (whereby we could\nnever know if the extrapolated values\nof the MLP are reasonable, but expe-\nrience shows that MLPs are suitable\nfor that matter). On the other hand,\nunlike the MLP the network is capa-Important!\nble to use this 0 to tell us "I don’t\nknow", which could be an advantage.\n\nLesion tolerance: For the output of an\nMLP, it is no so important if a weight\nor a neuron is missing. It will only\nworsen a little in total. If a weight\nor a neuron is missing in an RBF net-\nwork then large parts of the output\nremain practically uninfluenced. But\none part of the output is heavily af-\nfected because a Gaussian bell is di-\nrectly missing. Thus, we can choose\nbetween a strong local error for lesion\nand a weak but global error.\n\nSpread: Here the MLP is "advantaged"\nsince RBF networks are used consid-\nerably less often – which is not always\nunderstood by professionals (at least\nas far as low-dimensional input spaces\nare concerned). The MLPs seem to\nhave a considerably longer tradition\nand they are working too good to take\nthe effort to read some pages of this\nwork about RBF networks) :-).\n\nExercises\n\nExercise 13. An |I|-|H|-|O| RBF net-\nwork with fixed widths and centers of the\nneurons should approximate a target func-\ntion u. For this, |P | training samples of\nthe form (p, t) of the function u are given.\nLet |P | > |H| be true. The weights should\nbe analytically determined by means of\nthe Moore-Penrose pseudo inverse. Indi-\ncate the running time behavior regarding\n|P | and |O| as precisely as possible.\n\nNote: There are methods for matrix mul-\ntiplications and matrix inversions that are\nmore efficient than the canonical methods.\nFor better estimations, I recommend to\nlook for such methods (and their complex-\nity). In addition to your complexity calcu-\nlations, please indicate the used methods\ntogether with their complexity.\n\n120 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\nChapter 7\n\nRecurrent perceptron-like networks\nSome thoughts about networks with internal states.\n\nGenerally, recurrent networks are net-\nworks that are capable of influencing them-\nselves by means of recurrences, e.g. by\nincluding the network output in the follow-\ning computation steps. There are many\ntypes of recurrent networks of nearly arbi-\ntrary form, and nearly all of them are re-\nferred to as recurrent neural networks.\nAs a result, for the few paradigms in-\ntroduced here I use the name recurrent\nmultilayer perceptrons.\n\nApparently, such a recurrent network is ca-\npable to compute more than the ordinary\nMLP: If the recurrent weights are set to 0,\n\nmore capable\nthan MLP the recurrent network will be reduced to\n\nan ordinary MLP. Additionally, the recur-\nrence generates different network-internal\nstates so that different inputs can produce\ndifferent outputs in the context of the net-\nwork state.\n\nRecurrent networks in themselves have a\ngreat dynamic that is mathematically dif-\nficult to conceive and has to be discussed\nextensively. The aim of this chapter is\nonly to briefly discuss how recurrences can\n\nbe structured and how network-internal\nstates can be generated. Thus, I will\nbriefly introduce two paradigms of recur-\nrent networks and afterwards roughly out-\nline their training.\n\nWith a recurrent network an input x that\nis constant over time may lead to differ-\nent results: On the one hand, the network state\n\ndynamicscould converge, i.e. it could transform it-\nself into a fixed state and at some time re-\nturn a fixed output value y. On the other\nhand, it could never converge, or at least\nnot until a long time later, so that it can\nno longer be recognized, and as a conse-\nquence, y constantly changes.\n\nIf the network does not converge, it is, for\nexample, possible to check if periodicals\nor attractors (fig. 7.1 on the following\npage) are returned. Here, we can expect\nthe complete variety of dynamical sys-\ntems. That is the reason why I particu-\nlarly want to refer to the literature con-\ncerning dynamical systems.\n\n121\n\n\n\nChapter 7 Recurrent perceptron-like networks (depends on chapter 5) dkriesel.com\n\nFigure 7.1: The Roessler attractor\n\nFurther discussions could reveal what will\nhappen if the input of recurrent networks\nis changed.\n\nIn this chapter the related paradigms of\nrecurrent networks according to Jordan\nand Elman will be introduced.\n\n7.1 Jordan networks\n\nA Jordan network [Jor86] is a multi-\nlayer perceptron with a set K of so-called\ncontext neurons k1, k2, . . . , k|K|. There\nis one context neuron per output neuron\n(fig. 7.2 on the next page). In principle, a\ncontext neuron just memorizes an output\nuntil it can be processed in the next time output\n\nneurons\nare buffered\n\nstep. Therefore, there are weighted con-\nnections between each output neuron and\none context neuron. The stored values are\nreturned to the actual network by means\nof complete links between the context neu-\nrons and the input layer.\n\nIn the originial definition of a Jordan net-\nwork the context neurons are also recur-\nrent to themselves via a connecting weight\nλ. But most applications omit this recur-\nrence since the Jordan network is already\nvery dynamic and difficult to analyze, even\nwithout these additional recurrences.\n\nDefinition 7.1 (Context neuron). A con-\ntext neuron k receives the output value of\nanother neuron i at a time t and then reen-\nters it into the network at a time (t+ 1).\n\nDefinition 7.2 (Jordan network). A Jor-\ndan network is a multilayer perceptron\n\n122 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 7.2 Elman networks\n\n�� ��GFED@ABCi1\n\n~~}}}}}}}}}\n\n  AAAAAAAAA\n\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\n~~}}}}}}}}}\n\n  AAAAAAAAA\nGFED@ABCk2\n\n����xx\n\nGFED@ABCk1\n\n��{{vvGFED@ABCh1\n\n  AAAAAAAAA\n\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCh2\n\n~~}}}}}}}}}\n\n  AAAAAAAAA\nGFED@ABCh3\n\n~~}}}}}}}}}\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\nGFED@ABCΩ1\n\n��\n\n@A BC\n\nOO\n\nGFED@ABCΩ2\n\n��\n\n�� ��\n\nOO\n\nFigure 7.2: Illustration of a Jordan network. The network output is buffered in the context neurons\nand with the next time step it is entered into the network together with the new input.\n\nwith one context neuron per output neu-\nron. The set of context neurons is called\nK. The context neurons are completely\nlinked toward the input layer of the net-\nwork.\n\n7.2 Elman networks\n\nThe Elman networks (a variation of\nthe Jordan networks) [Elm90] have con-\ntext neurons, too, but one layer of context\nneurons per information processing neu-\nron layer (fig. 7.3 on the following page).\nThus, the outputs of each hidden neuron\n\nnearly every-\nthing is\nbuffered\n\nor output neuron are led into the associ-\nated context layer (again exactly one con-\ntext neuron per neuron) and from there it\nis reentered into the complete neuron layer\n\nduring the next time step (i.e. again a com-\nplete link on the way back). So the com-\nplete information processing part1 of the\nMLP exists a second time as a "context\nversion" – which once again considerably\nincreases dynamics and state variety.\n\nCompared with Jordan networks the El-\nman networks often have the advantage to\nact more purposeful since every layer can\naccess its own context.\n\nDefinition 7.3 (Elman network). An El-\nman network is an MLP with one con-\ntext neuron per information processing\nneuron. The set of context neurons is\ncalledK. This means that there exists one\ncontext layer per information processing\n\n1 Remember: The input layer does not process in-\nformation.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 123\n\n\n\nChapter 7 Recurrent perceptron-like networks (depends on chapter 5) dkriesel.com\n\n�� ��GFED@ABCi1\n\n~~~~~~~~~~~~\n\n  @@@@@@@@@@\n\n**UUUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\n~~~~~~~~~~~~\n\n  @@@@@@@@@@\n\nGFED@ABCh1\n\n��@@@@@@@@@\n\n**UUUUUUUUUUUUUUUUUUUUUUUUUU 44\nGFED@ABCh2\n\n��~~~~~~~~~\n\n��@@@@@@@@@ 55\nGFED@ABCh3\n\n��~~~~~~~~~\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii 55\nONMLHIJKkh1\n\nuu zzvv ONMLHIJKkh2\n\nwwuutt ONMLHIJKkh3\n\nvvuutt\n\nGFED@ABCΩ1\n\n��\n\n55\nGFED@ABCΩ2 55\n\n��\n\nONMLHIJKkΩ1\n\nuu ww ONMLHIJKkΩ2\n\nuu vv\n\nFigure 7.3: Illustration of an Elman network. The entire information processing part of the network\nexists, in a way, twice. The output of each neuron (except for the output of the input neurons)\nis buffered and reentered into the associated layer. For the reason of clarity I named the context\nneurons on the basis of their models in the actual network, but it is not mandatory to do so.\n\nneuron layer with exactly the same num-\nber of context neurons. Every neuron has\na weighted connection to exactly one con-\ntext neuron while the context layer is com-\npletely linked towards its original layer.\n\nNow it is interesting to take a look at the\ntraining of recurrent networks since, for in-\nstance, ordinary backpropagation of error\ncannot work on recurrent networks. Once\nagain, the style of the following part is\nrather informal, which means that I will\nnot use any formal definitions.\n\n7.3 Training recurrent\nnetworks\n\nIn order to explain the training as compre-\nhensible as possible, we have to agree on\nsome simplifications that do not affect the\nlearning principle itself.\n\nSo for the training let us assume that in\nthe beginning the context neurons are ini-\ntiated with an input, since otherwise they\nwould have an undefined input (this is no\nsimplification but reality).\n\nFurthermore, we use a Jordan network\nwithout a hidden neuron layer for our\ntraining attempts so that the output neu-\n\n124 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 7.3 Training recurrent networks\n\nrons can directly provide input. This ap-\nproach is a strong simplification because\ngenerally more complicated networks are\nused. But this does not change the learn-\ning principle.\n\n7.3.1 Unfolding in time\n\nRemember our actual learning procedure\nfor MLPs, the backpropagation of error,\nwhich backpropagates the delta values.\nSo, in case of recurrent networks the\ndelta values would backpropagate cycli-\ncally through the network again and again,\nwhich makes the training more difficult.\nOn the one hand we cannot know which\nof the many generated delta values for a\nweight should be selected for training, i.e.\nwhich values are useful. On the other hand\nwe cannot definitely know when learning\nshould be stopped. The advantage of re-\ncurrent networks are great state dynamics\nwithin the network; the disadvantage of\nrecurrent networks is that these dynamics\nare also granted to the training and there-\nfore make it difficult.\n\nOne learning approach would be the at-\ntempt to unfold the temporal states of\nthe network (fig. 7.4 on the next page):\nRecursions are deleted by putting a sim-\nilar network above the context neurons,\ni.e. the context neurons are, as a man-\nner of speaking, the output neurons of\nthe attached network. More generally spo-\nken, we have to backtrack the recurrences\nand place "‘earlier"’ instances of neurons\nin the network – thus creating a larger,\n\nbut forward-oriented network without re-\ncurrences. This enables training a recur-\nrent network with any training strategy\ndeveloped for non-recurrent ones. Here,\n\nattach\nthe same\nnetwork\nto each\ncontext\nlayer\n\nthe input is entered as teaching input into\nevery "copy" of the input neurons. This\ncan be done for a discrete number of time\nsteps. These training paradigms are called\nunfolding in time [MP69]. After the un-\nfolding a training by means of backpropa-\ngation of error is possible.\n\nBut obviously, for one weight wi,j sev-\neral changing values ∆wi,j are received,\nwhich can be treated differently: accumu-\nlation, averaging etc. A simple accumu-\nlation could possibly result in enormous\nchanges per weight if all changes have the\nsame sign. Hence, also the average is not\nto be underestimated. We could also intro-\nduce a discounting factor, which weakens\nthe influence of ∆wi,j of the past.\n\nUnfolding in time is particularly useful if\nwe receive the impression that the closer\npast is more important for the network\nthan the one being further away. The\nreason for this is that backpropagation\nhas only little influence in the layers far-\nther away from the output (remember:\nthe farther we are from the output layer,\nthe smaller the influence of backpropaga-\ntion).\n\nDisadvantages: the training of such an un-\nfolded network will take a long time since\na large number of layers could possibly be\nproduced. A problem that is no longer\nnegligible is the limited computational ac-\ncuracy of ordinary computers, which is\nexhausted very fast because of so many\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 125\n\n\n\nChapter 7 Recurrent perceptron-like networks (depends on chapter 5) dkriesel.com\n\n�� �� ��GFED@ABCi1\n\n\'\'OOOOOOOOOOOOOOOO\n\n**UUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\n\n��@@@@@@@@@\n\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCi3\n\n��   AAAAAAAAA\nGFED@ABCk1\n\nwwnnnnnnnnnnnnnnnnn\n\n~~}}}}}}}}}\nGFED@ABCk2\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\nwwnnnnnnnnnnnnnnnnn\n\nGFED@ABCΩ1@A BC\n\nOO\n\n��\n\nGFED@ABCΩ2�� ��\n\nOO\n\n��\n\n��\n\n...\n\n��\n\n...\n\n��\n\n...\n...\n\n...\n\n�� �� ��\n\n/.-,()*+\n\n((RRRRRRRRRRRRRRRRR\n\n**VVVVVVVVVVVVVVVVVVVVVVVV /.-,()*+\n\n!!CCCCCCCCC\n\n((PPPPPPPPPPPPPPP /.-,()*+\n�� ��???????? /.-,()*+\n\nwwoooooooooooooo\n\n����������\n/.-,()*+\n\nttjjjjjjjjjjjjjjjjjjjjj\n\nwwoooooooooooooo\n\n�� �� ��\n\n/.-,()*+\n\n((RRRRRRRRRRRRRRRRRR\n\n**VVVVVVVVVVVVVVVVVVVVVVVVVVV /.-,()*+\n\n!!DDDDDDDDDD\n\n((QQQQQQQQQQQQQQQQQQ /.-,()*+\n�� !!CCCCCCCCC /.-,()*+\n\nvvnnnnnnnnnnnnnnnn\n\n�����������\n/.-,()*+\n\nttjjjjjjjjjjjjjjjjjjjjjjj\n\nwwppppppppppppppp\n\nGFED@ABCi1\n\n\'\'OOOOOOOOOOOOOOOO\n\n**UUUUUUUUUUUUUUUUUUUUUUUUU GFED@ABCi2\n\n��@@@@@@@@@\n\n\'\'PPPPPPPPPPPPPPPPP GFED@ABCi3\n\n��   AAAAAAAAA\nGFED@ABCk1\n\nwwnnnnnnnnnnnnnnnnn\n\n~~}}}}}}}}}\nGFED@ABCk2\n\nttiiiiiiiiiiiiiiiiiiiiiiiiii\n\nwwnnnnnnnnnnnnnnnnn\n\nGFED@ABCΩ1\n\n��\n\nGFED@ABCΩ2\n\n��\n\nFigure 7.4: Illustration of the unfolding in time with a small exemplary recurrent MLP. Top: The\nrecurrent MLP. Bottom: The unfolded network. For reasons of clarity, I only added names to\nthe lowest part of the unfolded network. Dotted arrows leading into the network mark the inputs.\nDotted arrows leading out of the network mark the outputs. Each "network copy" represents a time\nstep of the network with the most recent time step being at the bottom.\n\n126 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 7.3 Training recurrent networks\n\nnested computations (the farther we are\nfrom the output layer, the smaller the in-\nfluence of backpropagation, so that this\nlimit is reached). Furthermore, with sev-\neral levels of context neurons this proce-\ndure could produce very large networks to\nbe trained.\n\n7.3.2 Teacher forcing\n\nOther procedures are the equivalent\nteacher forcing and open loop learn-\ning. They detach the recurrence during\nthe learning process: We simply pretend\n\nteaching\ninput\n\napplied at\ncontext\nneurons\n\nthat the recurrence does not exist and ap-\nply the teaching input to the context neu-\nrons during the training. So, backpropaga-\ntion becomes possible, too. Disadvantage:\nwith Elman networks a teaching input for\nnon-output-neurons is not given.\n\n7.3.3 Recurrent backpropagation\n\nAnother popular procedure without lim-\nited time horizon is the recurrent back-\npropagation using methods of differ-\nential calculus to solve the problem\n[Pin87].\n\n7.3.4 Training with evolution\n\nDue to the already long lasting train-\ning time, evolutionary algorithms have\nproved to be of value, especially with recur-\nrent networks. One reason for this is that\nthey are not only unrestricted with respect\nto recurrences but they also have other ad-\nvantages when the mutation mechanisms\n\nare chosen suitably: So, for example, neu-\nrons and weights can be adjusted and\nthe network topology can be optimized\n(of course the result of learning is not\nnecessarily a Jordan or Elman network).\nWith ordinary MLPs, however, evolution-\nary strategies are less popular since they\ncertainly need a lot more time than a di-\nrected learning procedure such as backpro-\npagation.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 127\n\n\n\n\n\nChapter 8\n\nHopfield networks\nIn a magnetic field, each particle applies a force to any other particle so that\nall particles adjust their movements in the energetically most favorable way.\nThis natural mechanism is copied to adjust noisy inputs in order to match\n\ntheir real models.\n\nAnother supervised learning example of\nthe wide range of neural networks was\ndeveloped by John Hopfield: the so-\ncalled Hopfield networks [Hop82]. Hop-\nfield and his physically motivated net-\nworks have contributed a lot to the renais-\nsance of neural networks.\n\n8.1 Hopfield networks are\ninspired by particles in a\nmagnetic field\n\nThe idea for the Hopfield networks origi-\nnated from the behavior of particles in a\nmagnetic field: Every particle "communi-\ncates" (by means of magnetic forces) with\nevery other particle (completely linked)\nwith each particle trying to reach an ener-\ngetically favorable state (i.e. a minimum\nof the energy function). As for the neurons\nthis state is known as activation. Thus,\nall particles or neurons rotate and thereby\n\nencourage each other to continue this rota-\ntion. As a manner of speaking, our neural\nnetwork is a cloud of particles\n\nBased on the fact that the particles auto-\nmatically detect the minima of the energy\nfunction, Hopfield had the idea to use the\n"spin" of the particles to process informa-\ntion: Why not letting the particles search\nminima on arbitrary functions? Even if we\nonly use two of those spins, i.e. a binary\nactivation, we will recognize that the devel-\noped Hopfield network shows considerable\ndynamics.\n\n8.2 In a hopfield network, all\nneurons influence each\nother symmetrically\n\nBriefly speaking, a Hopfield network con-\nsists of a set K of completely linked neu- JKrons with binary activation (since we only\n\n129\n\n\n\nChapter 8 Hopfield networks dkriesel.com\n\n?>=<89:;↑ ii\n\n��\n\nii\n\n))SSSSSSSSSSSSSSSSSSSSSSSS\nOO\n\n��\n\noo //\n^^\n\n��<<<<<<<<<\n?>=<89:;↓55\n\nuukkkkkkkkkkkkkkkkkkkkkkkk\nOO\n\n��\n\n@@\n\n����������� ^^\n\n��<<<<<<<<<\n\n?>=<89:;↑ ii\n\n))SSSSSSSSSSSSSSSSSSSSSSSSoo //\n��\n\n@@��������� ?>=<89:;↓ ?>=<89:;↑44jj 55\n\nuukkkkkkkkkkkkkkkkkkkkkkkk//oo\n@@\n\n�����������\n\n?>=<89:;↓\n\n\n\n\n66\n\n��\n\n@@�����������\n\n^^<<<<<<<<< ?>=<89:;↑//oo\n��\n\n^^<<<<<<<<<\n\nFigure 8.1: Illustration of an exemplary Hop-\nfield network. The arrows ↑ and ↓ mark the\nbinary "spin". Due to the completely linked neu-\nrons the layers cannot be separated, which means\nthat a Hopfield network simply includes a set of\nneurons.\n\nuse two spins), with the weights being\nsymmetric between the individual neurons\n\ncompletely\nlinked\nset of\n\nneurons\n\nand without any neuron being directly con-\nnected to itself (fig. 8.1). Thus, the state\nof |K| neurons with two possible states\n∈ {−1, 1} can be described by a string\nx ∈ {−1, 1}|K|.\n\nThe complete link provides a full square\nmatrix of weights between the neurons.\nThe meaning of the weights will be dis-\ncussed in the following. Furthermore, we\nwill soon recognize according to which\nrules the neurons are spinning, i.e. are\nchanging their state.\n\nAdditionally, the complete link leads to\nthe fact that we do not know any input,\noutput or hidden neurons. Thus, we have\nto think about how we can input some-\nthing into the |K| neurons.\n\nDefinition 8.1 (Hopfield network). A\nHopfield network consists of a set K of\ncompletely linked neurons without direct\nrecurrences. The activation function of\nthe neurons is the binary threshold func-\ntion with outputs ∈ {1,−1}.\n\nDefinition 8.2 (State of a Hopfield net-\nwork). The state of the network con-\nsists of the activation states of all neu-\nrons. Thus, the state of the network can\nbe understood as a binary string z ∈\n{−1, 1}|K|.\n\n8.2.1 Input and output of a\nHopfield network are\nrepresented by neuron states\n\nWe have learned that a network, i.e. a\nset of |K| particles, that is in a state\nis automatically looking for a minimum.\nAn input pattern of a Hopfield network\nis exactly such a state: A binary string\nx ∈ {−1, 1}|K| that initializes the neurons.\nThen the network is looking for the min-\nimum to be taken (which we have previ-\nously defined by the input of training sam-\nples) on its energy surface.\n\nBut when do we know that the minimum\nhas been found? This is simple, too: when\n\ninput and\noutput =\nnetwork\nstates\n\nthe network stops. It can be proven that a\nHopfield network with a symmetric weight\nmatrix that has zeros on its diagonal al-\nways converges [CG88], i.e. at some point\n\nalways\nconvergesit will stand still. Then the output is a\n\nbinary string y ∈ {−1, 1}|K|, namely the\nstate string of the network that has found\na minimum.\n\n130 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 8.2 Structure and functionality\n\nNow let us take a closer look at the con-\ntents of the weight matrix and the rules\nfor the state change of the neurons.\nDefinition 8.3 (Input and output of\na Hopfield network). The input of a\nHopfield network is binary string x ∈\n{−1, 1}|K| that initializes the state of the\nnetwork. After the convergence of the\nnetwork, the output is the binary string\ny ∈ {−1, 1}|K| generated from the new net-\nwork state.\n\n8.2.2 Significance of weights\n\nWe have already said that the neurons\nchange their states, i.e. their direction,\nfrom −1 to 1 or vice versa. These spins oc-\ncur dependent on the current states of the\nother neurons and the associated weights.\nThus, the weights are capable to control\nthe complete change of the network. The\nweights can be positive, negative, or 0.\nColloquially speaking, for a weight wi,j be-\ntween two neurons i and j the following\nholds:\n\nIf wi,j is positive, it will try to force the\ntwo neurons to become equal – the\nlarger they are, the harder the net-\nwork will try. If the neuron i is in\nstate 1 and the neuron j is in state\n−1, a high positive weight will advise\nthe two neurons that it is energeti-\ncally more favorable to be equal.\n\nIf wi,j is negative, its behavior will be\nanaloguous only that i and j are\nurged to be different. A neuron i in\nstate −1 would try to urge a neuron\nj into state 1.\n\nZero weights lead to the two involved\nneurons not influencing each other.\n\nThe weights as a whole apparently take\nthe way from the current state of the net-\nwork towards the next minimum of the en-\nergy function. We now want to discuss\nhow the neurons follow this way.\n\n8.2.3 A neuron changes its state\naccording to the influence of\nthe other neurons\n\nOnce a network has been trained and\ninitialized with some starting state, the\nchange of state xk of the individual neu-\nrons k occurs according to the scheme\n\nxk(t) = fact\n\n\uf8eb\uf8ed∑\nj∈K\n\nwj,k · xj(t− 1)\n\n\uf8f6\uf8f8 (8.1)\n\nin each time step, where the function fact\ngenerally is the binary threshold function\n(fig. 8.2 on the next page) with threshold\n0. Colloquially speaking: a neuron k cal-\nculates the sum of wj,k · xj(t − 1), which\nindicates how strong and into which direc-\ntion the neuron k is forced by the other\nneurons j. Thus, the new state of the net-\nwork (time t) results from the state of the\nnetwork at the previous time t − 1. This\nsum is the direction into which the neuron\nk is pushed. Depending on the sign of the\nsum the neuron takes state 1 or −1.\n\nAnother difference between Hopfield net-\nworks and other already known network\ntopologies is the asynchronous update: A\nneuron k is randomly chosen every time,\nwhich then recalculates the activation.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 131\n\n\n\nChapter 8 Hopfield networks dkriesel.com\n\n−1\n\n−0.5\n\n 0\n\n 0.5\n\n 1\n\n−4 −2  0  2  4\n\nf(\nx)\n\nx\n\nHeaviside Function\n\nFigure 8.2: Illustration of the binary threshold\nfunction.\n\nThus, the new activation of the previously\nchanged neurons immediately influences\nthe network, i.e. one time step indicates\nthe change of a single neuron.\n\nRegardless of the aforementioned random\nselection of the neuron, a Hopfield net-\nwork is often much easier to implement:\nThe neurons are simply processed one af-\nter the other and their activations are re-\ncalculated until no more changes occur.\n\nrandom\nneuron\n\ncalculates\nnew\n\nactivation\n\nDefinition 8.4 (Change in the state of\na Hopfield network). The change of state\nof the neurons occurs asynchronously with\nthe neuron to be updated being randomly\nchosen and the new state being generated\nby means of this rule:\n\nxk(t) = fact\n\n\uf8eb\uf8ed∑\nj∈J\n\nwj,k · xj(t− 1)\n\n\uf8f6\uf8f8 .\nNow that we know how the weights influ-\nence the changes in the states of the neu-\nrons and force the entire network towards\n\na minimum, then there is the question of\nhow to teach the weights to force the net-\nwork towards a certain minimum.\n\n8.3 The weight matrix is\ngenerated directly out of\nthe training patterns\n\nThe aim is to generate minima on the\nmentioned energy surface, so that at an\ninput the network can converge to them.\nAs with many other network paradigms,\nwe use a set P of training patterns p ∈\n{1,−1}|K|, representing the minima of our\nenergy surface.\n\nUnlike many other network paradigms, we\ndo not look for the minima of an unknown\nerror function but define minima on such a\nfunction. The purpose is that the network\nshall automatically take the closest min-\nimum when the input is presented. For\nnow this seems unusual, but we will un-\nderstand the whole purpose later.\n\nRoughly speaking, the training of a Hop-\nfield network is done by training each train-\ning pattern exactly once using the rule\ndescribed in the following (Single Shot\nLearning), where pi and pj are the states\nof the neurons i and j under p ∈ P :\n\nwi,j =\n∑\np∈P\n\npi · pj (8.2)\n\nThis results in the weight matrix W . Col-\nloquially speaking: We initialize the net-\nwork by means of a training pattern and\nthen process weights wi,j one after another.\n\n132 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 8.4 Autoassociation and traditional application\n\nFor each of these weights we verify: Are\nthe neurons i, j n the same state or do the\nstates vary? In the first case we add 1\nto the weight, in the second case we add\n−1.\n\nThis we repeat for each training pattern\np ∈ P . Finally, the values of the weights\nwi,j are high when i and j corresponded\nwith many training patterns. Colloquially\nspeaking, this high value tells the neurons:\n"Often, it is energetically favorable to hold\nthe same state". The same applies to neg-\native weights.\n\nDue to this training we can store a certain\nfixed number of patterns p in the weight\nmatrix. At an input x the network will\nconverge to the stored pattern that is clos-\nest to the input p.\n\nUnfortunately, the number of the maxi-\nmum storable and reconstructible patterns\np is limited to\n\n|P |MAX ≈ 0.139 · |K|, (8.3)\n\nwhich in turn only applies to orthogo-\nnal patterns. This was shown by precise\n(and time-consuming) mathematical anal-\nyses, which we do not want to specify\nnow. If more patterns are entered, already\nstored information will be destroyed.\n\nDefinition 8.5 (Learning rule for Hop-\nfield networks). The individual elements\nof the weight matrix W are defined by a\nsingle processing of the learning rule\n\nwi,j =\n∑\np∈P\n\npi · pj ,\n\nwhere the diagonal of the matrix is covered\nwith zeros. Here, no more than |P |MAX ≈\n\n0.139 · |K| training samples can be trained\nand at the same time maintain their func-\ntion.\n\nNow we know the functionality of Hopfield\nnetworks but nothing about their practical\nuse.\n\n8.4 Autoassociation and\ntraditional application\n\nHopfield networks, like those mentioned\nabove, are called autoassociators. An\nautoassociator a exactly shows the afore- Jamentioned behavior: Firstly, when a\nknown pattern p is entered, exactly this\nknown pattern is returned. Thus,\n\na(p) = p,\n\nwith a being the associative mapping. Sec-\nondly, and that is the practical use, this\nalso works with inputs that are close to a\npattern:\n\na(p+ ε) = p.\n\nAfterwards, the autoassociator is, in any\ncase, in a stable state, namely in the state\np.\n\nIf the set of patterns P consists of, for ex-\nnetwork\nrestores\ndamaged\ninputs\n\nample, letters or other characters in the\nform of pixels, the network will be able to\ncorrectly recognize deformed or noisy let-\nters with high probability (fig. 8.3 on the\nfollowing page).\n\nThe primary fields of application of Hop-\nfield networks are pattern recognition\nand pattern completion, such as the zip\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 133\n\n\n\nChapter 8 Hopfield networks dkriesel.com\n\nFigure 8.3: Illustration of the convergence of an\nexemplary Hopfield network. Each of the pic-\ntures has 10 × 12 = 120 binary pixels. In the\nHopfield network each pixel corresponds to one\nneuron. The upper illustration shows the train-\ning samples, the lower shows the convergence of\na heavily noisy 3 to the corresponding training\nsample.\n\ncode recognition on letters in the eighties.\nBut soon the Hopfield networks were re-\nplaced by other systems in most of their\nfields of application, for example by OCR\nsystems in the field of letter recognition.\nToday Hopfield networks are virtually no\nlonger used, they have not become estab-\nlished in practice.\n\n8.5 Heteroassociation and\nanalogies to neural data\nstorage\n\nSo far we have been introduced to Hopfield\nnetworks that converge from an arbitrary\ninput into the closest minimum of a static\nenergy surface.\n\nAnother variant is a dynamic energy sur-\nface: Here, the appearance of the energy\nsurface depends on the current state and\nwe receive a heteroassociator instead of\nan autoassociator. For a heteroassocia-\ntor\n\na(p+ ε) = p\n\nis no longer true, but rather\n\nh(p+ ε) = q,\n\nwhich means that a pattern is mapped\nonto another one. h is the heteroasso- Jhciative mapping. Such heteroassociations\nare achieved by means of an asymmetric\nweight matrix V .\n\n134 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 8.5 Heteroassociation and analogies to neural data storage\n\nHeteroassociations connected in series of\nthe form\n\nh(p+ ε) = q\n\nh(q + ε) = r\n\nh(r + ε) = s\n\n...\nh(z + ε) = p\n\ncan provoke a fast cycle of states\n\np→ q → r → s→ . . .→ z → p,\n\nwhereby a single pattern is never com-\npletely accepted: Before a pattern is en-\ntirely completed, the heteroassociation al-\nready tries to generate the successor of this\npattern. Additionally, the network would\nnever stop, since after having reached the\nlast state z, it would proceed to the first\nstate p again.\n\n8.5.1 Generating the\nheteroassociative matrix\n\nWe generate the matrix V by means of el-\nVI ements v very similar to the autoassocia-\nvI tive matrix with p being (per transition)\n\nthe training sample before the transition\nand q being the training sample to be gen-\n\nqI erated from p:\n\nvi,j =\n∑\n\np,q∈P,p6=q\npiqj (8.4)\n\nThe diagonal of the matrix is again filled\nwith zeros. The neuron states are, as al-\n\nnetword\nis instable\n\nwhile\nchanging\n\nstates\n\nways, adapted during operation. Several\ntransitions can be introduced into the ma-\ntrix by a simple addition, whereby the said\nlimitation exists here, too.\n\nDefinition 8.6 (Learning rule for the het-\neroassociative matrix). For two training\nsamples p being predecessor and q being\nsuccessor of a heteroassociative transition\nthe weights of the heteroassociative matrix\nV result from the learning rule\n\nvi,j =\n∑\n\np,q∈P,p6=q\npiqj ,\n\nwith several heteroassociations being intro-\nduced into the network by a simple addi-\ntion.\n\n8.5.2 Stabilizing the\nheteroassociations\n\nWe have already mentioned the problem\nthat the patterns are not completely gen-\nerated but that the next pattern is already\nbeginning before the generation of the pre-\nvious pattern is finished.\n\nThis problem can be avoided by not only\ninfluencing the network by means of the\nheteroassociative matrix V but also by\nthe already known autoassociative matrix\nW .\n\nAdditionally, the neuron adaptation rule\nis changed so that competing terms are\ngenerated: One term autoassociating an\nexisting pattern and one term trying to\nconvert the very same pattern into its suc-\ncessor. The associative rule provokes that\nthe network stabilizes a pattern, remains\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 135\n\n\n\nChapter 8 Hopfield networks dkriesel.com\n\nthere for a while, goes on to the next pat-\ntern, and so on.\n\nxi(t+ 1) = (8.5)\n\nfact\n\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n∑\nj∈K\n\nwi,jxj(t)︸ ︷︷ ︸\nautoassociation\n\n+\n∑\nk∈K\n\nvi,kxk(t−∆t)︸ ︷︷ ︸\nheteroassociation\n\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nHere, the value ∆t causes, descriptively\n\n∆tI\nstable change\n\nin states\n\nspeaking, the influence of the matrix V\nto be delayed, since it only refers to a\nnetwork being ∆t versions behind. The\nresult is a change in state, during which\nthe individual states are stable for a short\nwhile. If ∆t is set to, for example, twenty\nsteps, then the asymmetric weight matrix\nwill realize any change in the network only\ntwenty steps later so that it initially works\nwith the autoassociative matrix (since it\nstill perceives the predecessor pattern of\nthe current one), and only after that it will\nwork against it.\n\n8.5.3 Biological motivation of\nheterassociation\n\nFrom a biological point of view the transi-\ntion of stable states into other stable states\nis highly motivated: At least in the begin-\nning of the nineties it was assumed that\nthe Hopfield modell will achieve an ap-\nproximation of the state dynamics in the\nbrain, which realizes much by means of\nstate chains: When I would ask you, dear\nreader, to recite the alphabet, you gener-\nally will manage this better than (please\ntry it immediately) to answer the follow-\ning question:\n\nWhich letter in the alphabet follows the\nletter P?\n\nAnother example is the phenomenon that\none cannot remember a situation, but the\nplace at which one memorized it the last\ntime is perfectly known. If one returns\nto this place, the forgotten situation often\ncomes back to mind.\n\n8.6 Continuous Hopfield\nnetworks\n\nSo far, we only have discussed Hopfield net-\nworks with binary activations. But Hop-\nfield also described a version of his net-\nworks with continuous activations [Hop84],\nwhich we want to cover at least briefly:\ncontinuous Hopfield networks. Here,\nthe activation is no longer calculated by\nthe binary threshold function but by the\nFermi function with temperature parame-\nters (fig. 8.4 on the next page).\n\nHere, the network is stable for symmetric\nweight matrices with zeros on the diagonal,\ntoo.\n\nHopfield also stated, that continuous Hop-\nfield networks can be applied to find ac-\nceptable solutions for the NP-hard trav-\nelling salesman problem [HT85]. Accord-\ning to some verification trials [Zel94] this\nstatement can’t be kept up any more. But\ntoday there are faster algorithms for han-\ndling this problem and therefore the Hop-\nfield network is no longer used here.\n\n136 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 8.6 Continuous Hopfield networks\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−4 −2  0  2  4\n\nf(\nx)\n\nx\n\nFermi Function with Temperature Parameter\n\nFigure 8.4: The already known Fermi function\nwith different temperature parameter variations.\n\nExercises\n\nExercise 14. Indicate the storage re-\nquirements for a Hopfield network with\n|K| = 1000 neurons when the weights wi,j\nshall be stored as integers. Is it possible\nto limit the value range of the weights in\norder to save storage space?\n\nExercise 15. Compute the weights wi,j\nfor a Hopfield network using the training\nset\n\nP ={(−1,−1,−1,−1,−1, 1);\n(−1, 1, 1,−1,−1,−1);\n(1,−1,−1, 1,−1, 1)}.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 137\n\n\n\n\n\nChapter 9\n\nLearning vector quantization\nLearning Vector Quantization is a learning procedure with the aim to represent\n\nthe vector training sets divided into predefined classes as well as possible by\nusing a few representative vectors. If this has been managed, vectors which\n\nwere unkown until then could easily be assigned to one of these classes.\n\nSlowly, part II of this text is nearing its\nend – and therefore I want to write a last\nchapter for this part that will be a smooth\ntransition into the next one: A chapter\nabout the learning vector quantization\n(abbreviated LVQ) [Koh89] described by\nTeuvo Kohonen, which can be charac-\nterized as being related to the self orga-\nnizing feature maps. These SOMs are de-\nscribed in the next chapter that already\nbelongs to part III of this text, since SOMs\nlearn unsupervised. Thus, after the explo-\nration of LVQ I want to bid farewell to\nsupervised learning.\n\nPreviously, I want to announce that there\nare different variations of LVQ, which will\nbe mentioned but not exactly represented.\nThe goal of this chapter is rather to ana-\nlyze the underlying principle.\n\n9.1 About quantization\n\nIn order to explore the learning vec-\ntor quantization we should at first get\na clearer picture of what quantization\n(which can also be referred to as dis-\ncretization) is.\n\nEverybody knows the sequence of discrete\nnumbers\n\nN = {1, 2, 3, . . .},\n\nwhich contains the natural numbers. Dis-\ncrete means, that this sequence consists of\n\ndiscrete\n= separatedseparated elements that are not intercon-\n\nnected. The elements of our example are\nexactly such numbers, because the natural\nnumbers do not include, for example, num-\nbers between 1 and 2. On the other hand,\nthe sequence of real numbers R, for in-\nstance, is continuous: It does not matter\nhow close two selected numbers are, there\nwill always be a number between them.\n\n139\n\n\n\nChapter 9 Learning vector quantization dkriesel.com\n\nQuantization means that a continuous\nspace is divided into discrete sections: By\ndeleting, for example, all decimal places\nof the real number 2.71828, it could be\nassigned to the natural number 2. Here\nit is obvious that any other number hav-\ning a 2 in front of the comma would also\nbe assigned to the natural number 2, i.e.\n2 would be some kind of representative\nfor all real numbers within the interval\n[2; 3).\n\nIt must be noted that a sequence can be ir-\nregularly quantized, too: For instance, the\ntimeline for a week could be quantized into\nworking days and weekend.\n\nA special case of quantization is digiti-\nzation: In case of digitization we always\ntalk about regular quantization of a con-\ntinuous space into a number system with\nrespect to a certain basis. If we enter, for\nexample, some numbers into the computer,\nthese numbers will be digitized into the bi-\nnary system (basis 2).\n\nDefinition 9.1 (Quantization). Separa-\ntion of a continuous space into discrete sec-\ntions.\n\nDefinition 9.2 (Digitization). Regular\nquantization.\n\n9.2 LVQ divides the input\nspace into separate areas\n\nNow it is almost possible to describe by\nmeans of its name what LVQ should en-\nable us to do: A set of representatives\nshould be used to divide an input space\n\ninto classes that reflect the input space\nas well as possible (fig. 9.1 on the facing input space\n\nreduced to\nvector repre-\nsentatives\n\npage). Thus, each element of the input\nspace should be assigned to a vector as a\nrepresentative, i.e. to a class, where the\nset of these representatives should repre-\nsent the entire input space as precisely as\npossible. Such a vector is called codebook\nvector. A codebook vector is the represen-\ntative of exactly those input space vectors\nlying closest to it, which divides the input\nspace into the said discrete areas.\n\nIt is to be emphasized that we have to\nknow in advance how many classes we\nhave and which training sample belongs\nto which class. Furthermore, it is impor-\ntant that the classes must not be disjoint,\nwhich means they may overlap.\n\nSuch separation of data into classes is in-\nteresting for many problems for which it\nis useful to explore only some characteris-\ntic representatives instead of the possibly\nhuge set of all vectors – be it because it is\nless time-consuming or because it is suffi-\nciently precise.\n\n9.3 Using codebook vectors:\nthe nearest one is the\nwinner\n\nThe use of a prepared set of codebook vec-\ntors is very simple: For an input vector y\nthe class association is easily decided by\n\nclosest\nvector\nwins\n\nconsidering which codebook vector is the\nclosest – so, the codebook vectors build a\nvoronoi diagram out of the set. Since\n\n140 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 9.4 Adjusting codebook vectors\n\nFigure 9.1: BExamples for quantization of a two-dimensional input space. DThe lines represent\nthe class limit, the × mark the codebook vectors.\n\neach codebook vector can clearly be asso-\nciated to a class, each input vector is asso-\nciated to a class, too.\n\n9.4 Adjusting codebook\nvectors\n\nAs we have already indicated, the LVQ is\na supervised learning procedure. Thus, we\nhave a teaching input that tells the learn-\ning procedure whether the classification of\nthe input pattern is right or wrong: In\nother words, we have to know in advance\nthe number of classes to be represented or\nthe number of codebook vectors.\n\nRoughly speaking, it is the aim of the\nlearning procedure that training samples\n\nare used to cause a previously defined num-\nber of randomly initialized codebook vec-\ntors to reflect the training data as precisely\nas possible.\n\n9.4.1 The procedure of learning\n\nLearning works according to a simple\nscheme. We have (since learning is su-\npervised) a set P of |P | training samples.\nAdditionally, we already know that classes\nare predefined, too, i.e. we also have a set\nof classes C. A codebook vector is clearly\nassigned to each class. Thus, we can say\nthat the set of classes |C| contains many\ncodebook vectors C1, C2, . . . , C|C|.\n\nThis leads to the structure of the training\nsamples: They are of the form (p, c) and\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 141\n\n\n\nChapter 9 Learning vector quantization dkriesel.com\n\ntherefore contain the training input vector\np and its class affiliation c. For the class\naffiliation\n\nc ∈ {1, 2, . . . , |C|}\n\nholds, which means that it clearly assigns\nthe training sample to a class or a code-\nbook vector.\n\nIntuitively, we could say about learning:\n"Why a learning procedure? We calculate\nthe average of all class members and place\ntheir codebook vectors there – and that’s\nit." But we will see soon that our learning\nprocedure can do a lot more.\n\nI only want to briefly discuss the steps\nof the fundamental LVQ learning proce-\ndure:\n\nInitialization: We place our set of code-\nbook vectors on random positions in\nthe input space.\n\nTraining sample: A training sample p of\nour training set P is selected and pre-\nsented.\n\nDistance measurement: We measure the\ndistance ||p − C|| between all code-\nbook vectors C1, C2, . . . , C|C| and our\ninput p.\n\nWinner: The closest codebook vector\nwins, i.e. the one with\n\nmin\nCi∈C\n\n||p− Ci||.\n\nLearning process: The learning process\ntakes place according to the rule\n\n∆Ci = η(t) · h(p, Ci) · (p− Ci)\n(9.1)\n\nCi(t+ 1) = Ci(t) + ∆Ci, (9.2)\n\nwhich we now want to break down.\n\n. We have already seen that the first\nfactor η(t) is a time-dependent learn-\ning rate allowing us to differentiate\nbetween large learning steps and fine\ntuning.\n\n. The last factor (p − Ci) is obviously\nthe direction toward which the code-\nbook vector is moved.\n\n. But the function h(p, Ci) is the core of\nthe rule: It implements a distinction\nof cases.\n\nAssignment is correct: The winner\nvector is the codebook vector of\nthe class that includes p. In this Important!\ncase, the function provides posi-\ntive values and the codebook vec-\ntor moves towards p.\n\nAssignment is wrong: The winner\nvector does not represent the\nclass that includes p. Therefore\nit moves away from p.\n\nWe can see that our definition of the func-\ntion h was not precise enough. With good\nreason: From here on, the LVQ is divided\ninto different nuances, dependent of how\nexactly h and the learning rate should\nbe defined (called LVQ1, LVQ2, LVQ3,\n\n142 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 9.5 Connection to neural networks\n\nOLVQ, etc). The differences are, for in-\nstance, in the strength of the codebook vec-\ntor movements. They are not all based on\nthe same principle described here, and as\nannounced I don’t want to discuss them\nany further. Therefore I don’t give any\nformal definition regarding the aforemen-\ntioned learning rule and LVQ.\n\n9.5 Connection to neural\nnetworks\n\nUntil now, in spite of the learning process,\nthe question was what LVQ has to do with\nneural networks. The codebook vectors\ncan be understood as neurons with a fixed\nposition within the input space, similar to\nRBF networks. Additionally, in nature itvectors\n\n= neurons? often occurs that in a group one neuron\nmay fire (a winner neuron, here: a code-\nbook vector) and, in return, inhibits all\nother neurons.\n\nI decided to place this brief chapter about\nlearning vector quantization here so that\nthis approach can be continued in the fol-\nlowing chapter about self-organizing maps:\nWe will classify further inputs by means of\nneurons distributed throughout the input\nspace, only that this time, we do not know\nwhich input belongs to which class.\n\nNow let us take a look at the unsupervised\nlearning networks!\n\nExercises\n\nExercise 16. Indicate a quantization\nwhich equally distributes all vectors H ∈\nH in the five-dimensional unit cube H into\none of 1024 classes.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 143\n\n\n\n\n\nPart III\n\nUnsupervised learning network\nparadigms\n\n145\n\n\n\n\n\nChapter 10\n\nSelf-organizing feature maps\nA paradigm of unsupervised learning neural networks, which maps an input\n\nspace by its fixed topology and thus independently looks for simililarities.\nFunction, learning procedure, variations and neural gas.\n\nIf you take a look at the concepts of biologi-\ncal neural networks mentioned in the intro-\nduction, one question will arise: How does\nour brain store and recall the impressions\nit receives every day. Let me point out\nthat the brain does not have any training\n\nHow are\ndata stored\n\nin the\nbrain?\n\nsamples and therefore no "desired output".\nAnd while already considering this subject\nwe realize that there is no output in this\nsense at all, too. Our brain responds to\nexternal input by changes in state. These\nare, so to speak, its output.\n\nBased on this principle and exploring\nthe question of how biological neural net-\nworks organize themselves, Teuvo Ko-\nhonen developed in the Eighties his self-\norganizing feature maps [Koh82, Koh98],\nshortly referred to as self-organizing\nmaps or SOMs. A paradigm of neural\nnetworks where the output is the state of\nthe network, which learns completely un-\nsupervised, i.e. without a teacher.\n\nUnlike the other network paradigms we\nhave already got to know, for SOMs it is\nunnecessary to ask what the neurons calcu-\nlate. We only ask which neuron is active at\nthe moment. Biologically, this is very mo- no output,\n\nbut active\nneuron\n\ntivated: If in biology the neurons are con-\nnected to certain muscles, it will be less\ninteresting to know how strong a certain\nmuscle is contracted but which muscle is\nactivated. In other words: We are not in-\nterested in the exact output of the neuron\nbut in knowing which neuron provides out-\nput. Thus, SOMs are considerably more\nrelated to biology than, for example, the\nfeedforward networks, which are increas-\ningly used for calculations.\n\n10.1 Structure of a\nself-organizing map\n\nTypically, SOMs have – like our brain –\nthe task to map a high-dimensional in-\nput (N dimensions) onto areas in a low-\n\n147\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\ndimensional grid of cells (G dimensions)\nto draw a map of the high-dimensional\n\nhigh-dim.\ninput\n↓\n\nlow-dim.\nmap\n\nspace, so to speak. To generate this map,\nthe SOM simply obtains arbitrary many\npoints of the input space. During the in-\nput of the points the SOM will try to cover\nas good as possible the positions on which\nthe points appear by its neurons. This par-\nticularly means, that every neuron can be\nassigned to a certain position in the input\nspace.\n\nAt first, these facts seem to be a bit con-\nfusing, and it is recommended to briefly\nreflect about them. There are two spaces\nin which SOMs are working:\n\n. The N -dimensional input space and\n\n. the G-dimensional grid on which the\nneurons are lying and which indi-input space\n\nand topology cates the neighborhood relationships\nbetween the neurons and therefore\nthe network topology.\n\nIn a one-dimensional grid, the neurons\ncould be, for instance, like pearls on a\nstring. Every neuron would have exactly\ntwo neighbors (except for the two end neu-\nrons). A two-dimensional grid could be a\nsquare array of neurons (fig. 10.1). An-\nother possible array in two-dimensional\nspace would be some kind of honeycomb\nshape. Irregular topologies are possible,\ntoo, but not very often. Topolgies with\nmore dimensions and considerably more\nneighborhood relationships would also be\npossible, but due to their lack of visualiza-\ntion capability they are not employed very\noften.Important!\n\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\n\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\n\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\n\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\n\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\n\nFigure 10.1: Example topologies of a self-\norganizing map. Above we can see a one-\ndimensional topology, below a two-dimensional\none.\n\nEven if N = G is true, the two spaces are\nnot equal and have to be distinguished. In\nthis special case they only have the same\ndimension.\n\nInitially, we will briefly and formally re-\ngard the functionality of a self-organizing\nmap and then make it clear by means of\nsome examples.\n\nDefinition 10.1 (SOM neuron). Similar\nto the neurons in an RBF network a SOM\nneuron k does not occupy a fixed position\nck (a center) in the input space. Jc\nDefinition 10.2 (Self-organizing map).\nA self-organizing map is a set K of SOM\nneurons. If an input vector is entered, ex- JKactly that neuron k ∈ K is activated which\n\n148 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.3 Training\n\nis closest to the input pattern in the input\nspace. The dimension of the input space\nis referred to as N .\n\nNI\nDefinition 10.3 (Topology). The neu-\nrons are interconnected by neighborhood\nrelationships. These neighborhood rela-\ntionships are called topology. The train-\ning of a SOM is highly influenced by the\ntopology. It is defined by the topology\nfunction h(i, k, t), where i is the winner\n\niI neuron1 ist, k the neuron to be adapted\nkI (which will be discussed later) and t the\n\ntimestep. The dimension of the topology\nis referred to as G.\n\nGI\n\n10.2 SOMs always activate\nthe neuron with the\nleast distance to an\ninput pattern\n\nLike many other neural networks, the\nSOM has to be trained before it can be\nused. But let us regard the very simple\nfunctionality of a complete self-organizing\nmap before training, since there are many\nanalogies to the training. Functionality\nconsists of the following steps:\n\nInput of an arbitrary value p of the input\nspace RN .\n\nCalculation of the distance between ev-\nery neuron k and p by means of a\nnorm, i.e. calculation of ||p− ck||.\n\nOne neuron becomes active, namely\nsuch neuron i with the shortest\n\n1 We will learn soon what a winner neuron is.\n\ncalculated distance to the input. All\nother neurons remain inactive.This\nparadigm of activity is also called input\n\n↓\nwinner\n\nwinner-takes-all scheme. The output\nwe expect due to the input of a SOM\nshows which neuron becomes active.\n\nIn many literature citations, the descrip-\ntion of SOMs is more formal: Often an\ninput layer is described that is completely\nlinked towards an SOM layer. Then the in-\nput layer (N neurons) forwards all inputs\nto the SOM layer. The SOM layer is later-\nally linked in itself so that a winner neuron\ncan be established and inhibit the other\nneurons. I think that this explanation of\na SOM is not very descriptive and there-\nfore I tried to provide a clearer description\nof the network structure.\n\nNow the question is which neuron is ac-\ntivated by which input – and the answer\nis given by the network itself during train-\ning.\n\n10.3 Training\n\n[Training makes the SOM topology cover\nthe input space] The training of a SOM\nis nearly as straightforward as the func-\ntionality described above. Basically, it is\nstructured into five steps, which partially\ncorrespond to those of functionality.\n\nInitialization: The network starts with\nrandom neuron centers ck ∈ RN from\nthe input space.\n\nCreating an input pattern: A stimulus,\ni.e. a point p, is selected from the\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 149\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\ninput space RN . Now this stimulus istraining:\ninput,\n\n→ winner i,\nchange in\nposition\ni and\n\nneighbors\n\nentered into the network.\n\nDistance measurement: Then the dis-\ntance ||p−ck|| is determined for every\nneuron k in the network.\n\nWinner takes all: The winner neuron i\nis determined, which has the smallest\ndistance to p, i.e. which fulfills the\ncondition\n\n||p− ci|| ≤ ||p− ck|| ∀ k 6= i\n\n. You can see that from several win-\nner neurons one can be selected at\nwill.\n\nAdapting the centers: The neuron cen-\nters are moved within the input space\naccording to the rule2\n\n∆ck = η(t) · h(i, k, t) · (p− ck),\n\nwhere the values ∆ck are simply\nadded to the existing centers. The\nlast factor shows that the change in\nposition of the neurons k is propor-\ntional to the distance to the input\npattern p and, as usual, to a time-\ndependent learning rate η(t). The\nabove-mentioned network topology ex-\nerts its influence by means of the func-\ntion h(i, k, t), which will be discussed\nin the following.\n\n2 Note: In many sources this rule is written ηh(p−\nck), which wrongly leads the reader to believe that\nh is a constant. This problem can easily be solved\nby not omitting the multiplication dots ·.\n\nDefinition 10.4 (SOM learning rule). A\nSOM is trained by presenting an input pat-\ntern and determining the associated win-\nner neuron. The winner neuron and its\nneighbor neurons, which are defined by the\ntopology function, then adapt their cen-\nters according to the rule\n\n∆ck = η(t) · h(i, k, t) · (p− ck),\n(10.1)\n\nck(t+ 1) = ck(t) + ∆ck(t). (10.2)\n\n10.3.1 The topology function\ndefines, how a learning\nneuron influences its\nneighbors\n\nThe topology function h is not defined\non the input space but on the grid and rep-\nresents the neighborhood relationships be-\ntween the neurons, i.e. the topology of the\nnetwork. It can be time-dependent (which\nit often is) – which explains the parameter\n\ndefined on\nthe gridt. The parameter k is the index running\n\nthrough all neurons, and the parameter i\nis the index of the winner neuron.\n\nIn principle, the function shall take a large\nvalue if k is the neighbor of the winner neu-\nron or even the winner neuron itself, and\nsmall values if not. SMore precise defini-\ntion: The topology function must be uni-\nmodal, i.e. it must have exactly one maxi-\nmum. This maximum must be next to the\nwinner neuron i, for which the distance to\nitself certainly is 0.\n\nonly 1 maximum\nfor the winnerAdditionally, the time-dependence enables\n\nus, for example, to reduce the neighbor-\nhood in the course of time.\n\n150 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.3 Training\n\nIn order to be able to output large values\nfor the neighbors of i and small values for\nnon-neighbors, the function h needs some\nkind of distance notion on the grid because\nfrom somewhere it has to know how far i\nand k are apart from each other on the\ngrid. There are different methods to cal-\nculate this distance.\n\nOn a two-dimensional grid we could apply,\nfor instance, the Euclidean distance (lower\npart of fig. 10.2) or on a one-dimensional\ngrid we could simply use the number of the\nconnections between the neurons i and k\n(upper part of the same figure).\n\nDefinition 10.5 (Topology function).\nThe topology function h(i, k, t) describes\nthe neighborhood relationships in the\ntopology. It can be any unimodal func-\ntion that reaches its maximum when i = k\ngilt. Time-dependence is optional, but of-\nten used.\n\n10.3.1.1 Introduction of common\ndistance and topology\nfunctions\n\nA common distance function would be, for\nexample, the already known Gaussian\nbell (see fig. 10.3 on page 153). It is uni-\nmodal with a maximum close to 0. Addi-\ntionally, its width can be changed by ap-\nplying its parameter σ , which can be used\n\nσI to realize the neighborhood being reduced\nin the course of time: We simply relate the\ntime-dependence to the σ and the result is\n\n/.-,()*+ ?>=<89:;i oo 1 // ?>=<89:;k /.-,()*+ /.-,()*+\n\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\n\n/.-,()*+ /.-,()*+ /.-,()*+ ?>=<89:;k\nOO\n\n��\n\n/.-,()*+\n\n/.-,()*+ ?>=<89:;i xx\n2.23qqqqqqq\n\n88qqqqqq\n\noo ///.-,()*+oo ///.-,()*+ /.-,()*+\n\n/.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+ /.-,()*+\n\nFigure 10.2: Example distances of a one-\ndimensional SOM topology (above) and a two-\ndimensional SOM topology (below) between two\nneurons i and k. In the lower case the Euclidean\ndistance is determined (in two-dimensional space\nequivalent to the Pythagoream theorem). In the\nupper case we simply count the discrete path\nlength between i and k. To simplify matters I\nrequired a fixed grid edge length of 1 in both\ncases.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 151\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\na monotonically decreasing σ(t). Then our\ntopology function could look like this:\n\nh(i, k, t) = e\n(\n− ||gi−ck||\n\n2\n\n2·σ(t)2\n\n)\n, (10.3)\n\nwhere gi and gk represent the neuron po-\nsitions on the grid, not the neuron posi-\ntions in the input space, which would be\nreferred to as ci and ck.\n\nOther functions that can be used in-\nstead of the Gaussian function are, for\ninstance, the cone function, the cylin-\nder function or theMexican hat func-\ntion (fig. 10.3 on the facing page). Here,\nthe Mexican hat function offers a particu-\nlar biological motivation: Due to its neg-\native digits it rejects some neurons close\nto the winner neuron, a behavior that has\nalready been observed in nature. This can\ncause sharply separated map areas – and\nthat is exactly why the Mexican hat func-\ntion has been suggested by Teuvo Koho-\nnen himself. But this adjustment charac-\nteristic is not necessary for the functional-\nity of the map, it could even be possible\nthat the map would diverge, i.e. it could\nvirtually explode.\n\n10.3.2 Learning rates and\nneighborhoods can decrease\nmonotonically over time\n\nTo avoid that the later training phases\nforcefully pull the entire map towards\na new pattern, the SOMs often work\nwith temporally monotonically decreasing\nlearning rates and neighborhood sizes. At\nfirst, let us talk about the learning rate:\n\nTypical sizes of the target value of a learn-\ning rate are two sizes smaller than the ini-\ntial value, e.g\n\n0.01 < η < 0.6\n\ncould be true. But this size must also de-\npend on the network topology or the size\nof the neighborhood.\n\nAs we have already seen, a decreasing\nneighborhood size can be realized, for ex-\nample, by means of a time-dependent,\nmonotonically decreasing σ with the\nGaussin bell being used in the topology\nfunction.\n\nThe advantage of a decreasing neighbor-\nhood size is that in the beginning a moving\nneuron "pulls along" many neurons in its\nvicinity, i.e. the randomly initialized net-\nwork can unfold fast and properly in the\nbeginning. In the end of the learning pro-\ncess, only a few neurons are influenced at\nthe same time which stiffens the network\nas a whole but enables a good "fine tuning"\nof the individual neurons.\n\nIt must be noted that\n\nh · η ≤ 1\n\nmust always be true, since otherwise the\nneurons would constantly miss the current\ntraining sample.\n\nBut enough of theory – let us take a look\nat a SOM in action!\n\n152 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.3 Training\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−2 −1.5 −1 −0.5  0  0.5  1  1.5  2\n\nh(\nr)\n\nr\n\nGaussian in 1D\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−4 −2  0  2  4\n\nf(\nx)\n\nx\n\nCone Function\n\n 0\n\n 0.2\n\n 0.4\n\n 0.6\n\n 0.8\n\n 1\n\n−4 −2  0  2  4\n\nf(\nx)\n\nx\n\nCylinder Funktion\n\n−1.5\n\n−1\n\n−0.5\n\n 0\n\n 0.5\n\n 1\n\n 1.5\n\n 2\n\n 2.5\n\n 3\n\n 3.5\n\n−3 −2 −1  0  1  2  3\n\nf(\nx)\n\nx\n\nMexican Hat Function\n\nFigure 10.3: Gaussian bell, cone function, cylinder function and the Mexican hat function sug-\ngested by Kohonen as examples for topology functions of a SOM..\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 153\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\n?>=<89:;1 ?>=<89:;2\n\n����������������������������\n\n?>=<89:;7\n\n?>=<89:;4\n\n�� ��>>>>>>>>\n?>=<89:;6\n\n?>=<89:;3 // // p ?>=<89:;5\n\n?>=<89:;1\n\n?>=<89:;2\n\n?>=<89:;3\n\n?>=<89:;4\n\n?>=<89:;5\n\n?>=<89:;6\n\n?>=<89:;7\n\nFigure 10.4: Illustration of the two-dimensional input space (left) and the one-dimensional topolgy\nspace (right) of a self-organizing map. Neuron 3 is the winner neuron since it is closest to p. In\nthe topology, the neurons 2 and 4 are the neighbors of 3. The arrows mark the movement of the\nwinner neuron and its neighbors towards the training sample p.\n\nTo illustrate the one-dimensional topology of the network, it is plotted into the input space by the\ndotted line. The arrows mark the movement of the winner neuron and its neighbors towards the\npattern.\n\n154 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.4 Examples\n\n10.4 Examples for the\nfunctionality of SOMs\n\nLet us begin with a simple, mentally com-\nprehensible example.\n\nIn this example, we use a two-dimensional\ninput space, i.e. N = 2 is true. Let the\ngrid structure be one-dimensional (G = 1).\nFurthermore, our example SOM should\nconsist of 7 neurons and the learning rate\nshould be η = 0.5.\n\nThe neighborhood function is also kept\nsimple so that we will be able to mentally\ncomprehend the network:\n\nh(i, k, t) =\n\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n1 k direct neighbor of i,\n1 k = i,\n\n0 otherw.\n(10.4)\n\nNow let us take a look at the above-\nmentioned network with random initializa-\ntion of the centers (fig. 10.4 on the preced-\ning page) and enter a training sample p.\nObviously, in our example the input pat-\ntern is closest to neuron 3, i.e. this is the\nwinning neuron.\n\nWe remember the learning rule for\nSOMs\n\n∆ck = η(t) · h(i, k, t) · (p− ck)\n\nand process the three factors from the\nback:\n\nLearning direction: Remember that the\nneuron centers ck are vectors in the\ninput space, as well as the pattern p.\n\nThus, the factor (p−ck) indicates the\nvector of the neuron k to the pattern\np. This is now multiplied by different\nscalars:\n\nOur topology function h indicates that\nonly the winner neuron and its two\nclosest neighbors (here: 2 and 4) are\nallowed to learn by returning 0 for\nall other neurons. A time-dependence\nis not specified. Thus, our vector\n(p − ck) is multiplied by either 1 or\n0.\n\nThe learning rate indicates, as always,\nthe strength of learning. As already\nmentioned, η = 0.5, i. e. all in all, the\nresult is that the winner neuron and\nits neighbors (here: 2, 3 and 4) ap-\nproximate the pattern p half the way\n(in the figure marked by arrows).\n\nAlthough the center of neuron 7 – seen\nfrom the input space – is considerably\ncloser to the input pattern p than neuron\n2, neuron 2 is learning and neuron 7 is\nnot. I want to remind that the network\ntopology specifies which neuron is allowed\n\ntopology\nspecifies,\nwho will learn\n\nto learn and not its position in the input\nspace. This is exactly the mechanism by\nwhich a topology can significantly cover an\ninput space without having to be related\nto it by any sort.\n\nAfter the adaptation of the neurons 2, 3\nand 4 the next pattern is applied, and so\non. Another example of how such a one-\ndimensional SOM can develop in a two-\ndimensional input space with uniformly\ndistributed input patterns in the course of\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 155\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\ntime can be seen in figure 10.5 on the fac-\ning page.\n\nEnd states of one- and two-dimensional\nSOMs with differently shaped input spaces\ncan be seen in figure 10.6 on page 158.\nAs we can see, not every input space can\nbe neatly covered by every network topol-\nogy. There are so called exposed neurons\n– neurons which are located in an area\nwhere no input pattern has ever been oc-\ncurred. A one-dimensional topology gen-\nerally produces less exposed neurons than\na two-dimensional one: For instance, dur-\ning training on circularly arranged input\npatterns it is nearly impossible with a two-\ndimensional squared topology to avoid the\nexposed neurons in the center of the cir-\ncle. These are pulled in every direction\nduring the training so that they finally\nremain in the center. But this does not\nmake the one-dimensional topology an op-\ntimal topology since it can only find less\ncomplex neighborhood relationships than\na multi-dimensional one.\n\n10.4.1 Topological defects are\nfailures in SOM unfolding\n\nDuring the unfolding of a SOM it\ncould happen that a topological defect\n(fig. 10.7) occurs, i.e. the SOM does not\n\n"knot"\nin map unfold correctly. A topological defect can\n\nbe described at best by means of the word\n"knotting".\n\nA remedy for topological defects could\nbe to increase the initial values for the\n\nFigure 10.7: A topological defect in a two-\ndimensional SOM.\n\nneighborhood size, because the more com-\nplex the topology is (or the more neigh-\nbors each neuron has, respectively, since a\nthree-dimensional or a honeycombed two-\ndimensional topology could also be gener-\nated) the more difficult it is for a randomly\ninitialized map to unfold.\n\n10.5 It is possible to adjust\nthe resolution of certain\nareas in a SOM\n\nWe have seen that a SOM is trained by\nentering input patterns of the input space\n\n156 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.5 Adjustment of resolution and position-dependent learning rate\n\nFigure 10.5: Behavior of a SOM with one-dimensional topology (G = 1) after the input of 0, 100,\n300, 500, 5000, 50000, 70000 and 80000 randomly distributed input patterns p ∈ R2. During the\ntraining η decreased from 1.0 to 0.1, the σ parameter of the Gauss function decreased from 10.0\nto 0.2.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 157\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\nFigure 10.6: End states of one-dimensional (left column) and two-dimensional (right column)\nSOMs on different input spaces. 200 neurons were used for the one-dimensional topology, 10× 10\nneurons for the two-dimensionsal topology and 80.000 input patterns for all maps.\n\n158 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.6 Application\n\nRN one after another, again and again so\nthat the SOM will be aligned with these\npatterns and map them. It could happen\nthat we want a certain subset U of the in-\nput space to be mapped more precise than\nthe other ones.\n\nThis problem can easily be solved by\nmeans of SOMs: During the training dis-\nproportionally many input patterns of the\narea U are presented to the SOM. If the\nnumber of training patterns of U ⊂ RN\npresented to the SOM exceeds the number\nof those patterns of the remaining RN \\U ,\nthen more neurons will group there while\nthe remaining neurons are sparsely dis-\ntributed on RN \\ U (fig. 10.8 on the next\npage).more\n\npatterns\n↓\n\nhigher\nresolution\n\nAs you can see in the illustration, the edge\nof the SOM could be deformed. This can\nbe compensated by assigning to the edge\nof the input space a slightly higher proba-\nbility of being hit by training patterns (an\noften applied approach for reaching every\ncorner with the SOMs).\n\nAlso, a higher learning rate is often used\nfor edge and corner neurons, since they are\nonly pulled into the center by the topol-\nogy. This also results in a significantly im-\nproved corner coverage.\n\n10.6 Application of SOMs\n\nRegarding the biologically inspired asso-\nciative data storage, there are many\nfields of application for self-organizing\nmaps and their variations.\n\nFor example, the different phonemes of\nthe finnish language have successfully been\nmapped onto a SOM with a two dimen-\nsional discrete grid topology and therefore\nneighborhoods have been found (a SOM\ndoes nothing else than finding neighbor-\nhood relationships). So one tries once\nmore to break down a high-dimensional\nspace into a low-dimensional space (the\ntopology), looks if some structures have\nbeen developed – et voilà: clearly defined\nareas for the individual phenomenons are\nformed.\n\nTeuvo Kohonen himself made the ef-\nfort to search many papers mentioning his\nSOMs in their keywords. In this large in-\nput space the individual papers now indi-\nvidual positions, depending on the occur-\nrence of keywords. Then Kohonen created\na SOM with G = 2 and used it to map the\nhigh-dimensional "paper space" developed\nby him.\n\nThus, it is possible to enter any paper\ninto the completely trained SOM and look\nwhich neuron in the SOM is activated. It\nwill be likely to discover that the neigh-\nbored papers in the topology are interest-\ning, too. This type of brain-like context-\nbased search also works with many other\ninput spaces.\n\nSOM finds\nsimilarities\n\nIt is to be noted that the system itself\ndefines what is neighbored, i.e. similar,\nwithin the topology – and that’s why it\nis so interesting.\n\nThis example shows that the position c of\nthe neurons in the input space is not signif-\nicant. It is rather interesting to see which\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 159\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\nFigure 10.8: Training of a SOM with G = 2 on a two-dimensional input space. On the left side,\nthe chance to become a training pattern was equal for each coordinate of the input space. On the\nright side, for the central circle in the input space, this chance is more than ten times larger than\nfor the remaining input space (visible in the larger pattern density in the background). In this circle\nthe neurons are obviously more crowded and the remaining area is covered less dense but in both\ncases the neurons are still evenly distributed. The two SOMS were trained by means of 80.000\ntraining samples and decreasing η (1→ 0.2) as well as decreasing σ (5→ 0.5).\n\n160 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.7 Variations\n\nneuron is activated when an unknown in-\nput pattern is entered. Next, we can look\nat which of the previous inputs this neu-\nron was also activated – and will imme-\ndiately discover a group of very similar\ninputs. The more the inputs within the\ntopology are diverging, the less things they\nhave in common. Virtually, the topology\ngenerates a map of the input characteris-\ntics – reduced to descriptively few dimen-\nsions in relation to the input dimension.\n\nTherefore, the topology of a SOM often\nis two-dimensional so that it can be easily\nvisualized, while the input space can be\nvery high-dimensional.\n\n10.6.1 SOMs can be used to\ndetermine centers for RBF\nneurons\n\nSOMs arrange themselves exactly towards\nthe positions of the outgoing inputs. As a\nresult they are used, for example, to select\nthe centers of an RBF network. We have\nalready been introduced to the paradigm\nof the RBF network in chapter 6.\n\nAs we have already seen, it is possible\nto control which areas of the input space\nshould be covered with higher resolution\n- or, in connection with RBF networks,\non which areas of our function should the\nRBF network work with more neurons, i.e.\nwork more exactly. As a further useful fea-\nture of the combination of RBF networks\nwith SOMs one can use the topology ob-\ntained through the SOM: During the final\ntraining of a RBF neuron it can be used\n\nto influence neighboring RBF neurons in\ndifferent ways.\n\nFor this, many neural network simulators\noffer an additional so-called SOM layer\nin connection with the simulation of RBF\nnetworks.\n\n10.7 Variations of SOMs\n\nThere are different variations of SOMs\nfor different variations of representation\ntasks:\n\n10.7.1 A neural gas is a SOM\nwithout a static topology\n\nThe neural gas is a variation of the self-\norganizing maps of Thomas Martinetz\n[MBS93], which has been developed from\nthe difficulty of mapping complex input\ninformation that partially only occur in\nthe subspaces of the input space or even\nchange the subspaces (fig. 10.9 on the fol-\nlowing page).\n\nThe idea of a neural gas is, roughly speak-\ning, to realize a SOM without a grid struc-\nture. Due to the fact that they are de-\nrived from the SOMs the learning steps\nare very similar to the SOM learning steps,\nbut they include an additional intermedi-\nate step:\n\n. again, random initialization of ck ∈\nRn\n\n. selection and presentation of a pat-\ntern of the input space p ∈ Rn\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 161\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\nFigure 10.9: A figure filling different subspaces of the actual input space of different positions\ntherefore can hardly be filled by a SOM.\n\n. neuron distance measurement\n\n. identification of the winner neuron i\n\n. Intermediate step: generation of a list\nL of neurons sorted in ascending order\nby their distance to the winner neu-\nron. Thus, the first neuron in the list\nL is the neuron that is closest to the\nwinner neuron.\n\n. changing the centers by means of the\nknown rule but with the slightly mod-\nified topology function\n\nhL(i, k, t).\n\nThe function hL(i, k, t), which is slightly\nmodified compared with the original func-\ntion h(i, k, t), now regards the first el-\nements of the list as the neighborhood\n\nof the winner neuron i. The direct re-\nsult is that – similar to the free-floating\n\ndynamic\nneighborhoodmolecules in a gas – the neighborhood rela-\n\ntionships between the neurons can change\nanytime, and the number of neighbors is\nalmost arbitrary, too. The distance within\nthe neighborhood is now represented by\nthe distance within the input space.\n\nThe bulk of neurons can become as stiff-\nened as a SOM by means of a constantly\ndecreasing neighborhood size. It does not\nhave a fixed dimension but it can take the\ndimension that is locally needed at the mo-\nment, which can be very advantageous.\n\nA disadvantage could be that there is\nno fixed grid forcing the input space to\nbecome regularly covered, and therefore\nwholes can occur in the cover or neurons\ncan be isolated.\n\n162 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 10.7 Variations\n\nIn spite of all practical hints, it is as al-\nways the user’s responsibility not to un-\nderstand this text as a catalog for easy an-\nswers but to explore all advantages and\ndisadvantages himself.\n\nUnlike a SOM, the neighborhood of a neu-\nral gas must initially refer to all neurons\nsince otherwise some outliers of the ran-\ndom initialization may never reach the re-\nmaining group. To forget this is a popular\nerror during the implementation of a neu-\nral gas.\n\nWith a neural gas it is possible to learn a\nkind of complex input such as in fig. 10.9\n\ncan classify\ncomplex\n\nfigure\non the preceding page since we are not\nbound to a fixed-dimensional grid. But\nsome computational effort could be neces-\nsary for the permanent sorting of the list\n(here, it could be effective to store the list\nin an ordered data structure right from the\nstart).\n\nDefinition 10.6 (Neural gas). A neural\ngas differs from a SOM by a completely dy-\nnamic neighborhood function. With every\nlearning cycle it is decided anew which neu-\nrons are the neigborhood neurons of the\nwinner neuron. Generally, the criterion\nfor this decision is the distance between\nthe neurosn and the winner neuron in the\ninput space.\n\n10.7.2 A Multi-SOM consists of\nseveral separate SOMs\n\nIn order to present another variant of the\nSOMs, I want to formulate an extended\n\nproblem: What do we do with input pat-\nterns from which we know that they are\nconfined in different (maybe disjoint) ar-\neas?\n\nseveral SOMs\n\nHere, the idea is to use not only one\nSOM but several ones: A multi-self-\norganizing map, shortly referred to as\nM-SOM [GKE01b,GKE01a,GS06]. It is\nunnecessary that the SOMs have the same\ntopology or size, an M-SOM is just a com-\nbination of M SOMs.\n\nThis learning process is analog to that of\nthe SOMs. However, only the neurons be-\nlonging to the winner SOM of each train-\ning step are adapted. Thus, it is easy to\nrepresent two disjoint clusters of data by\nmeans of two SOMs, even if one of the\nclusters is not represented in every dimen-\nsion of the input space RN . Actually, the\nindividual SOMs exactly reflect these clus-\nters.\n\nDefinition 10.7 (Multi-SOM). A multi-\nSOM is nothing more than the simultane-\nous use of M SOMs.\n\n10.7.3 A multi-neural gas consists\nof several separate neural\ngases\n\nAnalogous to the multi-SOM, we also have\na set of M neural gases: a multi-neural\ngas [GS06, SG06]. This construct be-\n\nseveral gases\nhaves analogous to neural gas and M-SOM:\nAgain, only the neurons of the winner gas\nare adapted.\n\nThe reader certainly wonders what advan-\ntage is there to use a multi-neural gas since\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 163\n\n\n\nChapter 10 Self-organizing feature maps dkriesel.com\n\nan individual neural gas is already capa-\nble to divide into clusters and to work on\ncomplex input patterns with changing di-\nmensions. Basically, this is correct, but\na multi-neural gas has two serious advan-\ntages over a simple neural gas.\n\n1. With several gases, we can directly\ntell which neuron belongs to which\ngas. This is particularly important\nfor clustering tasks, for which multi-\nneural gases have been used recently.\nSimple neural gases can also find and\ncover clusters, but now we cannot rec-\nognize which neuron belongs to which\ncluster.\n\nless computa-\ntional effort 2. A lot of computational effort is saved\n\nwhen large original gases are divided\ninto several smaller ones since (as al-\nready mentioned) the sorting of the\nlist L could use a lot of computa-\ntional effort while the sorting of sev-\neral smaller lists L1, L2, . . . , LM is less\ntime-consuming – even if these lists in\ntotal contain the same number of neu-\nrons.\n\nAs a result we will only obtain local in-\nstead of global sortings, but in most cases\nthese local sortings are sufficient.\n\nNow we can choose between two extreme\ncases of multi-neural gases: One extreme\ncase is the ordinary neural gas M = 1, i.e.\nwe only use one single neural gas. Interest-\ning enough, the other extreme case (very\nlargeM , a few or only one neuron per gas)\nbehaves analogously to the K-means clus-\ntering (for more information on clustering\nprocedures see excursus A).\n\nDefinition 10.8 (Multi-neural gas). A\nmulti-neural gas is nothing more than the\nsimultaneous use of M neural gases.\n\n10.7.4 Growing neural gases can\nadd neurons to themselves\n\nA growing neural gas is a variation of\nthe aforementioned neural gas to which\nmore and more neurons are added accord-\ning to certain rules. Thus, this is an at-\ntempt to work against the isolation of neu-\nrons or the generation of larger wholes in\nthe cover.\n\nHere, this subject should only be men-\ntioned but not discussed.\n\nTo build a growing SOM is more difficult\nbecause new neurons have to be integrated\nin the neighborhood.\n\nExercises\n\nExercise 17. A regular, two-dimensional\ngrid shall cover a two-dimensional surface\nas "well" as possible.\n\n1. Which grid structure would suit best\nfor this purpose?\n\n2. Which criteria did you use for "well"\nand "best"?\n\nThe very imprecise formulation of this ex-\nercise is intentional.\n\n164 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\nChapter 11\n\nAdaptive resonance theory\nAn ART network in its original form shall classify binary input vectors, i.e. to\n\nassign them to a 1-out-of-n output. Simultaneously, the so far unclassified\npatterns shall be recognized and assigned to a new class.\n\nAs in the other smaller chapters, we want\nto try to figure out the basic idea of\nthe adaptive resonance theory (abbre-\nviated: ART) without discussing its the-\nory profoundly.\n\nIn several sections we have already men-\ntioned that it is difficult to use neural\nnetworks for the learning of new informa-\ntion in addition to but without destroying\nthe already existing information. This cir-\ncumstance is called stability / plasticity\ndilemma.\n\nIn 1987, Stephen Grossberg and Gail\nCarpenter published the first version of\ntheir ART network [Gro76] in order to al-\nleviate this problem. This was followed\nby a whole family of ART improvements\n(which we want to discuss briefly, too).\n\nIt is the idea of unsupervised learning,\nwhose aim is the (initially binary) pattern\nrecognition, or more precisely the catego-\nrization of patterns into classes. But addi-\n\ntionally an ART network shall be capable\nto find new classes.\n\n11.1 Task and structure of an\nART network\n\nAn ART network comprises exactly two\nlayers: the input layer I and the recog-\nnition layer O with the input layer be-\ning completely linked towards the recog-\nnition layer. This complete link induces\na top-down weight matrix W that con-\ntains the weight values of the connections\nbetween each neuron in the input layer\nand each neuron in the recognition layer\n(fig. 11.1 on the following page).\n\nSimple binary patterns are entered into\nthe input layer and transferred to the pattern\n\nrecognitionrecognition layer while the recognition\nlayer shall return a 1-out-of-|O| encoding,\ni.e. it should follow the winner-takes-all\n\n165\n\n\n\nChapter 11 Adaptive resonance theory dkriesel.com\n\n�� �� �� ��GFED@ABCi1\n\n��\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n�� ��4\n4444444444444\n\n##FFFFFFFFFFFFFFFFFFFFF\n\n\'\'OOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n\n))SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS GFED@ABCi2\n\n{{xxxxxxxxxxxxxxxxxxxxx\n\n��\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n�� ��4\n4444444444444\n\n##FFFFFFFFFFFFFFFFFFFFF\n\n\'\'OOOOOOOOOOOOOOOOOOOOOOOOOOOOO GFED@ABCi3\n\nwwooooooooooooooooooooooooooooo\n\n{{xxxxxxxxxxxxxxxxxxxxx\n\n��\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n�� ��4\n4444444444444\n\n##FFFFFFFFFFFFFFFFFFFFF GFED@ABCi4\n\nuukkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\n\nwwooooooooooooooooooooooooooooo\n\n{{xxxxxxxxxxxxxxxxxxxxx\n\n��\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n�� ��4\n4444444444444\n\nGFED@ABCΩ1\n\nEE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n;;xxxxxxxxxxxxxxxxxxxxx\n\n77ooooooooooooooooooooooooooooo\n\n55kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\n\n��\n\nGFED@ABCΩ2\n\nOO EE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n;;xxxxxxxxxxxxxxxxxxxxx\n\n77ooooooooooooooooooooooooooooo\n\n��\n\nGFED@ABCΩ3\n\nYY44444444444444\n\nOO EE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n;;xxxxxxxxxxxxxxxxxxxxx\n\n��\n\nGFED@ABCΩ4\n\nccFFFFFFFFFFFFFFFFFFFFF\n\nYY44444444444444\n\nOO EE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n��\n\nGFED@ABCΩ5\n\nggOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n\nccFFFFFFFFFFFFFFFFFFFFF\n\nYY44444444444444\n\nOO\n\n��\n\nGFED@ABCΩ6\n\niiSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n\nggOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n\nccFFFFFFFFFFFFFFFFFFFFF\n\nYY44444444444444\n\n��\n\nFigure 11.1: Simplified illustration of the ART network structure. Top: the input layer, bottom:\nthe recognition layer. In this illustration the lateral inhibition of the recognition layer and the control\nneurons are omitted.\n\nscheme. For instance, to realize this 1-\nout-of-|O| encoding the principle of lateral\ninhibition can be used – or in the imple-\nmentation the most activated neuron can\nbe searched. For practical reasons an IF\nquery would suit this task best.\n\n11.1.1 Resonance takes place by\nactivities being tossed and\nturned\n\nBut there also exists a bottom-up weight\nmatrix V , which propagates the activi-\n\nVI ties within the recognition layer back into\nthe input layer. Now it is obvious that\nthese activities are bounced forth and back\nagain and again, a fact that leads us to\nresonance. Every activity within the in-\n\nput layer causes an activity within the\nlayers\nactivate\none\nanother\n\nrecognition layer while in turn in the recog-\nnition layer every activity causes an activ-\nity within the input layer.\n\nIn addition to the two mentioned layers,\nin an ART network also exist a few neu-\nrons that exercise control functions such as\nsignal enhancement. But we do not want\nto discuss this theory further since here\nonly the basic principle of the ART net-\nwork should become explicit. I have only\nmentioned it to explain that in spite of the\nrecurrences, the ART network will achieve\na stable state after an input.\n\n166 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com 11.3 Extensions\n\n11.2 The learning process of\nan ART network is\ndivided to top-down and\nbottom-up learning\n\nThe trick of adaptive resonance theory is\nnot only the configuration of the ART net-\nwork but also the two-piece learning pro-\ncedure of the theory: On the one hand\nwe train the top-down matrix W , on the\nother hand we train the bottom-up matrix\nV (fig. 11.2 on the next page).\n\n11.2.1 Pattern input and top-down\nlearning\n\nWhen a pattern is entered into the net-\nwork it causes - as already mentioned - an\nactivation at the output neurons and thewinner\n\nneuron\nis\n\namplified\n\nstrongest neuron wins. Then the weights\nof the matrix W going towards the output\nneuron are changed such that the output\nof the strongest neuron Ω is still enhanced,\ni.e. the class affiliation of the input vector\nto the class of the output neuron Ω be-\ncomes enhanced.\n\n11.2.2 Resonance and bottom-up\nlearning\n\nThe training of the backward weights ofinput is\nteach. inp.\n\nfor backward\nweights\n\nthe matrix V is a bit tricky: Only the\nweights of the respective winner neuron\nare trained towards the input layer and\nour current input pattern is used as teach-\ning input. Thus, the network is trained to\nenhance input vectors.\n\n11.2.3 Adding an output neuron\n\nOf course, it could happen that the neu-\nrons are nearly equally activated or that\nseveral neurons are activated, i.e. that the\nnetwork is indecisive. In this case, the\nmechanisms of the control neurons acti-\nvate a signal that adds a new output neu-\nron. Then the current pattern is assigned\nto this output neuron and the weight sets\nof the new neuron are trained as usual.\n\nThus, the advantage of this system is not\nonly to divide inputs into classes and to\nfind new classes, it can also tell us after\nthe activation of an output neuron what a\ntypical representative of a class looks like\n- which is a significant feature.\n\nOften, however, the system can only mod-\nerately distinguish the patterns. The ques-\ntion is when a new neuron is permitted to\nbecome active and when it should learn.\nIn an ART network there are different ad-\nditional control neurons which answer this\nquestion according to different mathemat-\nical rules and which are responsible for in-\ntercepting special cases.\n\nAt the same time, one of the largest ob-\njections to an ART is the fact that an\nART network uses a special distinction of\ncases, similar to an IF query, that has been\nforced into the mechanism of a neural net-\nwork.\n\n11.3 Extensions\n\nAs already mentioned above, the ART net-\nworks have often been extended.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 167\n\n\n\nChapter 11 Adaptive resonance theory dkriesel.comKapitel 11 Adaptive Resonance Theory dkriesel.com\n\n�� �� �� ��GFED@ABCi1\n\n�� ""\n\nGFED@ABCi2\n\n�� ��\n\nGFED@ABCi3\n\n�� ��\n\nGFED@ABCi4\n\n|| ��GFED@ABCΩ1\n\nYY OO EE <<\n\n��\n\nGFED@ABCΩ2\n\nbb YY OO EE\n\n��\n0 1\n\n�� �� �� ��GFED@ABCi1\n\n�� ""FFFFFFFFFFFFFFFFFFFF GFED@ABCi2\n\n�� ��4\n4444444444444\nGFED@ABCi3\n\n�� ��\n\nGFED@ABCi4\n\n|| ����������������\n\nGFED@ABCΩ1\n\nYY OO EE <<\n\n��\n\nGFED@ABCΩ2\n\nbb YY OO EE\n\n��\n0 1\n\n�� �� �� ��GFED@ABCi1\n\n�� ""\n\nGFED@ABCi2\n\n�� ��\n\nGFED@ABCi3\n\n�� ��\n\nGFED@ABCi4\n\n|| ��GFED@ABCΩ1\n\nYY OO EE <<\n\n��\n\nGFED@ABCΩ2\n\nbbFFFFFFFFFFFFFFFFFFFF\n\nYY44444444444444\n\nOO EE��������������\n\n��\n0 1\n\nAbbildung 11.2: Vereinfachte Darstellung des\nzweigeteilten Trainings eines ART-Netzes: Die\njeweils trainierten Gewichte sind durchgezogen\ndargestellt. Nehmen wir an, ein Muster wurde in\ndas Netz eingegeben und die Zahlen markieren\nAusgaben. Oben: Wir wir sehen, ist Ω2 das Ge-\nwinnerneuron. Mitte: Also werden die Gewichte\nzum Gewinnerneuron hin trainiert und (unten)\ndie Gewichte vom Gewinnerneuron zur Eingangs-\nschicht trainiert.\n\neiner IF-Abfrage, die man in den Mecha-\nnismus eines Neuronalen Netzes gepresst\nhat.\n\n11.3 Erweiterungen\n\nWie schon eingangs erwähnt, wurden die\nART-Netze vielfach erweitert.\n\nART-2 [CG87] ist eine Erweiterung\nauf kontinuierliche Eingaben und bietet\nzusätzlich (in einer ART-2A genannten\nErweiterung) Verbesserungen der Lernge-\nschwindigkeit, was zusätzliche Kontroll-\nneurone und Schichten zur Folge hat.\n\nART-3 [CG90] verbessert die Lernfähig-\nkeit von ART-2, indem zusätzliche biolo-\ngische Vorgänge wie z.B. die chemischen\nVorgänge innerhalb der Synapsen adap-\ntiert werden1.\n\nZusätzlich zu den beschriebenen Erweite-\nrungen existieren noch viele mehr.\n\n1 Durch die häufigen Erweiterungen der Adaptive\nResonance Theory sprechen böse Zungen bereits\nvon ”ART-n-Netzen“.\n\n168 D. Kriesel – Ein kleiner Überblick über Neuronale Netze (EPSILON-DE)\n\nFigure 11.2: Simplified illustration of the two-\npiece training of an ART network: The trained\nweights are represented by solid lines. Let us as-\nsume that a pattern has been entered into the\nnetwork and that the numbers mark the outputs.\nTop: We can see that Ω2 is the winner neu-\nron. Middle: So the weights are trained towards\nthe winner neuron and (below) the weights of\nthe winner neuron are trained towards the input\nlayer.\n\nART-2 [CG87] is extended to continuous\ninputs and additionally offers (in an ex-\ntension called ART-2A) enhancements of\nthe learning speed which results in addi-\ntional control neurons and layers.\n\nART-3 [CG90] 3 improves the learning\nability of ART-2 by adapting additional\nbiological processes such as the chemical\nprocesses within the synapses1.\n\nApart from the described ones there exist\nmany other extensions.\n\n1 Because of the frequent extensions of the adap-\ntive resonance theory wagging tongues already call\nthem "ART-n networks".\n\n168 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\nPart IV\n\nExcursi, appendices and registers\n\n169\n\n\n\n\n\nAppendix A\n\nExcursus: Cluster analysis and regional and\nonline learnable fields\n\nIn Grimm’s dictionary the extinct German word "Kluster" is described by "was\ndicht und dick zusammensitzet (a thick and dense group of sth.)". In static\n\ncluster analysis, the formation of groups within point clouds is explored.\nIntroduction of some procedures, comparison of their advantages and\n\ndisadvantages. Discussion of an adaptive clustering method based on neural\nnetworks. A regional and online learnable field models from a point cloud,\npossibly with a lot of points, a comparatively small set of neurons being\n\nrepresentative for the point cloud.\n\nAs already mentioned, many problems can\nbe traced back to problems in cluster\nanalysis. Therefore, it is necessary to re-\nsearch procedures that examine whether\ngroups (so-called clusters) exist within\npoint clouds.\n\nSince cluster analysis procedures need a\nnotion of distance between two points, a\nmetric must be defined on the space\nwhere these points are situated.\n\nWe briefly want to specify what a metric\nis.\n\nDefinition A.1 (Metric). A relation\ndist(x1, x2) defined for two objects x1, x2\nis referred to as metric if each of the fol-\nlowing criteria applies:\n\n1. dist(x1, x2) = 0 if and only if x1 = x2,\n\n2. dist(x1, x2) = dist(x2, x1), i.e. sym-\nmetry,\n\n3. dist(x1, x3) ≤ dist(x1, x2) +\ndist(x2, x3), i.e. the triangle\ninequality holds.\n\nColloquially speaking, a metric is a tool\nfor determining distances between points\nin any space. Here, the distances have\nto be symmetrical, and the distance be-\ntween to points may only be 0 if the two\npoints are equal. Additionally, the trian-\ngle inequality must apply.\n\nMetrics are provided by, for example, the\nsquared distance and the Euclidean\ndistance, which have already been intro-\nduced. Based on such metrics we can de-\n\n171\n\n\n\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\n\nfine a clustering procedure that uses a met-\nric as distance measure.\n\nNow we want to introduce and briefly dis-\ncuss different clustering procedures.\n\nA.1 k-means clustering\nallocates data to a\npredefined number of\nclusters\n\nk-means clustering according to J.\nMacQueen [Mac67] is an algorithm that\nis often used because of its low computa-\ntion and storage complexity and which is\nregarded as "inexpensive and good". The\noperation sequence of the k-means cluster-\ning algorithm is the following:\n\n1. Provide data to be examined.\n\n2. Define k, which is the number of clus-\nter centers.\n\n3. Select k random vectors for the clus-\nter centers (also referred to as code-\nbook vectors).\n\n4. Assign each data point to the next\ncodebook vector1\n\n5. Compute cluster centers for all clus-\nters.\n\n6. Set codebook vectors to new cluster\ncenters.\n\n1 The name codebook vector was created because\nthe often used name cluster vector was too un-\nclear.\n\n7. Continue with 4 until the assignments\nare no longer changed.\n\nnumber of\ncluster\nmust be\nknown\npreviously\n\nStep 2 already shows one of the great ques-\ntions of the k-means algorithm: The num-\nber k of the cluster centers has to be de-\ntermined in advance. This cannot be done\nby the algorithm. The problem is that it\nis not necessarily known in advance how k\ncan be determined best. Another problem\nis that the procedure can become quite in-\nstable if the codebook vectors are badly\ninitialized. But since this is random, it\nis often useful to restart the procedure.\nThis has the advantage of not requiring\nmuch computational effort. If you are fully\naware of those weaknesses, you will receive\nquite good results.\n\nHowever, complex structures such as "clus-\nters in clusters" cannot be recognized. If k\nis high, the outer ring of the construction\nin the following illustration will be recog-\nnized as many single clusters. If k is low,\nthe ring with the small inner clusters will\nbe recognized as one cluster.\n\nFor an illustration see the upper right part\nof fig. A.1 on page 174.\n\nA.2 k-nearest neighboring\nlooks for the k nearest\nneighbors of each data\npoint\n\nThe k-nearest neighboring procedure\n[CH67] connects each data point to the k\nclosest neighbors, which often results in a\ndivision of the groups. Then such a group\n\n172 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com A.4 The silhouette coefficient\n\nbuilds a cluster. The advantage is that\nthe number of clusters occurs all by it-\nself. The disadvantage is that a large stor-\nage and computational effort is required to\nfind the next neighbor (the distances be-\ntween all data points must be computed\nand stored).\n\nclustering\nnext\n\npoints There are some special cases in which the\nprocedure combines data points belonging\nto different clusters, if k is too high. (see\nthe two small clusters in the upper right\nof the illustration). Clusters consisting of\nonly one single data point are basically\nconncted to another cluster, which is not\nalways intentional.\n\nFurthermore, it is not mandatory that the\nlinks between the points are symmetric.\n\nBut this procedure allows a recognition of\nrings and therefore of "clusters in clusters",\nwhich is a clear advantage. Another ad-\nvantage is that the procedure adaptively\nresponds to the distances in and between\nthe clusters.\n\nFor an illustration see the lower left part\nof fig. A.1.\n\nA.3 ε-nearest neighboring\nlooks for neighbors within\nthe radius ε for each\ndata point\n\nAnother approach of neighboring: here,\nthe neighborhood detection does not use a\nfixed number k of neighbors but a radius ε,\n\nwhich is the reason for the name epsilon-\nnearest neighboring. Points are neig-\nbors if they are at most ε apart from each\nother. Here, the storage and computa-\ntional effort is obviously very high, which\nis a disadvantage.\n\nclustering\nradii around\npointsBut note that there are some special cases:\n\nTwo separate clusters can easily be con-\nnected due to the unfavorable situation of\na single data point. This can also happen\nwith k-nearest neighboring, but it would\nbe more difficult since in this case the num-\nber of neighbors per point is limited.\n\nAn advantage is the symmetric nature of\nthe neighborhood relationships. Another\nadvantage is that the combination of min-\nimal clusters due to a fixed number of\nneighbors is avoided.\n\nOn the other hand, it is necessary to skill-\nfully initialize ε in order to be successful,\ni.e. smaller than half the smallest distance\nbetween two clusters. With variable clus-\nter and point distances within clusters this\ncan possibly be a problem.\n\nFor an illustration see the lower right part\nof fig. A.1.\n\nA.4 The silhouette coefficient\ndetermines how accurate\na given clustering is\n\nAs we can see above, there is no easy an-\nswer for clustering problems. Each proce-\ndure described has very specific disadvan-\ntages. In this respect it is useful to have\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 173\n\n\n\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\n\nFigure A.1: Top left: our set of points. We will use this set to explore the different clustering\nmethods. Top right: k-means clustering. Using this procedure we chose k = 6. As we can\nsee, the procedure is not capable to recognize "clusters in clusters" (bottom left of the illustration).\nLong "lines" of points are a problem, too: They would be recognized as many small clusters (if k\nis sufficiently large). Bottom left: k-nearest neighboring. If k is selected too high (higher than\nthe number of points in the smallest cluster), this will result in cluster combinations shown in the\nupper right of the illustration. Bottom right: ε-nearest neighboring. This procedure will cause\ndifficulties if ε is selected larger than the minimum distance between two clusters (see upper left of\nthe illustration), which will then be combined.\n\n174 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com A.5 Regional and online learnable fields\n\na criterion to decide how good our clus-\nter division is. This possibility is offered\nby the silhouette coefficient according\nto [Kau90]. This coefficient measures how\nwell the clusters are delimited from each\nother and indicates if points may be as-\nsigned to the wrong clusters.\n\nclustering\nquality is\n\nmeasureable Let P be a point cloud and p a point in\nP . Let c ⊆ P be a cluster within the\npoint cloud and p be part of this cluster,\ni.e. p ∈ c. The set of clusters is called C.\nSummary:\n\np ∈ c ⊆ P\n\napplies.\n\nTo calculate the silhouette coefficient, we\ninitially need the average distance between\npoint p and all its cluster neighbors. This\nvariable is referred to as a(p) and defined\nas follows:\n\na(p) = 1\n|c| − 1\n\n∑\nq∈c,q 6=p\n\ndist(p, q) (A.1)\n\nFurthermore, let b(p) be the average dis-\ntance between our point p and all points\nof the next cluster (g represents all clusters\nexcept for c):\n\nb(p) = min\ng∈C,g 6=c\n\n1\n|g|\n∑\nq∈g\n\ndist(p, q) (A.2)\n\nThe point p is classified well if the distance\nto the center of the own cluster is minimal\nand the distance to the centers of the other\nclusters is maximal. In this case, the fol-\nlowing term provides a value close to 1:\n\ns(p) = b(p)− a(p)\nmax{a(p), b(p)} (A.3)\n\nApparently, the whole term s(p) can only\nbe within the interval [−1; 1]. A value\nclose to -1 indicates a bad classification of\np.\n\nThe silhouette coefficient S(P ) results\nfrom the average of all values s(p):\n\nS(P ) = 1\n|P |\n\n∑\np∈P\n\ns(p). (A.4)\n\nAs above the total quality of the clus-\nter division is expressed by the interval\n[−1; 1].\n\nAs different clustering strategies with dif-\nferent characteristics have been presented\nnow (lots of further material is presented\nin [DHS01]), as well as a measure to in-\ndicate the quality of an existing arrange-\nment of given data into clusters, I want\nto introduce a clustering method based\non an unsupervised learning neural net-\nwork [SGE05] which was published in 2005.\nLike all the other methods this one may\nnot be perfect but it eliminates large stan-\ndard weaknesses of the known clustering\nmethods\n\nA.5 Regional and online\nlearnable fields are a\nneural clustering strategy\n\nThe paradigm of neural networks, which I\nwant to introduce now, are the regional\nand online learnable fields, shortly re-\nferred to as ROLFs.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 175\n\n\n\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\n\nA.5.1 ROLFs try to cover data with\nneurons\n\nRoughly speaking, the regional and online\nlearnable fields are a set K of neurons\n\nKI which try to cover a set of points as well\nas possible by means of their distribution\nin the input space. For this, neurons are\nadded, moved or changed in their size dur-\n\nnetwork\ncovers\n\npoint cloud\ning training if necessary. The parameters\nof the individual neurons will be discussed\nlater.\n\nDefinition A.2 (Regional and online\nlearnable field). A regional and on-\nline learnable field (abbreviated ROLF or\nROLF network) is a set K of neurons that\nare trained to cover a certain set in the\ninput space as well as possible.\n\nA.5.1.1 ROLF neurons feature a\nposition and a radius in the\ninput space\n\nHere, a ROLF neuron k ∈ K has two\nparameters: Similar to the RBF networks,\nit has a center ck, i.e. a position in the\n\ncI input space.\n\nBut it has yet another parameter: The ra-\ndius σ, which defines the radius of the per-\n\nσI ceptive surface surrounding the neuron2.\nA neuron covers the part of the input space\nthat is situated within this radius.\n\nck and σk are locally defined for each neu-neuron\nrepresents\n\nsurface 2 I write "defines" and not "is" because the actual\nradius is specified by σ · ρ.\n\nFigure A.2: Structure of a ROLF neuron.\n\nron. This particularly means that the neu-\nrons are capable to cover surfaces of differ-\nent sizes.\n\nThe radius of the perceptive surface is\nspecified by r = ρ · σ (fig. A.2) with\nthe multiplier ρ being globally defined and\npreviously specified for all neurons. Intu-\nitively, the reader will wonder what this\nmultiplicator is used for. Its significance\nwill be discussed later. Furthermore, the\nfollowing has to be observed: It is not nec-\nessary for the perceptive surface of the dif-\nferent neurons to be of the same size.\n\nDefinition A.3 (ROLF neuron). The pa-\nrameters of a ROLF neuron k are a center\nck and a radius σk.\n\nDefinition A.4 (Perceptive surface).\nThe perceptive surface of a ROLF neuron\n\n176 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com A.5 Regional and online learnable fields\n\nk consists of all points within the radius\nρ · σ in the input space.\n\nA.5.2 A ROLF learns unsupervised\nby presenting training\nsamples online\n\nLike many other paradigms of neural net-\nworks our ROLF network learns by receiv-\ning many training samples p of a training\nset P . The learning is unsupervised. For\neach training sample p entered into the net-\nwork two cases can occur:\n\n1. There is one accepting neuron k for p\nor\n\n2. there is no accepting neuron at all.\n\nIf in the first case several neurons are suit-\nable, then there will be exactly one ac-\ncepting neuron insofar as the closest neu-\nron is the accepting one. For the accepting\nneuron k ck and σk are adapted.\n\nDefinition A.5 (Accepting neuron). The\ncriterion for a ROLF neuron k to be an\naccepting neuron of a point p is that the\npoint p must be located within the percep-\ntive surface of k. If p is located in the per-\nceptive surfaces of several neurons, then\nthe closest neuron will be the accepting\none. If there are several closest neurons,\none can be chosen randomly.\n\nA.5.2.1 Both positions and radii are\nadapted throughout learning\n\nAdapting\nexisting\nneurons\n\nLet us assume that we entered a training\nsample p into the network and that there\n\nis an accepting neuron k. Then the radius\nmoves towards ||p − ck|| (i.e. towards the\ndistance between p and ck) and the center\nck towards p. Additionally, let us define\nthe two learning rates ησ and ηc for radii Jησ, ηcand centers.\n\nck(t+ 1) = ck(t) + ηc(p− ck(t))\nσk(t+ 1) = σk(t) + ησ(||p− ck(t)|| − σk(t))\n\nNote that here σk is a scalar while ck is a\nvector in the input space.\n\nDefinition A.6 (Adapting a ROLF neu-\nron). A neuron k accepted by a point p is\nadapted according to the following rules:\n\nck(t+ 1) = ck(t) + ηc(p− ck(t)) (A.5)\n\nσk(t+ 1) = σk(t) + ησ(||p− ck(t)|| − σk(t))\n(A.6)\n\nA.5.2.2 The radius multiplier allows\nneurons to be able not only to\nshrink\n\nNow we can understand the function of the\nmultiplier ρ: Due to this multiplier the per- Jρceptive surface of a neuron includes more\nthan only all points surrounding the neu-\nron in the radius σ. This means that due\nto the aforementioned learning rule σ can-\nnot only decrease but also increase.\n\nso the\nneurons\ncan growDefinition A.7 (Radius multiplier). The\n\nradius multiplier ρ > 1 is globally defined\nand expands the perceptive surface of a\nneuron k to a multiple of σk. So it is en-\nsured that the radius σk cannot only de-\ncrease but also increase.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 177\n\n\n\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\n\nGenerally, the radius multiplier is set to\nvalues in the lower one-digit range, such\nas 2 or 3.\n\nSo far we only have discussed the case in\nthe ROLF training that there is an accept-\ning neuron for the training sample p.\n\nA.5.2.3 As required, new neurons are\ngenerated\n\nThis suggests to discuss the approach for\nthe case that there is no accepting neu-\nron.\n\nIn this case a new accepting neuron k is\ngenerated for our training sample. The re-\nsult is of course that ck and σk have to be\ninitialized.\n\nThe initialization of ck can be understood\nintuitively: The center of the new neuron\nis simply set on the training sample, i.e.\n\nck = p.\n\nWe generate a new neuron because there\nis no neuron close to p – for logical reasons,\nwe place the neuron exactly on p.\n\nBut how to set a σ when a new neuron\nis generated? For this purpose there exist\ndifferent options:\n\nInit-σ: We always select a predefined\nstatic σ.\n\nMinimum σ: We take a look at the σ of\neach neuron and select the minimum.\n\nMaximum σ: We take a look at the σ of\neach neuron and select the maximum.\n\nMean σ: We select the mean σ of all neu-\nrons.\n\nCurrently, the mean-σ variant is the fa-\nvorite one although the learning procedure\nalso works with the other ones. In the\nminimum-σ variant the neurons tend to\ncover less of the surface, in the maximum-\nσ variant they tend to cover more of the\nsurface.\n\nDefinition A.8 (Generating a ROLF neu-\nron). If a new ROLF neuron k is gener-\nated by entering a training sample p, then\n\ninitialization\nof a\nneurons\n\nck is intialized with p and σk according to\none of the aforementioned strategies (init-\nσ, minimum-σ, maximum-σ, mean-σ).\n\nThe training is complete when after re-\npeated randomly permuted pattern presen-\ntation no new neuron has been generated\nin an epoch and the positions of the neu-\nrons barely change.\n\nA.5.3 Evaluating a ROLF\n\nThe result of the training algorithm is that\nthe training set is gradually covered well\nand precisely by the ROLF neurons and\nthat a high concentration of points on a\nspot of the input space does not automati-\ncally generate more neurons. Thus, a pos-\nsibly very large point cloud is reduced to\nvery few representatives (based on the in-\nput set).\n\nThen it is very easy to define the num-\ncluster =\nconnected\nneurons\n\nber of clusters: Two neurons are (accord-\ning to the definition of the ROLF) con-\nnected when their perceptive surfaces over-\n\n178 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com A.5 Regional and online learnable fields\n\nlap (i.e. some kind of nearest neighbor-\ning is executed with the variable percep-\ntive surfaces). A cluster is a group of\nconnected neurons or a group of points of\nthe input space covered by these neurons\n(fig. A.3).\n\nOf course, the complete ROLF network\ncan be evaluated by means of other clus-\ntering methods, i.e. the neurons can be\nsearched for clusters. Particularly with\nclustering methods whose storage effort\ngrows quadratic to |P | the storage effort\ncan be reduced dramatically since gener-\nally there are considerably less ROLF neu-\nrons than original data points, but the\nneurons represent the data points quite\nwell.\n\nA.5.4 Comparison with popular\nclustering methods\n\nIt is obvious, that storing the neurons\nrather than storing the input points takes\nthe biggest part of the storage effort of the\nROLFs. This is a great advantage for huge\n\nless\nstorage\neffort!\n\npoint clouds with a lot of points.\n\nSince it is unnecessary to store the en-\ntire point cloud, our ROLF, as a neural\nclustering method, has the capability to\nlearn online, which is definitely a great ad-\nvantage. Furthermore, it can (similar to\nε nearest neighboring or k nearest neigh-\nboring) distinguish clusters from enclosed\nclusters – but due to the online presenta-recognize\n\n"cluster in\nclusters"\n\ntion of the data without a quadratically\ngrowing storage effort, which is by far the\ngreatest disadvantage of the two neighbor-\ning methods.\n\nFigure A.3: The clustering process. Top: the\ninput set, middle: the input space covered by\nROLF neurons, bottom: the input space only\ncovered by the neurons (representatives).\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 179\n\n\n\nAppendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com\n\nAdditionally, the issue of the size of the in-\ndividual clusters proportional to their dis-\ntance from each other is addressed by us-\ning variable perceptive surfaces - which is\nalso not always the case for the two men-\ntioned methods.\n\nThe ROLF compares favorably with k-\nmeans clustering, as well: Firstly, it is un-\nnecessary to previously know the number\nof clusters and, secondly, k-means cluster-\ning recognizes clusters enclosed by other\nclusters as separate clusters.\n\nA.5.5 Initializing radii, learning\nrates and multiplier is not\ntrivial\n\nCertainly, the disadvantages of the ROLF\nshall not be concealed: It is not always\neasy to select the appropriate initial value\nfor σ and ρ. The previous knowledge\nabout the data set can so to say be in-\ncluded in ρ and the initial value of σ of the\nROLF: Fine-grained data clusters should\nuse a small ρ and a small σ initial value.\nBut the smaller the ρ the smaller, the\nchance that the neurons will grow if neces-\nsary. Here again, there is no easy answer,\njust like for the learning rates ηc and ησ.\n\nFor ρ the multipliers in the lower single-\ndigit range such as 2 or 3 are very popu-\nlar. ηc and ησ successfully work with val-\nues about 0.005 to 0.1, variations during\nrun-time are also imaginable for this type\nof network. Initial values for σ generally\ndepend on the cluster and data distribu-\ntion (i.e. they often have to be tested).\nBut compared to wrong initializations –\n\nat least with the mean-σ strategy – they\nare relatively robust after some training\ntime.\n\nAs a whole, the ROLF is on a par with\nthe other clustering methods and is par-\nticularly very interesting for systems with\nlow storage capacity or huge data sets.\n\nA.5.6 Application examples\n\nA first application example could be find-\ning color clusters in RGB images. Another\nfield of application directly described in\nthe ROLF publication is the recognition of\nwords transferred into a 720-dimensional\nfeature space. Thus, we can see that\nROLFs are relatively robust against higher\ndimensions. Further applications can be\nfound in the field of analysis of attacks on\nnetwork systems and their classification.\n\nExercises\n\nExercise 18. Determine at least four\nadaptation steps for one single ROLF neu-\nron k if the four patterns stated below\nare presented one after another in the in-\ndicated order. Let the initial values for\nthe ROLF neuron be ck = (0.1, 0.1) and\nσk = 1. Furthermore, let ηc = 0.5 and\nησ = 0. Let ρ = 3.\n\nP = {(0.1, 0.1);\n= (0.9, 0.1);\n= (0.1, 0.9);\n= (0.9, 0.9)}.\n\n180 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\nAppendix B\n\nExcursus: neural networks used for\nprediction\n\nDiscussion of an application of neural networks: a look ahead into the future\nof time series.\n\nAfter discussing the different paradigms of\nneural networks it is now useful to take a\nlook at an application of neural networks\nwhich is brought up often and (as we will\nsee) is also used for fraud: The applica-\ntion of time series prediction. This ex-\ncursus is structured into the description of\ntime series and estimations about the re-\nquirements that are actually needed to pre-\ndict the values of a time series. Finally, I\nwill say something about the range of soft-\nware which should predict share prices or\nother economic characteristics by means of\nneural networks or other procedures.\n\nThis chapter should not be a detailed\ndescription but rather indicate some ap-\nproaches for time series prediction. In this\nrespect I will again try to avoid formal def-\ninitions.\n\nB.1 About time series\n\nA time series is a series of values dis-\ncretized in time. For example, daily mea-\nsured temperature values or other meteo-\nrological data of a specific site could be\nrepresented by a time series. Share price\nvalues also represent a time series. Often\nthe measurement of time series is timely\nequidistant, and in many time series the\nfuture development of their values is very\ninteresting, e.g. the daily weather fore-\ncast. time\n\nseries of\nvaluesTime series can also be values of an actu-\n\nally continuous function read in a certain\ndistance of time ∆t (fig. B.1 on the next J∆tpage).\n\nIf we want to predict a time series, we will\nlook for a neural network that maps the\nprevious series values to future develop-\nments of the time series, i.e. if we know\nlonger sections of the time series, we will\n\n181\n\n\n\nAppendix B Excursus: neural networks used for prediction dkriesel.com\n\nFigure B.1: A function x that depends on the\ntime is sampled at discrete time steps (time dis-\ncretized), this means that the result is a time\nseries. The sampled values are entered into a\nneural network (in this example an SLP) which\nshall learn to predict the future values of the time\nseries.\n\nhave enough training samples. Of course,\nthese are not examples for the future to be\npredicted but it is tried to generalize and\nto extrapolate the past by means of the\nsaid samples.\n\nBut before we begin to predict a time\nseries we have to answer some questions\nabout this time series we are dealing with\nand ensure that it fulfills some require-\nments.\n\n1. Do we have any evidence which sug-\ngests that future values depend in any\nway on the past values of the time se-\nries? Does the past of a time series\ninclude information about its future?\n\n2. Do we have enough past values of the\ntime series that can be used as train-\ning patterns?\n\n3. In case of a prediction of a continuous\nfunction: What must a useful ∆t look\nlike?\n\nNow these questions shall be explored in\ndetail.\n\nHow much information about the future\nis included in the past values of a time se-\nries? This is the most important question\nto be answered for any time series that\nshould be mapped into the future. If the\nfuture values of a time series, for instance,\ndo not depend on the past values, then a\ntime series prediction based on them will\nbe impossible.\n\nIn this chapter, we assume systems whose\nfuture values can be deduced from their\nstates – the deterministic systems. This\n\n182 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com B.2 One-step-ahead prediction\n\nleads us to the question of what a system\nstate is.\n\nA system state completely describes a sys-\ntem for a certain point of time. The future\nof a deterministic system would be clearly\ndefined by means of the complete descrip-\ntion of its current state.\n\nThe problem in the real world is that such\na state concept includes all things that in-\nfluence our system by any means.\n\nIn case of our weather forecast for a spe-\ncific site we could definitely determine\nthe temperature, the atmospheric pres-\nsure and the cloud density as the mete-\norological state of the place at a time t.\nBut the whole state would include signifi-\ncantly more information. Here, the world-\nwide phenomena that control the weather\nwould be interesting as well as small local\npheonomena such as the cooling system of\nthe local power plant.\n\nSo we shall note that the system state is de-\nsirable for prediction but not always possi-\nble to obtain. Often only fragments of the\ncurrent states can be acquired, e.g. for a\nweather forecast these fragments are the\nsaid weather data.\n\nHowever, we can partially overcome these\nweaknesses by using not only one single\nstate (the last one) for the prediction, but\nby using several past states. From this\nwe want to derive our first prediction sys-\ntem:\n\nB.2 One-step-ahead\nprediction\n\nThe first attempt to predict the next fu-\nture value of a time series out of past val-\nues is called one-step-ahead prediction\n(fig. B.2 on the following page).\n\npredict\nthe next\nvalueSuch a predictor system receives the last\n\nn observed state parts of the system as\ninput and outputs the prediction for the\nnext state (or state part). The idea of\na state space with predictable states is\ncalled state space forecasting.\n\nThe aim of the predictor is to realize a\nfunction\n\nf(xt−n+1, . . . , xt−1, xt) = x̃t+1, (B.1)\n\nwhich receives exactly n past values in or-\nder to predict the future value. Predicted\nvalues shall be headed by a tilde (e.g. x̃) Jx̃to distinguish them from the actual future\nvalues.\n\nThe most intuitive and simplest approach\nwould be to find a linear combination\n\nx̃i+1 = a0xi + a1xi−1 + . . .+ ajxi−j\n(B.2)\n\nthat approximately fulfills our condi-\ntions.\n\nSuch a construction is called digital fil-\nter. Here we use the fact that time series\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 183\n\n\n\nAppendix B Excursus: neural networks used for prediction dkriesel.com\n\nxt−3\n\n..\n\nxt−2\n\n..\n\nxt−1\n\n--\n\nxt\n\n++\n\nx̃t+1\n\npredictor\n\nKK\n\nFigure B.2: Representation of the one-step-ahead prediction. It is tried to calculate the future\nvalue from a series of past values. The predicting element (in this case a neural network) is referred\nto as predictor.\n\nusually have a lot of past values so that we\ncan set up a series of equations1:\n\nxt = a0xt−1 + . . .+ ajxt−1−(n−1)\n\nxt−1 = a0xt−2 + . . .+ ajxt−2−(n−1)\n... (B.3)\n\nxt−n = a0xt−n + . . .+ ajxt−n−(n−1)\n\nThus, n equations could be found for n un-\nknown coefficients and solve them (if pos-\nsible). Or another, better approach: we\ncould usem > n equations for n unknowns\nin such a way that the sum of the mean\nsquared errors of the already known pre-\ndiction is minimized. This is called mov-\ning average procedure.\n\nBut this linear structure corresponds to a\nsinglelayer perceptron with a linear activa-\ntion function which has been trained by\nmeans of data from the past (The experi-\nmental setup would comply with fig. B.1\non page 182). In fact, the training by\n\n1 Without going into detail, I want to remark that\nthe prediction becomes easier the more past values\nof the time series are available. I would like to\nask the reader to read up on the Nyquist-Shannon\nsampling theorem\n\nmeans of the delta rule provides results\nvery close to the analytical solution.\n\nEven if this approach often provides satis-\nfying results, we have seen that many prob-\nlems cannot be solved by using a single-\nlayer perceptron. Additional layers with\nlinear activation function are useless, as\nwell, since a multilayer perceptron with\nonly linear activation functions can be re-\nduced to a singlelayer perceptron. Such\nconsiderations lead to a non-linear ap-\nproach.\n\nThe multilayer perceptron and non-linear\nactivation functions provide a universal\nnon-linear function approximator, i.e. we\ncan use an n-|H|-1-MLP for n n inputs out\nof the past. An RBF network could also be\nused. But remember that here the number\nn has to remain low since in RBF networks\nhigh input dimensions are very complex to\nrealize. So if we want to include many past\nvalues, a multilayer perceptron will require\nconsiderably less computational effort.\n\n184 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com B.4 Additional optimization approaches for prediction\n\nB.3 Two-step-ahead\nprediction\n\nWhat approaches can we use to to see far-\nther into the future?\n\nB.3.1 Recursive two-step-ahead\nprediction\n\npredict\nfuture\nvalues In order to extend the prediction to, for in-\n\nstance, two time steps into the future, we\ncould perform two one-step-ahead predic-\ntions in a row (fig. B.3 on the following\npage), i.e. a recursive two-step-ahead\nprediction. Unfortunately, the value de-\ntermined by means of a one-step-ahead\nprediction is generally imprecise so that\nerrors can be built up, and the more pre-\ndictions are performed in a row the more\nimprecise becomes the result.\n\nB.3.2 Direct two-step-ahead\nprediction\n\nWe have already guessed that there exists\na better approach: Just like the system\ncan be trained to predict the next value,\nwe can certainly train it to predict the\n\ndirect\nprediction\nis better\n\nnext but one value. This means we di-\nrectly train, for example, a neural network\nto look two time steps ahead into the fu-\nture, which is referred to as direct two-\nstep-ahead prediction (fig. B.4 on the\nnext page). Obviously, the direct two-step-\nahead prediction is technically identical to\nthe one-step-ahead prediction. The only\ndifference is the training.\n\nB.4 Additional optimization\napproaches for prediction\n\nThe possibility to predict values far away\nin the future is not only important because\nwe try to look farther ahead into the fu-\nture. There can also be periodic time se-\nries where other approaches are hardly pos-\nsible: If a lecture begins at 9 a.m. every\nThursday, it is not very useful to know how\nmany people sat in the lecture room on\nMonday to predict the number of lecture\nparticipants. The same applies, for ex-\nample, to periodically occurring commuter\njams.\n\nB.4.1 Changing temporal\nparameters\n\nThus, it can be useful to intentionally leave\ngaps in the future values as well as in the\npast values of the time series, i.e. to in-\ntroduce the parameter ∆t which indicates\nwhich past value is used for prediction.\nTechnically speaking, we still use a one- extent\n\ninput\nperiod\n\nstep-ahead prediction only that we extend\nthe input space or train the system to pre-\ndict values lying farther away.\n\nIt is also possible to combine different ∆t:\nIn case of the traffic jam prediction for a\nMonday the values of the last few days\ncould be used as data input in addition to\nthe values of the previous Mondays. Thus,\nwe use the last values of several periods,\nin this case the values of a weekly and a\ndaily period. We could also include an an-\nnual period in the form of the beginning of\nthe holidays (for sure, everyone of us has\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 185\n\n\n\nAppendix B Excursus: neural networks used for prediction dkriesel.com\n\npredictor\n\n��\nxt−3\n\n..\n\nxt−2\n\n00\n\n..\n\nxt−1\n\n00\n\n--\n\nxt\n\n++\n\n00\n\nx̃t+1\n\nOO\n\nx̃t+2\n\npredictor\n\nJJ\n\nFigure B.3: Representation of the two-step-ahead prediction. Attempt to predict the second future\nvalue out of a past value series by means of a second predictor and the involvement of an already\npredicted value.\n\nxt−3\n\n..\n\nxt−2\n\n..\n\nxt−1\n\n--\n\nxt\n\n++\n\nx̃t+1 x̃t+2\n\npredictor\n\nEE\n\nFigure B.4: Representation of the direct two-step-ahead prediction. Here, the second time step is\npredicted directly, the first one is omitted. Technically, it does not differ from a one-step-ahead\nprediction.\n\n186 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com B.5 Remarks on the prediction of share prices\n\nalready spent a lot of time on the highway\nbecause he forgot the beginning of the hol-\nidays).\n\nB.4.2 Heterogeneous prediction\n\nAnother prediction approach would be to\npredict the future values of a single time\nseries out of several time series, if it is\nassumed that the additional time seriesuse\n\ninformation\noutside of\ntime series\n\nis related to the future of the first one\n(heterogeneous one-step-ahead pre-\ndiction, fig. B.5 on the following page).\n\nIf we want to predict two outputs of two\nrelated time series, it is certainly possible\nto perform two parallel one-step-ahead pre-\ndictions (analytically this is done very of-\nten because otherwise the equations would\nbecome very confusing); or in case of\nthe neural networks an additional output\nneuron is attached and the knowledge of\nboth time series is used for both outputs\n(fig. B.6 on the next page).\n\nYou’ll find more and more general material\non time series in [WG94].\n\nB.5 Remarks on the\nprediction of share prices\n\nMany people observe the changes of a\nshare price in the past and try to con-\nclude the future from those values in or-\nder to benefit from this knowledge. Share\nprices are discontinuous and therefore they\nare principally difficult functions. Further-\nmore, the functions can only be used for\n\ndiscrete values – often, for example, in a\ndaily rhythm (including the maximum and\nminimum values per day, if we are lucky)\nwith the daily variations certainly being\neliminated. But this makes the whole\nthing even more difficult.\n\nThere are chartists, i.e. people who look\nat many diagrams and decide by means\nof a lot of background knowledge and\ndecade-long experience whether the equi-\nties should be bought or not (and often\nthey are very successful).\n\nApart from the share prices it is very in-\nteresting to predict the exchange rates of\ncurrencies: If we exchange 100 Euros into\nDollars, the Dollars into Pounds and the\nPounds back into Euros it could be pos-\nsible that we will finally receive 110 Eu-\nros. But once found out, we would do this\nmore often and thus we would change the\nexchange rates into a state in which such\nan increasing circulation would no longer\nbe possible (otherwise we could produce\nmoney by generating, so to speak, a finan-\ncial perpetual motion machine.\n\nAt the stock exchange, successful stock\nand currency brokers raise or lower their\nthumbs – and thereby indicate whether in\ntheir opinion a share price or an exchange\nrate will increase or decrease. Mathemat-\nically speaking, they indicate the first bit\n(sign) of the first derivative of the ex-\nchange rate. In that way excellent world-\nclass brokers obtain success rates of about\n70%.\n\nIn Great Britain, the heterogeneous one-\nstep-ahead prediction was successfully\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 187\n\n\n\nAppendix B Excursus: neural networks used for prediction dkriesel.com\n\nxt−3\n\n..\n\nxt−2\n\n..\n\nxt−1\n\n--\n\nxt\n\n++\n\nx̃t+1\n\npredictor\n\nKK\n\nyt−3\n\n00\n\nyt−2\n\n00\n\nyt−1\n\n11\n\nyt\n\n33\n\nFigure B.5: Representation of the heterogeneous one-step-ahead prediction. Prediction of a time\nseries under consideration of a second one.\n\nxt−3\n\n..\n\nxt−2\n\n..\n\nxt−1\n\n--\n\nxt\n\n++\n\nx̃t+1\n\npredictor\n\nKK\n\n��\nyt−3\n\n00\n\nyt−2\n\n00\n\nyt−1\n\n11\n\nyt\n\n33\n\nỹt+1\n\nFigure B.6: Heterogeneous one-step-ahead prediction of two time series at the same time.\n\n188 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com B.5 Remarks on the prediction of share prices\n\nused to increase the accuracy of such pre-\ndictions to 76%: In addition to the time\nseries of the values indicators such as the\noil price in Rotterdam or the US national\ndebt were included.\n\nThis is just an example to show the mag-\nnitude of the accuracy of stock-exchange\nevaluations, since we are still talking only\nabout the first bit of the first derivation!\nWe still do not know how strong the ex-\npected increase or decrease will be and\nalso whether the effort will pay off: Prob-\nably, one wrong prediction could nullify\nthe profit of one hundred correct predic-\ntions.\n\nHow can neural networks be used to pre-\ndict share prices? Intuitively, we assume\nthat future share prices are a function of\nthe previous share values.\n\nBut this assumption is wrong: Share\nprices are no function of their past val-\nues, but a function of their assumed fu-\n\nshare price\nfunction of\n\nassumed\nfuture\nvalue!\n\nture value. We do not buy shares be-\ncause their values have been increased\nduring the last days, but because we be-\nlieve that they will futher increase tomor-\nrow. If, as a consequence, many people\nbuy a share, they will boost the price.\nTherefore their assumption was right – a\nself-fulfilling prophecy has been gener-\nated, a phenomenon long known in eco-\nnomics.\n\nThe same applies the other way around:\nWe sell shares because we believe that to-\nmorrow the prices will decrease. This will\nbeat down the prices the next day and gen-\nerally even more the day after the next.\n\nAgain and again some software appears\nwhich uses scientific key words such as\n”neural networks” to purport that it is ca-\npable to predict where share prices are go-\ning. Do not buy such software! In addi-\ntion to the aforementioned scientific exclu-\nsions there is one simple reason for this:\nIf these tools work – why should the man-\nufacturer sell them? Normally, useful eco-\nnomic knowledge is kept secret. If we knew\na way to definitely gain wealth by means\nof shares, we would earn our millions by\nusing this knowledge instead of selling it\nfor 30 euros, wouldn’t we?\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 189\n\n\n\n\n\nAppendix C\n\nExcursus: reinforcement learning\nWhat if there were no training samples but it would nevertheless be possible\nto evaluate how well we have learned to solve a problem? Let us examine a\n\nlearning paradigm that is situated between supervised and unsupervised\nlearning.\n\nI now want to introduce a more exotic ap-\nproach of learning – just to leave the usual\npaths. We know learning procedures in\nwhich the network is exactly told what to\ndo, i.e. we provide exemplary output val-\nues. We also know learning procedures\nlike those of the self-organizing maps, into\nwhich only input values are entered.\n\nNow we want to explore something in-\nbetween: The learning paradigm of rein-\nforcement learning – reinforcement learn-\ning according to Sutton and Barto\n[SB98].\n\nReinforcement learning in itself is no neu-\nral network but only one of the three learn-\ning paradigms already mentioned in chap-\nter 4. In some sources it is counted among\nthe supervised learning procedures since a\nfeedback is given. Due to its very rudimen-no\n\nsamples\nbut\n\nfeedback\n\ntary feedback it is reasonable to separate\nit from the supervised learning procedures\n– apart from the fact that there are no\ntraining samples at all.\n\nWhile it is generally known that pro-\ncedures such as backpropagation cannot\nwork in the human brain itself, reinforce-\nment learning is usually considered as be-\ning biologically more motivated.\n\nThe term reinforcement learning\ncomes from cognitive science and\npsychology and it describes the learning\nsystem of carrot and stick, which occurs\neverywhere in nature, i.e. learning by\nmeans of good or bad experience, reward\nand punishment. But there is no learning\naid that exactly explains what we have\nto do: We only receive a total result\nfor a process (Did we win the game of\nchess or not? And how sure was this\nvictory?), but no results for the individual\nintermediate steps.\n\nFor example, if we ride our bike with worn\ntires and at a speed of exactly 21, 5kmh\nthrough a turn over some sand with a\ngrain size of 0.1mm, on the average, then\nnobody could tell us exactly which han-\n\n191\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\ndlebar angle we have to adjust or, even\nworse, how strong the great number of\nmuscle parts in our arms or legs have to\ncontract for this. Depending on whether\nwe reach the end of the curve unharmed or\nnot, we soon have to face the learning expe-\nrience, a feedback or a reward, be it good\nor bad. Thus, the reward is very simple\n- but on the other hand it is considerably\neasier to obtain. If we now have tested dif-\nferent velocities and turning angles often\nenough and received some rewards, we will\nget a feel for what works and what does\nnot. The aim of reinforcement learning is\nto maintain exactly this feeling.\n\nAnother example for the quasi-\nimpossibility to achieve a sort of cost or\nutility function is a tennis player who\ntries to maximize his athletic success\non the long term by means of complex\nmovements and ballistic trajectories in\nthe three-dimensional space including the\nwind direction, the importance of the\ntournament, private factors and many\nmore.\n\nTo get straight to the point: Since we\nreceive only little feedback, reinforcement\nlearning often means trial and error – and\ntherefore it is very slow.\n\nC.1 System structure\n\nNow we want to briefly discuss different\nsizes and components of the system. We\nwill define them more precisely in the fol-\nlowing sections. Broadly speaking, rein-\nforcement learning represents the mutual\n\ninteraction between an agent and an envi-\nronmental system (fig. C.2).\n\nThe agent shall solve some problem. He\ncould, for instance, be an autonomous\nrobot that shall avoid obstacles. The\nagent performs some actions within the\nenvironment and in return receives a feed-\nback from the environment, which in the\nfollowing is called reward. This cycle of ac-\ntion and reward is characteristic for rein-\nforcement learning. The agent influences\nthe system, the system provides a reward\nand then changes.\n\nThe reward is a real or discrete scalar\nwhich describes, as mentioned above, how\nwell we achieve our aim, but it does not\ngive any guidance how we can achieve it.\nThe aim is always to make the sum of\nrewards as high as possible on the long\nterm.\n\nC.1.1 The gridworld\n\nAs a learning example for reinforcement\nlearning I would like to use the so-called\ngridworld. We will see that its struc-\nture is very simple and easy to figure out\nand therefore reinforcement is actually not\nnecessary. However, it is very suitable\n\nsimple\nexamplary\nworld\n\nfor representing the approach of reinforce-\nment learning. Now let us exemplary de-\nfine the individual components of the re-\ninforcement system by means of the grid-\nworld. Later, each of these components\nwill be examined more exactly.\n\nEnvironment: The gridworld (fig. C.1 on\nthe facing page) is a simple, discrete\n\n192 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.1 System structure\n\nworld in two dimensions which in the\nfollowing we want to use as environ-\nmental system.\n\nAgent: As an Agent we use a simple robot\nbeing situated in our gridworld.\n\nState space: As we can see, our gridworld\nhas 5× 7 fields with 6 fields being un-\naccessible. Therefore, our agent can\noccupy 29 positions in the grid world.\nThese positions are regarded as states\nfor the agent.\n\nAction space: The actions are still miss-\ning. We simply define that the robot\ncould move one field up or down, to\nthe right or to the left (as long as\nthere is no obstacle or the edge of our\ngridworld).\n\nTask: Our agent’s task is to leave the grid-\nworld. The exit is located on the right\nof the light-colored field.\n\nNon-determinism: The two obstacles can\nbe connected by a "door". When the\ndoor is closed (lower part of the illus-\ntration), the corresponding field is in-\naccessible. The position of the door\ncannot change during a cycle but only\nbetween the cycles.\n\nWe now have created a small world that\nwill accompany us through the following\nlearning strategies and illustrate them.\n\nC.1.2 Agent und environment\n\nOur aim is that the agent learns what hap-\npens by means of the reward. Thus, it\n\n×\n\n×\n\nFigure C.1: A graphical representation of our\ngridworld. Dark-colored cells are obstacles and\ntherefore inaccessible. The exit is located on the\nright side of the light-colored field. The symbol\n× marks the starting position of our agent. In\nthe upper part of our figure the door is open, in\nthe lower part it is closed.\n\nAgent\n\naction\n\n__\nenvironment\n\nreward / new situation\n\n??\n\nFigure C.2: The agent performs some actions\nwithin the environment and in return receives a\nreward.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 193\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\nis trained over, of and by means of a dy-\nnamic system, the environment, in order\nto reach an aim. But what does learning\nmean in this context?\n\nThe agent shall learn a mapping of sit-agent\nacts in\n\nenvironment\nuations to actions (called policy), i.e. it\nshall learn what to do in which situation\nto achieve a certain (given) aim. The aim\nis simply shown to the agent by giving an\naward for the achievement.\n\nSuch an award must not be mistaken for\nthe reward – on the agent’s way to the\nsolution it may sometimes be useful to\nreceive a smaller award or a punishment\nwhen in return the longterm result is max-\nimum (similar to the situation when an\ninvestor just sits out the downturn of the\nshare price or to a pawn sacrifice in a chess\ngame). So, if the agent is heading into\nthe right direction towards the target, it\nreceives a positive reward, and if not it re-\nceives no reward at all or even a negative\nreward (punishment). The award is, so to\nspeak, the final sum of all rewards – which\nis also called return.\n\nAfter having colloquially named all the ba-\nsic components, we want to discuss more\nprecisely which components can be used to\nmake up our abstract reinforcement learn-\ning system.\n\nIn the gridworld: In the gridworld, the\nagent is a simple robot that should find the\nexit of the gridworld. The environment\nis the gridworld itself, which is a discrete\ngridworld.\n\nDefinition C.1 (Agent). In reinforce-\nment learning the agent can be formally\n\ndescribed as a mapping of the situation\nspace S into the action space A(st). The\nmeaning of situations st will be defined\nlater and should only indicate that the ac-\ntion space depends on the current situa-\ntion.\n\nAgent: S → A(st) (C.1)\n\nDefinition C.2 (Environment). The en-\nvironment represents a stochastic map-\nping of an action A in the current situa-\ntion st to a reward rt and a new situation\nst+1.\n\nEnvironment: S ×A→ P (S × rt) (C.2)\n\nC.1.3 States, situations and actions\n\nAs already mentioned, an agent can be in\ndifferent states: In case of the gridworld,\nfor example, it can be in different positions\n(here we get a two-dimensional state vec-\ntor).\n\nFor an agent is ist not always possible to\nrealize all information about its current\nstate so that we have to introduce the term\nsituation. A situation is a state from the\nagent’s point of view, i.e. only a more or\nless precise approximation of a state.\n\nTherefore, situations generally do not al-\nlow to clearly "predict" successor situa-\ntions – even with a completely determin-\nistic system this may not be applicable.\nIf we knew all states and the transitions\nbetween them exactly (thus, the complete\nsystem), it would be possible to plan op-\ntimally and also easy to find an optimal\n\n194 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.1 System structure\n\npolicy (methods are provided, for example,\nby dynamic programming).\n\nNow we know that reinforcement learning\nis an interaction between the agent and\nthe system including actions at and sit-\nuations st. The agent cannot determine\nby itself whether the current situation is\ngood or bad: This is exactly the reason\nwhy it receives the said reward from the\nenvironment.\n\nIn the gridworld: States are positions\nwhere the agent can be situated. Sim-\nply said, the situations equal the states\nin the gridworld. Possible actions would\nbe to move towards north, south, east or\nwest.\n\nSituation and action can be vectorial, the\nreward is always a scalar (in an extreme\ncase even only a binary value) since the\naim of reinforcement learning is to get\nalong with little feedback. A complex vec-\ntorial reward would equal a real teaching\ninput.\n\nBy the way, the cost function should be\nminimized, which would not be possible,\nhowever, with a vectorial reward since we\ndo not have any intuitive order relations\nin multi-dimensional space, i.e. we do not\ndirectly know what is better or worse.\n\nDefinition C.3 (State). Within its en-\nvironment the agent is in a state. States\ncontain any information about the agent\nwithin the environmental system. Thus,\nit is theoretically possible to clearly pre-\ndict a successor state to a performed ac-\ntion within a deterministic system out of\nthis godlike state knowledge.\n\nDefinition C.4 (Situation). Situations\nst (here at time t) of a situation space JstS are the agent’s limited, approximate\n\nJSknowledge about its state. This approx-\nimation (about which the agent cannot\neven know how good it is) makes clear pre-\ndictions impossible.\n\nDefinition C.5 (Action). Actions at can Jatbe performed by the agent (whereupon it\ncould be possible that depending on the\nsituation another action space A(S) ex-\n\nJA(S)ists). They cause state transitions and\ntherefore a new situation from the agent’s\npoint of view.\n\nC.1.4 Reward and return\n\nAs in real life it is our aim to receive\nan award that is as high as possible, i.e.\nto maximize the sum of the expected re-\nwards r, called return R, on the long\nterm. For finitely many time steps1 the\nrewards can simply be added:\n\nRt = rt+1 + rt+2 + . . . (C.3)\n\n=\n∞∑\nx=1\n\nrt+x (C.4)\n\nCertainly, the return is only estimated\nhere (if we knew all rewards and therefore\nthe return completely, it would no longer\nbe necessary to learn).\n\nDefinition C.6 (Reward). A reward rt is Jrta scalar, real or discrete (even sometimes\nonly binary) reward or punishment which\n\n1 In practice, only finitely many time steps will be\npossible, even though the formulas are stated with\nan infinite sum in the first place\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 195\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\nthe environmental system returns to the\nagent as reaction to an action.\n\nDefinition C.7 (Return). The return Rt\nis the accumulation of all received rewards\n\nRtI until time t.\n\nC.1.4.1 Dealing with long periods of\ntime\n\nHowever, not every problem has an ex-\nplicit target and therefore a finite sum (e.g.\nour agent can be a robot having the task\nto drive around again and again and to\navoid obstacles). In order not to receive a\ndiverging sum in case of an infinite series\nof reward estimations a weakening factor\n0 < γ < 1 is used, which weakens the in-\n\nγI fluence of future rewards. This is not only\nuseful if there exists no target but also if\nthe target is very far away:\n\nRt = rt+1 + γ1rt+2 + γ2rt+3 + . . . (C.5)\n\n=\n∞∑\nx=1\n\nγx−1rt+x (C.6)\n\nThe farther the reward is away, the smaller\nis the influence it has in the agent’s deci-\nsions.\n\nAnother possibility to handle the return\nsum would be a limited time horizon\nτ so that only τ many following rewards\n\nτI\nrt+1, . . . , rt+τ are regarded:\n\nRt = rt+1 + . . .+ γτ−1rt+τ (C.7)\n\n=\nτ∑\nx=1\n\nγx−1rt+x (C.8)\n\nThus, we divide the timeline into\nepisodes. Usually, one of the two meth-\nods is used to limit the sum, if not both\nmethods together.\n\nAs in daily living we try to approximate\nour current situation to a desired state.\nSince it is not mandatory that only the\nnext expected reward but the expected to-\ntal sum decides what the agent will do, it\nis also possible to perform actions that, on\nshort notice, result in a negative reward\n(e.g. the pawn sacrifice in a chess game)\nbut will pay off later.\n\nC.1.5 The policy\n\nAfter having considered and formalized\nsome system components of reinforcement\nlearning the actual aim is still to be dis-\ncussed:\n\nDuring reinforcement learning the agent\nlearns a policy JΠ\n\nΠ : S → P (A),\n\nThus, it continuously adjusts a mapping\nof the situations to the probabilities P (A),\nwith which any action A is performed in\nany situation S. A policy can be defined\nas a strategy to select actions that would\nmaximize the reward in the long term.\n\nIn the gridworld: In the gridworld the pol-\nicy is the strategy according to which the\nagent tries to exit the gridworld.\n\nDefinition C.8 (Policy). The policy Π\ns a mapping of situations to probabilities\n\n196 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.1 System structure\n\nto perform every action out of the action\nspace. So it can be formalized as\n\nΠ : S → P (A). (C.9)\n\nBasically, we distinguish between two pol-\nicy paradigms: An open loop policy rep-\nresents an open control chain and creates\nout of an initial situation s0 a sequence of\nactions a0, a1, . . . with ai 6= ai(si); i > 0.\nThus, in the beginning the agent develops\na plan and consecutively executes it to the\nend without considering the intermediate\nsituations (therefore ai 6= ai(si), actions af-\nter a0 do not depend on the situations).\n\nIn the gridworld: In the gridworld, an\nopen-loop policy would provide a precise\ndirection towards the exit, such as the way\nfrom the given starting position to (in ab-\nbreviations of the directions) EEEEN.\n\nSo an open-loop policy is a sequence of\nactions without interim feedback. A se-\nquence of actions is generated out of a\nstarting situation. If the system is known\nwell and truly, such an open-loop policy\ncan be used successfully and lead to use-\nful results. But, for example, to know the\nchess game well and truly it would be nec-\nessary to try every possible move, which\nwould be very time-consuming. Thus, for\nsuch problems we have to find an alterna-\ntive to the open-loop policy, which incorpo-\nrates the current situations into the action\nplan:\n\nA closed loop policy is a closed loop, a\nfunction\n\nΠ : si → ai with ai = ai(si),\n\nin a manner of speaking. Here, the envi-\nronment influences our action or the agent\nresponds to the input of the environment,\nrespectively, as already illustrated in fig.\nC.2. A closed-loop policy, so to speak, is\na reactive plan to map current situations\nto actions to be performed.\n\nIn the gridworld: A closed-loop policy\nwould be responsive to the current posi-\ntion and choose the direction according to\nthe action. In particular, when an obsta-\ncle appears dynamically, such a policy is\nthe better choice.\n\nWhen selecting the actions to be per-\nformed, again two basic strategies can be\nexamined.\n\nC.1.5.1 Exploitation vs. exploration\n\nAs in real life, during reinforcement learn-\ning often the question arises whether the\nexisiting knowledge is only willfully ex-\nploited or new ways are also explored.\nInitially, we want to discuss the two ex-\ntremes:\n\nresearch\nor safety?A greedy policy always chooses the way\n\nof the highest reward that can be deter-\nmined in advance, i.e. the way of the high-\nest known reward. This policy represents\nthe exploitation approach and is very\npromising when the used system is already\nknown.\n\nIn contrast to the exploitation approach it\nis the aim of the exploration approach\nto explore a system as detailed as possible\nso that also such paths leading to the tar-\nget can be found which may be not very\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 197\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\npromising at first glance but are in fact\nvery successful.\n\nLet us assume that we are looking for the\nway to a restaurant, a safe policy would\nbe to always take the way we already\nknow, not matter how unoptimal and long\nit may be, and not to try to explore bet-\nter ways. Another approach would be to\nexplore shorter ways every now and then,\neven at the risk of taking a long time and\nbeing unsuccessful, and therefore finally\nhaving to take the original way and arrive\ntoo late at the restaurant.\n\nIn reality, often a combination of both\nmethods is applied: In the beginning of\nthe learning process it is researched with\na higher probability while at the end more\nexisting knowledge is exploited. Here, a\nstatic probability distribution is also pos-\nsible and often applied.\n\nIn the gridworld: For finding the way in\nthe gridworld, the restaurant example ap-\nplies equally.\n\nC.2 Learning process\n\nLet us again take a look at daily life. Ac-\ntions can lead us from one situation into\ndifferent subsituations, from each subsit-\nuation into further sub-subsituations. In\na sense, we get a situation tree where\nlinks between the nodes must be consid-\nered (often there are several ways to reach\na situation – so the tree could more accu-\nrately be referred to as a situation graph).\n\nhe leaves of such a tree are the end situ-\nations of the system. The exploration ap-\nproach would search the tree as thoroughly\nas possible and become acquainted with all\nleaves. The exploitation approach would\nunerringly go to the best known leave.\n\nAnalogous to the situation tree, we also\ncan create an action tree. Here, the re-\nwards for the actions are within the nodes.\nNow we have to adapt from daily life how\nwe learn exactly.\n\nC.2.1 Rewarding strategies\n\nInteresting and very important is the ques-\ntion for what a reward and what kind of\nreward is awarded since the design of the\nreward significantly controls system behav-\nior. As we have seen above, there gener-\nally are (again as in daily life) various ac-\ntions that can be performed in any situa-\ntion. There are different strategies to eval-\nuate the selected situations and to learn\nwhich series of actions would lead to the\ntarget. First of all, this principle should\nbe explained in the following.\n\nWe now want to indicate some extreme\ncases as design examples for the reward:\n\nA rewarding similar to the rewarding in a\nchess game is referred to as pure delayed\nreward: We only receive the reward at\nthe end of and not during the game. This\nmethod is always advantageous when we\nfinally can say whether we were succesful\nor not, but the interim steps do not allow\n\n198 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.2 Learning process\n\nan estimation of our situation. If we win,\nthen\n\nrt = 0 ∀t < τ (C.10)\n\nas well as rτ = 1. If we lose, then rτ = −1.\nWith this rewarding strategy a reward is\nonly returned by the leaves of the situation\ntree.\n\nPure negative reward: Here,\n\nrt = −1 ∀t < τ. (C.11)\n\nThis system finds the most rapid way to\nreach the target because this way is auto-\nmatically the most favorable one in respect\nof the reward. The agent receives punish-\nment for anything it does – even if it does\nnothing. As a result it is the most inex-\npensive method for the agent to reach the\ntarget fast.\n\nAnother strategy is the avoidance strat-\negy: Harmful situations are avoided.\nHere,\n\nrt ∈ {0,−1}, (C.12)\n\nMost situations do not receive any reward,\nonly a few of them receive a negative re-\nward. The agent agent will avoid getting\ntoo close to such negative situations\n\nWarning: Rewarding strategies can have\nunexpected consequences. A robot that is\ntold "have it your own way but if you touch\nan obstacle you will be punished" will sim-\nply stand still. If standing still is also pun-\nished, it will drive in small circles. Recon-\nsidering this, we will understand that this\nbehavior optimally fulfills the return of the\n\nrobot but unfortunately was not intended\nto do so.\n\nFurthermore, we can show that especially\nsmall tasks can be solved better by means\nof negative rewards while positive, more\ndifferentiated rewards are useful for large,\ncomplex tasks.\n\nFor our gridworld we want to apply the\npure negative reward strategy: The robot\nshall find the exit as fast as possible.\n\nC.2.2 The state-value function\n\nUnlike our agent we have a godlike view state\nevaluationof our gridworld so that we can swiftly de-\n\ntermine which robot starting position can\nprovide which optimal return.\n\nIn figure C.3 on the next page these opti-\nmal returns are applied per field.\n\nIn the gridworld: The state-value function\nfor our gridworld exactly represents such\na function per situation (= position) with\nthe difference being that here the function\nis unknown and has to be learned.\n\nThus, we can see that it would be more\npractical for the robot to be capable to\nevaluate the current and future situations.\nSo let us take a look at another system\ncomponent of reinforcement learning: the\nstate-value function V (s), which with\nregard to a policy Π is often called VΠ(s).\nBecause whether a situation is bad often\ndepends on the general behavior Π of the\nagent.\n\nA situation being bad under a policy that\nis searching risks and checking out limits\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 199\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\n-6 -5 -4 -3 -2\n-7 -1\n-6 -5 -4 -3 -2\n-7 -6 -5 -3\n-8 -7 -6 -4\n-9 -8 -7 -5\n-10 -9 -8 -7 -6\n\n-6 -5 -4 -3 -2\n-7 -1\n-8 -9 -10 -2\n-9 -10 -11 -3\n-10 -11 -10 -4\n-11 -10 -9 -5\n-10 -9 -8 -7 -6\n\nFigure C.3: Representation of each optimal re-\nturn per field in our gridworld by means of pure\nnegative reward awarding, at the top with an\nopen and at the bottom with a closed door.\n\nwould be, for instance, if an agent on a bi-\ncycle turns a corner and the front wheel\nbegins to slide out. And due to its dare-\ndevil policy the agent would not brake in\nthis situation. With a risk-aware policy\nthe same situations would look much bet-\nter, thus it would be evaluated higher by\na good state-value function\n\nVΠ(s) simply returns the value the current\nVΠ(s)I situation s has for the agent under policy\n\nΠ. Abstractly speaking, according to the\nabove definitions, the value of the state-\nvalue function corresponds to the return\nRt (the expected value) of a situation st.\n\nEΠ denotes the set of the expected returns\nunder Π and the current situation st.\n\nVΠ(s) = EΠ{Rt|s = st}\n\nDefinition C.9 (State-value function).\nThe state-value function VΠ(s) has the\ntask of determining the value of situations\nunder a policy, i.e. to answer the agent’s\nquestion of whether a situation s is good\nor bad or how good or bad it is. For this\npurpose it returns the expectation of the\nreturn under the situation:\n\nVΠ(s) = EΠ{Rt|s = st} (C.13)\n\nThe optimal state-value function is called\nV ∗Π(s).\n\nJV ∗Π(s)\n\nUnfortunaely, unlike us our robot does not\nhave a godlike view of its environment. It\ndoes not have a table with optimal returns\nlike the one shown above to orient itself.\nThe aim of reinforcement learning is that\nthe robot generates its state-value func-\ntion bit by bit on the basis of the returns of\nmany trials and approximates the optimal\nstate-value function V ∗ (if there is one).\n\nIn this context I want to introduce two\nterms closely related to the cycle between\nstate-value function and policy:\n\nC.2.2.1 Policy evaluation\n\nPolicy evaluation is the approach to try\na policy a few times, to provide many re-\nwards that way and to gradually accumu-\nlate a state-value function by means of\nthese rewards.\n\n200 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.2 Learning process\n\nV\n))\n\n��\n\nΠii\n\n��\nV ∗ Π∗\n\nFigure C.4: The cycle of reinforcement learning\nwhich ideally leads to optimal Π∗ and V ∗.\n\nC.2.2.2 Policy improvement\n\nPolicy improvement means to improve\na policy itself, i.e. to turn it into a new and\nbetter one. In order to improve the policy\nwe have to aim at the return finally having\na larger value than before, i.e. until we\nhave found a shorter way to the restaurant\nand have walked it successfully\n\nThe principle of reinforcement learning is\nto realize an interaction. It is tried to eval-\nuate how good a policy is in individual\nsituations. The changed state-value func-\ntion provides information about the sys-\ntem with which we again improve our pol-\nicy. These two values lift each other, which\ncan mathematically be proved, so that the\nfinal result is an optimal policy Π∗ and an\noptimal state-value function V ∗ (fig. C.4).\nThis cycle sounds simple but is very time-\nconsuming.\n\nAt first, let us regard a simple, random pol-\nicy by which our robot could slowly fulfill\nand improve its state-value function with-\nout any previous knowledge.\n\nC.2.3 Monte Carlo method\n\nThe easiest approach to accumulate a\nstate-value function is mere trial and er-\nror. Thus, we select a randomly behaving\npolicy which does not consider the accumu-\nlated state-value function for its random\ndecisions. It can be proved that at some\npoint we will find the exit of our gridworld\nby chance.\n\nInspired by random-based games of chance\nthis approach is called Monte Carlo\nmethod.\n\nIf we additionally assume a pure negative\nreward, it is obvious that we can receive\nan optimum value of −6 for our starting\nfield in the state-value function. Depend-\ning on the random way the random policy\ntakes values other (smaller) than −6 can\noccur for the starting field. Intuitively, we\nwant to memorize only the better value for\none state (i.e. one field). But here caution\nis advised: In this way, the learning proce-\ndure would work only with deterministic\nsystems. Our door, which can be open or\nclosed during a cycle, would produce oscil-\nlations for all fields and such oscillations\nwould influence their shortest way to the\ntarget.\n\nWith the Monte Carlo method we prefer\nto use the learning rule2\n\nV (st)new = V (st)alt + α(Rt − V (st)alt),\n\nin which the update of the state-value func-\ntion is obviously influenced by both the\n\n2 The learning rule is, among others, derived by\nmeans of the Bellman equation, but this deriva-\ntion is not discussed in this chapter.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 201\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\nold state value and the received return (α\nis the learning rate). Thus, the agent gets\n\nαI some kind of memory, new findings always\nchange the situation value just a little bit.\nAn exemplary learning step is shown in\nfig. C.5.\n\nIn this example, the computation of the\nstate value was applied for only one single\nstate (our initial state). It should be ob-\nvious that it is possible (and often done)\nto train the values for the states visited in-\nbetween (in case of the gridworld our ways\nto the target) at the same time. The result\nof such a calculation related to our exam-\nple is illustrated in fig. C.6 on the facing\npage.\n\nThe Monte Carlo method seems to be\nsuboptimal and usually it is significantly\nslower than the following methods of re-\ninforcement learning. But this method is\nthe only one for which it can be mathemat-\nically proved that it works and therefore\nit is very useful for theoretical considera-\ntions.\n\nDefinition C.10 (Monte Carlo learning).\nActions are randomly performed regard-\nless of the state-value function and in the\nlong term an expressive state-value func-\ntion is accumulated by means of the fol-\nlowing learning rule.\n\nV (st)new = V (st)alt + α(Rt − V (st)alt),\n\nC.2.4 Temporal difference learning\n\nMost of the learning is the result of ex-\nperiences; e.g. walking or riding a bicycle\n\n-1\n-6 -5 -4 -3 -2\n\n-1\n-14 -13 -12 -2\n\n-11 -3\n-10 -4\n-9 -5\n-8 -7 -6\n\n-10\n\nFigure C.5: Application of the Monte Carlo\nlearning rule with a learning rate of α = 0.5.\nTop: two exemplary ways the agent randomly\nselects are applied (one with an open and one\nwith a closed door). Bottom: The result of the\nlearning rule for the value of the initial state con-\nsidering both ways. Due to the fact that in the\ncourse of time many different ways are walked\ngiven a random policy, a very expressive state-\nvalue function is obtained.\n\n202 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.2 Learning process\n\n-1\n-10 -9 -8 -3 -2\n\n-11 -3\n-10 -4\n-9 -5\n-8 -7 -6\n\nFigure C.6: Extension of the learning example\nin fig. C.5 in which the returns for intermedi-\nate states are also used to accumulate the state-\nvalue function. Here, the low value on the door\nfield can be seen very well: If this state is possi-\nble, it must be very positive. If the door is closed,\nthis state is impossible.\n\nΠ\n\nEvaluation\n\n!!\nQ\n\npolicy improvement\n\naa\n\nFigure C.7: We try different actions within the\nenvironment and as a result we learn and improve\nthe policy.\n\nwithout getting injured (or not), even men-\ntal skills like mathematical problem solv-\ning benefit a lot from experience and sim-\nple trial and error. Thus, we initialize our\npolicy with arbitrary values – we try, learn\nand improve the policy due to experience\n(fig. C.7). In contrast to the Monte Carlo\nmethod we want to do this in a more di-\nrected manner.\n\nJust as we learn from experience to re-\nact on different situations in different ways\n\nthe temporal difference learning (abbre-\nviated: TD learning), does the same by\ntraining VΠ(s) (i.e. the agent learns to esti-\nmate which situations are worth a lot and\nwhich are not). Again the current situa-\ntion is identified with st, the following sit-\nuations with st+1 and so on. Thus, the\nlearning formula for the state-value func-\ntion VΠ(st) is\n\nV (st)new =V (st)\n+ α(rt+1 + γV (st+1)− V (st))︸ ︷︷ ︸\n\nchange of previous value\n\nWe can see that the change in value of the\ncurrent situation st, which is proportional\nto the learning rate α, is influenced by\n\n. the received reward rt+1,\n\n. the previous return weighted with a\nfactor γ of the following situation\nV (st+1),\n\n. the previous value of the situation\nV (st).\n\nDefinition C.11 (Temporal difference\nlearning). Unlike the Monte Carlo\nmethod, TD learning looks ahead by re-\ngarding the following situation st+1. Thus,\nthe learning rule is given by\n\nV (st)new =V (st) (C.14)\n\n+ α(rt+1 + γV (st+1)− V (st))︸ ︷︷ ︸\nchange of previous value\n\n.\n\nC.2.5 The action-value function\n\nAnalogous to the state-value function\nVΠ(s), the action-value function action\n\nevaluation\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 203\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\n0\n× +1\n-1\n\nFigure C.8: Exemplary values of an action-\nvalue function for the position ×. Moving right,\none remains on the fastest way towards the tar-\nget, moving up is still a quite fast way, moving\ndown is not a good way at all (provided that the\ndoor is open for all cases).\n\nQΠ(s, a) is another system component of\nQΠ(s, a)I reinforcement learning, which evaluates a\n\ncertain action a under a certain situation\ns and the policy Π.\n\nIn the gridworld: In the gridworld, the\naction-value function tells us how good it\nis to move from a certain field into a cer-\ntain direction (fig. C.8).\n\nDefinition C.12 (Action-value function).\nLike the state-value function, the action-\nvalue function QΠ(st, a) evaluates certain\nactions on the basis of certain situations\nunder a policy. The optimal action-value\nfunction is called Q∗Π(st, a).\n\nQ∗Π(s, a)I\n\nAs shown in fig. C.9, the actions are per-\nformed until a target situation (here re-\nferred to as sτ ) is achieved (if there exists a\ntarget situation, otherwise the actions are\nsimply performed again and again).\n\nC.2.6 Q learning\n\nThis implies QΠ(s, a) as learning fomula\nfor the action-value function, and – analo-\ngously to TD learning – its application is\ncalled Q learning:\n\nQ(st, a)new =Q(st, a)\n+ α(rt+1 + γmax\n\na\nQ(st+1, a)︸ ︷︷ ︸\n\ngreedy strategy\n\n−Q(st, a))\n\n︸ ︷︷ ︸\nchange of previous value\n\n.\n\nAgain we break down the change of the\ncurrent action value (proportional to the\nlearning rate α) under the current situa-\ntion. It is influenced by\n\n. the received reward rt+1,\n\n. the maximum action over the follow-\ning actions weighted with γ (Here, a\ngreedy strategy is applied since it can\nbe assumed that the best known ac-\ntion is selected. With TD learning,\non the other hand, we do not mind to\nalways get into the best known next\nsituation.),\n\n. the previous value of the action under\nour situation st known as Q(st, a) (re-\nmember that this is also weighted by\nmeans of α).\n\nUsually, the action-value function learns\nconsiderably faster than the state-value\nfunction. But we must not disregard that\nreinforcement learning is generally quite\nslow: The system has to find out itself\nwhat is good. But the advantage of Q\n\n204 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.3 Example applications\n\nGFED@ABCs0\na0 //\n\ndirection of actions\n\n((GFED@ABCs1\na1 //\n\nr1\nkk GFED@ABC· · · aτ−2 //\n\nr2\nkk ONMLHIJKsτ−1\n\naτ−1 //\nrτ−1\nkk GFED@ABCsτ\n\nrτ\nll\n\ndirection of reward\n\nhh\n\nFigure C.9: Actions are performed until the desired target situation is achieved. Attention should\nbe paid to numbering: Rewards are numbered beginning with 1, actions and situations beginning\nwith 0 (This has simply been adopted as a convention).\n\nlearning is: Π can be initialized arbitrar-\nily, and by means of Q learning the result\nis always Q∗.\n\nDefinition C.13 (Q learning). Q learn-\ning trains the action-value function by\nmeans of the learning rule\n\nQ(st, a)new =Q(st, a) (C.15)\n+ α(rt+1 + γmax\n\na\nQ(st+1, a) −Q(st, a)).\n\nand thus finds Q∗ in any case.\n\nC.3 Example applications\n\nC.3.1 TD gammon\n\nTD gammon is a very successful\nbackgammon game based on TD learn-\ning invented by Gerald Tesauro. The\nsituation here is the current configura-\ntion of the board. Anyone who has ever\n\nplayed backgammon knows that the situ-\nation space is huge (approx. 1020 situa-\ntions). As a result, the state-value func-\ntions cannot be computed explicitly (par-\nticularly in the late eighties when TD gam-\nmon was introduced). The selected re-\nwarding strategy was the pure delayed re-\nward, i.e. the system receives the reward\nnot before the end of the game and at the\nsame time the reward is the return. Then\nthe system was allowed to practice itself\n(initially against a backgammon program,\nthen against an entity of itself). The result\nwas that it achieved the highest ranking in\na computer-backgammon league and strik-\ningly disproved the theory that a computer\nprogramm is not capable to master a task\nbetter than its programmer.\n\nC.3.2 The car in the pit\n\nLet us take a look at a car parking on a\none-dimensional road at the bottom of a\ndeep pit without being able to get over\nthe slope on both sides straight away by\nmeans of its engine power in order to leave\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 205\n\n\n\nAppendix C Excursus: reinforcement learning dkriesel.com\n\nthe pit. Trivially, the executable actions\nhere are the possibilities to drive forwards\nand backwards. The intuitive solution we\nthink of immediately is to move backwards,\nto gain momentum at the opposite slope\nand oscillate in this way several times to\ndash out of the pit.\n\nThe actions of a reinforcement learning\nsystem would be "full throttle forward",\n"full reverse" and "doing nothing".\n\nHere, "everything costs" would be a good\nchoice for awarding the reward so that the\nsystem learns fast how to leave the pit and\nrealizes that our problem cannot be solved\nby means of mere forward directed engine\npower. So the system will slowly build up\nthe movement.\n\nThe policy can no longer be stored as a\ntable since the state space is hard to dis-\ncretize. As policy a function has to be\ngenerated.\n\nC.3.3 The pole balancer\n\nThe pole balancer was developed by\nBarto, Sutton and Anderson.\n\nLet be given a situation including a vehicle\nthat is capable to move either to the right\nat full throttle or to the left at full throt-\ntle (bang bang control). Only these two\nactions can be performed, standing still\nis impossible. On the top of this car is\nhinged an upright pole that could tip over\nto both sides. The pole is built in such a\nway that it always tips over to one side so\nit never stands still (let us assume that the\npole is rounded at the lower end).\n\nThe angle of the pole relative to the verti-\ncal line is referred to as α. Furthermore,\nthe vehicle always has a fixed position x an\nour one-dimensional world and a velocity\nof ẋ. Our one-dimensional world is lim-\nited, i.e. there are maximum values and\nminimum values x can adopt.\n\nThe aim of our system is to learn to steer\nthe car in such a way that it can balance\nthe pole, to prevent the pole from tipping\nover. This is achieved best by an avoid-\nance strategy: As long as the pole is bal-\nanced the reward is 0. If the pole tips over,\nthe reward is -1.\n\nInterestingly, the system is soon capable\nto keep the pole balanced by tilting it suf-\nficiently fast and with small movements.\nAt this the system mostly is in the cen-\nter of the space since this is farthest from\nthe walls which it understands as negative\n(if it touches the wall, the pole will tip\nover).\n\nC.3.3.1 Swinging up an inverted\npendulum\n\nMore difficult for the system is the fol-\nlowing initial situation: the pole initially\nhangs down, has to be swung up over the\nvehicle and finally has to be stabilized. In\nthe literature this task is called swing up\nan inverted pendulum.\n\n206 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com C.4 Reinforcement learning in connection with neural networks\n\nC.4 Reinforcement learning in\nconnection with neural\nnetworks\n\nFinally, the reader would like to ask why a\ntext on "neural networks" includes a chap-\nter about reinforcement learning.\n\nThe answer is very simple. We have al-\nready been introduced to supervised and\nunsupervised learning procedures. Al-\nthough we do not always have an om-\nniscient teacher who makes unsupervised\nlearning possible, this does not mean that\nwe do not receive any feedback at all.\nThere is often something in between, some\nkind of criticism or school mark. Problems\nlike this can be solved by means of rein-\nforcement learning.\n\nBut not every problem is that easily solved\nlike our gridworld: In our backgammon ex-\nample we have approx. 1020 situations and\nthe situation tree has a large branching fac-\ntor, let alone other games. Here, the tables\nused in the gridworld can no longer be re-\nalized as state- and action-value functions.\nThus, we have to find approximators for\nthese functions.\n\nAnd which learning approximators for\nthese reinforcement learning components\ncome immediately into our mind? Exactly:\nneural networks.\n\nExercises\n\nExercise 19. A robot control system\nshall be persuaded by means of reinforce-\n\nment learning to find a strategy in order\nto exit a maze as fast as possible.\n\n. What could an appropriate state-\nvalue function look like?\n\n. How would you generate an appropri-\nate reward?\n\nAssume that the robot is capable to avoid\nobstacles and at any time knows its posi-\ntion (x, y) and orientation φ.\n\nExercise 20. Describe the function of\nthe two components ASE and ACE as\nthey have been proposed by Barto, Sut-\nton and Anderson to control the pole\nbalancer.\n\nBibliography: [BSA83].\n\nExercise 21. Indicate several "classical"\nproblems of informatics which could be\nsolved efficiently by means of reinforce-\nment learning. Please give reasons for\nyour answers.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 207\n\n\n\n\n\nBibliography\n\n[And72] James A. Anderson. A simple neural network generating an interactive\nmemory. Mathematical Biosciences, 14:197–220, 1972.\n\n[APZ93] D. Anguita, G. Parodi, and R. Zunino. Speed improvement of the back-\npropagation on current-generation workstations. In WCNN’93, Portland:\nWorld Congress on Neural Networks, July 11-15, 1993, Oregon Convention\nCenter, Portland, Oregon, volume 1. Lawrence Erlbaum, 1993.\n\n[BSA83] A. Barto, R. Sutton, and C. Anderson. Neuron-like adaptive elements\nthat can solve difficult learning control problems. IEEE Transactions on\nSystems, Man, and Cybernetics, 13(5):834–846, September 1983.\n\n[CG87] G. A. Carpenter and S. Grossberg. ART2: Self-organization of stable cate-\ngory recognition codes for analog input patterns. Applied Optics, 26:4919–\n4930, 1987.\n\n[CG88] M.A. Cohen and S. Grossberg. Absolute stability of global pattern forma-\ntion and parallel memory storage by competitive neural networks. Com-\nputer Society Press Technology Series Neural Networks, pages 70–81, 1988.\n\n[CG90] G. A. Carpenter and S. Grossberg. ART 3: Hierarchical search using\nchemical transmitters in self-organising pattern recognition architectures.\nNeural Networks, 3(2):129–152, 1990.\n\n[CH67] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE\nTransactions on Information Theory, 13(1):21–27, 1967.\n\n[CR00] N.A. Campbell and JB Reece. Biologie. Spektrum. Akademischer Verlag,\n2000.\n\n[Cyb89] G. Cybenko. Approximation by superpositions of a sigmoidal function.\nMathematics of Control, Signals, and Systems (MCSS), 2(4):303–314,\n1989.\n\n[DHS01] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern classification. Wiley New\nYork, 2001.\n\n209\n\n\n\nBibliography dkriesel.com\n\n[Elm90] Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–\n211, April 1990.\n\n[Fah88] S. E. Fahlman. An empirical sudy of learning speed in back-propagation\nnetworks. Technical Report CMU-CS-88-162, CMU, 1988.\n\n[FMI83] K. Fukushima, S. Miyake, and T. Ito. Neocognitron: A neural network\nmodel for a mechanism of visual pattern recognition. IEEE Transactions\non Systems, Man, and Cybernetics, 13(5):826–834, September/October\n1983.\n\n[Fri94] B. Fritzke. Fast learning with incremental RBF networks. Neural Process-\ning Letters, 1(1):2–5, 1994.\n\n[GKE01a] N. Goerke, F. Kintzler, and R. Eckmiller. Self organized classification of\nchaotic domains from a nonlinearattractor. In Neural Networks, 2001. Pro-\nceedings. IJCNN’01. International Joint Conference on, volume 3, 2001.\n\n[GKE01b] N. Goerke, F. Kintzler, and R. Eckmiller. Self organized partitioning of\nchaotic attractors for control. Lecture notes in computer science, pages\n851–856, 2001.\n\n[Gro76] S. Grossberg. Adaptive pattern classification and universal recoding, I:\nParallel development and coding of neural feature detectors. Biological\nCybernetics, 23:121–134, 1976.\n\n[GS06] Nils Goerke and Alexandra Scherbart. Classification using multi-soms and\nmulti-neural gas. In IJCNN, pages 3895–3902, 2006.\n\n[Heb49] Donald O. Hebb. The Organization of Behavior: A Neuropsychological\nTheory. Wiley, New York, 1949.\n\n[Hop82] John J. Hopfield. Neural networks and physical systems with emergent col-\nlective computational abilities. Proc. of the National Academy of Science,\nUSA, 79:2554–2558, 1982.\n\n[Hop84] JJ Hopfield. Neurons with graded response have collective computational\nproperties like those of two-state neurons. Proceedings of the National\nAcademy of Sciences, 81(10):3088–3092, 1984.\n\n[HT85] JJ Hopfield and DW Tank. Neural computation of decisions in optimiza-\ntion problems. Biological cybernetics, 52(3):141–152, 1985.\n\n[Jor86] M. I. Jordan. Attractor dynamics and parallelism in a connectionist se-\nquential machine. In Proceedings of the Eighth Conference of the Cognitive\nScience Society, pages 531–546. Erlbaum, 1986.\n\n210 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Bibliography\n\n[Kau90] L. Kaufman. Finding groups in data: an introduction to cluster analysis.\nIn Finding Groups in Data: An Introduction to Cluster Analysis. Wiley,\nNew York, 1990.\n\n[Koh72] T. Kohonen. Correlation matrix memories. IEEEtC, C-21:353–359, 1972.\n\n[Koh82] Teuvo Kohonen. Self-organized formation of topologically correct feature\nmaps. Biological Cybernetics, 43:59–69, 1982.\n\n[Koh89] Teuvo Kohonen. Self-Organization and Associative Memory. Springer-\nVerlag, Berlin, third edition, 1989.\n\n[Koh98] T. Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1–6, 1998.\n\n[KSJ00] E.R. Kandel, J.H. Schwartz, and T.M. Jessell. Principles of neural science.\nAppleton & Lange, 2000.\n\n[lCDS90] Y. le Cun, J. S. Denker, and S. A. Solla. Optimal brain damage. In\nD. Touretzky, editor, Advances in Neural Information Processing Systems\n2, pages 598–605. Morgan Kaufmann, 1990.\n\n[Mac67] J. MacQueen. Some methods for classification and analysis of multivariate\nobservations. In Proceedings of the Fifth Berkeley Symposium on Mathe-\nmatics, Statistics and Probability, Vol. 1, pages 281–296, 1967.\n\n[MBS93] Thomas M. Martinetz, Stanislav G. Berkovich, and Klaus J. Schulten.\n’Neural-gas’ network for vector quantization and its application to time-\nseries prediction. IEEE Trans. on Neural Networks, 4(4):558–569, 1993.\n\n[MBW+10] K.D. Micheva, B. Busse, N.C. Weiler, N. O’Rourke, and S.J. Smith. Single-\nsynapse analysis of a diverse synapse population: proteomic imaging meth-\nods and markers. Neuron, 68(4):639–653, 2010.\n\n[MP43] W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent\nin nervous activity. Bulletin of Mathematical Biology, 5(4):115–133, 1943.\n\n[MP69] M. Minsky and S. Papert. Perceptrons. MIT Press, Cambridge, Mass,\n1969.\n\n[MR86] J. L. McClelland and D. E. Rumelhart. Parallel Distributed Processing:\nExplorations in the Microstructure of Cognition, volume 2. MIT Press,\nCambridge, 1986.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 211\n\n\n\nBibliography dkriesel.com\n\n[Par87] David R. Parker. Optimal algorithms for adaptive networks: Second or-\nder back propagation, second order direct propagation, and second order\nhebbian learning. In Maureen Caudill and Charles Butler, editors, IEEE\nFirst International Conference on Neural Networks (ICNN’87), volume II,\npages II–593–II–600, San Diego, CA, June 1987. IEEE.\n\n[PG89] T. Poggio and F. Girosi. A theory of networks for approximation and\nlearning. MIT Press, Cambridge Mass., 1989.\n\n[Pin87] F. J. Pineda. Generalization of back-propagation to recurrent neural net-\nworks. Physical Review Letters, 59:2229–2232, 1987.\n\n[PM47] W. Pitts and W.S. McCulloch. How we know universals the perception of\nauditory and visual forms. Bulletin of Mathematical Biology, 9(3):127–147,\n1947.\n\n[Pre94] L. Prechelt. Proben1: A set of neural network benchmark problems and\nbenchmarking rules. Technical Report, 21:94, 1994.\n\n[RB93] M. Riedmiller and H. Braun. A direct adaptive method for faster back-\npropagation learning: The rprop algorithm. In Neural Networks, 1993.,\nIEEE International Conference on, pages 586–591. IEEE, 1993.\n\n[RD05] G. Roth and U. Dicke. Evolution of the brain and intelligence. Trends in\nCognitive Sciences, 9(5):250–257, 2005.\n\n[RHW86a] D. Rumelhart, G. Hinton, and R. Williams. Learning representations by\nback-propagating errors. Nature, 323:533–536, October 1986.\n\n[RHW86b] David E. Rumelhart, Geoffrey E. Hinton, and R. J. Williams. Learning\ninternal representations by error propagation. In D. E. Rumelhart, J. L.\nMcClelland, and the PDP research group., editors, Parallel distributed pro-\ncessing: Explorations in the microstructure of cognition, Volume 1: Foun-\ndations. MIT Press, 1986.\n\n[Rie94] M. Riedmiller. Rprop - description and implementation details. Technical\nreport, University of Karlsruhe, 1994.\n\n[Ros58] F. Rosenblatt. The perceptron: a probabilistic model for information\nstorage and organization in the brain. Psychological Review, 65:386–408,\n1958.\n\n[Ros62] F. Rosenblatt. Principles of Neurodynamics. Spartan, New York, 1962.\n\n[SB98] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction.\nMIT Press, Cambridge, MA, 1998.\n\n212 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Bibliography\n\n[SG06] A. Scherbart and N. Goerke. Unsupervised system for discovering patterns\nin time-series, 2006.\n\n[SGE05] Rolf Schatten, Nils Goerke, and Rolf Eckmiller. Regional and online learn-\nable fields. In Sameer Singh, Maneesha Singh, Chidanand Apté, and Petra\nPerner, editors, ICAPR (2), volume 3687 of Lecture Notes in Computer\nScience, pages 74–83. Springer, 2005.\n\n[Ste61] K. Steinbuch. Die lernmatrix. Kybernetik (Biological Cybernetics), 1:36–45,\n1961.\n\n[vdM73] C. von der Malsburg. Self-organizing of orientation sensitive cells in striate\ncortex. Kybernetik, 14:85–100, 1973.\n\n[Was89] P. D. Wasserman. Neural Computing Theory and Practice. New York :\nVan Nostrand Reinhold, 1989.\n\n[Wer74] P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis\nin the Behavioral Sciences. PhD thesis, Harvard University, 1974.\n\n[Wer88] P. J. Werbos. Backpropagation: Past and future. In Proceedings ICNN-88,\nSan Diego, pages 343–353, 1988.\n\n[WG94] A.S. Weigend and N.A. Gershenfeld. Time series prediction. Addison-\nWesley, 1994.\n\n[WH60] B. Widrow and M. E. Hoff. Adaptive switching circuits. In Proceedings\nWESCON, pages 96–104, 1960.\n\n[Wid89] R. Widner. Single-stage logic. AIEE Fall General Meeting, 1960. Wasser-\nman, P. Neural Computing, Theory and Practice, Van Nostrand Reinhold,\n1989.\n\n[Zel94] Andreas Zell. Simulation Neuronaler Netze. Addison-Wesley, 1994. Ger-\nman.\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 213\n\n\n\n\n\nList of Figures\n\n1.1 Robot with 8 sensors and 2 motors . . . . . . . . . . . . . . . . . . . . . 6\n1.3 Black box with eight inputs and two outputs . . . . . . . . . . . . . . . 7\n1.2 Learning samples for the example robot . . . . . . . . . . . . . . . . . . 8\n1.4 Institutions of the field of neural networks . . . . . . . . . . . . . . . . . 9\n\n2.1 Central nervous system . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.2 Brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.3 Biological neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.4 Action potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.5 Compound eye . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n\n3.1 Data processing of a neuron . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.2 Various popular activation functions . . . . . . . . . . . . . . . . . . . . 38\n3.3 Feedforward network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.4 Feedforward network with shortcuts . . . . . . . . . . . . . . . . . . . . 41\n3.5 Directly recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n3.6 Indirectly recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.7 Laterally recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.8 Completely linked network . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.10 Examples for different types of neurons . . . . . . . . . . . . . . . . . . 45\n3.9 Example network with and without bias neuron . . . . . . . . . . . . . . 46\n\n4.1 Training samples and network capacities . . . . . . . . . . . . . . . . . . 56\n4.2 Learning curve with different scalings . . . . . . . . . . . . . . . . . . . 60\n4.3 Gradient descent, 2D visualization . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Possible errors during a gradient descent . . . . . . . . . . . . . . . . . . 63\n4.5 The 2-spiral problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.6 Checkerboard problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n\n5.1 The perceptron in three different views . . . . . . . . . . . . . . . . . . . 72\n5.2 Singlelayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n5.3 Singlelayer perceptron with several output neurons . . . . . . . . . . . . 74\n5.4 AND and OR singlelayer perceptron . . . . . . . . . . . . . . . . . . . . 75\n\n215\n\n\n\nList of Figures dkriesel.com\n\n5.5 Error surface of a network with 2 connections . . . . . . . . . . . . . . . 78\n5.6 Sketch of a XOR-SLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.7 Two-dimensional linear separation . . . . . . . . . . . . . . . . . . . . . 82\n5.8 Three-dimensional linear separation . . . . . . . . . . . . . . . . . . . . 83\n5.9 The XOR network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.10 Multilayer perceptrons and output sets . . . . . . . . . . . . . . . . . . . 85\n5.11 Position of an inner neuron for derivation of backpropagation . . . . . . 87\n5.12 Illustration of the backpropagation derivation . . . . . . . . . . . . . . . 89\n5.13 Momentum term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.14 Fermi function and hyperbolic tangent . . . . . . . . . . . . . . . . . . . 102\n5.15 Functionality of 8-2-8 encoding . . . . . . . . . . . . . . . . . . . . . . . 103\n\n6.1 RBF network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.2 Distance function in the RBF network . . . . . . . . . . . . . . . . . . . 108\n6.3 Individual Gaussian bells in one- and two-dimensional space . . . . . . . 109\n6.4 Accumulating Gaussian bells in one-dimensional space . . . . . . . . . . 109\n6.5 Accumulating Gaussian bells in two-dimensional space . . . . . . . . . . 110\n6.6 Even coverage of an input space with radial basis functions . . . . . . . 116\n6.7 Uneven coverage of an input space with radial basis functions . . . . . . 117\n6.8 Random, uneven coverage of an input space with radial basis functions . 117\n\n7.1 Roessler attractor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n7.2 Jordan network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n7.3 Elman network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n7.4 Unfolding in time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n\n8.1 Hopfield network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n8.2 Binary threshold function . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n8.3 Convergence of a Hopfield network . . . . . . . . . . . . . . . . . . . . . 134\n8.4 Fermi function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n\n9.1 Examples for quantization . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n\n10.1 Example topologies of a SOM . . . . . . . . . . . . . . . . . . . . . . . . 148\n10.2 Example distances of SOM topologies . . . . . . . . . . . . . . . . . . . 151\n10.3 SOM topology functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n10.4 First example of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n10.7 Topological defect of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . 156\n10.5 Training a SOM with one-dimensional topology . . . . . . . . . . . . . . 157\n10.6 SOMs with one- and two-dimensional topologies and different input spaces158\n10.8 Resolution optimization of a SOM to certain areas . . . . . . . . . . . . 160\n\n216 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com List of Figures\n\n10.9 Shape to be classified by neural gas . . . . . . . . . . . . . . . . . . . . . 162\n\n11.1 Structure of an ART network . . . . . . . . . . . . . . . . . . . . . . . . 166\n11.2 Learning process of an ART network . . . . . . . . . . . . . . . . . . . . 168\n\nA.1 Comparing cluster analysis methods . . . . . . . . . . . . . . . . . . . . 174\nA.2 ROLF neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\nA.3 Clustering by means of a ROLF . . . . . . . . . . . . . . . . . . . . . . . 179\n\nB.1 Neural network reading time series . . . . . . . . . . . . . . . . . . . . . 182\nB.2 One-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 184\nB.3 Two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 186\nB.4 Direct two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . 186\nB.5 Heterogeneous one-step-ahead prediction . . . . . . . . . . . . . . . . . . 188\nB.6 Heterogeneous one-step-ahead prediction with two outputs . . . . . . . . 188\n\nC.1 Gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\nC.2 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\nC.3 Gridworld with optimal returns . . . . . . . . . . . . . . . . . . . . . . . 200\nC.4 Reinforcement learning cycle . . . . . . . . . . . . . . . . . . . . . . . . 201\nC.5 The Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . . 202\nC.6 Extended Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . 203\nC.7 Improving the policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\nC.8 Action-value function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\nC.9 Reinforcement learning timeline . . . . . . . . . . . . . . . . . . . . . . . 205\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 217\n\n\n\n\n\nIndex\n\n*\n100-step rule . . . . . . . . . . . . . . . . . . . . . . . . 5\n\nA\nAction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\naction potential . . . . . . . . . . . . . . . . . . . . 21\naction space . . . . . . . . . . . . . . . . . . . . . . . 195\naction-value function . . . . . . . . . . . . . . 203\nactivation . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nactivation function . . . . . . . . . . . . . . . . . 36\n\nselection of . . . . . . . . . . . . . . . . . . . . . 98\nADALINE . . see adaptive linear neuron\nadaptive linear element . . . see adaptive\n\nlinear neuron\nadaptive linear neuron . . . . . . . . . . . . . . 10\nadaptive resonance theory . . . . . 11, 165\nagent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .194\nalgorithm. . . . . . . . . . . . . . . . . . . . . . . . . . .50\namacrine cell . . . . . . . . . . . . . . . . . . . . . . . 28\napproximation. . . . . . . . . . . . . . . . . . . . .110\nART . . . . see adaptive resonance theory\nART-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\nART-2A. . . . . . . . . . . . . . . . . . . . . . . . . . .168\nART-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\nartificial intelligence . . . . . . . . . . . . . . . . 10\nassociative data storage . . . . . . . . . . . 157\n\nATP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nattractor . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nautoassociator . . . . . . . . . . . . . . . . . . . . . 131\naxon . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18, 23\n\nB\nbackpropagation . . . . . . . . . . . . . . . . . . . . 88\n\nsecond order . . . . . . . . . . . . . . . . . . . 95\nbackpropagation of error. . . . . . . . . . . .84\n\nrecurrent . . . . . . . . . . . . . . . . . . . . . . 125\nbar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nbasis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\nbias neuron. . . . . . . . . . . . . . . . . . . . . . . . .44\nbinary threshold function . . . . . . . . . . 37\nbipolar cell . . . . . . . . . . . . . . . . . . . . . . . . . 27\nblack box . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\nbrain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nbrainstem . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\nC\ncapability to learn . . . . . . . . . . . . . . . . . . . 4\ncenter\n\nof a ROLF neuron . . . . . . . . . . . . 176\nof a SOM neuron. . . . . . . . . . . . . .146\n\n219\n\n\n\nIndex dkriesel.com\n\nof an RBF neuron . . . . . . . . . . . . . 104\ndistance to the . . . . . . . . . . . . . . 107\n\ncentral nervous system . . . . . . . . . . . . . 14\ncerebellum . . . . . . . . . . . . . . . . . . . . . . . . . 15\ncerebral cortex . . . . . . . . . . . . . . . . . . . . . 14\ncerebrum . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nchange in weight. . . . . . . . . . . . . . . . . . . .64\ncluster analysis . . . . . . . . . . . . . . . . . . . . 171\nclusters . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\nCNS . . . . . . . see central nervous system\ncodebook vector . . . . . . . . . . . . . . 138, 172\ncomplete linkage. . . . . . . . . . . . . . . . . . . .39\ncompound eye . . . . . . . . . . . . . . . . . . . . . . 26\nconcentration gradient . . . . . . . . . . . . . . 19\ncone function . . . . . . . . . . . . . . . . . . . . . .150\nconnection. . . . . . . . . . . . . . . . . . . . . . . . . .34\ncontext-based search . . . . . . . . . . . . . . 157\ncontinuous . . . . . . . . . . . . . . . . . . . . . . . . 137\ncortex . . . . . . . . . . . . . . see cerebral cortex\n\nvisual . . . . . . . . . . . . . . . . . . . . . . . . . . 15\ncortical field . . . . . . . . . . . . . . . . . . . . . . . . 14\n\nassociation . . . . . . . . . . . . . . . . . . . . . 15\nprimary . . . . . . . . . . . . . . . . . . . . . . . . 15\n\ncylinder function . . . . . . . . . . . . . . . . . . 150\n\nD\nDartmouth Summer Research Project9\ndeep networks . . . . . . . . . . . . . . . . . . 93, 97\nDelta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\ndelta rule . . . . . . . . . . . . . . . . . . . . . . . . . . .79\ndendrite . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n\ntree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\ndepolarization . . . . . . . . . . . . . . . . . . . . . . 21\ndiencephalon . . . . . . . . . . . . see interbrain\ndifference vector . . . . . . . see error vector\ndigital filter . . . . . . . . . . . . . . . . . . . . . . . 183\n\ndigitization . . . . . . . . . . . . . . . . . . . . . . . . 138\ndiscrete . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\ndiscretization . . . . . . . . . see quantization\ndistance\n\nEuclidean . . . . . . . . . . . . . . . . . 56, 171\nsquared. . . . . . . . . . . . . . . . . . . .76, 171\n\ndynamical system . . . . . . . . . . . . . . . . . 119\n\nE\nearly stopping . . . . . . . . . . . . . . . . . . . . . . 59\nelectronic brain . . . . . . . . . . . . . . . . . . . . . . 9\nElman network . . . . . . . . . . . . . . . . . . . . 121\nenvironment . . . . . . . . . . . . . . . . . . . . . . .193\nepisode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\nepoch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nepsilon-nearest neighboring . . . . . . . . 173\nerror\n\nspecific . . . . . . . . . . . . . . . . . . . . . . . . . 56\ntotal . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\nerror function . . . . . . . . . . . . . . . . . . . . . . 75\nspecific . . . . . . . . . . . . . . . . . . . . . . . . . 75\n\nerror vector . . . . . . . . . . . . . . . . . . . . . . . . 53\nevolutionary algorithms . . . . . . . . . . . 125\nexploitation approach . . . . . . . . . . . . . 197\nexploration approach . . . . . . . . . . . . . . 197\nexteroceptor . . . . . . . . . . . . . . . . . . . . . . . . 24\n\nF\nfastprop . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nfault tolerance . . . . . . . . . . . . . . . . . . . . . . . 4\nfeedforward. . . . . . . . . . . . . . . . . . . . . . . . .39\nFermi function . . . . . . . . . . . . . . . . . . . . . 37\nflat spot elimination . . . . . . . . . . . . . . . . 95\n\n220 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Index\n\nfudging . . . . . . . see flat spot elimination\nfunction approximation . . . . . . . . . . . . . 98\nfunction approximator\n\nuniversal . . . . . . . . . . . . . . . . . . . . . . . 82\n\nG\nganglion cell . . . . . . . . . . . . . . . . . . . . . . . . 27\nGauss-Markov model . . . . . . . . . . . . . . 111\nGaussian bell . . . . . . . . . . . . . . . . . . . . . .149\ngeneralization . . . . . . . . . . . . . . . . . . . . 4, 49\nglial cell . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\ngradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\ngradient descent . . . . . . . . . . . . . . . . . . . . 59\n\nproblems . . . . . . . . . . . . . . . . . . . . . . . 60\ngrid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\ngridworld. . . . . . . . . . . . . . . . . . . . . . . . . .192\n\nH\nHeaviside function see binary threshold\n\nfunction\nHebbian rule . . . . . . . . . . . . . . . . . . . . . . . 64\n\ngeneralized form. . . . . . . . . . . . . . . .65\nheteroassociator . . . . . . . . . . . . . . . . . . . 132\nHinton diagram . . . . . . . . . . . . . . . . . . . . 34\nhistory of development. . . . . . . . . . . . . . .8\nHopfield networks . . . . . . . . . . . . . . . . . 127\n\ncontinuous . . . . . . . . . . . . . . . . . . . . 134\nhorizontal cell . . . . . . . . . . . . . . . . . . . . . . 28\nhyperbolic tangent . . . . . . . . . . . . . . . . . 37\nhyperpolarization . . . . . . . . . . . . . . . . . . . 21\nhypothalamus . . . . . . . . . . . . . . . . . . . . . . 15\n\nI\nindividual eye . . . . . . . . see ommatidium\ninput dimension . . . . . . . . . . . . . . . . . . . . 48\ninput patterns . . . . . . . . . . . . . . . . . . . . . . 50\ninput vector . . . . . . . . . . . . . . . . . . . . . . . . 48\ninterbrain . . . . . . . . . . . . . . . . . . . . . . . . . . 15\ninternodes . . . . . . . . . . . . . . . . . . . . . . . . . . 23\ninteroceptor . . . . . . . . . . . . . . . . . . . . . . . . 24\ninterpolation\n\nprecise . . . . . . . . . . . . . . . . . . . . . . . . 110\nion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\niris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n\nJ\nJordan network. . . . . . . . . . . . . . . . . . . .120\n\nK\nk-means clustering . . . . . . . . . . . . . . . . 172\nk-nearest neighboring. . . . . . . . . . . . . .172\n\nL\nlayer\n\nhidden . . . . . . . . . . . . . . . . . . . . . . . . . 39\ninput . . . . . . . . . . . . . . . . . . . . . . . . . . .39\noutput . . . . . . . . . . . . . . . . . . . . . . . . . 39\n\nlearnability . . . . . . . . . . . . . . . . . . . . . . . . . 97\nlearning\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 221\n\n\n\nIndex dkriesel.com\n\nbatch . . . . . . . . . . see learning, offline\noffline . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nonline . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nreinforcement . . . . . . . . . . . . . . . . . . 51\nsupervised. . . . . . . . . . . . . . . . . . . . . .51\nunsupervised . . . . . . . . . . . . . . . . . . . 50\n\nlearning rate . . . . . . . . . . . . . . . . . . . . . . . 89\nvariable . . . . . . . . . . . . . . . . . . . . . . . . 90\n\nlearning strategy . . . . . . . . . . . . . . . . . . . 39\nlearning vector quantization . . . . . . . 137\nlens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nlinear separability . . . . . . . . . . . . . . . . . . 81\nlinearer associator . . . . . . . . . . . . . . . . . . 11\nlocked-in syndrome . . . . . . . . . . . . . . . . . 16\nlogistic function . . . . see Fermi function\n\ntemperature parameter . . . . . . . . . 37\nLVQ . . see learning vector quantization\nLVQ1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\nLVQ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\nLVQ3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n\nM\nM-SOM. see self-organizing map, multi\nMark I perceptron . . . . . . . . . . . . . . . . . . 10\nMathematical Symbols\n\n(t) . . . . . . . . . . . . . . . see time concept\nA(S) . . . . . . . . . . . . . see action space\nEp . . . . . . . . . . . . . . . . see error vector\nG . . . . . . . . . . . . . . . . . . . . see topology\nN . . see self-organizing map, input\n\ndimension\nP . . . . . . . . . . . . . . . . . see training set\nQ∗Π(s, a) . see action-value function,\n\noptimal\nQΠ(s, a) . see action-value function\nRt . . . . . . . . . . . . . . . . . . . . . . see return\n\nS . . . . . . . . . . . . . . see situation space\nT . . . . . . see temperature parameter\nV ∗Π(s) . . . . . see state-value function,\n\noptimal\nVΠ(s) . . . . . see state-value function\nW . . . . . . . . . . . . . . see weight matrix\n∆wi,j . . . . . . . . see change in weight\nΠ . . . . . . . . . . . . . . . . . . . . . . . see policy\nΘ . . . . . . . . . . . . . .see threshold value\nα . . . . . . . . . . . . . . . . . . see momentum\nβ . . . . . . . . . . . . . . . . see weight decay\nδ . . . . . . . . . . . . . . . . . . . . . . . . see Delta\nη . . . . . . . . . . . . . . . . .see learning rate\nη↑ . . . . . . . . . . . . . . . . . . . . . . see Rprop\nη↓ . . . . . . . . . . . . . . . . . . . . . . see Rprop\nηmax . . . . . . . . . . . . . . . . . . . . see Rprop\nηmin . . . . . . . . . . . . . . . . . . . . see Rprop\nηi,j . . . . . . . . . . . . . . . . . . . . . see Rprop\n∇ . . . . . . . . . . . . . . see nabla operator\nρ . . . . . . . . . . . . . see radius multiplier\nErr . . . . . . . . . . . . . . . . see error, total\nErr(W ) . . . . . . . . . see error function\nErrp . . . . . . . . . . . . . see error, specific\nErrp(W )see error function, specific\nErrWD . . . . . . . . . . . see weight decay\nat . . . . . . . . . . . . . . . . . . . . . . see action\nc . . . . . . . . . . . . . . . . . . . . . . . .see center\n\nof an RBF neuron, see neuron,\nself-organizing map, center\n\nm . . . . . . . . . . . see output dimension\nn . . . . . . . . . . . . . see input dimension\np . . . . . . . . . . . . . see training pattern\nrh . . . see center of an RBF neuron,\n\ndistance to the\nrt . . . . . . . . . . . . . . . . . . . . . . see reward\nst . . . . . . . . . . . . . . . . . . . . see situation\nt . . . . . . . . . . . . . . . see teaching input\nwi,j . . . . . . . . . . . . . . . . . . . . see weight\nx . . . . . . . . . . . . . . . . . see input vector\ny . . . . . . . . . . . . . . . . see output vector\n\n222 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Index\n\nfact . . . . . . . . see activation function\nfout . . . . . . . . . . . see output function\n\nmembrane . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n-potential . . . . . . . . . . . . . . . . . . . . . . 19\n\nmemorized . . . . . . . . . . . . . . . . . . . . . . . . . 54\nmetric . . . . . . . . . . . . . . . . . . . . . . . . . . . . .171\nMexican hat function . . . . . . . . . . . . . . 150\nMLP. . . . . . . .see perceptron, multilayer\nmomentum . . . . . . . . . . . . . . . . . . . . . . . . . 94\nmomentum term. . . . . . . . . . . . . . . . . . . .94\nMonte Carlo method . . . . . . . . . . . . . . 201\nMoore-Penrose pseudo inverse . . . . . 110\nmoving average procedure . . . . . . . . . 184\nmyelin sheath . . . . . . . . . . . . . . . . . . . . . . 23\n\nN\nnabla operator. . . . . . . . . . . . . . . . . . . . . .59\nNeocognitron . . . . . . . . . . . . . . . . . . . . . . . 12\nnervous system . . . . . . . . . . . . . . . . . . . . . 13\nnetwork input . . . . . . . . . . . . . . . . . . . . . . 35\nneural gas . . . . . . . . . . . . . . . . . . . . . . . . . 159\n\ngrowing . . . . . . . . . . . . . . . . . . . . . . . 162\nmulti- . . . . . . . . . . . . . . . . . . . . . . . . . 161\n\nneural network . . . . . . . . . . . . . . . . . . . . . 34\nrecurrent . . . . . . . . . . . . . . . . . . . . . . 119\n\nneuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\naccepting . . . . . . . . . . . . . . . . . . . . . 177\nbinary. . . . . . . . . . . . . . . . . . . . . . . . . .71\ncontext. . . . . . . . . . . . . . . . . . . . . . . .120\nFermi . . . . . . . . . . . . . . . . . . . . . . . . . . 71\nidentity . . . . . . . . . . . . . . . . . . . . . . . . 71\ninformation processing . . . . . . . . . 71\ninput . . . . . . . . . . . . . . . . . . . . . . . . . . .71\nRBF . . . . . . . . . . . . . . . . . . . . . . . . . . 104\noutput . . . . . . . . . . . . . . . . . . . . . . 104\n\nROLF. . . . . . . . . . . . . . . . . . . . . . . . .176\n\nself-organizing map. . . . . . . . . . . .146\ntanh . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\nwinner . . . . . . . . . . . . . . . . . . . . . . . . 148\n\nneuron layers . . . . . . . . . . . . . . . . . see layer\nneurotransmitters . . . . . . . . . . . . . . . . . . 17\nnodes of Ranvier . . . . . . . . . . . . . . . . . . . 23\n\nO\noligodendrocytes . . . . . . . . . . . . . . . . . . . 23\nOLVQ. . . . . . . . . . . . . . . . . . . . . . . . . . . . .141\non-neuron . . . . . . . . . . . . . see bias neuron\none-step-ahead prediction . . . . . . . . . 183\n\nheterogeneous . . . . . . . . . . . . . . . . . 187\nopen loop learning. . . . . . . . . . . . . . . . .125\noptimal brain damage . . . . . . . . . . . . . . 96\norder of activation . . . . . . . . . . . . . . . . . . 45\n\nasynchronous\nfixed order . . . . . . . . . . . . . . . . . . . 47\nrandom order . . . . . . . . . . . . . . . . 46\nrandomly permuted order . . . . 46\ntopological order . . . . . . . . . . . . . 47\n\nsynchronous . . . . . . . . . . . . . . . . . . . . 46\noutput dimension . . . . . . . . . . . . . . . . . . . 48\noutput function. . . . . . . . . . . . . . . . . . . . .38\noutput vector. . . . . . . . . . . . . . . . . . . . . . .48\n\nP\nparallelism . . . . . . . . . . . . . . . . . . . . . . . . . . 5\npattern . . . . . . . . . . . see training pattern\npattern recognition . . . . . . . . . . . . 98, 131\nperceptron . . . . . . . . . . . . . . . . . . . . . . . . . 71\n\nmultilayer . . . . . . . . . . . . . . . . . . . . . . 82\nrecurrent . . . . . . . . . . . . . . . . . . . . 119\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 223\n\n\n\nIndex dkriesel.com\n\nsinglelayer . . . . . . . . . . . . . . . . . . . . . .72\nperceptron convergence theorem . . . . 73\nperceptron learning algorithm . . . . . . 73\nperiod . . . . . . . . . . . . . . . . . . . . . . . . . . . . .119\nperipheral nervous system . . . . . . . . . . 13\nPersons\n\nAnderson . . . . . . . . . . . . . . . . . . . . 206 f.\nAnderson, James A. . . . . . . . . . . . . 11\nAnguita . . . . . . . . . . . . . . . . . . . . . . . . 37\nBarto . . . . . . . . . . . . . . . . . . . 191, 206 f.\nCarpenter, Gail . . . . . . . . . . . .11, 165\nElman . . . . . . . . . . . . . . . . . . . . . . . . 120\nFukushima . . . . . . . . . . . . . . . . . . . . . 12\nGirosi . . . . . . . . . . . . . . . . . . . . . . . . . 103\nGrossberg, Stephen . . . . . . . . 11, 165\nHebb, Donald O. . . . . . . . . . . . . 9, 64\nHinton . . . . . . . . . . . . . . . . . . . . . . . . . 12\nHoff, Marcian E. . . . . . . . . . . . . . . . 10\nHopfield, John . . . . . . . . . . . 11 f., 127\nIto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nJordan . . . . . . . . . . . . . . . . . . . . . . . . 120\nKohonen, Teuvo . 11, 137, 145, 157\nLashley, Karl . . . . . . . . . . . . . . . . . . . . 9\nMacQueen, J. . . . . . . . . . . . . . . . . . 172\nMartinetz, Thomas . . . . . . . . . . . . 159\nMcCulloch, Warren . . . . . . . . . . . . 8 f.\nMinsky, Marvin . . . . . . . . . . . . . . . . 9 f.\nMiyake . . . . . . . . . . . . . . . . . . . . . . . . . 12\nNilsson, Nils. . . . . . . . . . . . . . . . . . . .10\nPapert, Seymour . . . . . . . . . . . . . . . 10\nParker, David . . . . . . . . . . . . . . . . . . 95\nPitts, Walter . . . . . . . . . . . . . . . . . . . 8 f.\nPoggio . . . . . . . . . . . . . . . . . . . . . . . . 103\nPythagoras . . . . . . . . . . . . . . . . . . . . . 56\nRiedmiller, Martin . . . . . . . . . . . . . 90\nRosenblatt, Frank . . . . . . . . . . 10, 69\nRumelhart . . . . . . . . . . . . . . . . . . . . . 12\nSteinbuch, Karl . . . . . . . . . . . . . . . . 10\nSutton . . . . . . . . . . . . . . . . . . 191, 206 f.\nTesauro, Gerald . . . . . . . . . . . . . . . 205\n\nvon der Malsburg, Christoph . . . 11\nWerbos, Paul . . . . . . . . . . . 11, 84, 96\nWidrow, Bernard . . . . . . . . . . . . . . . 10\nWightman, Charles . . . . . . . . . . . . .10\nWilliams . . . . . . . . . . . . . . . . . . . . . . . 12\nZuse, Konrad . . . . . . . . . . . . . . . . . . . . 9\n\npinhole eye . . . . . . . . . . . . . . . . . . . . . . . . . 26\nPNS . . . . see peripheral nervous system\npole balancer . . . . . . . . . . . . . . . . . . . . . . 206\npolicy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n\nclosed loop . . . . . . . . . . . . . . . . . . . . 197\nevaluation . . . . . . . . . . . . . . . . . . . . . 200\ngreedy . . . . . . . . . . . . . . . . . . . . . . . . 197\nimprovement . . . . . . . . . . . . . . . . . . 200\nopen loop . . . . . . . . . . . . . . . . . . . . . 197\n\npons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\npropagation function . . . . . . . . . . . . . . . 35\npruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\npupil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n\nQ\nQ learning . . . . . . . . . . . . . . . . . . . . . . . . 204\nquantization. . . . . . . . . . . . . . . . . . . . . . .137\nquickpropagation . . . . . . . . . . . . . . . . . . . 95\n\nR\nRBF network. . . . . . . . . . . . . . . . . . . . . .104\n\ngrowing . . . . . . . . . . . . . . . . . . . . . . . 115\nreceptive field . . . . . . . . . . . . . . . . . . . . . . 27\nreceptor cell . . . . . . . . . . . . . . . . . . . . . . . . 24\n\nphoto-. . . . . . . . . . . . . . . . . . . . . . . . . .27\nprimary . . . . . . . . . . . . . . . . . . . . . . . . 24\nsecondary . . . . . . . . . . . . . . . . . . . . . . 24\n\n224 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\ndkriesel.com Index\n\nrecurrence . . . . . . . . . . . . . . . . . . . . . 40, 119\ndirect . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nindirect . . . . . . . . . . . . . . . . . . . . . . . . 41\nlateral . . . . . . . . . . . . . . . . . . . . . . . . . .42\n\nrefractory period . . . . . . . . . . . . . . . . . . . 23\nregional and online learnable fields 175\nreinforcement learning . . . . . . . . . . . . . 191\nrepolarization . . . . . . . . . . . . . . . . . . . . . . 21\nrepresentability . . . . . . . . . . . . . . . . . . . . . 97\nresilient backpropagation . . . . . . . . . . . 90\nresonance . . . . . . . . . . . . . . . . . . . . . . . . . 166\nretina. . . . . . . . . . . . . . . . . . . . . . . . . . .27, 71\nreturn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\nreward . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\n\navoidance strategy . . . . . . . . . . . . 199\npure delayed . . . . . . . . . . . . . . . . . . 198\npure negative . . . . . . . . . . . . . . . . . 198\n\nRMS . . . . . . . . . . . . see root mean square\nROLFs . . . . . . . . . see regional and online\n\nlearnable fields\nroot mean square . . . . . . . . . . . . . . . . . . . 56\nRprop . . . see resilient backpropagation\n\nS\nsaltatory conductor . . . . . . . . . . . . . . . . . 23\nSchwann cell . . . . . . . . . . . . . . . . . . . . . . . 23\nself-fulfilling prophecy . . . . . . . . . . . . . 189\nself-organizing feature maps . . . . . . . . 11\nself-organizing map . . . . . . . . . . . . . . . . 145\n\nmulti- . . . . . . . . . . . . . . . . . . . . . . . . . 161\nsensory adaptation . . . . . . . . . . . . . . . . . 25\nsensory transduction. . . . . . . . . . . . . . . .24\nshortcut connections . . . . . . . . . . . . . . . .39\nsilhouette coefficient . . . . . . . . . . . . . . . 175\nsingle lense eye . . . . . . . . . . . . . . . . . . . . . 27\nSingle Shot Learning . . . . . . . . . . . . . . 130\n\nsituation . . . . . . . . . . . . . . . . . . . . . . . . . . 194\nsituation space . . . . . . . . . . . . . . . . . . . . 195\nsituation tree . . . . . . . . . . . . . . . . . . . . . . 198\nSLP . . . . . . . . see perceptron, singlelayer\nSnark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nSNIPE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vi\nsodium-potassium pump. . . . . . . . . . . . 20\nSOM . . . . . . . . . . see self-organizing map\nsoma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nspin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\nspinal cord . . . . . . . . . . . . . . . . . . . . . . . . . 14\nstability / plasticity dilemma . . . . . . 165\nstate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\nstate space forecasting . . . . . . . . . . . . .183\nstate-value function . . . . . . . . . . . . . . . 200\nstimulus . . . . . . . . . . . . . . . . . . . . . . . 21, 147\nstimulus-conducting apparatus. . . . . .24\nsurface, perceptive. . . . . . . . . . . . . . . . .176\nswing up an inverted pendulum. . . .206\nsymmetry breaking . . . . . . . . . . . . . . . . . 98\nsynapse\n\nchemical . . . . . . . . . . . . . . . . . . . . . . . 17\nelectrical . . . . . . . . . . . . . . . . . . . . . . . 17\n\nsynapses. . . . . . . . . . . . . . . . . . . . . . . . . . . .17\nsynaptic cleft . . . . . . . . . . . . . . . . . . . . . . . 17\n\nT\ntarget . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nTD gammon . . . . . . . . . . . . . . . . . . . . . . 205\nTD learning. . . .see temporal difference\n\nlearning\nteacher forcing . . . . . . . . . . . . . . . . . . . . 125\nteaching input . . . . . . . . . . . . . . . . . . . . . . 53\ntelencephalon . . . . . . . . . . . . see cerebrum\ntemporal difference learning . . . . . . . 202\nthalamus . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\nD. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN) 225\n\n\n\nIndex dkriesel.com\n\nthreshold potential . . . . . . . . . . . . . . . . . 21\nthreshold value . . . . . . . . . . . . . . . . . . . . . 36\ntime concept . . . . . . . . . . . . . . . . . . . . . . . 33\ntime horizon . . . . . . . . . . . . . . . . . . . . . . 196\ntime series . . . . . . . . . . . . . . . . . . . . . . . . 181\ntime series prediction . . . . . . . . . . . . . . 181\ntopological defect. . . . . . . . . . . . . . . . . .154\ntopology . . . . . . . . . . . . . . . . . . . . . . . . . . 147\ntopology function . . . . . . . . . . . . . . . . . 148\ntraining pattern . . . . . . . . . . . . . . . . . . . . 53\n\nset of . . . . . . . . . . . . . . . . . . . . . . . . . . .53\ntraining set . . . . . . . . . . . . . . . . . . . . . . . . . 50\ntransfer functionsee activation function\ntruncus cerebri . . . . . . . . . . see brainstem\ntwo-step-ahead prediction . . . . . . . . . 185\n\ndirect . . . . . . . . . . . . . . . . . . . . . . . . . 185\n\nU\nunfolding in time . . . . . . . . . . . . . . . . . . 123\n\nV\nvoronoi diagram . . . . . . . . . . . . . . . . . . . 138\n\nW\nweight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nweight matrix . . . . . . . . . . . . . . . . . . . . . . 34\n\nbottom-up . . . . . . . . . . . . . . . . . . . . 166\ntop-down. . . . . . . . . . . . . . . . . . . . . .165\n\nweight vector . . . . . . . . . . . . . . . . . . . . . . . 34\n\nweighted sum. . . . . . . . . . . . . . . . . . . . . . . 35\nWidrow-Hoff rule . . . . . . . . see delta rule\nwinner-takes-all scheme . . . . . . . . . . . . . 42\n\n226 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)\n\n\n\tA small preface\n\tI From biology to formalization – motivation, philosophy, history and realization of neural models\n\t1 Introduction, motivation and history\n\t1.1 Why neural networks?\n\t1.1.1 The 100-step rule\n\t1.1.2 Simple application examples\n\n\t1.2 History of neural networks\n\t1.2.1 The beginning\n\t1.2.2 Golden age\n\t1.2.3 Long silence and slow reconstruction\n\t1.2.4 Renaissance\n\n\tExercises\n\n\t2 Biological neural networks\n\t2.1 The vertebrate nervous system\n\t2.1.1 Peripheral and central nervous system\n\t2.1.2 Cerebrum\n\t2.1.3 Cerebellum\n\t2.1.4 Diencephalon\n\t2.1.5 Brainstem\n\n\t2.2 The neuron\n\t2.2.1 Components\n\t2.2.2 Electrochemical processes in the neuron\n\n\t2.3 Receptor cells\n\t2.3.1 Various types\n\t2.3.2 Information processing within the nervous system\n\t2.3.3 Light sensing organs\n\n\t2.4 The amount of neurons in living organisms\n\t2.5 Technical neurons as caricature of biology\n\tExercises\n\n\t3 Components of artificial neural networks (fundamental)\n\t3.1 The concept of time in neural networks\n\t3.2 Components of neural networks\n\t3.2.1 Connections\n\t3.2.2 Propagation function and network input\n\t3.2.3 Activation\n\t3.2.4 Threshold value\n\t3.2.5 Activation function\n\t3.2.6 Common activation functions\n\t3.2.7 Output function\n\t3.2.8 Learning strategy\n\n\t3.3 Network topologies\n\t3.3.1 Feedforward\n\t3.3.2 Recurrent networks\n\t3.3.3 Completely linked networks\n\n\t3.4 The bias neuron\n\t3.5 Representing neurons\n\t3.6 Orders of activation\n\t3.6.1 Synchronous activation\n\t3.6.2 Asynchronous activation\n\n\t3.7 Input and output of data\n\tExercises\n\n\t4 Fundamentals on learning and training samples (fundamental)\n\t4.1 Paradigms of learning\n\t4.1.1 Unsupervised learning\n\t4.1.2 Reinforcement learning\n\t4.1.3 Supervised learning\n\t4.1.4 Offline or online learning?\n\t4.1.5 Questions in advance\n\n\t4.2 Training patterns and teaching input\n\t4.3 Using training samples\n\t4.3.1 Division of the training set\n\t4.3.2 Order of pattern representation\n\n\t4.4 Learning curve and error measurement\n\t4.4.1 When do we stop learning?\n\n\t4.5 Gradient optimization procedures\n\t4.5.1 Problems of gradient procedures\n\n\t4.6 Exemplary problems\n\t4.6.1 Boolean functions\n\t4.6.2 The parity function\n\t4.6.3 The 2-spiral problem\n\t4.6.4 The checkerboard problem\n\t4.6.5 The identity function\n\t4.6.6 Other exemplary problems\n\n\t4.7 Hebbian rule\n\t4.7.1 Original rule\n\t4.7.2 Generalized form\n\n\tExercises\n\n\n\tII Supervised learning network paradigms\n\t5 The perceptron, backpropagation and its variants\n\t5.1 The singlelayer perceptron\n\t5.1.1 Perceptron learning algorithm and convergence theorem\n\t5.1.2 Delta rule\n\n\t5.2 Linear separability\n\t5.3 The multilayer perceptron\n\t5.4 Backpropagation of error\n\t5.4.1 Derivation\n\t5.4.2 Boiling backpropagation down to the delta rule\n\t5.4.3 Selecting a learning rate\n\n\t5.5 Resilient backpropagation\n\t5.5.1 Adaption of weights\n\t5.5.2 Dynamic learning rate adjustment\n\t5.5.3 Rprop in practice\n\n\t5.6 Further variations and extensions to backpropagation\n\t5.6.1 Momentum term\n\t5.6.2 Flat spot elimination\n\t5.6.3 Second order backpropagation\n\t5.6.4 Weight decay\n\t5.6.5 Pruning and Optimal Brain Damage\n\n\t5.7 Initial configuration of a multilayer perceptron\n\t5.7.1 Number of layers\n\t5.7.2 The number of neurons\n\t5.7.3 Selecting an activation function\n\t5.7.4 Initializing weights\n\n\t5.8 The 8-3-8 encoding problem and related problems\n\tExercises\n\n\t6 Radial basis functions\n\t6.1 Components and structure\n\t6.2 Information processing of an RBF network\n\t6.2.1 Information processing in RBF neurons\n\t6.2.2 Analytical thoughts prior to the training\n\n\t6.3 Training of RBF networks\n\t6.3.1 Centers and widths of RBF neurons\n\n\t6.4 Growing RBF networks\n\t6.4.1 Adding neurons\n\t6.4.2 Limiting the number of neurons\n\t6.4.3 Deleting neurons\n\n\t6.5 Comparing RBF networks and multilayer perceptrons\n\tExercises\n\n\t7 Recurrent perceptron-like networks (depends on chapter 5)\n\t7.1 Jordan networks\n\t7.2 Elman networks\n\t7.3 Training recurrent networks\n\t7.3.1 Unfolding in time\n\t7.3.2 Teacher forcing\n\t7.3.3 Recurrent backpropagation\n\t7.3.4 Training with evolution\n\n\n\t8 Hopfield networks\n\t8.1 Inspired by magnetism\n\t8.2 Structure and functionality\n\t8.2.1 Input and output of a Hopfield network\n\t8.2.2 Significance of weights\n\t8.2.3 Change in the state of neurons\n\n\t8.3 Generating the weight matrix\n\t8.4 Autoassociation and traditional application\n\t8.5 Heteroassociation and analogies to neural data storage\n\t8.5.1 Generating the heteroassociative matrix\n\t8.5.2 Stabilizing the heteroassociations\n\t8.5.3 Biological motivation of heterassociation\n\n\t8.6 Continuous Hopfield networks\n\tExercises\n\n\t9 Learning vector quantization\n\t9.1 About quantization\n\t9.2 Purpose of LVQ\n\t9.3 Using codebook vectors\n\t9.4 Adjusting codebook vectors\n\t9.4.1 The procedure of learning\n\n\t9.5 Connection to neural networks\n\tExercises\n\n\n\tIII Unsupervised learning network paradigms\n\t10 Self-organizing feature maps\n\t10.1 Structure\n\t10.2 Functionality and output interpretation\n\t10.3 Training\n\t10.3.1 The topology function\n\t10.3.2 Monotonically decreasing learning rate and neighborhood\n\n\t10.4 Examples\n\t10.4.1 Topological defects\n\n\t10.5 Adjustment of resolution and position-dependent learning rate\n\t10.6 Application\n\t10.6.1 Interaction with RBF networks\n\n\t10.7 Variations\n\t10.7.1 Neural gas\n\t10.7.2 Multi-SOMs\n\t10.7.3 Multi-neural gas\n\t10.7.4 Growing neural gas\n\n\tExercises\n\n\t11 Adaptive resonance theory\n\t11.1 Task and structure of an ART network\n\t11.1.1 Resonance\n\n\t11.2 Learning process\n\t11.2.1 Pattern input and top-down learning\n\t11.2.2 Resonance and bottom-up learning\n\t11.2.3 Adding an output neuron\n\n\t11.3 Extensions\n\n\n\tIV Excursi, appendices and registers\n\tA Excursus: Cluster analysis and regional and online learnable fields\n\tA.1 k-means clustering\n\tA.2 k-nearest neighboring\n\tA.3 -nearest neighboring\n\tA.4 The silhouette coefficient\n\tA.5 Regional and online learnable fields\n\tA.5.1 Structure of a ROLF\n\tA.5.2 Training a ROLF\n\tA.5.3 Evaluating a ROLF\n\tA.5.4 Comparison with popular clustering methods\n\tA.5.5 Initializing radii, learning rates and multiplier\n\tA.5.6 Application examples\n\n\tExercises\n\n\tB Excursus: neural networks used for prediction\n\tB.1 About time series\n\tB.2 One-step-ahead prediction\n\tB.3 Two-step-ahead prediction\n\tB.3.1 Recursive two-step-ahead prediction\n\tB.3.2 Direct two-step-ahead prediction\n\n\tB.4 Additional optimization approaches for prediction\n\tB.4.1 Changing temporal parameters\n\tB.4.2 Heterogeneous prediction\n\n\tB.5 Remarks on the prediction of share prices\n\n\tC Excursus: reinforcement learning\n\tC.1 System structure\n\tC.1.1 The gridworld\n\tC.1.2 Agent und environment\n\tC.1.3 States, situations and actions\n\tC.1.4 Reward and return\n\tC.1.5 The policy\n\n\tC.2 Learning process\n\tC.2.1 Rewarding strategies\n\tC.2.2 The state-value function\n\tC.2.3 Monte Carlo method\n\tC.2.4 Temporal difference learning\n\tC.2.5 The action-value function\n\tC.2.6 Q learning\n\n\tC.3 Example applications\n\tC.3.1 TD gammon\n\tC.3.2 The car in the pit\n\tC.3.3 The pole balancer\n\n\tC.4 Reinforcement learning in connection with neural networks\n\tExercises\n\n\tBibliography\n\tList of Figures\n\tIndex\n\n\n', 'metadata_pdf:docinfo:creator': 'DavidKriesel', 'metadata_resourceName': "b'A Brief Introduction to Neural Networks (neuronalenetze-en-zeta2-2col-dkrieselcom).pdf'", 'metadata_pdf:docinfo:modified': '2012-05-16T07:01:55Z'}}, {'_index': 'mongo_index', '_id': 'E_9p5IUB9nynXRNhVMxa', '_score': 0.7813254, '_ignored': ['content.keyword', 'log_entry.keyword'], '_source': {'log_entry': '{"_id"=>BSON::ObjectId(\'63cffa2978b994746c729cea\'), "content"=>"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA block-sorting lossless data compression algorithm\\n\\n\\nMay 10, 1994\\n\\nSRC\\nResearch\\nReport 124\\n\\nA Block-sorting Lossless\\nData Compression Algorithm\\n\\nM. Burrows and D.J. Wheeler\\n\\nd i g i t a l\\nSystems Research Center\\n130 Lytton Avenue\\nPalo Alto, California 94301\\n\\n\\n\\nSystems Research Center\\n\\nThe charter of SRC is to advance both the state of knowledge and the state of the\\nart in computer systems. From our establishment in 1984, we have performed basic\\nand applied research to support Digital’s business objectives. Our current work\\nincludes exploring distributed personal computing on multiple platforms, network-\\ning, programming technology, system modelling and management techniques, and\\nselected applications.\\n\\nOur strategy is to test the technical and practical value of our ideas by building\\nhardware and software prototypes and using them as daily tools. Interesting systems\\nare too complex to be evaluated solely in the abstract; extended use allows us to\\ninvestigate their properties in depth. This experience is useful in the short term in\\nrefining our designs, and invaluable in the long term in advancing our knowledge.\\nMost of the major advances in information systems have come through this strategy,\\nincluding personal computing, distributed systems, and the Internet.\\n\\nWe also perform complementary work of a more mathematical flavor. Some of\\nit is in established fields of theoretical computer science, such as the analysis\\nof algorithms, computational geometry, and logics of programming. Other work\\nexplores new ground motivated by problems that arise in our systems research.\\n\\nWe have a strong commitment to communicating our results; exposing and testing\\nour ideas in the research and development communities leads to improved un-\\nderstanding. Our research report series supplements publication in professional\\njournals and conferences. We seek users for our prototype systems among those\\nwith whom we have common interests, and we encourage collaboration with uni-\\nversity researchers.\\n\\nRobert W. Taylor, Director\\n\\n\\n\\nA Block-sorting Lossless\\nData Compression Algorithm\\n\\nM. Burrows and D.J. Wheeler\\n\\nMay 10, 1994\\n\\n\\n\\nDavid Wheeler is a Professor of Computer Science at the University of Cambridge,\\nU.K. His electronic mail address is: djw3@cl.cam.ac.uk\\n\\nc\\rDigital Equipment Corporation 1994.\\n\\nThis work may not be copied or reproduced in whole or in part for any commercial\\npurpose. Permission to copy in whole or in part without payment of fee is granted\\nfor nonprofit educational and research purposes provided that all such whole or\\npartial copies include the following: a notice that such copying is by permission\\nof the Systems Research Center of Digital Equipment Corporation in Palo Alto,\\nCalifornia; an acknowledgment of the authors and individual contributors to the\\nwork; and all applicable portions of the copyright notice. Copying, reproducing,\\nor republishing for any other purpose shall require a license with payment of fee to\\nthe Systems Research Center. All rights reserved.\\n\\n\\n\\nAuthors’ abstract\\n\\nWe describe a block-sorting, lossless data compression algorithm, and our imple-\\nmentation of that algorithm. We compare the performance of our implementation\\nwith widely available data compressors running on the same hardware.\\n\\nThe algorithm works by applying a reversible transformation to a block of input\\ntext. The transformation does not itself compress the data, but reorders it to make\\nit easy to compress with simple algorithms such as move-to-front coding.\\n\\nOur algorithm achieves speed comparable to algorithms based on the techniques\\nof Lempel and Ziv, but obtains compression close to the best statistical modelling\\ntechniques. The size of the input block must be large (a few kilobytes) to achieve\\ngood compression.\\n\\n\\n\\nContents\\n\\n1 Introduction 1\\n\\n2 The reversible transformation 2\\n\\n3 Why the transformed string compresses well 5\\n\\n4 An efficient implementation 8\\n4.1 Compression : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8\\n4.2 Decompression : : : : : : : : : : : : : : : : : : : : : : : : : : : 12\\n\\n5 Algorithm variants 13\\n\\n6 Performance of implementation 15\\n\\n7 Conclusions 16\\n\\n\\n\\n1 Introduction\\n\\nThe most widely used data compression algorithms are based on the sequential\\ndata compressors of Lempel and Ziv [1, 2]. Statistical modelling techniques may\\nproduce superior compression [3], but are significantly slower.\\n\\nIn this paper, we present a technique that achieves compression within a percent\\nor so of that achieved by statistical modelling techniques, but at speeds comparable\\nto those of algorithms based on Lempel and Ziv’s.\\n\\nOur algorithm does not process its input sequentially, but instead processes a\\nblock of text as a single unit. The idea is to apply a reversible transformation to a\\nblock of text to form a new block that contains the same characters, but is easier\\nto compress by simple compression algorithms. The transformation tends to group\\ncharacters together so that the probability of finding a character close to another\\ninstance of the same character is increased substantially. Text of this kind can\\neasily be compressed with fast locally-adaptive algorithms, such as move-to-front\\ncoding [4] in combination with Huffman or arithmetic coding.\\n\\nBriefly, our algorithm transforms a string S of N characters by forming the N\\nrotations (cyclic shifts) of S, sorting them lexicographically, and extracting the last\\ncharacter of each of the rotations. A string L is formed from these characters, where\\nthe ith character of L is the last character of the ith sorted rotation. In addition to\\nL, the algorithm computes the index I of the original string S in the sorted list of\\nrotations. Surprisingly, there is an efficient algorithm to compute the original string\\nS given only L and I .\\n\\nThe sorting operation brings together rotations with the same initial characters.\\nSince the initial characters of the rotations are adjacent to the final characters,\\nconsecutive characters in L are adjacent to similar strings in S. If the context of a\\ncharacter is a good predictor for the character, L will be easy to compress with a\\nsimple locally-adaptive compression algorithm.\\n\\nIn the following sections, we describe the transformation in more detail, and\\nshow that it can be inverted. We explain more carefully why this transformation\\ntends to group characters to allow a simple compression algorithm to work more\\neffectively. We then describe efficient techniques for implementing the transfor-\\nmation and its inverse, allowing this algorithm to be competitive in speed with\\nLempel-Ziv-based algorithms, but achieving better compression. Finally, we give\\nthe performance of our implementation of this algorithm, and compare it with\\nwell-known compression programmes.\\n\\nThe algorithm described here was discovered by one of the authors (Wheeler) in\\n1983 while he was working at AT&T Bell Laboratories, though it has not previously\\nbeen published.\\n\\n1\\n\\n\\n\\n2 The reversible transformation\\n\\nThis section describes two sub-algorithms. Algorithm C performs the reversible\\ntransformation that we apply to a block of text before compressing it, and Algorithm\\nD performs the inverse operation. A later section suggests a method for compressing\\nthe transformed block of text.\\n\\nIn the description below, we treat strings as vectors whose elements are char-\\nacters.\\n\\nAlgorithm C: Compression transformation\\n\\nThis algorithm takes as input a string S of N characters S[0]; : : : ; S[N�1] selected\\nfrom an ordered alphabet X of characters. To illustrate the technique, we also give\\na running example, using the string S D ‘abraca’, N D 6, and the alphabet\\nX D f0a0;0b0;0c0;0r0g.\\n\\nC1. [sort rotations]\\nForm a conceptual N ð N matrix M whose elements are characters, and\\nwhose rows are the rotations (cyclic shifts) of S, sorted in lexicographical\\norder. At least one of the rows of M contains the original string S. Let I be\\nthe index of the first such row, numbering from zero.\\n\\nIn our example, the index I D 1 and the matrix M is\\n\\nrow\\n0 aabrac\\n\\n1 abraca\\n\\n2 acaabr\\n\\n3 bracaa\\n\\n4 caabra\\n\\n5 racaab\\n\\nC2. [find last characters of rotations]\\nLet the string L be the last column of M , with characters L[0]; : : : ; L[N�1]\\n(equal to M[0; N � 1]; : : : ;M[N � 1; N � 1]). The output of the transfor-\\nmation is the pair .L; I/.\\n\\nIn our example, L D ‘caraab’ and I D 1 (from step C1).\\n\\n2\\n\\n\\n\\nAlgorithm D: Decompression transformation\\n\\nWe use the example and notation introduced in Algorithm C. Algorithm D uses the\\noutput .L; I/ of Algorithm C to reconstruct its input, the string S of length N .\\n\\nD1. [find first characters of rotations]\\nThis step calculates the first column F of the matrix M of Algorithm C. This\\nis done by sorting the characters of L to form F. We observe that any column\\nof the matrix M is a permutation of the original string S. Thus, L and F are\\nboth permutations of S, and therefore of one another. Furthermore, because\\nthe rows of M are sorted, and F is the first column of M , the characters in F\\nare also sorted.\\n\\nIn our example, F D ‘aaabcr’.\\n\\nD2. [build list of predecessor characters]\\nTo assist our explanation, we describe this step in terms of the contents of\\nthe matrix M . The reader should remember that the complete matrix is not\\navailable to the decompressor; only the strings F, L, and the index I (from\\nthe input) are needed by this step.\\n\\nConsider the rows of the matrix M that start with some given character ch.\\nAlgorithm C ensured that the rows of matrix M are sorted lexicographically,\\nso the rows that start with ch are ordered lexicographically.\\n\\nWe define the matrix M 0 formed by rotating each row of M one character to\\nthe right, so for each i D 0; : : : ; N � 1, and each j D 0; : : : ; N � 1,\\n\\nM 0[i; j ] D M[i; . j � 1/ mod N ]\\n\\nIn our example, M and M 0 are:\\n\\nrow M M 0\\n\\n0 aabrac caabra\\n\\n1 abraca aabrac\\n\\n2 acaabr racaab\\n\\n3 bracaa abraca\\n\\n4 caabra acaabr\\n\\n5 racaab bracaa\\n\\nLike M , each row of M 0 is a rotation of S, and for each row of M there is\\na corresponding row in M 0. We constructed M 0 from M so that the rows\\n\\n3\\n\\n\\n\\nof M 0 are sorted lexicographically starting with their second character. So,\\nif we consider only those rows in M 0 that start with a character ch, they\\nmust appear in lexicographical order relative to one another; they have been\\nsorted lexicographically starting with their second characters, and their first\\ncharacters are all the same and so do not affect the sort order. Therefore, for\\nany given character ch, the rows in M that begin with ch appear in the same\\norder as the rows in M 0 that begin with ch.\\n\\nIn our example, this is demonstrated by the rows that begin with ‘a’. The rows\\n‘aabrac’, ‘abraca’, and ‘acaabr’ are rows 0, 1, 2 in M and correspond to\\nrows 1, 3, 4 in M 0.\\n\\nUsing F and L, the first columns of M and M 0 respectively, we calculate\\na vector T that indicates the correspondence between the rows of the two\\nmatrices, in the sense that for each j D 0; : : : ; N�1, row j of M 0 corresponds\\nto row T [ j ] of M .\\n\\nIf L[ j ] is the kth instance of ch in L, then T [ j ] D i where F[i] is the kth\\ninstance of ch in F. Note that T represents a one-to-one correspondence\\nbetween elements of F and elements of L, and F[T [ j ]] D L[ j ].\\n\\nIn our example, T is: (4 0 5 1 2 3).\\n\\nD3. [form output S]\\nNow, for each i D 0; : : : ; N � 1, the characters L[i] and F[i] are the last\\nand first characters of the row i of M . Since each row is a rotation of\\nS, the character L[i] cyclicly precedes the character F[i] in S. From the\\nconstruction of T , we have F[T [ j ]] D L[ j ]. Substituting i D T [ j ], we have\\nL[T [ j ]] cyclicly precedes L[ j ] in S.\\n\\nThe index I is defined by Algorithm C such that row I of M is S. Thus, the\\nlast character of S is L[I ]. We use the vector T to give the predecessors of\\neach character:\\n\\nfor each i D 0; : : : ; N � 1: S[N � 1� i] D L[T i [I ]].\\n\\nwhere T 0[x ] D x , and T iC1[x ] D T [T i [x ]]. This yields S, the original input\\nto the compressor.\\n\\nIn our example, S D ‘abraca’.\\n\\nWe could have defined T so that the string S would be generated from front to back,\\nrather than the other way around. The description above matches the pseudo-code\\ngiven in Section 4.2.\\n\\n4\\n\\n\\n\\nThe sequence T i [I ] for i D 0; : : : ; N � 1 is not necessarily a permutation of\\nthe numbers 0; : : : ; N � 1. If the original string S is of the form Z p for some\\nsubstring Z and some p > 1, then the sequence T i [I ] for i D 0; : : : ; N � 1 will\\nalso be of the form Z 0p for some subsequence Z 0. That is, the repetitions in S will\\nbe generated by visiting the same elements of T repeatedly. For example, if S D\\n‘cancan’, Z D ‘can’ and p D 2, the sequence T i [I ] for i D 0; : : : ; N � 1 will be\\n[2; 4; 0; 2; 4; 0].\\n\\n3 Why the transformed string compresses well\\n\\nAlgorithm C sorts the rotations of an input string S, and generates the string L\\nconsisting of the last character of each rotation.\\n\\nTo see why this might lead to effective compression, consider the effect on a\\nsingle letter in a common word in a block of English text. We will use the example\\nof the letter ‘t’ in the word ‘the’, and assume an input string containing many\\ninstances of ‘the’.\\n\\nWhen the list of rotations of the input is sorted, all the rotations starting with\\n‘he ’ will sort together; a large proportion of them are likely to end in ‘t’. One\\nregion of the string L will therefore contain a disproportionately large number of ‘t’\\ncharacters, intermingled with other characters that can proceed ‘he ’ in English,\\nsuch as space, ‘s’, ‘T’, and ‘S’.\\n\\nThe same argument can be applied to all characters in all words, so any localized\\nregion of the string L is likely to contain a large number of a few distinct characters.\\nThe overall effect is that the probability that given character ch will occur at a given\\npoint in L is very high if ch occurs near that point in L, and is low otherwise.\\nThis property is exactly the one needed for effective compression by a move-to-\\nfront coder [4], which encodes an instance of character ch by the count of distinct\\ncharacters seen since the next previous occurrence of ch. When applied to the\\nstring L, the output of a move-to-front coder will be dominated by low numbers,\\nwhich can be efficiently encoded with a Huffman or arithmetic coder.\\n\\nFigure 1 shows an example of the algorithm at work. Each line is the first few\\ncharacters and final character of a rotation of a version of this document. Note the\\ngrouping of similar characters in the column of final characters.\\n\\nFor completeness, we now give details of one possible way to encode the output\\nof Algorithm C, and the corresponding inverse operation. A complete compression\\nalgorithm is created by combining these encoding and decoding operations with\\nAlgorithms C and D.\\n\\n5\\n\\n\\n\\nfinal\\nchar sorted rotations\\n(L)\\na n to decompress. It achieves compression\\no n to perform only comparisons to a depth\\no n transformation} This section describes\\no n transformation} We use the example and\\no n treats the right-hand side as the most\\na n tree for each 16 kbyte input block, enc\\na n tree in the output stream, then encodes\\ni n turn, set $L[i]$ to be the\\ni n turn, set $R[i]$ to the\\no n unusual data. Like the algorithm of Man\\na n use a single set of probabilities table\\ne n using the positions of the suffixes in\\ni n value at a given point in the vector $R\\ne n we present modifications that improve t\\ne n when the block size is quite large. Ho\\ni n which codes that have not been seen in\\ni n with $ch$ appear in the {\\\\em same order\\ni n with $ch$. In our exam\\no n with Huffman or arithmetic coding. Bri\\no n with figures given by Bell˜\\\\cite{bell}.\\n\\nFigure 1: Example of sorted rotations. Twenty consecutive rotations from the\\nsorted list of rotations of a version of this paper are shown, together with the final\\ncharacter of each rotation.\\n\\n6\\n\\n\\n\\nAlgorithm M: Move-to-front coding\\n\\nThis algorithm encodes the output .L; I/ of Algorithm C, where L is a string of\\nlength N and I is an index. It encodes L using a move-to-front algorithm and a\\nHuffman or arithmetic coder.\\n\\nThe running example of Algorithm C is continued here.\\n\\nM1. [move-to-front coding]\\nThis step encodes each of the characters in L by applying the move-to-\\nfront technique to the individual characters. We define a vector of integers\\nR[0]; : : : ; R[N�1], which are the codes for the characters L[0]; : : : ; L[N�\\n1].\\n\\nInitialize a list Y of characters to contain each character in the alphabet X\\nexactly once.\\n\\nFor each i D 0; : : : ; N � 1 in turn, set R[i] to the number of characters\\npreceding character L[i] in the list Y , then move character L[i] to the front\\nof Y .\\n\\nTaking Y D [0a0;0b0;0c0;0r0] initially, and L D‘caraab’, we compute the\\nvector R: (2 1 3 1 0 3).\\n\\nM2. [encode]\\nApply Huffman or arithmetic coding to the elements of R, treating each\\nelement as a separate token to be coded. Any coding technique can be\\napplied as long as the decompressor can perform the inverse operation. Call\\nthe output of this coding process OUT. The output of Algorithm C is the pair\\n.OUT; I/ where I is the value computed in step C1.\\n\\nAlgorithm W: Move-to-front decoding\\n\\nThis algorithm is the inverse of Algorithm M. It computes the pair .L; I/ from the\\npair .OUT; I/.\\n\\nWe assume that the initial value of the list Y used in step M1 is available to the\\ndecompressor, and that the coding scheme used in step M2 has an inverse operation.\\n\\nW1. [decode]\\nDecode the coded stream OUT using the inverse of the coding scheme used\\nin step M2. The result is a vector R of N integers.\\n\\nIn our example, R is: (2 1 3 1 0 3).\\n\\n7\\n\\n\\n\\nW2. [inverse move-to-front coding]\\nThe goal of this step is to calculate a string L of N characters, given the\\nmove-to-front codes R[0]; : : : ; R[N � 1].\\n\\nInitialize a list Y of characters to contain the characters of the alphabet X in\\nthe same order as in step M1.\\n\\nFor each i D 0; : : : ; N � 1 in turn, set L[i] to be the character at position\\nR[i] in list Y (numbering from 0), then move that character to the front of Y .\\nThe resulting string L is the last column of matrix M of Algorithm C. The\\noutput of this algorithm is the pair .L; I/, which is the input to Algorithm D.\\n\\nTaking Y D [0a0;0b0;0c0;0r0] initially (as in Algorithm M), we compute the\\nstring L D ‘caraab’.\\n\\n4 An efficient implementation\\n\\nEfficient implementations of move-to-front, Huffman, and arithmetic coding are\\nwell known. Here we concentrate on the unusual steps in Algorithms C and D.\\n\\n4.1 Compression\\n\\nThe most important factor in compression speed is the time taken to sort the rotations\\nof the input block. A simple application of quicksort requires little additional space\\n(one word per character), and works fairly well on most input data. However, its\\nworst-case performance is poor.\\n\\nA faster way to implement Algorithm C is to reduce the problem of sorting the\\nrotations to the problem of sorting the suffixes of a related string.\\n\\nTo compress a string S, first form the string S0, which is the concatenation of S\\nwith EOF, a new character that does not appear in S. Now apply Algorithm C to S0.\\nBecause EOF is different from all other characters in S0, the suffixes of S0 sort in the\\nsame order as the rotations of S0. Hence, to sort the rotations, it suffices to sort the\\nsuffixes of S0. This can be done in linear time and space by building a suffix tree,\\nwhich then can be walked in lexicographical order to recover the sorted suffixes.\\nWe used McCreight’s suffix tree construction algorithm [5] in an implementation of\\nAlgorithm C. Its performance is within 40% of the fastest technique we have found\\nwhen operating on text files. Unfortunately, suffix tree algorithms need space for\\nmore than four machine words per input character.\\n\\nManber and Myers give a simple algorithm for sorting the suffixes of a string\\nin O.N log N/ time [6]. The algorithm they describe requires only two words per\\n\\n8\\n\\n\\n\\ninput character. The algorithm works by sorting suffixes on their first i characters,\\nthen using the positions of the suffixes in the sorted array as the sort key for another\\nsort on the first 2i characters. Unfortunately, the performance of their algorithm is\\nsignificantly worse than the suffix tree approach.\\n\\nThe fastest scheme we have tried so far uses a variant of quicksort to generate\\nthe sorted list of suffixes. Its performance is significantly better than the suffix tree\\nwhen operating on text, and it uses significantly less space. Unlike the suffix tree\\nhowever, its performance can degrade considerably when operating on some types\\nof data. Like the algorithm of Manber and Myers, our algorithm uses only two\\nwords per input character.\\n\\nAlgorithm Q: Fast quicksorting on suffixes\\n\\nThis algorithm sorts the suffixes of the string S, which contains N characters\\nS[0; : : : ; N�1]. The algorithm works by applying a modified version of quicksort\\nto the suffixes of S.\\n\\nFirst we present a simple version of the algorithm. Then we present modifica-\\ntions that improve the speed and make bad performance less likely.\\n\\nQ1. [form extended string]\\nLet k be the number of characters that fit in a machine word.\\n\\nForm the string S0 from S by appending k additional EOF characters to S,\\nwhere EOF does not appear in S.\\n\\nQ2. [form array of words]\\nInitialize an array W of N words W[0; : : : ; N � 1], such that W[i] contains\\nthe characters S0[i; : : : ; i C k � 1] arranged so that integer comparisons on\\nthe words agree with lexicographic comparisons on the k-character strings.\\n\\nPacking characters into words has two benefits: It allows two prefixes to be\\ncompared k bytes at a time using aligned memory accesses, and it allows\\nmany slow cases to be eliminated (described in step Q7).\\n\\nQ3. [form array of suffixes]\\nIn this step we initialize an array V of N integers. If an element of V\\ncontains j , it represents the suffix of S0 whose first character is S0[ j ]. When\\nthe algorithm is complete, suffix V [i] will be the ith suffix in lexicographical\\norder.\\n\\nInitialize an array V of integers so that for each i D 0; : : : ; N�1 : V [i] D i.\\n\\n9\\n\\n\\n\\nQ4. [radix sort]\\nSort the elements of V , using the first two characters of each suffix as the\\nsort key. This can be done efficiently using radix sort.\\n\\nQ5. [iterate over each character in the alphabet]\\nFor each character ch in the alphabet, perform steps Q6, Q7.\\n\\nOnce this iteration is complete, V represents the sorted suffixes of S, and the\\nalgorithm terminates.\\n\\nQ6. [quicksort suffixes starting with ch]\\nFor each character ch0 in the alphabet: Apply quicksort to the elements of V\\nstarting with ch followed by ch0. In the implementation of quicksort, compare\\nthe elements of V by comparing the suffixes they represent by indexing into\\nthe array W . At each recursion step, keep track of the number of characters\\nthat have compared equal in the group, so that they need not be compared\\nagain.\\n\\nAll the suffixes starting with ch have now been sorted into their final positions\\nin V .\\n\\nQ7. [update sort keys]\\nFor each element V [i] corresponding to a suffix starting with ch (that is, for\\nwhich S[V [i]] D ch), set W[V [i]] to a value with ch in its high-order bits\\n(as before) and with i in its low-order bits (which step Q2 set to the k � 1\\ncharacters following the initial ch). The new value of W[V [i]] sorts into the\\nsame position as the old value, but has the desirable property that it is distinct\\nfrom all other values in W . This speeds up subsequent sorting operations,\\nsince comparisons with the new elements cannot compare equal.\\n\\nThis basic algorithm can be refined in a number of ways. An obvious improve-\\nment is to pick the character ch in step Q5 starting with the least common character\\nin S, and proceeding to the most common. This allows the updates of step Q7 to\\nhave the greatest effect.\\n\\nAs described, the algorithm performs poorly when S contains many repeated\\nsubstrings. We deal with this problem with two mechanisms.\\n\\nThe first mechanism handles strings of a single repeated character by replacing\\nstep Q6 with the following steps.\\n\\nQ6a. [quicksort suffixes starting ch; ch0 where ch 6D ch0]\\nFor each character ch0 6D ch in the alphabet: Apply quicksort to the elements\\nof V starting with ch followed by ch0. In the implementation of quicksort,\\n\\n10\\n\\n\\n\\ncompare the elements of V by comparing the suffixes they represent by\\nindexing into the array W . At each recursion step, keep track of the number\\nof characters that have compared equal in the group, so that they need not be\\ncompared again.\\n\\nQ6b. [sort low suffixes starting ch; ch]\\nThis step sorts the suffixes starting runs which would sort before an infinite\\nstring of ch characters. We observe that among such suffixes, long initial runs\\nof character ch sort after shorter initial runs, and runs of equal length sort in\\nthe same order as the characters at the ends of the run. This ordering can be\\nobtained with a simple loop over the suffixes, starting with the shortest.\\n\\nLet V[i] represent the lexicographically least suffix starting with ch. Let j\\nbe the lowest value such that suffix V [ j ] starts with ch; ch.\\n\\nwhile not (i = j) do\\nif (V[i] > 0) and (S[V[i]-1] = ch) then\\n\\nV[j] := V[i]-1;\\nj := j + 1\\n\\nend;\\ni := i + 1\\n\\nend;\\n\\nAll the suffixes that start with ch; ch and that are lexicographically less than\\nan infinite sequence of ch characters are now sorted.\\n\\nQ6c. [sort high suffixes starting ch; ch]\\nThis step sorts the suffixes starting runs which would sort after an infinite\\nstring of ch characters. This step is very like step Q6b, but with the direction\\nof the loop reversed.\\n\\nLet V [i] represent the lexicographically greatest suffix starting with ch. Let\\nj be the highest value such that suffix V [ j ] starts with ch; ch.\\n\\nwhile not (i = j) do\\nif (V[i] > 0) and (S[V[i]-1] = ch) then\\n\\nV[j] := V[i]-1;\\nj := j - 1\\n\\nend;\\ni := i - 1\\n\\nend;\\n\\nAll the suffixes that start with ch; ch and that are lexicographically greater\\nthan an infinite sequence of ch characters are now sorted.\\n\\n11\\n\\n\\n\\nThe second mechanism handles long strings of a repeated substring of more\\nthan one character. For such strings, we use a doubling technique similar to that\\nused in Manber and Myers’ scheme. We limit our quicksort implementation to\\nperform comparisons only to a depth D. We record where suffixes have been left\\nunsorted with a bit in the corresponding elements of V .\\n\\nOnce steps Q1 to Q7 are complete, the elements of the array W indicate sorted\\npositions up to D ð k characters, since each comparison compares k characters. If\\nunsorted portions of V remain, we simply sort them again, limiting the comparison\\ndepth to D as before. This time, each comparison compares D ð k characters, to\\nyield a list of suffixes sorted on their first D2ð k characters. We repeat this process\\nuntil the entire string is sorted.\\n\\nThe combination of radix sort, a careful implementation of quicksort, and the\\nmechanisms for dealing with repeated strings produce a sorting algorithm that is\\nextremely fast on most inputs, and is quite unlikely to perform very badly on real\\ninput data.\\n\\nWe are currently investigating further variations of quicksort. The following\\napproach seems promising. By applying the technique of Q6a–Q6c to all overlap-\\nping repeated strings, and by caching previously computed sort orders in a hash\\ntable, we can produce an algorithm that sorts the suffixes of a string in approxi-\\nmately the same time as the modified algorithm Q, but using only 6 bytes of space\\nper input character (4 bytes for a pointer, 1 byte for the input character itself, and\\n1 byte amortized space in the hash table). This approach has poor performance in\\nthe worst case, but bad cases seem to be rare in practice.\\n\\n4.2 Decompression\\n\\nSteps D1 and D2 can be accomplished efficiently with only two passes over the\\ndata, and one pass over the alphabet, as shown in the pseudo-code below. In the\\nfirst pass, we construct two arrays: C[alphabet] and P[0; : : : ; N �1]. C[ch] is the\\ntotal number of instances in L of characters preceding character ch in the alphabet.\\nP[i] is the number of instances of character L[i] in the prefix L[0; : : : ; i � 1] of\\nL. In practice, this first pass could be combined with the move-to-front decoding\\nstep. Given the arrays L, C, P, the array T defined in step D2 is given by:\\n\\nfor each i D 0; : : : ; N � 1 : T [i] D P[i] C C[L[i]]\\n\\nIn a second pass over the data, we use the starting position I to complete Algorithm\\nD to generate S.\\n\\nInitially, all elements of C are zero, and the last column of matrix M is the vector\\nL[0; : : : ; N � 1].\\n\\n12\\n\\n\\n\\nfor i := 0 to N-1 do\\nP[i] := C[L[i]];\\nC[L[i]] := C[L[i]] + 1\\n\\nend;\\nNow C[ch] is the number of instances in L of character ch. The value P[i] is the\\nnumber of instances of character L[i] in the prefix L[0; : : : ; i � 1] of L.\\nsum := 0;\\nfor ch := FIRST(alphabet) to LAST(alphabet) do\\n\\nsum := sum + C[ch];\\nC[ch] := sum - C[ch];\\n\\nend;\\nNow C[ch] is the total number of instances in L of characters preceding ch in the\\nalphabet.\\ni := I;\\nfor j := N-1 downto 0 do\\n\\nS[j] := L[i];\\ni := P[i] + C[L[i]]\\n\\nend\\n\\nThe decompressed result is now S[0; : : : ; N � 1].\\n\\n5 Algorithm variants\\n\\nIn the example given in step C1, we treated the left-hand side of each rotation as\\nthe most significant when sorting. In fact, our implementation treats the right-hand\\nside as the most significant, so that the decompressor will generate its output S\\nfrom left to right using the code of Section 4.2. This choice has almost no effect\\non the compression achieved.\\n\\nThe algorithm can be modified to use a different coding scheme instead of\\nmove-to-front in step M1. Compression can improve slightly if move-to-front\\nis replaced by a scheme in which codes that have not been seen in a very long\\ntime are moved only part-way to the front. We have not exploited this in our\\nimplementations.\\n\\nAlthough simple Huffman and arithmetic coders do well in step M2, a more\\ncomplex coding scheme can do slightly better. This is because the probability of\\na certain value at a given point in the vector R depends to a certain extent on the\\nimmediately preceding value. In practice, the most important effect is that zeroes\\ntend to occur in runs in R. We can take advantage of this effect by representing each\\nrun of zeroes by a code indicating the length of the run. A second set of Huffman\\ncodes can be used for values immediately following a run of zeroes, since the next\\nvalue cannot be another run of zeroes.\\n\\n13\\n\\n\\n\\nSize CPU time/s Compressed bits/\\nFile (bytes) compress decompress size (bytes) char\\nbib 111261 1.6 0.3 28750 2.07\\nbook1 768771 14.4 2.5 238989 2.49\\nbook2 610856 10.9 1.8 162612 2.13\\ngeo 102400 1.9 0.6 56974 4.45\\nnews 377109 6.5 1.2 122175 2.59\\nobj1 21504 0.4 0.1 10694 3.98\\nobj2 246814 4.1 0.8 81337 2.64\\npaper1 53161 0.7 0.1 16965 2.55\\npaper2 82199 1.1 0.2 25832 2.51\\npic 513216 5.4 1.2 53562 0.83\\nprogc 39611 0.6 0.1 12786 2.58\\nprogl 71646 1.1 0.2 16131 1.80\\nprogp 49379 0.8 0.1 11043 1.79\\ntrans 93695 1.6 0.2 18383 1.57\\nTotal 3141622 51.1 9.4 856233 -\\n\\nTable 1: Results of compressing fourteen files of the Calgary Compression Corpus.\\n\\nBlock size bits/character\\n(bytes) book1 Hector corpus\\n\\n1k 4.34 4.35\\n4k 3.86 3.83\\n\\n16k 3.43 3.39\\n64k 3.00 2.98\\n256k 2.68 2.65\\n750k 2.49 -\\n1M - 2.43\\n4M - 2.26\\n\\n16M - 2.13\\n64M - 2.04\\n103M - 2.01\\n\\nTable 2: The effect of varying block size (N ) on compression of book1 from the\\nCalgary Compression Corpus and of the entire Hector corpus.\\n\\n14\\n\\n\\n\\n6 Performance of implementation\\n\\nIn Table 1 we give the results of compressing the fourteen commonly used files\\nof the Calgary Compression Corpus [7] with our algorithm. Comparison of these\\nfigures with those given by Bell, Witten and Cleary [3] indicate that our algorithm\\ndoes well on non-text inputs as well as text inputs.\\n\\nOur implementation of Algorithm C uses the techniques described in Sec-\\ntion 4.1, and a simple move-to-front coder. In each case, the block size N is the\\nlength of the file being compressed.\\n\\nWe compress the output of the move-to-front coder with a modified Huffman\\ncoder that uses the technique described in Section 5. Our coder calculates new\\nHuffman trees for each 16 kbyte input block, rather than computing one tree for its\\nwhole input.\\n\\nIn Table 1, compression effectiveness is expressed as output bits per input\\ncharacter. The CPU time measurements were made on a DECstation 5000/200,\\nwhich has a MIPS R3000 processor clocked at 25MHz with a 64 kbyte cache. CPU\\ntime is given rather than elapsed time so the time spent performing I/O is excluded.\\n\\nIn Table 2, we show the variation of compression effectiveness with input block\\nsize for two inputs. The first input is the file book1 from the Calgary Compression\\nCorpus. The second is the entire Hector corpus [8], which consists of a little over\\n100 MBytes of modern English text. The table shows that compression improves\\nwith increasing block size, even when the block size is quite large. However,\\nincreasing the block size above a few tens of millions of characters has only a small\\neffect.\\n\\nIf the block size were increased indefinitely, we expect that the algorithm would\\nnot approach the theoretical optimum compression because our simple byte-wise\\nmove-to-front coder introduces some loss. A better coding scheme might achieve\\noptimum compression asymptotically, but this is not likely to be of great practical\\nimportance.\\n\\nComparison with other compression programmes\\n\\nTable 3 compares our implementation of Algorithm C with three other compression\\nprogrammes, chosen for their wide availability. The same fourteen files were\\ncompressed individually with each algorithm, and the results totalled. The bits per\\ncharacter values are the means of the values for the individual files. This metric\\nwas chosen to allow easy comparison with figures given by Bell [3].\\n\\n15\\n\\n\\n\\nTotal CPU time/s Total compressed mean\\nProgramme compress decompress size (bytes) bits/char\\ncompress 9.6 5.2 1246286 3.63\\ngzip 42.6 4.9 1024887 2.71\\nAlg-C/D 51.1 9.4 856233 2.43\\ncomp-2 603.2 614.1 848885 2.47\\n\\ncompress is version 4.2.3 of the well-known LZW-based tool [9, 10].\\ngzip is version 1.2.4 of Gailly’s LZ77-based tool [1, 11].\\nAlg-C/D is Algorithms C and D, with our back-end Huffman coder.\\ncomp-2 is Nelson’s comp-2 coder, limited to a fourth-order model [12].\\n\\nTable 3: Comparison with other compression programmes.\\n\\n7 Conclusions\\n\\nWe have described a compression technique that works by applying a reversible\\ntransformation to a block of text to make redundancy in the input more accessible\\nto simple coding schemes. Our algorithm is general-purpose, in that it does well on\\nboth text and non-text inputs. The transformation uses sorting to group characters\\ntogether based on their contexts; this technique makes use of the context on only\\none side of each character.\\n\\nTo achieve good compression, input blocks of several thousand characters are\\nneeded. The effectiveness of the algorithm continues to improve with increasing\\nblock size at least up to blocks of several million characters.\\n\\nOur algorithm achieves compression comparable with good statistical mod-\\nellers, yet is closer in speed to coders based on the algorithms of Lempel and\\nZiv. Like Lempel and Ziv’s algorithms, our algorithm decompresses faster than it\\ncompresses.\\n\\nAcknowledgements\\n\\nWe would like to thank Andrei Broder, Cynthia Hibbard, Greg Nelson, and Jim\\nSaxe for their suggestions on the algorithms and the paper.\\n\\n16\\n\\n\\n\\nReferences\\n\\n[1] J. Ziv and A. Lempel. A universal algorithm for sequential data compression.\\nIEEE Transactions on Information Theory. Vol. IT-23, No. 3, May 1977,\\npp. 337–343.\\n\\n[2] J. Ziv and A. Lempel. Compression of individual sequences via variable\\nrate coding. IEEE Transactions on Information Theory. Vol. IT-24, No. 5,\\nSeptember 1978, pp. 530–535.\\n\\n[3] T. Bell, I. H. Witten, and J.G. Cleary. Modeling for text compression. ACM\\nComputing Surveys, Vol. 21, No. 4, December 1989, pp. 557–589.\\n\\n[4] J.L. Bentley, D.D. Sleator, R.E. Tarjan, and V.K. Wei. A locally adaptive data\\ncompression algorithm. Communications of the ACM, Vol. 29, No. 4, April\\n1986, pp. 320–330.\\n\\n[5] E.M. McCreight. A space economical suffix tree construction algorithm. Jour-\\nnal of the ACM, Vol. 32, No. 2, April 1976, pp. 262–272.\\n\\n[6] U. Manber and E. W. Myers, Suffix arrays: A new method for on-line string\\nsearches. SIAM Journal on Computing, Volume 22, No. 5, October 1993, pp.\\n935-948.\\n\\n[7] I. H. Witten and T. Bell. The Calgary/Canterbury text compression cor-\\npus. Anonymous ftp from ftp.cpsc.ucalgary.ca: /pub/text.com-\\n\\npression.corpus/text.compression.corpus.tar.Z\\n\\n[8] L. Glassman, D. Grinberg, C. Hibbard, J. Meehan, L. Guarino Reid, and\\nM-C. van Leunen. Hector: Connecting Words with Definitions. Proceedings\\nof the 8th Annual Conference of the UW Centre for the New Oxford English\\nDictionary and Text Research, Waterloo, Canada, October, 1992. pp. 37–\\n73. Also available as Research Report 92a, Digital Equipment Corporation\\nSystems Research Center, 130 Lytton Ave, Palo Alto, CA. 94301.\\n\\n[9] T.A. Welch. A technique for high performance data compression. IEEE Com-\\nputer, Vol. 17, No. 6, June 1984, pp. 8–19.\\n\\n[10] Peter Jannesen et al. Compress, Version 4.2.3. Posted to the Internet news-\\ngroup comp.sources.reviewed, 28th August, 1992. Anonymous ftp from\\ngatekeeper.dec.com: /pub/misc/ncompress-4.2.3\\n\\n17\\n\\n\\n\\n[11] J. Gailly et al. Gzip,Version 1.2.4. Anonymous ftp from prep.ai.mit.edu:\\n\\n/pub/gnu/gzip-1.2.4.tar.gz\\n\\n[12] Mark Nelson. Arithmetic coding and statistical modeling. Dr. Dobbs Journal.\\nFebruary, 1991. Anonymous ftp from wuarchive.wustl.edu: /mir-\\n\\nrors/msdos/ddjmag/ddj9102.zip\\n\\n18\\n\\n\\n", "metadata"=>{"pdf:docinfo:title"=>"A block-sorting lossless data compression algorithm", "pdf:docinfo:modified"=>"2007-09-08T00:26:14Z", "pdf:docinfo:creator"=>"Burrows, M.; Wheeler, David J", "pdf:docinfo:custom:SPDF"=>"1112", "resourceName"=>"b\'A Block-sorting Lossless Data Compression Algorithm - May 10th, 1994 (SRC-RR-124).pdf\'", "pdf:docinfo:created"=>"1996-12-23T14:56:00Z"}, "filename"=>"A Block-sorting Lossless Data Compression Algorithm - May 10th, 1994 (SRC-RR-124).pdf"}', 'filename': 'A Block-sorting Lossless Data Compression Algorithm - May 10th, 1994 (SRC-RR-124).pdf', 'metadata_pdf:docinfo:created': '1996-12-23T14:56:00Z', 'mongo_id': '63cffa2978b994746c729cea', '@version': '1', 'host': 'bdvm', 'logdate': '2023-01-24T15:32:57+00:00', '@timestamp': '2023-01-24T15:32:58.674527247Z', 'content': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA block-sorting lossless data compression algorithm\n\n\nMay 10, 1994\n\nSRC\nResearch\nReport 124\n\nA Block-sorting Lossless\nData Compression Algorithm\n\nM. Burrows and D.J. Wheeler\n\nd i g i t a l\nSystems Research Center\n130 Lytton Avenue\nPalo Alto, California 94301\n\n\n\nSystems Research Center\n\nThe charter of SRC is to advance both the state of knowledge and the state of the\nart in computer systems. From our establishment in 1984, we have performed basic\nand applied research to support Digital’s business objectives. Our current work\nincludes exploring distributed personal computing on multiple platforms, network-\ning, programming technology, system modelling and management techniques, and\nselected applications.\n\nOur strategy is to test the technical and practical value of our ideas by building\nhardware and software prototypes and using them as daily tools. Interesting systems\nare too complex to be evaluated solely in the abstract; extended use allows us to\ninvestigate their properties in depth. This experience is useful in the short term in\nrefining our designs, and invaluable in the long term in advancing our knowledge.\nMost of the major advances in information systems have come through this strategy,\nincluding personal computing, distributed systems, and the Internet.\n\nWe also perform complementary work of a more mathematical flavor. Some of\nit is in established fields of theoretical computer science, such as the analysis\nof algorithms, computational geometry, and logics of programming. Other work\nexplores new ground motivated by problems that arise in our systems research.\n\nWe have a strong commitment to communicating our results; exposing and testing\nour ideas in the research and development communities leads to improved un-\nderstanding. Our research report series supplements publication in professional\njournals and conferences. We seek users for our prototype systems among those\nwith whom we have common interests, and we encourage collaboration with uni-\nversity researchers.\n\nRobert W. Taylor, Director\n\n\n\nA Block-sorting Lossless\nData Compression Algorithm\n\nM. Burrows and D.J. Wheeler\n\nMay 10, 1994\n\n\n\nDavid Wheeler is a Professor of Computer Science at the University of Cambridge,\nU.K. His electronic mail address is: djw3@cl.cam.ac.uk\n\nc\rDigital Equipment Corporation 1994.\n\nThis work may not be copied or reproduced in whole or in part for any commercial\npurpose. Permission to copy in whole or in part without payment of fee is granted\nfor nonprofit educational and research purposes provided that all such whole or\npartial copies include the following: a notice that such copying is by permission\nof the Systems Research Center of Digital Equipment Corporation in Palo Alto,\nCalifornia; an acknowledgment of the authors and individual contributors to the\nwork; and all applicable portions of the copyright notice. Copying, reproducing,\nor republishing for any other purpose shall require a license with payment of fee to\nthe Systems Research Center. All rights reserved.\n\n\n\nAuthors’ abstract\n\nWe describe a block-sorting, lossless data compression algorithm, and our imple-\nmentation of that algorithm. We compare the performance of our implementation\nwith widely available data compressors running on the same hardware.\n\nThe algorithm works by applying a reversible transformation to a block of input\ntext. The transformation does not itself compress the data, but reorders it to make\nit easy to compress with simple algorithms such as move-to-front coding.\n\nOur algorithm achieves speed comparable to algorithms based on the techniques\nof Lempel and Ziv, but obtains compression close to the best statistical modelling\ntechniques. The size of the input block must be large (a few kilobytes) to achieve\ngood compression.\n\n\n\nContents\n\n1 Introduction 1\n\n2 The reversible transformation 2\n\n3 Why the transformed string compresses well 5\n\n4 An efficient implementation 8\n4.1 Compression : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8\n4.2 Decompression : : : : : : : : : : : : : : : : : : : : : : : : : : : 12\n\n5 Algorithm variants 13\n\n6 Performance of implementation 15\n\n7 Conclusions 16\n\n\n\n1 Introduction\n\nThe most widely used data compression algorithms are based on the sequential\ndata compressors of Lempel and Ziv [1, 2]. Statistical modelling techniques may\nproduce superior compression [3], but are significantly slower.\n\nIn this paper, we present a technique that achieves compression within a percent\nor so of that achieved by statistical modelling techniques, but at speeds comparable\nto those of algorithms based on Lempel and Ziv’s.\n\nOur algorithm does not process its input sequentially, but instead processes a\nblock of text as a single unit. The idea is to apply a reversible transformation to a\nblock of text to form a new block that contains the same characters, but is easier\nto compress by simple compression algorithms. The transformation tends to group\ncharacters together so that the probability of finding a character close to another\ninstance of the same character is increased substantially. Text of this kind can\neasily be compressed with fast locally-adaptive algorithms, such as move-to-front\ncoding [4] in combination with Huffman or arithmetic coding.\n\nBriefly, our algorithm transforms a string S of N characters by forming the N\nrotations (cyclic shifts) of S, sorting them lexicographically, and extracting the last\ncharacter of each of the rotations. A string L is formed from these characters, where\nthe ith character of L is the last character of the ith sorted rotation. In addition to\nL, the algorithm computes the index I of the original string S in the sorted list of\nrotations. Surprisingly, there is an efficient algorithm to compute the original string\nS given only L and I .\n\nThe sorting operation brings together rotations with the same initial characters.\nSince the initial characters of the rotations are adjacent to the final characters,\nconsecutive characters in L are adjacent to similar strings in S. If the context of a\ncharacter is a good predictor for the character, L will be easy to compress with a\nsimple locally-adaptive compression algorithm.\n\nIn the following sections, we describe the transformation in more detail, and\nshow that it can be inverted. We explain more carefully why this transformation\ntends to group characters to allow a simple compression algorithm to work more\neffectively. We then describe efficient techniques for implementing the transfor-\nmation and its inverse, allowing this algorithm to be competitive in speed with\nLempel-Ziv-based algorithms, but achieving better compression. Finally, we give\nthe performance of our implementation of this algorithm, and compare it with\nwell-known compression programmes.\n\nThe algorithm described here was discovered by one of the authors (Wheeler) in\n1983 while he was working at AT&T Bell Laboratories, though it has not previously\nbeen published.\n\n1\n\n\n\n2 The reversible transformation\n\nThis section describes two sub-algorithms. Algorithm C performs the reversible\ntransformation that we apply to a block of text before compressing it, and Algorithm\nD performs the inverse operation. A later section suggests a method for compressing\nthe transformed block of text.\n\nIn the description below, we treat strings as vectors whose elements are char-\nacters.\n\nAlgorithm C: Compression transformation\n\nThis algorithm takes as input a string S of N characters S[0]; : : : ; S[N�1] selected\nfrom an ordered alphabet X of characters. To illustrate the technique, we also give\na running example, using the string S D ‘abraca’, N D 6, and the alphabet\nX D f0a0;0b0;0c0;0r0g.\n\nC1. [sort rotations]\nForm a conceptual N ð N matrix M whose elements are characters, and\nwhose rows are the rotations (cyclic shifts) of S, sorted in lexicographical\norder. At least one of the rows of M contains the original string S. Let I be\nthe index of the first such row, numbering from zero.\n\nIn our example, the index I D 1 and the matrix M is\n\nrow\n0 aabrac\n\n1 abraca\n\n2 acaabr\n\n3 bracaa\n\n4 caabra\n\n5 racaab\n\nC2. [find last characters of rotations]\nLet the string L be the last column of M , with characters L[0]; : : : ; L[N�1]\n(equal to M[0; N � 1]; : : : ;M[N � 1; N � 1]). The output of the transfor-\nmation is the pair .L; I/.\n\nIn our example, L D ‘caraab’ and I D 1 (from step C1).\n\n2\n\n\n\nAlgorithm D: Decompression transformation\n\nWe use the example and notation introduced in Algorithm C. Algorithm D uses the\noutput .L; I/ of Algorithm C to reconstruct its input, the string S of length N .\n\nD1. [find first characters of rotations]\nThis step calculates the first column F of the matrix M of Algorithm C. This\nis done by sorting the characters of L to form F. We observe that any column\nof the matrix M is a permutation of the original string S. Thus, L and F are\nboth permutations of S, and therefore of one another. Furthermore, because\nthe rows of M are sorted, and F is the first column of M , the characters in F\nare also sorted.\n\nIn our example, F D ‘aaabcr’.\n\nD2. [build list of predecessor characters]\nTo assist our explanation, we describe this step in terms of the contents of\nthe matrix M . The reader should remember that the complete matrix is not\navailable to the decompressor; only the strings F, L, and the index I (from\nthe input) are needed by this step.\n\nConsider the rows of the matrix M that start with some given character ch.\nAlgorithm C ensured that the rows of matrix M are sorted lexicographically,\nso the rows that start with ch are ordered lexicographically.\n\nWe define the matrix M 0 formed by rotating each row of M one character to\nthe right, so for each i D 0; : : : ; N � 1, and each j D 0; : : : ; N � 1,\n\nM 0[i; j ] D M[i; . j � 1/ mod N ]\n\nIn our example, M and M 0 are:\n\nrow M M 0\n\n0 aabrac caabra\n\n1 abraca aabrac\n\n2 acaabr racaab\n\n3 bracaa abraca\n\n4 caabra acaabr\n\n5 racaab bracaa\n\nLike M , each row of M 0 is a rotation of S, and for each row of M there is\na corresponding row in M 0. We constructed M 0 from M so that the rows\n\n3\n\n\n\nof M 0 are sorted lexicographically starting with their second character. So,\nif we consider only those rows in M 0 that start with a character ch, they\nmust appear in lexicographical order relative to one another; they have been\nsorted lexicographically starting with their second characters, and their first\ncharacters are all the same and so do not affect the sort order. Therefore, for\nany given character ch, the rows in M that begin with ch appear in the same\norder as the rows in M 0 that begin with ch.\n\nIn our example, this is demonstrated by the rows that begin with ‘a’. The rows\n‘aabrac’, ‘abraca’, and ‘acaabr’ are rows 0, 1, 2 in M and correspond to\nrows 1, 3, 4 in M 0.\n\nUsing F and L, the first columns of M and M 0 respectively, we calculate\na vector T that indicates the correspondence between the rows of the two\nmatrices, in the sense that for each j D 0; : : : ; N�1, row j of M 0 corresponds\nto row T [ j ] of M .\n\nIf L[ j ] is the kth instance of ch in L, then T [ j ] D i where F[i] is the kth\ninstance of ch in F. Note that T represents a one-to-one correspondence\nbetween elements of F and elements of L, and F[T [ j ]] D L[ j ].\n\nIn our example, T is: (4 0 5 1 2 3).\n\nD3. [form output S]\nNow, for each i D 0; : : : ; N � 1, the characters L[i] and F[i] are the last\nand first characters of the row i of M . Since each row is a rotation of\nS, the character L[i] cyclicly precedes the character F[i] in S. From the\nconstruction of T , we have F[T [ j ]] D L[ j ]. Substituting i D T [ j ], we have\nL[T [ j ]] cyclicly precedes L[ j ] in S.\n\nThe index I is defined by Algorithm C such that row I of M is S. Thus, the\nlast character of S is L[I ]. We use the vector T to give the predecessors of\neach character:\n\nfor each i D 0; : : : ; N � 1: S[N � 1� i] D L[T i [I ]].\n\nwhere T 0[x ] D x , and T iC1[x ] D T [T i [x ]]. This yields S, the original input\nto the compressor.\n\nIn our example, S D ‘abraca’.\n\nWe could have defined T so that the string S would be generated from front to back,\nrather than the other way around. The description above matches the pseudo-code\ngiven in Section 4.2.\n\n4\n\n\n\nThe sequence T i [I ] for i D 0; : : : ; N � 1 is not necessarily a permutation of\nthe numbers 0; : : : ; N � 1. If the original string S is of the form Z p for some\nsubstring Z and some p > 1, then the sequence T i [I ] for i D 0; : : : ; N � 1 will\nalso be of the form Z 0p for some subsequence Z 0. That is, the repetitions in S will\nbe generated by visiting the same elements of T repeatedly. For example, if S D\n‘cancan’, Z D ‘can’ and p D 2, the sequence T i [I ] for i D 0; : : : ; N � 1 will be\n[2; 4; 0; 2; 4; 0].\n\n3 Why the transformed string compresses well\n\nAlgorithm C sorts the rotations of an input string S, and generates the string L\nconsisting of the last character of each rotation.\n\nTo see why this might lead to effective compression, consider the effect on a\nsingle letter in a common word in a block of English text. We will use the example\nof the letter ‘t’ in the word ‘the’, and assume an input string containing many\ninstances of ‘the’.\n\nWhen the list of rotations of the input is sorted, all the rotations starting with\n‘he ’ will sort together; a large proportion of them are likely to end in ‘t’. One\nregion of the string L will therefore contain a disproportionately large number of ‘t’\ncharacters, intermingled with other characters that can proceed ‘he ’ in English,\nsuch as space, ‘s’, ‘T’, and ‘S’.\n\nThe same argument can be applied to all characters in all words, so any localized\nregion of the string L is likely to contain a large number of a few distinct characters.\nThe overall effect is that the probability that given character ch will occur at a given\npoint in L is very high if ch occurs near that point in L, and is low otherwise.\nThis property is exactly the one needed for effective compression by a move-to-\nfront coder [4], which encodes an instance of character ch by the count of distinct\ncharacters seen since the next previous occurrence of ch. When applied to the\nstring L, the output of a move-to-front coder will be dominated by low numbers,\nwhich can be efficiently encoded with a Huffman or arithmetic coder.\n\nFigure 1 shows an example of the algorithm at work. Each line is the first few\ncharacters and final character of a rotation of a version of this document. Note the\ngrouping of similar characters in the column of final characters.\n\nFor completeness, we now give details of one possible way to encode the output\nof Algorithm C, and the corresponding inverse operation. A complete compression\nalgorithm is created by combining these encoding and decoding operations with\nAlgorithms C and D.\n\n5\n\n\n\nfinal\nchar sorted rotations\n(L)\na n to decompress. It achieves compression\no n to perform only comparisons to a depth\no n transformation} This section describes\no n transformation} We use the example and\no n treats the right-hand side as the most\na n tree for each 16 kbyte input block, enc\na n tree in the output stream, then encodes\ni n turn, set $L[i]$ to be the\ni n turn, set $R[i]$ to the\no n unusual data. Like the algorithm of Man\na n use a single set of probabilities table\ne n using the positions of the suffixes in\ni n value at a given point in the vector $R\ne n we present modifications that improve t\ne n when the block size is quite large. Ho\ni n which codes that have not been seen in\ni n with $ch$ appear in the {\\em same order\ni n with $ch$. In our exam\no n with Huffman or arithmetic coding. Bri\no n with figures given by Bell˜\\cite{bell}.\n\nFigure 1: Example of sorted rotations. Twenty consecutive rotations from the\nsorted list of rotations of a version of this paper are shown, together with the final\ncharacter of each rotation.\n\n6\n\n\n\nAlgorithm M: Move-to-front coding\n\nThis algorithm encodes the output .L; I/ of Algorithm C, where L is a string of\nlength N and I is an index. It encodes L using a move-to-front algorithm and a\nHuffman or arithmetic coder.\n\nThe running example of Algorithm C is continued here.\n\nM1. [move-to-front coding]\nThis step encodes each of the characters in L by applying the move-to-\nfront technique to the individual characters. We define a vector of integers\nR[0]; : : : ; R[N�1], which are the codes for the characters L[0]; : : : ; L[N�\n1].\n\nInitialize a list Y of characters to contain each character in the alphabet X\nexactly once.\n\nFor each i D 0; : : : ; N � 1 in turn, set R[i] to the number of characters\npreceding character L[i] in the list Y , then move character L[i] to the front\nof Y .\n\nTaking Y D [0a0;0b0;0c0;0r0] initially, and L D‘caraab’, we compute the\nvector R: (2 1 3 1 0 3).\n\nM2. [encode]\nApply Huffman or arithmetic coding to the elements of R, treating each\nelement as a separate token to be coded. Any coding technique can be\napplied as long as the decompressor can perform the inverse operation. Call\nthe output of this coding process OUT. The output of Algorithm C is the pair\n.OUT; I/ where I is the value computed in step C1.\n\nAlgorithm W: Move-to-front decoding\n\nThis algorithm is the inverse of Algorithm M. It computes the pair .L; I/ from the\npair .OUT; I/.\n\nWe assume that the initial value of the list Y used in step M1 is available to the\ndecompressor, and that the coding scheme used in step M2 has an inverse operation.\n\nW1. [decode]\nDecode the coded stream OUT using the inverse of the coding scheme used\nin step M2. The result is a vector R of N integers.\n\nIn our example, R is: (2 1 3 1 0 3).\n\n7\n\n\n\nW2. [inverse move-to-front coding]\nThe goal of this step is to calculate a string L of N characters, given the\nmove-to-front codes R[0]; : : : ; R[N � 1].\n\nInitialize a list Y of characters to contain the characters of the alphabet X in\nthe same order as in step M1.\n\nFor each i D 0; : : : ; N � 1 in turn, set L[i] to be the character at position\nR[i] in list Y (numbering from 0), then move that character to the front of Y .\nThe resulting string L is the last column of matrix M of Algorithm C. The\noutput of this algorithm is the pair .L; I/, which is the input to Algorithm D.\n\nTaking Y D [0a0;0b0;0c0;0r0] initially (as in Algorithm M), we compute the\nstring L D ‘caraab’.\n\n4 An efficient implementation\n\nEfficient implementations of move-to-front, Huffman, and arithmetic coding are\nwell known. Here we concentrate on the unusual steps in Algorithms C and D.\n\n4.1 Compression\n\nThe most important factor in compression speed is the time taken to sort the rotations\nof the input block. A simple application of quicksort requires little additional space\n(one word per character), and works fairly well on most input data. However, its\nworst-case performance is poor.\n\nA faster way to implement Algorithm C is to reduce the problem of sorting the\nrotations to the problem of sorting the suffixes of a related string.\n\nTo compress a string S, first form the string S0, which is the concatenation of S\nwith EOF, a new character that does not appear in S. Now apply Algorithm C to S0.\nBecause EOF is different from all other characters in S0, the suffixes of S0 sort in the\nsame order as the rotations of S0. Hence, to sort the rotations, it suffices to sort the\nsuffixes of S0. This can be done in linear time and space by building a suffix tree,\nwhich then can be walked in lexicographical order to recover the sorted suffixes.\nWe used McCreight’s suffix tree construction algorithm [5] in an implementation of\nAlgorithm C. Its performance is within 40% of the fastest technique we have found\nwhen operating on text files. Unfortunately, suffix tree algorithms need space for\nmore than four machine words per input character.\n\nManber and Myers give a simple algorithm for sorting the suffixes of a string\nin O.N log N/ time [6]. The algorithm they describe requires only two words per\n\n8\n\n\n\ninput character. The algorithm works by sorting suffixes on their first i characters,\nthen using the positions of the suffixes in the sorted array as the sort key for another\nsort on the first 2i characters. Unfortunately, the performance of their algorithm is\nsignificantly worse than the suffix tree approach.\n\nThe fastest scheme we have tried so far uses a variant of quicksort to generate\nthe sorted list of suffixes. Its performance is significantly better than the suffix tree\nwhen operating on text, and it uses significantly less space. Unlike the suffix tree\nhowever, its performance can degrade considerably when operating on some types\nof data. Like the algorithm of Manber and Myers, our algorithm uses only two\nwords per input character.\n\nAlgorithm Q: Fast quicksorting on suffixes\n\nThis algorithm sorts the suffixes of the string S, which contains N characters\nS[0; : : : ; N�1]. The algorithm works by applying a modified version of quicksort\nto the suffixes of S.\n\nFirst we present a simple version of the algorithm. Then we present modifica-\ntions that improve the speed and make bad performance less likely.\n\nQ1. [form extended string]\nLet k be the number of characters that fit in a machine word.\n\nForm the string S0 from S by appending k additional EOF characters to S,\nwhere EOF does not appear in S.\n\nQ2. [form array of words]\nInitialize an array W of N words W[0; : : : ; N � 1], such that W[i] contains\nthe characters S0[i; : : : ; i C k � 1] arranged so that integer comparisons on\nthe words agree with lexicographic comparisons on the k-character strings.\n\nPacking characters into words has two benefits: It allows two prefixes to be\ncompared k bytes at a time using aligned memory accesses, and it allows\nmany slow cases to be eliminated (described in step Q7).\n\nQ3. [form array of suffixes]\nIn this step we initialize an array V of N integers. If an element of V\ncontains j , it represents the suffix of S0 whose first character is S0[ j ]. When\nthe algorithm is complete, suffix V [i] will be the ith suffix in lexicographical\norder.\n\nInitialize an array V of integers so that for each i D 0; : : : ; N�1 : V [i] D i.\n\n9\n\n\n\nQ4. [radix sort]\nSort the elements of V , using the first two characters of each suffix as the\nsort key. This can be done efficiently using radix sort.\n\nQ5. [iterate over each character in the alphabet]\nFor each character ch in the alphabet, perform steps Q6, Q7.\n\nOnce this iteration is complete, V represents the sorted suffixes of S, and the\nalgorithm terminates.\n\nQ6. [quicksort suffixes starting with ch]\nFor each character ch0 in the alphabet: Apply quicksort to the elements of V\nstarting with ch followed by ch0. In the implementation of quicksort, compare\nthe elements of V by comparing the suffixes they represent by indexing into\nthe array W . At each recursion step, keep track of the number of characters\nthat have compared equal in the group, so that they need not be compared\nagain.\n\nAll the suffixes starting with ch have now been sorted into their final positions\nin V .\n\nQ7. [update sort keys]\nFor each element V [i] corresponding to a suffix starting with ch (that is, for\nwhich S[V [i]] D ch), set W[V [i]] to a value with ch in its high-order bits\n(as before) and with i in its low-order bits (which step Q2 set to the k � 1\ncharacters following the initial ch). The new value of W[V [i]] sorts into the\nsame position as the old value, but has the desirable property that it is distinct\nfrom all other values in W . This speeds up subsequent sorting operations,\nsince comparisons with the new elements cannot compare equal.\n\nThis basic algorithm can be refined in a number of ways. An obvious improve-\nment is to pick the character ch in step Q5 starting with the least common character\nin S, and proceeding to the most common. This allows the updates of step Q7 to\nhave the greatest effect.\n\nAs described, the algorithm performs poorly when S contains many repeated\nsubstrings. We deal with this problem with two mechanisms.\n\nThe first mechanism handles strings of a single repeated character by replacing\nstep Q6 with the following steps.\n\nQ6a. [quicksort suffixes starting ch; ch0 where ch 6D ch0]\nFor each character ch0 6D ch in the alphabet: Apply quicksort to the elements\nof V starting with ch followed by ch0. In the implementation of quicksort,\n\n10\n\n\n\ncompare the elements of V by comparing the suffixes they represent by\nindexing into the array W . At each recursion step, keep track of the number\nof characters that have compared equal in the group, so that they need not be\ncompared again.\n\nQ6b. [sort low suffixes starting ch; ch]\nThis step sorts the suffixes starting runs which would sort before an infinite\nstring of ch characters. We observe that among such suffixes, long initial runs\nof character ch sort after shorter initial runs, and runs of equal length sort in\nthe same order as the characters at the ends of the run. This ordering can be\nobtained with a simple loop over the suffixes, starting with the shortest.\n\nLet V[i] represent the lexicographically least suffix starting with ch. Let j\nbe the lowest value such that suffix V [ j ] starts with ch; ch.\n\nwhile not (i = j) do\nif (V[i] > 0) and (S[V[i]-1] = ch) then\n\nV[j] := V[i]-1;\nj := j + 1\n\nend;\ni := i + 1\n\nend;\n\nAll the suffixes that start with ch; ch and that are lexicographically less than\nan infinite sequence of ch characters are now sorted.\n\nQ6c. [sort high suffixes starting ch; ch]\nThis step sorts the suffixes starting runs which would sort after an infinite\nstring of ch characters. This step is very like step Q6b, but with the direction\nof the loop reversed.\n\nLet V [i] represent the lexicographically greatest suffix starting with ch. Let\nj be the highest value such that suffix V [ j ] starts with ch; ch.\n\nwhile not (i = j) do\nif (V[i] > 0) and (S[V[i]-1] = ch) then\n\nV[j] := V[i]-1;\nj := j - 1\n\nend;\ni := i - 1\n\nend;\n\nAll the suffixes that start with ch; ch and that are lexicographically greater\nthan an infinite sequence of ch characters are now sorted.\n\n11\n\n\n\nThe second mechanism handles long strings of a repeated substring of more\nthan one character. For such strings, we use a doubling technique similar to that\nused in Manber and Myers’ scheme. We limit our quicksort implementation to\nperform comparisons only to a depth D. We record where suffixes have been left\nunsorted with a bit in the corresponding elements of V .\n\nOnce steps Q1 to Q7 are complete, the elements of the array W indicate sorted\npositions up to D ð k characters, since each comparison compares k characters. If\nunsorted portions of V remain, we simply sort them again, limiting the comparison\ndepth to D as before. This time, each comparison compares D ð k characters, to\nyield a list of suffixes sorted on their first D2ð k characters. We repeat this process\nuntil the entire string is sorted.\n\nThe combination of radix sort, a careful implementation of quicksort, and the\nmechanisms for dealing with repeated strings produce a sorting algorithm that is\nextremely fast on most inputs, and is quite unlikely to perform very badly on real\ninput data.\n\nWe are currently investigating further variations of quicksort. The following\napproach seems promising. By applying the technique of Q6a–Q6c to all overlap-\nping repeated strings, and by caching previously computed sort orders in a hash\ntable, we can produce an algorithm that sorts the suffixes of a string in approxi-\nmately the same time as the modified algorithm Q, but using only 6 bytes of space\nper input character (4 bytes for a pointer, 1 byte for the input character itself, and\n1 byte amortized space in the hash table). This approach has poor performance in\nthe worst case, but bad cases seem to be rare in practice.\n\n4.2 Decompression\n\nSteps D1 and D2 can be accomplished efficiently with only two passes over the\ndata, and one pass over the alphabet, as shown in the pseudo-code below. In the\nfirst pass, we construct two arrays: C[alphabet] and P[0; : : : ; N �1]. C[ch] is the\ntotal number of instances in L of characters preceding character ch in the alphabet.\nP[i] is the number of instances of character L[i] in the prefix L[0; : : : ; i � 1] of\nL. In practice, this first pass could be combined with the move-to-front decoding\nstep. Given the arrays L, C, P, the array T defined in step D2 is given by:\n\nfor each i D 0; : : : ; N � 1 : T [i] D P[i] C C[L[i]]\n\nIn a second pass over the data, we use the starting position I to complete Algorithm\nD to generate S.\n\nInitially, all elements of C are zero, and the last column of matrix M is the vector\nL[0; : : : ; N � 1].\n\n12\n\n\n\nfor i := 0 to N-1 do\nP[i] := C[L[i]];\nC[L[i]] := C[L[i]] + 1\n\nend;\nNow C[ch] is the number of instances in L of character ch. The value P[i] is the\nnumber of instances of character L[i] in the prefix L[0; : : : ; i � 1] of L.\nsum := 0;\nfor ch := FIRST(alphabet) to LAST(alphabet) do\n\nsum := sum + C[ch];\nC[ch] := sum - C[ch];\n\nend;\nNow C[ch] is the total number of instances in L of characters preceding ch in the\nalphabet.\ni := I;\nfor j := N-1 downto 0 do\n\nS[j] := L[i];\ni := P[i] + C[L[i]]\n\nend\n\nThe decompressed result is now S[0; : : : ; N � 1].\n\n5 Algorithm variants\n\nIn the example given in step C1, we treated the left-hand side of each rotation as\nthe most significant when sorting. In fact, our implementation treats the right-hand\nside as the most significant, so that the decompressor will generate its output S\nfrom left to right using the code of Section 4.2. This choice has almost no effect\non the compression achieved.\n\nThe algorithm can be modified to use a different coding scheme instead of\nmove-to-front in step M1. Compression can improve slightly if move-to-front\nis replaced by a scheme in which codes that have not been seen in a very long\ntime are moved only part-way to the front. We have not exploited this in our\nimplementations.\n\nAlthough simple Huffman and arithmetic coders do well in step M2, a more\ncomplex coding scheme can do slightly better. This is because the probability of\na certain value at a given point in the vector R depends to a certain extent on the\nimmediately preceding value. In practice, the most important effect is that zeroes\ntend to occur in runs in R. We can take advantage of this effect by representing each\nrun of zeroes by a code indicating the length of the run. A second set of Huffman\ncodes can be used for values immediately following a run of zeroes, since the next\nvalue cannot be another run of zeroes.\n\n13\n\n\n\nSize CPU time/s Compressed bits/\nFile (bytes) compress decompress size (bytes) char\nbib 111261 1.6 0.3 28750 2.07\nbook1 768771 14.4 2.5 238989 2.49\nbook2 610856 10.9 1.8 162612 2.13\ngeo 102400 1.9 0.6 56974 4.45\nnews 377109 6.5 1.2 122175 2.59\nobj1 21504 0.4 0.1 10694 3.98\nobj2 246814 4.1 0.8 81337 2.64\npaper1 53161 0.7 0.1 16965 2.55\npaper2 82199 1.1 0.2 25832 2.51\npic 513216 5.4 1.2 53562 0.83\nprogc 39611 0.6 0.1 12786 2.58\nprogl 71646 1.1 0.2 16131 1.80\nprogp 49379 0.8 0.1 11043 1.79\ntrans 93695 1.6 0.2 18383 1.57\nTotal 3141622 51.1 9.4 856233 -\n\nTable 1: Results of compressing fourteen files of the Calgary Compression Corpus.\n\nBlock size bits/character\n(bytes) book1 Hector corpus\n\n1k 4.34 4.35\n4k 3.86 3.83\n\n16k 3.43 3.39\n64k 3.00 2.98\n256k 2.68 2.65\n750k 2.49 -\n1M - 2.43\n4M - 2.26\n\n16M - 2.13\n64M - 2.04\n103M - 2.01\n\nTable 2: The effect of varying block size (N ) on compression of book1 from the\nCalgary Compression Corpus and of the entire Hector corpus.\n\n14\n\n\n\n6 Performance of implementation\n\nIn Table 1 we give the results of compressing the fourteen commonly used files\nof the Calgary Compression Corpus [7] with our algorithm. Comparison of these\nfigures with those given by Bell, Witten and Cleary [3] indicate that our algorithm\ndoes well on non-text inputs as well as text inputs.\n\nOur implementation of Algorithm C uses the techniques described in Sec-\ntion 4.1, and a simple move-to-front coder. In each case, the block size N is the\nlength of the file being compressed.\n\nWe compress the output of the move-to-front coder with a modified Huffman\ncoder that uses the technique described in Section 5. Our coder calculates new\nHuffman trees for each 16 kbyte input block, rather than computing one tree for its\nwhole input.\n\nIn Table 1, compression effectiveness is expressed as output bits per input\ncharacter. The CPU time measurements were made on a DECstation 5000/200,\nwhich has a MIPS R3000 processor clocked at 25MHz with a 64 kbyte cache. CPU\ntime is given rather than elapsed time so the time spent performing I/O is excluded.\n\nIn Table 2, we show the variation of compression effectiveness with input block\nsize for two inputs. The first input is the file book1 from the Calgary Compression\nCorpus. The second is the entire Hector corpus [8], which consists of a little over\n100 MBytes of modern English text. The table shows that compression improves\nwith increasing block size, even when the block size is quite large. However,\nincreasing the block size above a few tens of millions of characters has only a small\neffect.\n\nIf the block size were increased indefinitely, we expect that the algorithm would\nnot approach the theoretical optimum compression because our simple byte-wise\nmove-to-front coder introduces some loss. A better coding scheme might achieve\noptimum compression asymptotically, but this is not likely to be of great practical\nimportance.\n\nComparison with other compression programmes\n\nTable 3 compares our implementation of Algorithm C with three other compression\nprogrammes, chosen for their wide availability. The same fourteen files were\ncompressed individually with each algorithm, and the results totalled. The bits per\ncharacter values are the means of the values for the individual files. This metric\nwas chosen to allow easy comparison with figures given by Bell [3].\n\n15\n\n\n\nTotal CPU time/s Total compressed mean\nProgramme compress decompress size (bytes) bits/char\ncompress 9.6 5.2 1246286 3.63\ngzip 42.6 4.9 1024887 2.71\nAlg-C/D 51.1 9.4 856233 2.43\ncomp-2 603.2 614.1 848885 2.47\n\ncompress is version 4.2.3 of the well-known LZW-based tool [9, 10].\ngzip is version 1.2.4 of Gailly’s LZ77-based tool [1, 11].\nAlg-C/D is Algorithms C and D, with our back-end Huffman coder.\ncomp-2 is Nelson’s comp-2 coder, limited to a fourth-order model [12].\n\nTable 3: Comparison with other compression programmes.\n\n7 Conclusions\n\nWe have described a compression technique that works by applying a reversible\ntransformation to a block of text to make redundancy in the input more accessible\nto simple coding schemes. Our algorithm is general-purpose, in that it does well on\nboth text and non-text inputs. The transformation uses sorting to group characters\ntogether based on their contexts; this technique makes use of the context on only\none side of each character.\n\nTo achieve good compression, input blocks of several thousand characters are\nneeded. The effectiveness of the algorithm continues to improve with increasing\nblock size at least up to blocks of several million characters.\n\nOur algorithm achieves compression comparable with good statistical mod-\nellers, yet is closer in speed to coders based on the algorithms of Lempel and\nZiv. Like Lempel and Ziv’s algorithms, our algorithm decompresses faster than it\ncompresses.\n\nAcknowledgements\n\nWe would like to thank Andrei Broder, Cynthia Hibbard, Greg Nelson, and Jim\nSaxe for their suggestions on the algorithms and the paper.\n\n16\n\n\n\nReferences\n\n[1] J. Ziv and A. Lempel. A universal algorithm for sequential data compression.\nIEEE Transactions on Information Theory. Vol. IT-23, No. 3, May 1977,\npp. 337–343.\n\n[2] J. Ziv and A. Lempel. Compression of individual sequences via variable\nrate coding. IEEE Transactions on Information Theory. Vol. IT-24, No. 5,\nSeptember 1978, pp. 530–535.\n\n[3] T. Bell, I. H. Witten, and J.G. Cleary. Modeling for text compression. ACM\nComputing Surveys, Vol. 21, No. 4, December 1989, pp. 557–589.\n\n[4] J.L. Bentley, D.D. Sleator, R.E. Tarjan, and V.K. Wei. A locally adaptive data\ncompression algorithm. Communications of the ACM, Vol. 29, No. 4, April\n1986, pp. 320–330.\n\n[5] E.M. McCreight. A space economical suffix tree construction algorithm. Jour-\nnal of the ACM, Vol. 32, No. 2, April 1976, pp. 262–272.\n\n[6] U. Manber and E. W. Myers, Suffix arrays: A new method for on-line string\nsearches. SIAM Journal on Computing, Volume 22, No. 5, October 1993, pp.\n935-948.\n\n[7] I. H. Witten and T. Bell. The Calgary/Canterbury text compression cor-\npus. Anonymous ftp from ftp.cpsc.ucalgary.ca: /pub/text.com-\n\npression.corpus/text.compression.corpus.tar.Z\n\n[8] L. Glassman, D. Grinberg, C. Hibbard, J. Meehan, L. Guarino Reid, and\nM-C. van Leunen. Hector: Connecting Words with Definitions. Proceedings\nof the 8th Annual Conference of the UW Centre for the New Oxford English\nDictionary and Text Research, Waterloo, Canada, October, 1992. pp. 37–\n73. Also available as Research Report 92a, Digital Equipment Corporation\nSystems Research Center, 130 Lytton Ave, Palo Alto, CA. 94301.\n\n[9] T.A. Welch. A technique for high performance data compression. IEEE Com-\nputer, Vol. 17, No. 6, June 1984, pp. 8–19.\n\n[10] Peter Jannesen et al. Compress, Version 4.2.3. Posted to the Internet news-\ngroup comp.sources.reviewed, 28th August, 1992. Anonymous ftp from\ngatekeeper.dec.com: /pub/misc/ncompress-4.2.3\n\n17\n\n\n\n[11] J. Gailly et al. Gzip,Version 1.2.4. Anonymous ftp from prep.ai.mit.edu:\n\n/pub/gnu/gzip-1.2.4.tar.gz\n\n[12] Mark Nelson. Arithmetic coding and statistical modeling. Dr. Dobbs Journal.\nFebruary, 1991. Anonymous ftp from wuarchive.wustl.edu: /mir-\n\nrors/msdos/ddjmag/ddj9102.zip\n\n18\n\n\n', 'metadata_pdf:docinfo:creator': 'Burrows, M.; Wheeler, David J', 'metadata_resourceName': "b'A Block-sorting Lossless Data Compression Algorithm - May 10th, 1994 (SRC-RR-124).pdf'", 'metadata_pdf:docinfo:modified': '2007-09-08T00:26:14Z', 'metadata_pdf:docinfo:custom:SPDF': 1112, 'metadata_pdf:docinfo:title': 'A block-sorting lossless data compression algorithm'}}]
