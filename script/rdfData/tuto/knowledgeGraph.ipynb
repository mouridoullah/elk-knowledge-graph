{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import des bibliothèques nécessaires\n",
    "import pymongo\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import math\n",
    "import torch\n",
    "from elasticsearch import Elasticsearch\n",
    "import uuid\n",
    "from neo4j import GraphDatabase\n",
    "import wikipediaapi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract (s, p, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def merge_relations(self, r1):\n",
    "        r2 = [r for r in self.relations\n",
    "              if self.are_relations_equal(r1, r)][0]\n",
    "        spans_to_add = [span for span in r1[\"meta\"][\"spans\"]\n",
    "                        if span not in r2[\"meta\"][\"spans\"]]\n",
    "        r2[\"meta\"][\"spans\"] += spans_to_add\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "        else:\n",
    "            self.merge_relations(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "\n",
    "    def get_relations(self):\n",
    "        return self.relations\n",
    "\n",
    "    def get_entities(self):\n",
    "        return self.entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformation to KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text_to_kb(text, span_length=128, verbose=False):\n",
    "    # tokenize whole text\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "    # compute span boundaries\n",
    "    num_tokens = len(inputs[\"input_ids\"][0])\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_tokens} tokens\")\n",
    "    num_spans = math.ceil(num_tokens / span_length)\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_spans} spans\")\n",
    "    overlap = math.ceil((num_spans * span_length - num_tokens) / \n",
    "                        max(num_spans - 1, 1))\n",
    "    spans_boundaries = []\n",
    "    start = 0\n",
    "    for i in range(num_spans):\n",
    "        spans_boundaries.append([start + span_length * i,\n",
    "                                 start + span_length * (i + 1)])\n",
    "        start -= overlap\n",
    "    if verbose:\n",
    "        print(f\"Span boundaries are {spans_boundaries}\")\n",
    "\n",
    "    # transform input with spans\n",
    "    tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]]\n",
    "                  for boundary in spans_boundaries]\n",
    "    tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]]\n",
    "                    for boundary in spans_boundaries]\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.stack(tensor_ids),\n",
    "        \"attention_mask\": torch.stack(tensor_masks)\n",
    "    }\n",
    "\n",
    "    # generate relations\n",
    "    num_return_sequences = 3\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 256,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": num_return_sequences\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "    # decode relations\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens,\n",
    "                                           skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    kb = KB()\n",
    "    i = 0\n",
    "    for sentence_pred in decoded_preds:\n",
    "        current_span_index = i // num_return_sequences\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for relation in relations:\n",
    "            relation[\"meta\"] = {\n",
    "                \"spans\": [spans_boundaries[current_span_index]]\n",
    "            }\n",
    "            kb.add_relation(relation)\n",
    "        i += 1\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get wikipédia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text(text, chunk_size):\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def get_wikipedia_data(title):\n",
    "    # Initialiser la connexion à Wikipédia\n",
    "    wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "    # Récupérer la page correspondant au titre\n",
    "    page = wiki.page(title)\n",
    "\n",
    "    # Vérifier si la page existe\n",
    "    if page.exists():\n",
    "        # Extraire les données de la page\n",
    "        data = {\n",
    "            'title': page.title,\n",
    "            'summary': \" \".join(page.summary.rsplit(\".\", 1)[0].replace(\"\\n\", \" \").replace(\"  \", \" \").split()),\n",
    "            'content': \" \".join(page.text.rsplit(\".\", 1)[0].replace(\"\\n\", \" \").replace(\"  \", \" \").split()),\n",
    "        }\n",
    "        return data\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data_in_mongodb(data):\n",
    "    # Initialiser la connexion à MongoDB\n",
    "    client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['dbDocument']\n",
    "    collection = db['test']\n",
    "\n",
    "    # Insérer les données dans la base de données\n",
    "    result = collection.insert_one(data)\n",
    "    print('Page enregistrée dans MongoDB :', result.inserted_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_documents(collection):\n",
    "    documents = collection.find()\n",
    "    for document in documents:\n",
    "        collection.delete_one(document)\n",
    "\n",
    "def delete_all_nodes():\n",
    "    # Se connecter à la base de données Neo4j\n",
    "    uri = \"bolt://localhost:7687\"\n",
    "    username = \"neo4j\"\n",
    "    password = \"password\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    # Ouvrir une session Neo4j\n",
    "    with driver.session() as session:\n",
    "        # Supprimer tous les nœuds\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "    # Fermer le pilote Neo4j\n",
    "    driver.close()\n",
    "\n",
    "def delete_all_documentsES(index_name):\n",
    "    # Créer une connexion à ElasticSearch\n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "    # Utiliser la méthode delete_by_query pour supprimer tous les documents de l'index spécifié\n",
    "    es.delete_by_query(index=index_name, body={\"query\": {\"match_all\": {}}})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez une instance de GraphDatabase\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Connexion à la base de données MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"dbDocument\"]\n",
    "collection = db[\"test\"]\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "\n",
    "\n",
    "term = input(\"Entrez une chaîne de caractères (appuyez sur Entrée pour quitter) : \")\n",
    "while term != \"\":\n",
    "    data = get_wikipedia_data(term)\n",
    "    if data:\n",
    "        id = uuid.uuid4()\n",
    "        store_data_in_mongodb(data)\n",
    "\n",
    "        segments = segment_text(data['summary'], 2048)\n",
    "\n",
    "        for segment in segments:\n",
    "            text = \" \".join(segment.replace(\"\\n\", \" \").replace(\"  \", \" \").split())\n",
    "            kb = from_text_to_kb(text)\n",
    "\n",
    "            # kb.print()\n",
    "            # for r in kb.get_relations():\n",
    "            #     print(r)\n",
    "\n",
    "\n",
    "            triplets = kb.get_relations()\n",
    "\n",
    "            # Exécutez une transaction pour ajouter des nœuds et des relations au graphe\n",
    "            with driver.session() as session:\n",
    "                # Parcourez la liste de dictionnaires et créez des nœuds pour chaque \"head\" et \"tail\"\n",
    "                for triplet in triplets:\n",
    "                    print(triplet)\n",
    "                    try:\n",
    "                        session.run(\"MERGE (head:Node {name: $head}) \"\n",
    "                                    \"MERGE (tail:Node {name: $tail}) \"\n",
    "                                    \"MERGE (head)-[r:\" + triplet['type'].strip().replace(' ', '_') + \"]->(tail) \"\n",
    "                                    \"WITH head, tail \"\n",
    "                                    \"MERGE (a:Document{id: $id, title: $title}) \"\n",
    "                                    \"WITH a, head, tail \"\n",
    "                                    \"MERGE (a)-[:MENTIONED]->(head) \"\n",
    "                                    \"MERGE (a)-[:MENTIONED]->(tail)\",\n",
    "                                    head=triplet['head'], tail=triplet['tail'], id=str(id), title=data[\"title\"])\n",
    "                    except:\n",
    "                        continue\n",
    "            # Fermez la connexion du driver\n",
    "            driver.close()\n",
    "\n",
    "    else:\n",
    "        print('La page', term, \"n'existe pas sur Wikipédia\")\n",
    "    \n",
    "    term = input(\"Entrez une chaîne de caractères (appuyez sur Entrée pour quitter) : \")\n",
    "\n",
    "\n",
    "# fonctions pour supprimer nœuds et documents\n",
    "delete_all_documents(collection)\n",
    "delete_all_nodes()\n",
    "delete_all_documentsES(\"mongo_index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
